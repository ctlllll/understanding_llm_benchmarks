{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Chatbot Arena\n",
    "chatbot_arena_files = glob('data/chatbot-arena-leaderboard/*.csv')\n",
    "chatbot_arena_files.sort(key=lambda x: int(x[-12:-4]))\n",
    "# Head: key,Model,MT-bench (score),MMLU,License,Organization,Link\n",
    "chatbot_arena = pd.read_csv(chatbot_arena_files[-1], sep=',', header=0)\n",
    "\n",
    "chatbot_arena_elo_files = glob('data/chatbot-arena-leaderboard/*.pkl')\n",
    "chatbot_arena_elo_files.sort(key=lambda x: int(x[-12:-4]))\n",
    "chatbot_arena_elo = pickle.load(open(chatbot_arena_elo_files[-1], 'rb'))\n",
    "\n",
    "# Use elo as ground truth\n",
    "elo = chatbot_arena_elo['leaderboard_table_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>Model</th>\n",
       "      <th>MT-bench (score)</th>\n",
       "      <th>MMLU</th>\n",
       "      <th>License</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wizardlm-30b</td>\n",
       "      <td>WizardLM-30B</td>\n",
       "      <td>7.01</td>\n",
       "      <td>0.587</td>\n",
       "      <td>Non-commercial</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>https://huggingface.co/WizardLM/WizardLM-30B-V1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vicuna-13b-16k</td>\n",
       "      <td>Vicuna-13B-16k</td>\n",
       "      <td>6.92</td>\n",
       "      <td>0.545</td>\n",
       "      <td>Llama 2 Community</td>\n",
       "      <td>LMSYS</td>\n",
       "      <td>https://huggingface.co/lmsys/vicuna-13b-v1.5-16k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wizardlm-13b-v1.1</td>\n",
       "      <td>WizardLM-13B-v1.1</td>\n",
       "      <td>6.76</td>\n",
       "      <td>0.500</td>\n",
       "      <td>Non-commercial</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>https://huggingface.co/WizardLM/WizardLM-13B-V1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tulu-30b</td>\n",
       "      <td>Tulu-30B</td>\n",
       "      <td>6.43</td>\n",
       "      <td>0.581</td>\n",
       "      <td>Non-commercial</td>\n",
       "      <td>AllenAI/UW</td>\n",
       "      <td>https://huggingface.co/allenai/tulu-30b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>guanaco-65b</td>\n",
       "      <td>Guanaco-65B</td>\n",
       "      <td>6.41</td>\n",
       "      <td>0.621</td>\n",
       "      <td>Non-commercial</td>\n",
       "      <td>UW</td>\n",
       "      <td>https://huggingface.co/timdettmers/guanaco-65b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>StableLM-Tuned-Alpha-7B</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.244</td>\n",
       "      <td>CC-BY-NC-SA-4.0</td>\n",
       "      <td>Stability AI</td>\n",
       "      <td>https://huggingface.co/stabilityai/stablelm-tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>dolly-v2-12b</td>\n",
       "      <td>Dolly-V2-12B</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.257</td>\n",
       "      <td>MIT</td>\n",
       "      <td>Databricks</td>\n",
       "      <td>https://huggingface.co/databricks/dolly-v2-12b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>llama-13b</td>\n",
       "      <td>LLaMA-13B</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.470</td>\n",
       "      <td>Non-commercial</td>\n",
       "      <td>Meta</td>\n",
       "      <td>https://arxiv.org/abs/2302.13971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>Mistral Medium</td>\n",
       "      <td>8.61</td>\n",
       "      <td>0.753</td>\n",
       "      <td>Proprietary</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>https://mistral.ai/news/la-plateforme/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>llama2-70b-steerlm-chat</td>\n",
       "      <td>Llama2-70B-SteerLM-Chat</td>\n",
       "      <td>7.54</td>\n",
       "      <td>-</td>\n",
       "      <td>Llama 2 Community</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>https://huggingface.co/nvidia/Llama2-70B-Steer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        key                    Model MT-bench (score)   MMLU  \\\n",
       "0              wizardlm-30b             WizardLM-30B             7.01  0.587   \n",
       "1            vicuna-13b-16k           Vicuna-13B-16k             6.92  0.545   \n",
       "2         wizardlm-13b-v1.1        WizardLM-13B-v1.1             6.76  0.500   \n",
       "3                  tulu-30b                 Tulu-30B             6.43  0.581   \n",
       "4               guanaco-65b              Guanaco-65B             6.41  0.621   \n",
       "..                      ...                      ...              ...    ...   \n",
       "63  stablelm-tuned-alpha-7b  StableLM-Tuned-Alpha-7B             2.75  0.244   \n",
       "64             dolly-v2-12b             Dolly-V2-12B             3.28  0.257   \n",
       "65                llama-13b                LLaMA-13B             2.61  0.470   \n",
       "66           mistral-medium           Mistral Medium             8.61  0.753   \n",
       "67  llama2-70b-steerlm-chat  Llama2-70B-SteerLM-Chat             7.54      -   \n",
       "\n",
       "              License  Organization  \\\n",
       "0      Non-commercial     Microsoft   \n",
       "1   Llama 2 Community         LMSYS   \n",
       "2      Non-commercial     Microsoft   \n",
       "3      Non-commercial    AllenAI/UW   \n",
       "4      Non-commercial            UW   \n",
       "..                ...           ...   \n",
       "63    CC-BY-NC-SA-4.0  Stability AI   \n",
       "64                MIT    Databricks   \n",
       "65     Non-commercial          Meta   \n",
       "66        Proprietary       Mistral   \n",
       "67  Llama 2 Community        Nvidia   \n",
       "\n",
       "                                                 Link  \n",
       "0   https://huggingface.co/WizardLM/WizardLM-30B-V1.0  \n",
       "1    https://huggingface.co/lmsys/vicuna-13b-v1.5-16k  \n",
       "2   https://huggingface.co/WizardLM/WizardLM-13B-V1.1  \n",
       "3             https://huggingface.co/allenai/tulu-30b  \n",
       "4   https://huggingface.co/timdettmers/guanaco-65b...  \n",
       "..                                                ...  \n",
       "63  https://huggingface.co/stabilityai/stablelm-tu...  \n",
       "64     https://huggingface.co/databricks/dolly-v2-12b  \n",
       "65                   https://arxiv.org/abs/2302.13971  \n",
       "66             https://mistral.ai/news/la-plateforme/  \n",
       "67  https://huggingface.co/nvidia/Llama2-70B-Steer...  \n",
       "\n",
       "[68 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Alpaca Eval\n",
    "# Head: name,win_rate,avg_length,link,samples,filter\n",
    "alpaca_eval = pd.read_csv('data/alpaca_eval_gpt4_leaderboard.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>win_rate</th>\n",
       "      <th>avg_length</th>\n",
       "      <th>link</th>\n",
       "      <th>samples</th>\n",
       "      <th>filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-4 Turbo</td>\n",
       "      <td>97.699005</td>\n",
       "      <td>2049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XwinLM 70b V0.1</td>\n",
       "      <td>95.568040</td>\n",
       "      <td>1775</td>\n",
       "      <td>https://github.com/Xwin-LM/Xwin-LM</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PairRM+Tulu 2+DPO 70B (best-of-16)</td>\n",
       "      <td>95.398010</td>\n",
       "      <td>1607</td>\n",
       "      <td>https://huggingface.co/llm-blender/PairRM</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>95.279503</td>\n",
       "      <td>1365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tulu 2+DPO 70B</td>\n",
       "      <td>95.031056</td>\n",
       "      <td>1418</td>\n",
       "      <td>https://huggingface.co/allenai/tulu-2-dpo-70b</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Alpaca 7B</td>\n",
       "      <td>26.459627</td>\n",
       "      <td>396</td>\n",
       "      <td>https://huggingface.co/tatsu-lab/alpaca-7b-wdiff</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Pythia 12B OASST SFT</td>\n",
       "      <td>25.962733</td>\n",
       "      <td>726</td>\n",
       "      <td>https://huggingface.co/OpenAssistant/oasst-sft...</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Falcon 7B Instruct</td>\n",
       "      <td>23.602484</td>\n",
       "      <td>478</td>\n",
       "      <td>https://huggingface.co/tiiuae/falcon-7b-instruct</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Baichuan-13B-Chat</td>\n",
       "      <td>21.801242</td>\n",
       "      <td>1727</td>\n",
       "      <td>https://huggingface.co/baichuan-inc/Baichuan-1...</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Davinci001</td>\n",
       "      <td>15.174129</td>\n",
       "      <td>296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>minimal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name   win_rate  avg_length  \\\n",
       "0                          GPT-4 Turbo  97.699005        2049   \n",
       "1                      XwinLM 70b V0.1  95.568040        1775   \n",
       "2   PairRM+Tulu 2+DPO 70B (best-of-16)  95.398010        1607   \n",
       "3                                GPT-4  95.279503        1365   \n",
       "4                       Tulu 2+DPO 70B  95.031056        1418   \n",
       "..                                 ...        ...         ...   \n",
       "92                           Alpaca 7B  26.459627         396   \n",
       "93                Pythia 12B OASST SFT  25.962733         726   \n",
       "94                  Falcon 7B Instruct  23.602484         478   \n",
       "95                   Baichuan-13B-Chat  21.801242        1727   \n",
       "96                          Davinci001  15.174129         296   \n",
       "\n",
       "                                                 link  \\\n",
       "0                                                 NaN   \n",
       "1                  https://github.com/Xwin-LM/Xwin-LM   \n",
       "2           https://huggingface.co/llm-blender/PairRM   \n",
       "3                                                 NaN   \n",
       "4       https://huggingface.co/allenai/tulu-2-dpo-70b   \n",
       "..                                                ...   \n",
       "92   https://huggingface.co/tatsu-lab/alpaca-7b-wdiff   \n",
       "93  https://huggingface.co/OpenAssistant/oasst-sft...   \n",
       "94   https://huggingface.co/tiiuae/falcon-7b-instruct   \n",
       "95  https://huggingface.co/baichuan-inc/Baichuan-1...   \n",
       "96                                                NaN   \n",
       "\n",
       "                                              samples     filter  \n",
       "0   https://github.com/tatsu-lab/alpaca_eval/blob/...    minimal  \n",
       "1   https://github.com/tatsu-lab/alpaca_eval/blob/...  community  \n",
       "2   https://github.com/tatsu-lab/alpaca_eval/blob/...  community  \n",
       "3   https://github.com/tatsu-lab/alpaca_eval/blob/...    minimal  \n",
       "4   https://github.com/tatsu-lab/alpaca_eval/blob/...  community  \n",
       "..                                                ...        ...  \n",
       "92  https://github.com/tatsu-lab/alpaca_eval/blob/...    minimal  \n",
       "93  https://github.com/tatsu-lab/alpaca_eval/blob/...   verified  \n",
       "94  https://github.com/tatsu-lab/alpaca_eval/blob/...   verified  \n",
       "95  https://github.com/tatsu-lab/alpaca_eval/blob/...  community  \n",
       "96  https://github.com/tatsu-lab/alpaca_eval/blob/...    minimal  \n",
       "\n",
       "[97 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of lines in elo\n",
    "len(elo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>variance</th>\n",
       "      <th>rating_q975</th>\n",
       "      <th>rating_q025</th>\n",
       "      <th>num_battles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RWKV-4-Raven-14B</th>\n",
       "      <td>924.231727</td>\n",
       "      <td>56.102110</td>\n",
       "      <td>939.938277</td>\n",
       "      <td>910.571472</td>\n",
       "      <td>5231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpaca-13b</th>\n",
       "      <td>903.741314</td>\n",
       "      <td>48.294819</td>\n",
       "      <td>918.853719</td>\n",
       "      <td>890.549086</td>\n",
       "      <td>6206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatglm-6b</th>\n",
       "      <td>882.464210</td>\n",
       "      <td>52.359596</td>\n",
       "      <td>896.432612</td>\n",
       "      <td>868.335295</td>\n",
       "      <td>5266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatglm2-6b</th>\n",
       "      <td>928.856627</td>\n",
       "      <td>73.475978</td>\n",
       "      <td>946.275697</td>\n",
       "      <td>913.007148</td>\n",
       "      <td>2924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatglm3-6b</th>\n",
       "      <td>959.975825</td>\n",
       "      <td>66.490085</td>\n",
       "      <td>977.777208</td>\n",
       "      <td>945.182976</td>\n",
       "      <td>3696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-1</th>\n",
       "      <td>1149.379341</td>\n",
       "      <td>47.590691</td>\n",
       "      <td>1164.315198</td>\n",
       "      <td>1136.601676</td>\n",
       "      <td>16956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-2.0</th>\n",
       "      <td>1130.616704</td>\n",
       "      <td>52.432423</td>\n",
       "      <td>1145.070539</td>\n",
       "      <td>1117.537633</td>\n",
       "      <td>11204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-2.1</th>\n",
       "      <td>1118.772077</td>\n",
       "      <td>46.599912</td>\n",
       "      <td>1133.145271</td>\n",
       "      <td>1106.276687</td>\n",
       "      <td>20883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-instant-1</th>\n",
       "      <td>1109.380031</td>\n",
       "      <td>47.385636</td>\n",
       "      <td>1124.636944</td>\n",
       "      <td>1096.643598</td>\n",
       "      <td>16182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codellama-34b-instruct</th>\n",
       "      <td>1041.413340</td>\n",
       "      <td>51.029353</td>\n",
       "      <td>1057.412081</td>\n",
       "      <td>1029.251071</td>\n",
       "      <td>6464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dolly-v2-12b</th>\n",
       "      <td>822.353912</td>\n",
       "      <td>62.582951</td>\n",
       "      <td>838.585948</td>\n",
       "      <td>808.132744</td>\n",
       "      <td>3716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dolphin-2.2.1-mistral-7b</th>\n",
       "      <td>1064.768876</td>\n",
       "      <td>78.750698</td>\n",
       "      <td>1083.265298</td>\n",
       "      <td>1047.314159</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-180b-chat</th>\n",
       "      <td>1031.113032</td>\n",
       "      <td>106.252106</td>\n",
       "      <td>1052.685022</td>\n",
       "      <td>1012.965179</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastchat-t5-3b</th>\n",
       "      <td>873.504920</td>\n",
       "      <td>54.712636</td>\n",
       "      <td>888.513353</td>\n",
       "      <td>860.286757</td>\n",
       "      <td>4592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-pro</th>\n",
       "      <td>1113.770208</td>\n",
       "      <td>55.986817</td>\n",
       "      <td>1129.495825</td>\n",
       "      <td>1100.714562</td>\n",
       "      <td>6981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-pro-dev-api</th>\n",
       "      <td>1120.446710</td>\n",
       "      <td>87.216253</td>\n",
       "      <td>1138.509585</td>\n",
       "      <td>1102.130002</td>\n",
       "      <td>1898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0314</th>\n",
       "      <td>1104.473206</td>\n",
       "      <td>51.538926</td>\n",
       "      <td>1118.994330</td>\n",
       "      <td>1091.161583</td>\n",
       "      <td>5961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0613</th>\n",
       "      <td>1116.167858</td>\n",
       "      <td>46.265192</td>\n",
       "      <td>1129.292757</td>\n",
       "      <td>1103.156929</td>\n",
       "      <td>26583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-1106</th>\n",
       "      <td>1072.312769</td>\n",
       "      <td>51.981770</td>\n",
       "      <td>1086.650442</td>\n",
       "      <td>1058.491161</td>\n",
       "      <td>11892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0314</th>\n",
       "      <td>1190.498739</td>\n",
       "      <td>46.843298</td>\n",
       "      <td>1204.909427</td>\n",
       "      <td>1176.584615</td>\n",
       "      <td>16237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0613</th>\n",
       "      <td>1159.625234</td>\n",
       "      <td>45.537522</td>\n",
       "      <td>1173.198148</td>\n",
       "      <td>1147.474296</td>\n",
       "      <td>20884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <td>1249.334661</td>\n",
       "      <td>47.673083</td>\n",
       "      <td>1263.406855</td>\n",
       "      <td>1236.312400</td>\n",
       "      <td>23069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4all-13b-snoozy</th>\n",
       "      <td>936.706295</td>\n",
       "      <td>90.889155</td>\n",
       "      <td>954.252226</td>\n",
       "      <td>917.672765</td>\n",
       "      <td>1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guanaco-33b</th>\n",
       "      <td>1030.921900</td>\n",
       "      <td>70.252167</td>\n",
       "      <td>1048.594417</td>\n",
       "      <td>1016.927777</td>\n",
       "      <td>3243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koala-13b</th>\n",
       "      <td>965.468002</td>\n",
       "      <td>47.989863</td>\n",
       "      <td>980.399131</td>\n",
       "      <td>952.830071</td>\n",
       "      <td>7420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-13b</th>\n",
       "      <td>800.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-2-13b-chat</th>\n",
       "      <td>1036.726952</td>\n",
       "      <td>52.534565</td>\n",
       "      <td>1051.544813</td>\n",
       "      <td>1024.016891</td>\n",
       "      <td>10926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-2-70b-chat</th>\n",
       "      <td>1079.026713</td>\n",
       "      <td>49.730905</td>\n",
       "      <td>1094.483283</td>\n",
       "      <td>1065.938859</td>\n",
       "      <td>13514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-2-7b-chat</th>\n",
       "      <td>1017.263710</td>\n",
       "      <td>54.535924</td>\n",
       "      <td>1031.644764</td>\n",
       "      <td>1003.435207</td>\n",
       "      <td>6579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-steerlm-chat</th>\n",
       "      <td>1076.677334</td>\n",
       "      <td>136.489899</td>\n",
       "      <td>1098.699928</td>\n",
       "      <td>1052.097510</td>\n",
       "      <td>902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-7b-instruct</th>\n",
       "      <td>1016.125896</td>\n",
       "      <td>59.753307</td>\n",
       "      <td>1031.987495</td>\n",
       "      <td>1002.254832</td>\n",
       "      <td>6499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-medium</th>\n",
       "      <td>1149.866992</td>\n",
       "      <td>54.689082</td>\n",
       "      <td>1164.881520</td>\n",
       "      <td>1137.110738</td>\n",
       "      <td>6586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixtral-8x7b-instruct-v0.1</th>\n",
       "      <td>1123.483354</td>\n",
       "      <td>50.099729</td>\n",
       "      <td>1138.422271</td>\n",
       "      <td>1110.213375</td>\n",
       "      <td>12469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mpt-30b-chat</th>\n",
       "      <td>1042.337339</td>\n",
       "      <td>66.770031</td>\n",
       "      <td>1058.679541</td>\n",
       "      <td>1027.374729</td>\n",
       "      <td>2872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mpt-7b-chat</th>\n",
       "      <td>930.081278</td>\n",
       "      <td>59.190757</td>\n",
       "      <td>946.299573</td>\n",
       "      <td>915.892727</td>\n",
       "      <td>4274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oasst-pythia-12b</th>\n",
       "      <td>896.371111</td>\n",
       "      <td>47.305148</td>\n",
       "      <td>910.567532</td>\n",
       "      <td>882.906863</td>\n",
       "      <td>6711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openchat-3.5</th>\n",
       "      <td>1076.346554</td>\n",
       "      <td>57.231535</td>\n",
       "      <td>1090.433089</td>\n",
       "      <td>1062.961093</td>\n",
       "      <td>6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openhermes-2.5-mistral-7b</th>\n",
       "      <td>1078.870526</td>\n",
       "      <td>63.656642</td>\n",
       "      <td>1095.151793</td>\n",
       "      <td>1064.536235</td>\n",
       "      <td>3077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>palm-2</th>\n",
       "      <td>1004.453952</td>\n",
       "      <td>50.836705</td>\n",
       "      <td>1019.682167</td>\n",
       "      <td>991.498447</td>\n",
       "      <td>9420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pplx-70b-online</th>\n",
       "      <td>1072.953906</td>\n",
       "      <td>58.767088</td>\n",
       "      <td>1089.549251</td>\n",
       "      <td>1059.026585</td>\n",
       "      <td>4996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pplx-7b-online</th>\n",
       "      <td>1034.725132</td>\n",
       "      <td>70.789004</td>\n",
       "      <td>1052.130330</td>\n",
       "      <td>1019.844920</td>\n",
       "      <td>4421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen-14b-chat</th>\n",
       "      <td>1036.205943</td>\n",
       "      <td>57.965385</td>\n",
       "      <td>1052.433991</td>\n",
       "      <td>1021.953429</td>\n",
       "      <td>4315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solar-10.7b-instruct-v1.0</th>\n",
       "      <td>1064.646974</td>\n",
       "      <td>62.711149</td>\n",
       "      <td>1080.911189</td>\n",
       "      <td>1049.839653</td>\n",
       "      <td>4598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stablelm-tuned-alpha-7b</th>\n",
       "      <td>844.426061</td>\n",
       "      <td>56.977440</td>\n",
       "      <td>860.575227</td>\n",
       "      <td>830.496042</td>\n",
       "      <td>3518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starling-lm-7b-alpha</th>\n",
       "      <td>1092.284114</td>\n",
       "      <td>70.860354</td>\n",
       "      <td>1109.471498</td>\n",
       "      <td>1076.054019</td>\n",
       "      <td>3947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tulu-2-dpo-70b</th>\n",
       "      <td>1106.076435</td>\n",
       "      <td>61.743800</td>\n",
       "      <td>1122.131282</td>\n",
       "      <td>1090.985676</td>\n",
       "      <td>4494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b</th>\n",
       "      <td>1040.659617</td>\n",
       "      <td>43.058316</td>\n",
       "      <td>1053.391708</td>\n",
       "      <td>1028.132395</td>\n",
       "      <td>15049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-33b</th>\n",
       "      <td>1093.778618</td>\n",
       "      <td>49.367996</td>\n",
       "      <td>1108.368994</td>\n",
       "      <td>1081.385716</td>\n",
       "      <td>15632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-7b</th>\n",
       "      <td>1004.065525</td>\n",
       "      <td>49.960610</td>\n",
       "      <td>1018.228388</td>\n",
       "      <td>991.467511</td>\n",
       "      <td>7562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizardlm-13b</th>\n",
       "      <td>1057.001411</td>\n",
       "      <td>54.522107</td>\n",
       "      <td>1072.514809</td>\n",
       "      <td>1042.792519</td>\n",
       "      <td>7767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizardlm-70b</th>\n",
       "      <td>1104.218972</td>\n",
       "      <td>54.607665</td>\n",
       "      <td>1120.951499</td>\n",
       "      <td>1090.371642</td>\n",
       "      <td>7531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi-34b-chat</th>\n",
       "      <td>1110.896308</td>\n",
       "      <td>61.806389</td>\n",
       "      <td>1127.163691</td>\n",
       "      <td>1096.357334</td>\n",
       "      <td>5055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyr-7b-alpha</th>\n",
       "      <td>1037.230490</td>\n",
       "      <td>95.319603</td>\n",
       "      <td>1056.172817</td>\n",
       "      <td>1016.866037</td>\n",
       "      <td>1943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyr-7b-beta</th>\n",
       "      <td>1049.230704</td>\n",
       "      <td>53.183853</td>\n",
       "      <td>1064.177045</td>\n",
       "      <td>1035.788742</td>\n",
       "      <td>10531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 rating    variance  rating_q975  rating_q025  \\\n",
       "RWKV-4-Raven-14B             924.231727   56.102110   939.938277   910.571472   \n",
       "alpaca-13b                   903.741314   48.294819   918.853719   890.549086   \n",
       "chatglm-6b                   882.464210   52.359596   896.432612   868.335295   \n",
       "chatglm2-6b                  928.856627   73.475978   946.275697   913.007148   \n",
       "chatglm3-6b                  959.975825   66.490085   977.777208   945.182976   \n",
       "claude-1                    1149.379341   47.590691  1164.315198  1136.601676   \n",
       "claude-2.0                  1130.616704   52.432423  1145.070539  1117.537633   \n",
       "claude-2.1                  1118.772077   46.599912  1133.145271  1106.276687   \n",
       "claude-instant-1            1109.380031   47.385636  1124.636944  1096.643598   \n",
       "codellama-34b-instruct      1041.413340   51.029353  1057.412081  1029.251071   \n",
       "dolly-v2-12b                 822.353912   62.582951   838.585948   808.132744   \n",
       "dolphin-2.2.1-mistral-7b    1064.768876   78.750698  1083.265298  1047.314159   \n",
       "falcon-180b-chat            1031.113032  106.252106  1052.685022  1012.965179   \n",
       "fastchat-t5-3b               873.504920   54.712636   888.513353   860.286757   \n",
       "gemini-pro                  1113.770208   55.986817  1129.495825  1100.714562   \n",
       "gemini-pro-dev-api          1120.446710   87.216253  1138.509585  1102.130002   \n",
       "gpt-3.5-turbo-0314          1104.473206   51.538926  1118.994330  1091.161583   \n",
       "gpt-3.5-turbo-0613          1116.167858   46.265192  1129.292757  1103.156929   \n",
       "gpt-3.5-turbo-1106          1072.312769   51.981770  1086.650442  1058.491161   \n",
       "gpt-4-0314                  1190.498739   46.843298  1204.909427  1176.584615   \n",
       "gpt-4-0613                  1159.625234   45.537522  1173.198148  1147.474296   \n",
       "gpt-4-turbo                 1249.334661   47.673083  1263.406855  1236.312400   \n",
       "gpt4all-13b-snoozy           936.706295   90.889155   954.252226   917.672765   \n",
       "guanaco-33b                 1030.921900   70.252167  1048.594417  1016.927777   \n",
       "koala-13b                    965.468002   47.989863   980.399131   952.830071   \n",
       "llama-13b                    800.000000    0.000000   800.000000   800.000000   \n",
       "llama-2-13b-chat            1036.726952   52.534565  1051.544813  1024.016891   \n",
       "llama-2-70b-chat            1079.026713   49.730905  1094.483283  1065.938859   \n",
       "llama-2-7b-chat             1017.263710   54.535924  1031.644764  1003.435207   \n",
       "llama2-70b-steerlm-chat     1076.677334  136.489899  1098.699928  1052.097510   \n",
       "mistral-7b-instruct         1016.125896   59.753307  1031.987495  1002.254832   \n",
       "mistral-medium              1149.866992   54.689082  1164.881520  1137.110738   \n",
       "mixtral-8x7b-instruct-v0.1  1123.483354   50.099729  1138.422271  1110.213375   \n",
       "mpt-30b-chat                1042.337339   66.770031  1058.679541  1027.374729   \n",
       "mpt-7b-chat                  930.081278   59.190757   946.299573   915.892727   \n",
       "oasst-pythia-12b             896.371111   47.305148   910.567532   882.906863   \n",
       "openchat-3.5                1076.346554   57.231535  1090.433089  1062.961093   \n",
       "openhermes-2.5-mistral-7b   1078.870526   63.656642  1095.151793  1064.536235   \n",
       "palm-2                      1004.453952   50.836705  1019.682167   991.498447   \n",
       "pplx-70b-online             1072.953906   58.767088  1089.549251  1059.026585   \n",
       "pplx-7b-online              1034.725132   70.789004  1052.130330  1019.844920   \n",
       "qwen-14b-chat               1036.205943   57.965385  1052.433991  1021.953429   \n",
       "solar-10.7b-instruct-v1.0   1064.646974   62.711149  1080.911189  1049.839653   \n",
       "stablelm-tuned-alpha-7b      844.426061   56.977440   860.575227   830.496042   \n",
       "starling-lm-7b-alpha        1092.284114   70.860354  1109.471498  1076.054019   \n",
       "tulu-2-dpo-70b              1106.076435   61.743800  1122.131282  1090.985676   \n",
       "vicuna-13b                  1040.659617   43.058316  1053.391708  1028.132395   \n",
       "vicuna-33b                  1093.778618   49.367996  1108.368994  1081.385716   \n",
       "vicuna-7b                   1004.065525   49.960610  1018.228388   991.467511   \n",
       "wizardlm-13b                1057.001411   54.522107  1072.514809  1042.792519   \n",
       "wizardlm-70b                1104.218972   54.607665  1120.951499  1090.371642   \n",
       "yi-34b-chat                 1110.896308   61.806389  1127.163691  1096.357334   \n",
       "zephyr-7b-alpha             1037.230490   95.319603  1056.172817  1016.866037   \n",
       "zephyr-7b-beta              1049.230704   53.183853  1064.177045  1035.788742   \n",
       "\n",
       "                            num_battles  \n",
       "RWKV-4-Raven-14B                   5231  \n",
       "alpaca-13b                         6206  \n",
       "chatglm-6b                         5266  \n",
       "chatglm2-6b                        2924  \n",
       "chatglm3-6b                        3696  \n",
       "claude-1                          16956  \n",
       "claude-2.0                        11204  \n",
       "claude-2.1                        20883  \n",
       "claude-instant-1                  16182  \n",
       "codellama-34b-instruct             6464  \n",
       "dolly-v2-12b                       3716  \n",
       "dolphin-2.2.1-mistral-7b           1817  \n",
       "falcon-180b-chat                   1420  \n",
       "fastchat-t5-3b                     4592  \n",
       "gemini-pro                         6981  \n",
       "gemini-pro-dev-api                 1898  \n",
       "gpt-3.5-turbo-0314                 5961  \n",
       "gpt-3.5-turbo-0613                26583  \n",
       "gpt-3.5-turbo-1106                11892  \n",
       "gpt-4-0314                        16237  \n",
       "gpt-4-0613                        20884  \n",
       "gpt-4-turbo                       23069  \n",
       "gpt4all-13b-snoozy                 1938  \n",
       "guanaco-33b                        3243  \n",
       "koala-13b                          7420  \n",
       "llama-13b                          2600  \n",
       "llama-2-13b-chat                  10926  \n",
       "llama-2-70b-chat                  13514  \n",
       "llama-2-7b-chat                    6579  \n",
       "llama2-70b-steerlm-chat             902  \n",
       "mistral-7b-instruct                6499  \n",
       "mistral-medium                     6586  \n",
       "mixtral-8x7b-instruct-v0.1        12469  \n",
       "mpt-30b-chat                       2872  \n",
       "mpt-7b-chat                        4274  \n",
       "oasst-pythia-12b                   6711  \n",
       "openchat-3.5                       6701  \n",
       "openhermes-2.5-mistral-7b          3077  \n",
       "palm-2                             9420  \n",
       "pplx-70b-online                    4996  \n",
       "pplx-7b-online                     4421  \n",
       "qwen-14b-chat                      4315  \n",
       "solar-10.7b-instruct-v1.0          4598  \n",
       "stablelm-tuned-alpha-7b            3518  \n",
       "starling-lm-7b-alpha               3947  \n",
       "tulu-2-dpo-70b                     4494  \n",
       "vicuna-13b                        15049  \n",
       "vicuna-33b                        15632  \n",
       "vicuna-7b                          7562  \n",
       "wizardlm-13b                       7767  \n",
       "wizardlm-70b                       7531  \n",
       "yi-34b-chat                        5055  \n",
       "zephyr-7b-alpha                    1943  \n",
       "zephyr-7b-beta                    10531  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out models with benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for key in elo.index:\n",
    "    if chatbot_arena.loc[chatbot_arena['key'] == key].empty:\n",
    "        print(key)\n",
    "    else:\n",
    "        model = chatbot_arena.loc[chatbot_arena['key'] == key].iloc[0].to_dict()\n",
    "        models[key] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_models = {}\n",
    "for key in models.keys():\n",
    "    if 'huggingface' in models[key]['Link']:\n",
    "        hf_models[key] = models[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hf_models = {}\n",
    "\n",
    "# Retrieve hf model results from open-llm-leaderboard\n",
    "for key in hf_models.keys():\n",
    "    link = hf_models[key]['Link']\n",
    "    path = 'data/open-llm-leaderboard/' + '/'.join(link.split('/')[-2:])\n",
    "    if os.path.exists(path):\n",
    "        result_files = glob(path + '/*.json')\n",
    "        results = {}\n",
    "        for file in result_files:\n",
    "            with open(file) as f:\n",
    "                result = json.load(f)\n",
    "                results |= result.get('results', {})\n",
    "\n",
    "        filtered_hf_models[key] = hf_models[key]\n",
    "        filtered_hf_models[key]['results'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_hf_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chatglm2-6b': {'key': 'chatglm2-6b',\n",
       "  'Model': 'ChatGLM2-6B',\n",
       "  'MT-bench (score)': '4.96',\n",
       "  'MMLU': '0.455',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'Tsinghua',\n",
       "  'Link': 'https://huggingface.co/THUDM/chatglm2-6b',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.37457337883959047,\n",
       "    'acc_stderr': 0.014144193471893444,\n",
       "    'acc_norm': 0.38822525597269625,\n",
       "    'acc_norm_stderr': 0.014241614207414044},\n",
       "   'harness|hellaswag|10': {'acc': 0.4596693885680143,\n",
       "    'acc_stderr': 0.004973522582431221,\n",
       "    'acc_norm': 0.5902210714997013,\n",
       "    'acc_norm_stderr': 0.004907877144720029},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4148148148148148,\n",
       "    'acc_stderr': 0.042561937679014075,\n",
       "    'acc_norm': 0.4148148148148148,\n",
       "    'acc_norm_stderr': 0.042561937679014075},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5328947368421053,\n",
       "    'acc_stderr': 0.04060127035236395,\n",
       "    'acc_norm': 0.5328947368421053,\n",
       "    'acc_norm_stderr': 0.04060127035236395},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.61,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.61,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.4867924528301887,\n",
       "    'acc_stderr': 0.030762134874500476,\n",
       "    'acc_norm': 0.4867924528301887,\n",
       "    'acc_norm_stderr': 0.030762134874500476},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.04076663253918567,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.04076663253918567},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.42196531791907516,\n",
       "    'acc_stderr': 0.0376574669386515,\n",
       "    'acc_norm': 0.42196531791907516,\n",
       "    'acc_norm_stderr': 0.0376574669386515},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.21568627450980393,\n",
       "    'acc_stderr': 0.040925639582376556,\n",
       "    'acc_norm': 0.21568627450980393,\n",
       "    'acc_norm_stderr': 0.040925639582376556},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.03202563076101736,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.03202563076101736},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.30701754385964913,\n",
       "    'acc_stderr': 0.043391383225798615,\n",
       "    'acc_norm': 0.30701754385964913,\n",
       "    'acc_norm_stderr': 0.043391383225798615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4689655172413793,\n",
       "    'acc_stderr': 0.04158632762097828,\n",
       "    'acc_norm': 0.4689655172413793,\n",
       "    'acc_norm_stderr': 0.04158632762097828},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.31746031746031744,\n",
       "    'acc_stderr': 0.023973861998992072,\n",
       "    'acc_norm': 0.31746031746031744,\n",
       "    'acc_norm_stderr': 0.023973861998992072},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3492063492063492,\n",
       "    'acc_stderr': 0.04263906892795133,\n",
       "    'acc_norm': 0.3492063492063492,\n",
       "    'acc_norm_stderr': 0.04263906892795133},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.5580645161290323,\n",
       "    'acc_stderr': 0.02825155790684973,\n",
       "    'acc_norm': 0.5580645161290323,\n",
       "    'acc_norm_stderr': 0.02825155790684973},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.45320197044334976,\n",
       "    'acc_stderr': 0.03502544650845872,\n",
       "    'acc_norm': 0.45320197044334976,\n",
       "    'acc_norm_stderr': 0.03502544650845872},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.42,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.42,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.038254602783800246,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.038254602783800246},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.035402943770953675,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.035402943770953675},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.5699481865284974,\n",
       "    'acc_stderr': 0.03572954333144809,\n",
       "    'acc_norm': 0.5699481865284974,\n",
       "    'acc_norm_stderr': 0.03572954333144809},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.44871794871794873,\n",
       "    'acc_stderr': 0.025217315184846482,\n",
       "    'acc_norm': 0.44871794871794873,\n",
       "    'acc_norm_stderr': 0.025217315184846482},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.26296296296296295,\n",
       "    'acc_stderr': 0.026842057873833706,\n",
       "    'acc_norm': 0.26296296296296295,\n",
       "    'acc_norm_stderr': 0.026842057873833706},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.4327731092436975,\n",
       "    'acc_stderr': 0.03218358107742613,\n",
       "    'acc_norm': 0.4327731092436975,\n",
       "    'acc_norm_stderr': 0.03218358107742613},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2913907284768212,\n",
       "    'acc_stderr': 0.03710185726119994,\n",
       "    'acc_norm': 0.2913907284768212,\n",
       "    'acc_norm_stderr': 0.03710185726119994},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.5926605504587156,\n",
       "    'acc_stderr': 0.021065986244412895,\n",
       "    'acc_norm': 0.5926605504587156,\n",
       "    'acc_norm_stderr': 0.021065986244412895},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.33796296296296297,\n",
       "    'acc_stderr': 0.032259413526312945,\n",
       "    'acc_norm': 0.33796296296296297,\n",
       "    'acc_norm_stderr': 0.032259413526312945},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.5882352941176471,\n",
       "    'acc_stderr': 0.03454236585380609,\n",
       "    'acc_norm': 0.5882352941176471,\n",
       "    'acc_norm_stderr': 0.03454236585380609},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6624472573839663,\n",
       "    'acc_stderr': 0.03078154910202621,\n",
       "    'acc_norm': 0.6624472573839663,\n",
       "    'acc_norm_stderr': 0.03078154910202621},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.4663677130044843,\n",
       "    'acc_stderr': 0.033481800170603065,\n",
       "    'acc_norm': 0.4663677130044843,\n",
       "    'acc_norm_stderr': 0.033481800170603065},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.48091603053435117,\n",
       "    'acc_stderr': 0.04382094705550988,\n",
       "    'acc_norm': 0.48091603053435117,\n",
       "    'acc_norm_stderr': 0.04382094705550988},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.6033057851239669,\n",
       "    'acc_stderr': 0.04465869780531009,\n",
       "    'acc_norm': 0.6033057851239669,\n",
       "    'acc_norm_stderr': 0.04465869780531009},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04766075165356461,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04766075165356461},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.49079754601226994,\n",
       "    'acc_stderr': 0.03927705600787443,\n",
       "    'acc_norm': 0.49079754601226994,\n",
       "    'acc_norm_stderr': 0.03927705600787443},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.4375,\n",
       "    'acc_stderr': 0.04708567521880525,\n",
       "    'acc_norm': 0.4375,\n",
       "    'acc_norm_stderr': 0.04708567521880525},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6213592233009708,\n",
       "    'acc_stderr': 0.048026946982589726,\n",
       "    'acc_norm': 0.6213592233009708,\n",
       "    'acc_norm_stderr': 0.048026946982589726},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.6923076923076923,\n",
       "    'acc_stderr': 0.030236389942173102,\n",
       "    'acc_norm': 0.6923076923076923,\n",
       "    'acc_norm_stderr': 0.030236389942173102},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.5964240102171137,\n",
       "    'acc_stderr': 0.017544332237926417,\n",
       "    'acc_norm': 0.5964240102171137,\n",
       "    'acc_norm_stderr': 0.017544332237926417},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5289017341040463,\n",
       "    'acc_stderr': 0.026874085883518348,\n",
       "    'acc_norm': 0.5289017341040463,\n",
       "    'acc_norm_stderr': 0.026874085883518348},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.23575418994413408,\n",
       "    'acc_stderr': 0.014196375686290804,\n",
       "    'acc_norm': 0.23575418994413408,\n",
       "    'acc_norm_stderr': 0.014196375686290804},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5326797385620915,\n",
       "    'acc_stderr': 0.028568699752225868,\n",
       "    'acc_norm': 0.5326797385620915,\n",
       "    'acc_norm_stderr': 0.028568699752225868},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.4983922829581994,\n",
       "    'acc_stderr': 0.02839794490780661,\n",
       "    'acc_norm': 0.4983922829581994,\n",
       "    'acc_norm_stderr': 0.02839794490780661},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.49382716049382713,\n",
       "    'acc_stderr': 0.02781862396258329,\n",
       "    'acc_norm': 0.49382716049382713,\n",
       "    'acc_norm_stderr': 0.02781862396258329},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.02812163604063988,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.02812163604063988},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3559322033898305,\n",
       "    'acc_stderr': 0.012228645537277566,\n",
       "    'acc_norm': 0.3559322033898305,\n",
       "    'acc_norm_stderr': 0.012228645537277566},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.35294117647058826,\n",
       "    'acc_stderr': 0.029029422815681393,\n",
       "    'acc_norm': 0.35294117647058826,\n",
       "    'acc_norm_stderr': 0.029029422815681393},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.020102583895887188,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.020102583895887188},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5818181818181818,\n",
       "    'acc_stderr': 0.04724577405731572,\n",
       "    'acc_norm': 0.5818181818181818,\n",
       "    'acc_norm_stderr': 0.04724577405731572},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5673469387755102,\n",
       "    'acc_stderr': 0.031717528240626645,\n",
       "    'acc_norm': 0.5673469387755102,\n",
       "    'acc_norm_stderr': 0.031717528240626645},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6218905472636815,\n",
       "    'acc_stderr': 0.034288678487786564,\n",
       "    'acc_norm': 0.6218905472636815,\n",
       "    'acc_norm_stderr': 0.034288678487786564},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.41566265060240964,\n",
       "    'acc_stderr': 0.03836722176598052,\n",
       "    'acc_norm': 0.41566265060240964,\n",
       "    'acc_norm_stderr': 0.03836722176598052},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.038295098689947286,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.038295098689947286},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.2998776009791922,\n",
       "    'mc1_stderr': 0.016040352966713616,\n",
       "    'mc2': 0.48081249614578586,\n",
       "    'mc2_stderr': 0.01612488256095027},\n",
       "   'all': {'acc': 0.46492840933020524,\n",
       "    'acc_stderr': 0.03535258865761692,\n",
       "    'acc_norm': 0.46737253746689666,\n",
       "    'acc_norm_stderr': 0.035353127221986566,\n",
       "    'mc1': 0.2998776009791922,\n",
       "    'mc1_stderr': 0.016040352966713616,\n",
       "    'mc2': 0.48081249614578586,\n",
       "    'mc2_stderr': 0.01612488256095027}}},\n",
       " 'codellama-34b-instruct': {'key': 'codellama-34b-instruct',\n",
       "  'Model': 'CodeLlama-34B-instruct',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.537',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf',\n",
       "  'results': {'harness|drop|3': {'em': 0.0014681208053691276,\n",
       "    'em_stderr': 0.00039210421902985756,\n",
       "    'f1': 0.05437604865771826,\n",
       "    'f1_stderr': 0.0011898517432786567},\n",
       "   'harness|gsm8k|5': {'acc': 0.3100833965125095,\n",
       "    'acc_stderr': 0.012740305717376268},\n",
       "   'harness|winogrande|5': {'acc': 0.745067087608524,\n",
       "    'acc_stderr': 0.012248806969376422},\n",
       "   'all': {'em': 0.0014681208053691276,\n",
       "    'em_stderr': 0.00039210421902985756,\n",
       "    'f1': 0.05437604865771826,\n",
       "    'f1_stderr': 0.0011898517432786567,\n",
       "    'acc': 0.5275752420605168,\n",
       "    'acc_stderr': 0.012494556343376345},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5093856655290102,\n",
       "    'acc_stderr': 0.014608816322065,\n",
       "    'acc_norm': 0.5426621160409556,\n",
       "    'acc_norm_stderr': 0.01455810654392406},\n",
       "   'harness|hellaswag|10': {'acc': 0.5637323242381995,\n",
       "    'acc_stderr': 0.004949080334816024,\n",
       "    'acc_norm': 0.7691694881497709,\n",
       "    'acc_norm_stderr': 0.004205030476886528},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.45925925925925926,\n",
       "    'acc_stderr': 0.04304979692464244,\n",
       "    'acc_norm': 0.45925925925925926,\n",
       "    'acc_norm_stderr': 0.04304979692464244},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5921052631578947,\n",
       "    'acc_stderr': 0.03999309712777472,\n",
       "    'acc_norm': 0.5921052631578947,\n",
       "    'acc_norm_stderr': 0.03999309712777472},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.61,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.61,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.49056603773584906,\n",
       "    'acc_stderr': 0.0307673947078081,\n",
       "    'acc_norm': 0.49056603773584906,\n",
       "    'acc_norm_stderr': 0.0307673947078081},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.04179596617581,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.04179596617581},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4508670520231214,\n",
       "    'acc_stderr': 0.03794012674697028,\n",
       "    'acc_norm': 0.4508670520231214,\n",
       "    'acc_norm_stderr': 0.03794012674697028},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3431372549019608,\n",
       "    'acc_stderr': 0.047240073523838876,\n",
       "    'acc_norm': 0.3431372549019608,\n",
       "    'acc_norm_stderr': 0.047240073523838876},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.69,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.69,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.49361702127659574,\n",
       "    'acc_stderr': 0.03268335899936336,\n",
       "    'acc_norm': 0.49361702127659574,\n",
       "    'acc_norm_stderr': 0.03268335899936336},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.42105263157894735,\n",
       "    'acc_stderr': 0.046446020912223177,\n",
       "    'acc_norm': 0.42105263157894735,\n",
       "    'acc_norm_stderr': 0.046446020912223177},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5241379310344828,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.5241379310344828,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3915343915343915,\n",
       "    'acc_stderr': 0.025138091388851102,\n",
       "    'acc_norm': 0.3915343915343915,\n",
       "    'acc_norm_stderr': 0.025138091388851102},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4603174603174603,\n",
       "    'acc_stderr': 0.04458029125470973,\n",
       "    'acc_norm': 0.4603174603174603,\n",
       "    'acc_norm_stderr': 0.04458029125470973},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6096774193548387,\n",
       "    'acc_stderr': 0.027751256636969576,\n",
       "    'acc_norm': 0.6096774193548387,\n",
       "    'acc_norm_stderr': 0.027751256636969576},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3891625615763547,\n",
       "    'acc_stderr': 0.034304624161038716,\n",
       "    'acc_norm': 0.3891625615763547,\n",
       "    'acc_norm_stderr': 0.034304624161038716},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.036639749943912434,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.036639749943912434},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.03191178226713549,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.03191178226713549},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7616580310880829,\n",
       "    'acc_stderr': 0.03074890536390989,\n",
       "    'acc_norm': 0.7616580310880829,\n",
       "    'acc_norm_stderr': 0.03074890536390989},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5153846153846153,\n",
       "    'acc_stderr': 0.02533900301010651,\n",
       "    'acc_norm': 0.5153846153846153,\n",
       "    'acc_norm_stderr': 0.02533900301010651},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.34074074074074073,\n",
       "    'acc_stderr': 0.02889774874113114,\n",
       "    'acc_norm': 0.34074074074074073,\n",
       "    'acc_norm_stderr': 0.02889774874113114},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5168067226890757,\n",
       "    'acc_stderr': 0.03246013680375308,\n",
       "    'acc_norm': 0.5168067226890757,\n",
       "    'acc_norm_stderr': 0.03246013680375308},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.36423841059602646,\n",
       "    'acc_stderr': 0.03929111781242742,\n",
       "    'acc_norm': 0.36423841059602646,\n",
       "    'acc_norm_stderr': 0.03929111781242742},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7064220183486238,\n",
       "    'acc_stderr': 0.019525151122639667,\n",
       "    'acc_norm': 0.7064220183486238,\n",
       "    'acc_norm_stderr': 0.019525151122639667},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.033247089118091176,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.033247089118091176},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.029102254389674082,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.029102254389674082},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7637130801687764,\n",
       "    'acc_stderr': 0.027652153144159256,\n",
       "    'acc_norm': 0.7637130801687764,\n",
       "    'acc_norm_stderr': 0.027652153144159256},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6098654708520179,\n",
       "    'acc_stderr': 0.03273766725459156,\n",
       "    'acc_norm': 0.6098654708520179,\n",
       "    'acc_norm_stderr': 0.03273766725459156},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6259541984732825,\n",
       "    'acc_stderr': 0.042438692422305246,\n",
       "    'acc_norm': 0.6259541984732825,\n",
       "    'acc_norm_stderr': 0.042438692422305246},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7355371900826446,\n",
       "    'acc_stderr': 0.040261875275912046,\n",
       "    'acc_norm': 0.7355371900826446,\n",
       "    'acc_norm_stderr': 0.040261875275912046},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.04557239513497751,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.04557239513497751},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6993865030674846,\n",
       "    'acc_stderr': 0.03602511318806771,\n",
       "    'acc_norm': 0.6993865030674846,\n",
       "    'acc_norm_stderr': 0.03602511318806771},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.38392857142857145,\n",
       "    'acc_stderr': 0.04616143075028547,\n",
       "    'acc_norm': 0.38392857142857145,\n",
       "    'acc_norm_stderr': 0.04616143075028547},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.046561471100123514,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.046561471100123514},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.811965811965812,\n",
       "    'acc_stderr': 0.025598193686652265,\n",
       "    'acc_norm': 0.811965811965812,\n",
       "    'acc_norm_stderr': 0.025598193686652265},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.0498887651569859,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.0498887651569859},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7075351213282248,\n",
       "    'acc_stderr': 0.016267000684598645,\n",
       "    'acc_norm': 0.7075351213282248,\n",
       "    'acc_norm_stderr': 0.016267000684598645},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5780346820809249,\n",
       "    'acc_stderr': 0.02658923114217426,\n",
       "    'acc_norm': 0.5780346820809249,\n",
       "    'acc_norm_stderr': 0.02658923114217426},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.31620111731843575,\n",
       "    'acc_stderr': 0.015551673652172554,\n",
       "    'acc_norm': 0.31620111731843575,\n",
       "    'acc_norm_stderr': 0.015551673652172554},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5915032679738562,\n",
       "    'acc_stderr': 0.028146405993096358,\n",
       "    'acc_norm': 0.5915032679738562,\n",
       "    'acc_norm_stderr': 0.028146405993096358},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.594855305466238,\n",
       "    'acc_stderr': 0.027882383791325963,\n",
       "    'acc_norm': 0.594855305466238,\n",
       "    'acc_norm_stderr': 0.027882383791325963},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6141975308641975,\n",
       "    'acc_stderr': 0.027085401226132143,\n",
       "    'acc_norm': 0.6141975308641975,\n",
       "    'acc_norm_stderr': 0.027085401226132143},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.40425531914893614,\n",
       "    'acc_stderr': 0.029275532159704725,\n",
       "    'acc_norm': 0.40425531914893614,\n",
       "    'acc_norm_stderr': 0.029275532159704725},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3891786179921773,\n",
       "    'acc_stderr': 0.012452613934287012,\n",
       "    'acc_norm': 0.3891786179921773,\n",
       "    'acc_norm_stderr': 0.012452613934287012},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.47794117647058826,\n",
       "    'acc_stderr': 0.030343264224213535,\n",
       "    'acc_norm': 0.47794117647058826,\n",
       "    'acc_norm_stderr': 0.030343264224213535},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.020227834851568375,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.020227834851568375},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6489795918367347,\n",
       "    'acc_stderr': 0.03055531675557364,\n",
       "    'acc_norm': 0.6489795918367347,\n",
       "    'acc_norm_stderr': 0.03055531675557364},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7611940298507462,\n",
       "    'acc_stderr': 0.03014777593540922,\n",
       "    'acc_norm': 0.7611940298507462,\n",
       "    'acc_norm_stderr': 0.03014777593540922},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.041633319989322626,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.041633319989322626},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.42168674698795183,\n",
       "    'acc_stderr': 0.03844453181770917,\n",
       "    'acc_norm': 0.42168674698795183,\n",
       "    'acc_norm_stderr': 0.03844453181770917},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7485380116959064,\n",
       "    'acc_stderr': 0.033275044238468436,\n",
       "    'acc_norm': 0.7485380116959064,\n",
       "    'acc_norm_stderr': 0.033275044238468436},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.44437538633055657,\n",
       "    'mc2_stderr': 0.014550940721814704}}},\n",
       " 'dolly-v2-12b': {'key': 'dolly-v2-12b',\n",
       "  'Model': 'Dolly-V2-12B',\n",
       "  'MT-bench (score)': '3.28',\n",
       "  'MMLU': '0.257',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'Databricks',\n",
       "  'Link': 'https://huggingface.co/databricks/dolly-v2-12b',\n",
       "  'results': {'harness|drop|3': {'em': 0.0016778523489932886,\n",
       "    'em_stderr': 0.0004191330178826844,\n",
       "    'f1': 0.06285968959731549,\n",
       "    'f1_stderr': 0.0014820300080071475},\n",
       "   'harness|gsm8k|5': {'acc': 0.012130401819560273,\n",
       "    'acc_stderr': 0.0030152942428909495},\n",
       "   'harness|winogrande|5': {'acc': 0.6085240726124704,\n",
       "    'acc_stderr': 0.013717487071290854},\n",
       "   'all': {'acc': 0.2661068958576839,\n",
       "    'acc_stderr': 0.03186337801557282,\n",
       "    'acc_norm': 0.26986300658126056,\n",
       "    'acc_norm_stderr': 0.0318588678937482,\n",
       "    'mc1': 0.200734394124847,\n",
       "    'mc1_stderr': 0.014022045717482156,\n",
       "    'mc2': 0.33827081326692277,\n",
       "    'mc2_stderr': 0.014862542606576355},\n",
       "   'harness|arc:challenge|25': {'acc': 0.38139931740614336,\n",
       "    'acc_stderr': 0.01419438908668526,\n",
       "    'acc_norm': 0.42406143344709896,\n",
       "    'acc_norm_stderr': 0.0144418896274644},\n",
       "   'harness|hellaswag|10': {'acc': 0.5463055168293168,\n",
       "    'acc_stderr': 0.0049683371441363675,\n",
       "    'acc_norm': 0.7252539334793866,\n",
       "    'acc_norm_stderr': 0.00445473941570504},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.04072314811876837,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.04072314811876837},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.2631578947368421,\n",
       "    'acc_stderr': 0.03583496176361064,\n",
       "    'acc_norm': 0.2631578947368421,\n",
       "    'acc_norm_stderr': 0.03583496176361064},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816507,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816507},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.22264150943396227,\n",
       "    'acc_stderr': 0.0256042334708991,\n",
       "    'acc_norm': 0.22264150943396227,\n",
       "    'acc_norm_stderr': 0.0256042334708991},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2638888888888889,\n",
       "    'acc_stderr': 0.03685651095897532,\n",
       "    'acc_norm': 0.2638888888888889,\n",
       "    'acc_norm_stderr': 0.03685651095897532},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.18,\n",
       "    'acc_stderr': 0.038612291966536955,\n",
       "    'acc_norm': 0.18,\n",
       "    'acc_norm_stderr': 0.038612291966536955},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.2,\n",
       "    'acc_stderr': 0.04020151261036845,\n",
       "    'acc_norm': 0.2,\n",
       "    'acc_norm_stderr': 0.04020151261036845},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.23699421965317918,\n",
       "    'acc_stderr': 0.03242414757483099,\n",
       "    'acc_norm': 0.23699421965317918,\n",
       "    'acc_norm_stderr': 0.03242414757483099},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.23529411764705882,\n",
       "    'acc_stderr': 0.04220773659171452,\n",
       "    'acc_norm': 0.23529411764705882,\n",
       "    'acc_norm_stderr': 0.04220773659171452},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.23829787234042554,\n",
       "    'acc_stderr': 0.027851252973889788,\n",
       "    'acc_norm': 0.23829787234042554,\n",
       "    'acc_norm_stderr': 0.027851252973889788},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2543859649122807,\n",
       "    'acc_stderr': 0.040969851398436695,\n",
       "    'acc_norm': 0.2543859649122807,\n",
       "    'acc_norm_stderr': 0.040969851398436695},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.2827586206896552,\n",
       "    'acc_stderr': 0.03752833958003337,\n",
       "    'acc_norm': 0.2827586206896552,\n",
       "    'acc_norm_stderr': 0.03752833958003337},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.2698412698412698,\n",
       "    'acc_stderr': 0.02286083830923207,\n",
       "    'acc_norm': 0.2698412698412698,\n",
       "    'acc_norm_stderr': 0.02286083830923207},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.1984126984126984,\n",
       "    'acc_stderr': 0.035670166752768635,\n",
       "    'acc_norm': 0.1984126984126984,\n",
       "    'acc_norm_stderr': 0.035670166752768635},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.26129032258064516,\n",
       "    'acc_stderr': 0.024993053397764822,\n",
       "    'acc_norm': 0.26129032258064516,\n",
       "    'acc_norm_stderr': 0.024993053397764822},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.2512315270935961,\n",
       "    'acc_stderr': 0.030516530732694436,\n",
       "    'acc_norm': 0.2512315270935961,\n",
       "    'acc_norm_stderr': 0.030516530732694436},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.20606060606060606,\n",
       "    'acc_stderr': 0.03158415324047708,\n",
       "    'acc_norm': 0.20606060606060606,\n",
       "    'acc_norm_stderr': 0.03158415324047708},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.20707070707070707,\n",
       "    'acc_stderr': 0.02886977846026705,\n",
       "    'acc_norm': 0.20707070707070707,\n",
       "    'acc_norm_stderr': 0.02886977846026705},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.23316062176165803,\n",
       "    'acc_stderr': 0.03051611137147601,\n",
       "    'acc_norm': 0.23316062176165803,\n",
       "    'acc_norm_stderr': 0.03051611137147601},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.23076923076923078,\n",
       "    'acc_stderr': 0.02136202772522273,\n",
       "    'acc_norm': 0.23076923076923078,\n",
       "    'acc_norm_stderr': 0.02136202772522273},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.26666666666666666,\n",
       "    'acc_stderr': 0.026962424325073835,\n",
       "    'acc_norm': 0.26666666666666666,\n",
       "    'acc_norm_stderr': 0.026962424325073835},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.226890756302521,\n",
       "    'acc_stderr': 0.02720537153827949,\n",
       "    'acc_norm': 0.226890756302521,\n",
       "    'acc_norm_stderr': 0.02720537153827949},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.23178807947019867,\n",
       "    'acc_stderr': 0.03445406271987054,\n",
       "    'acc_norm': 0.23178807947019867,\n",
       "    'acc_norm_stderr': 0.03445406271987054},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.26605504587155965,\n",
       "    'acc_stderr': 0.018946022322225593,\n",
       "    'acc_norm': 0.26605504587155965,\n",
       "    'acc_norm_stderr': 0.018946022322225593},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.20833333333333334,\n",
       "    'acc_stderr': 0.02769691071309394,\n",
       "    'acc_norm': 0.20833333333333334,\n",
       "    'acc_norm_stderr': 0.02769691071309394},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.2696078431372549,\n",
       "    'acc_stderr': 0.031145570659486782,\n",
       "    'acc_norm': 0.2696078431372549,\n",
       "    'acc_norm_stderr': 0.031145570659486782},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.32489451476793246,\n",
       "    'acc_stderr': 0.030486039389105307,\n",
       "    'acc_norm': 0.32489451476793246,\n",
       "    'acc_norm_stderr': 0.030486039389105307},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.29596412556053814,\n",
       "    'acc_stderr': 0.0306365913486998,\n",
       "    'acc_norm': 0.29596412556053814,\n",
       "    'acc_norm_stderr': 0.0306365913486998},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.22137404580152673,\n",
       "    'acc_stderr': 0.03641297081313729,\n",
       "    'acc_norm': 0.22137404580152673,\n",
       "    'acc_norm_stderr': 0.03641297081313729},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.38016528925619836,\n",
       "    'acc_stderr': 0.04431324501968431,\n",
       "    'acc_norm': 0.38016528925619836,\n",
       "    'acc_norm_stderr': 0.04431324501968431},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.2222222222222222,\n",
       "    'acc_stderr': 0.040191074725573483,\n",
       "    'acc_norm': 0.2222222222222222,\n",
       "    'acc_norm_stderr': 0.040191074725573483},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2392638036809816,\n",
       "    'acc_stderr': 0.03351953879521272,\n",
       "    'acc_norm': 0.2392638036809816,\n",
       "    'acc_norm_stderr': 0.03351953879521272},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.3482142857142857,\n",
       "    'acc_stderr': 0.045218299028335865,\n",
       "    'acc_norm': 0.3482142857142857,\n",
       "    'acc_norm_stderr': 0.045218299028335865},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.24271844660194175,\n",
       "    'acc_stderr': 0.04245022486384493,\n",
       "    'acc_norm': 0.24271844660194175,\n",
       "    'acc_norm_stderr': 0.04245022486384493},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.27350427350427353,\n",
       "    'acc_stderr': 0.029202540153431187,\n",
       "    'acc_norm': 0.27350427350427353,\n",
       "    'acc_norm_stderr': 0.029202540153431187},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.041633319989322695,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.041633319989322695},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.29246487867177523,\n",
       "    'acc_stderr': 0.016267000684598645,\n",
       "    'acc_norm': 0.29246487867177523,\n",
       "    'acc_norm_stderr': 0.016267000684598645},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.2630057803468208,\n",
       "    'acc_stderr': 0.023703099525258176,\n",
       "    'acc_norm': 0.2630057803468208,\n",
       "    'acc_norm_stderr': 0.023703099525258176},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961443,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961443},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.24509803921568626,\n",
       "    'acc_stderr': 0.02463004897982478,\n",
       "    'acc_norm': 0.24509803921568626,\n",
       "    'acc_norm_stderr': 0.02463004897982478},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.3054662379421222,\n",
       "    'acc_stderr': 0.026160584450140495,\n",
       "    'acc_norm': 0.3054662379421222,\n",
       "    'acc_norm_stderr': 0.026160584450140495},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.02409347123262133,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.02409347123262133},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2695035460992908,\n",
       "    'acc_stderr': 0.02646903681859062,\n",
       "    'acc_norm': 0.2695035460992908,\n",
       "    'acc_norm_stderr': 0.02646903681859062},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.2692307692307692,\n",
       "    'acc_stderr': 0.011328734403140308,\n",
       "    'acc_norm': 0.2692307692307692,\n",
       "    'acc_norm_stderr': 0.011328734403140308},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.18382352941176472,\n",
       "    'acc_stderr': 0.02352924218519311,\n",
       "    'acc_norm': 0.18382352941176472,\n",
       "    'acc_norm_stderr': 0.02352924218519311},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.01784808957491322,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.01784808957491322},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.2818181818181818,\n",
       "    'acc_stderr': 0.0430911870994646,\n",
       "    'acc_norm': 0.2818181818181818,\n",
       "    'acc_norm_stderr': 0.0430911870994646},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.17551020408163265,\n",
       "    'acc_stderr': 0.024352800722970015,\n",
       "    'acc_norm': 0.17551020408163265,\n",
       "    'acc_norm_stderr': 0.024352800722970015},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.2935323383084577,\n",
       "    'acc_stderr': 0.03220024104534205,\n",
       "    'acc_norm': 0.2935323383084577,\n",
       "    'acc_norm_stderr': 0.03220024104534205},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.044084400227680794,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.044084400227680794},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.26506024096385544,\n",
       "    'acc_stderr': 0.03436024037944967,\n",
       "    'acc_norm': 0.26506024096385544,\n",
       "    'acc_norm_stderr': 0.03436024037944967},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.03565079670708311,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.03565079670708311},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.200734394124847,\n",
       "    'mc1_stderr': 0.014022045717482156,\n",
       "    'mc2': 0.33827081326692277,\n",
       "    'mc2_stderr': 0.014862542606576355}}},\n",
       " 'dolphin-2.2.1-mistral-7b': {'key': 'dolphin-2.2.1-mistral-7b',\n",
       "  'Model': 'Dolphin-2.2.1-Mistral-7B',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '-',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'Cognitive Computations',\n",
       "  'Link': 'https://huggingface.co/ehartford/dolphin-2.2.1-mistral-7b',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6049488054607508,\n",
       "    'acc_stderr': 0.014285898292938167,\n",
       "    'acc_norm': 0.6331058020477816,\n",
       "    'acc_norm_stderr': 0.014084133118104301},\n",
       "   'harness|hellaswag|10': {'acc': 0.6431985660227046,\n",
       "    'acc_stderr': 0.004780764443411322,\n",
       "    'acc_norm': 0.8375821549492133,\n",
       "    'acc_norm_stderr': 0.0036807989505319113},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.04218506215368879,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.04218506215368879},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6776315789473685,\n",
       "    'acc_stderr': 0.03803510248351585,\n",
       "    'acc_norm': 0.6776315789473685,\n",
       "    'acc_norm_stderr': 0.03803510248351585},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.028727502957880267,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.028727502957880267},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03621034121889507,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03621034121889507},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.04878317312145633,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.04878317312145633},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6358381502890174,\n",
       "    'acc_stderr': 0.03669072477416907,\n",
       "    'acc_norm': 0.6358381502890174,\n",
       "    'acc_norm_stderr': 0.03669072477416907},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.35294117647058826,\n",
       "    'acc_stderr': 0.04755129616062947,\n",
       "    'acc_norm': 0.35294117647058826,\n",
       "    'acc_norm_stderr': 0.04755129616062947},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.04093601807403326,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.04093601807403326},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5446808510638298,\n",
       "    'acc_stderr': 0.03255525359340354,\n",
       "    'acc_norm': 0.5446808510638298,\n",
       "    'acc_norm_stderr': 0.03255525359340354},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.45614035087719296,\n",
       "    'acc_stderr': 0.04685473041907789,\n",
       "    'acc_norm': 0.45614035087719296,\n",
       "    'acc_norm_stderr': 0.04685473041907789},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.593103448275862,\n",
       "    'acc_stderr': 0.04093793981266236,\n",
       "    'acc_norm': 0.593103448275862,\n",
       "    'acc_norm_stderr': 0.04093793981266236},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3862433862433862,\n",
       "    'acc_stderr': 0.025075981767601684,\n",
       "    'acc_norm': 0.3862433862433862,\n",
       "    'acc_norm_stderr': 0.025075981767601684},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3968253968253968,\n",
       "    'acc_stderr': 0.04375888492727062,\n",
       "    'acc_norm': 0.3968253968253968,\n",
       "    'acc_norm_stderr': 0.04375888492727062},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7709677419354839,\n",
       "    'acc_stderr': 0.02390491431178265,\n",
       "    'acc_norm': 0.7709677419354839,\n",
       "    'acc_norm_stderr': 0.02390491431178265},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4876847290640394,\n",
       "    'acc_stderr': 0.035169204442208966,\n",
       "    'acc_norm': 0.4876847290640394,\n",
       "    'acc_norm_stderr': 0.035169204442208966},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.69,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.69,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7696969696969697,\n",
       "    'acc_stderr': 0.0328766675860349,\n",
       "    'acc_norm': 0.7696969696969697,\n",
       "    'acc_norm_stderr': 0.0328766675860349},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7929292929292929,\n",
       "    'acc_stderr': 0.028869778460267025,\n",
       "    'acc_norm': 0.7929292929292929,\n",
       "    'acc_norm_stderr': 0.028869778460267025},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8704663212435233,\n",
       "    'acc_stderr': 0.02423353229775873,\n",
       "    'acc_norm': 0.8704663212435233,\n",
       "    'acc_norm_stderr': 0.02423353229775873},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6512820512820513,\n",
       "    'acc_stderr': 0.02416278028401772,\n",
       "    'acc_norm': 0.6512820512820513,\n",
       "    'acc_norm_stderr': 0.02416278028401772},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028593,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.030388353551886783,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.030388353551886783},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.304635761589404,\n",
       "    'acc_stderr': 0.03757949922943343,\n",
       "    'acc_norm': 0.304635761589404,\n",
       "    'acc_norm_stderr': 0.03757949922943343},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8348623853211009,\n",
       "    'acc_stderr': 0.015919557829976044,\n",
       "    'acc_norm': 0.8348623853211009,\n",
       "    'acc_norm_stderr': 0.015919557829976044},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.034076320938540516,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.034076320938540516},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7696078431372549,\n",
       "    'acc_stderr': 0.02955429260569508,\n",
       "    'acc_norm': 0.7696078431372549,\n",
       "    'acc_norm_stderr': 0.02955429260569508},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7763713080168776,\n",
       "    'acc_stderr': 0.027123298205229966,\n",
       "    'acc_norm': 0.7763713080168776,\n",
       "    'acc_norm_stderr': 0.027123298205229966},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6771300448430493,\n",
       "    'acc_stderr': 0.03138147637575499,\n",
       "    'acc_norm': 0.6771300448430493,\n",
       "    'acc_norm_stderr': 0.03138147637575499},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7786259541984732,\n",
       "    'acc_stderr': 0.0364129708131373,\n",
       "    'acc_norm': 0.7786259541984732,\n",
       "    'acc_norm_stderr': 0.0364129708131373},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7933884297520661,\n",
       "    'acc_stderr': 0.03695980128098823,\n",
       "    'acc_norm': 0.7933884297520661,\n",
       "    'acc_norm_stderr': 0.03695980128098823},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8055555555555556,\n",
       "    'acc_stderr': 0.03826076324884866,\n",
       "    'acc_norm': 0.8055555555555556,\n",
       "    'acc_norm_stderr': 0.03826076324884866},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7607361963190185,\n",
       "    'acc_stderr': 0.033519538795212696,\n",
       "    'acc_norm': 0.7607361963190185,\n",
       "    'acc_norm_stderr': 0.033519538795212696},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7669902912621359,\n",
       "    'acc_stderr': 0.04185832598928315,\n",
       "    'acc_norm': 0.7669902912621359,\n",
       "    'acc_norm_stderr': 0.04185832598928315},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8632478632478633,\n",
       "    'acc_stderr': 0.022509033937077805,\n",
       "    'acc_norm': 0.8632478632478633,\n",
       "    'acc_norm_stderr': 0.022509033937077805},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768078,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768078},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8135376756066411,\n",
       "    'acc_stderr': 0.013927751372001506,\n",
       "    'acc_norm': 0.8135376756066411,\n",
       "    'acc_norm_stderr': 0.013927751372001506},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7138728323699421,\n",
       "    'acc_stderr': 0.02433214677913413,\n",
       "    'acc_norm': 0.7138728323699421,\n",
       "    'acc_norm_stderr': 0.02433214677913413},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.37988826815642457,\n",
       "    'acc_stderr': 0.016232826818678502,\n",
       "    'acc_norm': 0.37988826815642457,\n",
       "    'acc_norm_stderr': 0.016232826818678502},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7058823529411765,\n",
       "    'acc_stderr': 0.026090162504279053,\n",
       "    'acc_norm': 0.7058823529411765,\n",
       "    'acc_norm_stderr': 0.026090162504279053},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7202572347266881,\n",
       "    'acc_stderr': 0.025494259350694912,\n",
       "    'acc_norm': 0.7202572347266881,\n",
       "    'acc_norm_stderr': 0.025494259350694912},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7098765432098766,\n",
       "    'acc_stderr': 0.025251173936495026,\n",
       "    'acc_norm': 0.7098765432098766,\n",
       "    'acc_norm_stderr': 0.025251173936495026},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4787234042553192,\n",
       "    'acc_stderr': 0.029800481645628693,\n",
       "    'acc_norm': 0.4787234042553192,\n",
       "    'acc_norm_stderr': 0.029800481645628693},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4471968709256845,\n",
       "    'acc_stderr': 0.012698825252435111,\n",
       "    'acc_norm': 0.4471968709256845,\n",
       "    'acc_norm_stderr': 0.012698825252435111},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6470588235294118,\n",
       "    'acc_stderr': 0.029029422815681397,\n",
       "    'acc_norm': 0.6470588235294118,\n",
       "    'acc_norm_stderr': 0.029029422815681397},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6503267973856209,\n",
       "    'acc_stderr': 0.01929196189506638,\n",
       "    'acc_norm': 0.6503267973856209,\n",
       "    'acc_norm_stderr': 0.01929196189506638},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.726530612244898,\n",
       "    'acc_stderr': 0.02853556033712844,\n",
       "    'acc_norm': 0.726530612244898,\n",
       "    'acc_norm_stderr': 0.02853556033712844},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.835820895522388,\n",
       "    'acc_stderr': 0.026193923544454125,\n",
       "    'acc_norm': 0.835820895522388,\n",
       "    'acc_norm_stderr': 0.026193923544454125},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.03379976689896309,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.03379976689896309},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5481927710843374,\n",
       "    'acc_stderr': 0.03874371556587953,\n",
       "    'acc_norm': 0.5481927710843374,\n",
       "    'acc_norm_stderr': 0.03874371556587953},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8304093567251462,\n",
       "    'acc_stderr': 0.02878210810540171,\n",
       "    'acc_norm': 0.8304093567251462,\n",
       "    'acc_norm_stderr': 0.02878210810540171},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3659730722154223,\n",
       "    'mc1_stderr': 0.016862941684088365,\n",
       "    'mc2': 0.5311447373702662,\n",
       "    'mc2_stderr': 0.015062742496541512},\n",
       "   'harness|winogrande|5': {'acc': 0.7813733228097869,\n",
       "    'acc_stderr': 0.01161619821577323},\n",
       "   'harness|drop|3': {'em': 0.004299496644295302,\n",
       "    'em_stderr': 0.0006700586558630193,\n",
       "    'f1': 0.081867659395973,\n",
       "    'f1_stderr': 0.0016972961971096978},\n",
       "   'harness|gsm8k|5': {'acc': 0.4806671721000758,\n",
       "    'acc_stderr': 0.013762185709851354},\n",
       "   'all': {'acc': 0.6314567324183159,\n",
       "    'acc_stderr': 0.032318316802746,\n",
       "    'acc_norm': 0.6352434028495076,\n",
       "    'acc_norm_stderr': 0.032961647633460475,\n",
       "    'mc1': 0.3659730722154223,\n",
       "    'mc1_stderr': 0.016862941684088365,\n",
       "    'mc2': 0.5311447373702662,\n",
       "    'mc2_stderr': 0.015062742496541512}}},\n",
       " 'falcon-180b-chat': {'key': 'falcon-180b-chat',\n",
       "  'Model': 'falcon-180b-chat',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.680',\n",
       "  'License': 'Falcon-180B TII License',\n",
       "  'Organization': 'TII',\n",
       "  'Link': 'https://huggingface.co/tiiuae/falcon-180B-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6143344709897611,\n",
       "    'acc_stderr': 0.014224250973257182,\n",
       "    'acc_norm': 0.64419795221843,\n",
       "    'acc_norm_stderr': 0.01399057113791876},\n",
       "   'harness|hellaswag|10': {'acc': 0.6904999004182434,\n",
       "    'acc_stderr': 0.004613427745209517,\n",
       "    'acc_norm': 0.8804023102967536,\n",
       "    'acc_norm_stderr': 0.0032382732952847414},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.04725815626252606,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.04725815626252606},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6518518518518519,\n",
       "    'acc_stderr': 0.041153246103369526,\n",
       "    'acc_norm': 0.6518518518518519,\n",
       "    'acc_norm_stderr': 0.041153246103369526},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.743421052631579,\n",
       "    'acc_stderr': 0.0355418036802569,\n",
       "    'acc_norm': 0.743421052631579,\n",
       "    'acc_norm_stderr': 0.0355418036802569},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.7018867924528301,\n",
       "    'acc_stderr': 0.028152837942493857,\n",
       "    'acc_norm': 0.7018867924528301,\n",
       "    'acc_norm_stderr': 0.028152837942493857},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.8263888888888888,\n",
       "    'acc_stderr': 0.03167473383795718,\n",
       "    'acc_norm': 0.8263888888888888,\n",
       "    'acc_norm_stderr': 0.03167473383795718},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6473988439306358,\n",
       "    'acc_stderr': 0.036430371689585475,\n",
       "    'acc_norm': 0.6473988439306358,\n",
       "    'acc_norm_stderr': 0.036430371689585475},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3137254901960784,\n",
       "    'acc_stderr': 0.04617034827006717,\n",
       "    'acc_norm': 0.3137254901960784,\n",
       "    'acc_norm_stderr': 0.04617034827006717},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932263,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932263},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.6510638297872341,\n",
       "    'acc_stderr': 0.03115852213135778,\n",
       "    'acc_norm': 0.6510638297872341,\n",
       "    'acc_norm_stderr': 0.03115852213135778},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.4824561403508772,\n",
       "    'acc_stderr': 0.04700708033551038,\n",
       "    'acc_norm': 0.4824561403508772,\n",
       "    'acc_norm_stderr': 0.04700708033551038},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.6137931034482759,\n",
       "    'acc_stderr': 0.04057324734419036,\n",
       "    'acc_norm': 0.6137931034482759,\n",
       "    'acc_norm_stderr': 0.04057324734419036},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.455026455026455,\n",
       "    'acc_stderr': 0.02564692836104939,\n",
       "    'acc_norm': 0.455026455026455,\n",
       "    'acc_norm_stderr': 0.02564692836104939},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4365079365079365,\n",
       "    'acc_stderr': 0.04435932892851466,\n",
       "    'acc_norm': 0.4365079365079365,\n",
       "    'acc_norm_stderr': 0.04435932892851466},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7967741935483871,\n",
       "    'acc_stderr': 0.02289168798455495,\n",
       "    'acc_norm': 0.7967741935483871,\n",
       "    'acc_norm_stderr': 0.02289168798455495},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.541871921182266,\n",
       "    'acc_stderr': 0.03505630140785741,\n",
       "    'acc_norm': 0.541871921182266,\n",
       "    'acc_norm_stderr': 0.03505630140785741},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.03123475237772117,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.03123475237772117},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8484848484848485,\n",
       "    'acc_stderr': 0.025545650426603617,\n",
       "    'acc_norm': 0.8484848484848485,\n",
       "    'acc_norm_stderr': 0.025545650426603617},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9481865284974094,\n",
       "    'acc_stderr': 0.01599622932024412,\n",
       "    'acc_norm': 0.9481865284974094,\n",
       "    'acc_norm_stderr': 0.01599622932024412},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6820512820512821,\n",
       "    'acc_stderr': 0.02361088430892786,\n",
       "    'acc_norm': 0.6820512820512821,\n",
       "    'acc_norm_stderr': 0.02361088430892786},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028597,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028597},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7689075630252101,\n",
       "    'acc_stderr': 0.027381406927868886,\n",
       "    'acc_norm': 0.7689075630252101,\n",
       "    'acc_norm_stderr': 0.027381406927868886},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3973509933774834,\n",
       "    'acc_stderr': 0.039955240076816806,\n",
       "    'acc_norm': 0.3973509933774834,\n",
       "    'acc_norm_stderr': 0.039955240076816806},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8715596330275229,\n",
       "    'acc_stderr': 0.014344977542914318,\n",
       "    'acc_norm': 0.8715596330275229,\n",
       "    'acc_norm_stderr': 0.014344977542914318},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5416666666666666,\n",
       "    'acc_stderr': 0.033981108902946366,\n",
       "    'acc_norm': 0.5416666666666666,\n",
       "    'acc_norm_stderr': 0.033981108902946366},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8578431372549019,\n",
       "    'acc_stderr': 0.02450980392156862,\n",
       "    'acc_norm': 0.8578431372549019,\n",
       "    'acc_norm_stderr': 0.02450980392156862},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8607594936708861,\n",
       "    'acc_stderr': 0.0225355263526927,\n",
       "    'acc_norm': 0.8607594936708861,\n",
       "    'acc_norm_stderr': 0.0225355263526927},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7668161434977578,\n",
       "    'acc_stderr': 0.028380391147094702,\n",
       "    'acc_norm': 0.7668161434977578,\n",
       "    'acc_norm_stderr': 0.028380391147094702},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8396946564885496,\n",
       "    'acc_stderr': 0.03217829420744631,\n",
       "    'acc_norm': 0.8396946564885496,\n",
       "    'acc_norm_stderr': 0.03217829420744631},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03520893951097653,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03520893951097653},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8611111111111112,\n",
       "    'acc_stderr': 0.0334327006286962,\n",
       "    'acc_norm': 0.8611111111111112,\n",
       "    'acc_norm_stderr': 0.0334327006286962},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.8343558282208589,\n",
       "    'acc_stderr': 0.029208296231259104,\n",
       "    'acc_norm': 0.8343558282208589,\n",
       "    'acc_norm_stderr': 0.029208296231259104},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.5446428571428571,\n",
       "    'acc_stderr': 0.04726835553719098,\n",
       "    'acc_norm': 0.5446428571428571,\n",
       "    'acc_norm_stderr': 0.04726835553719098},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8446601941747572,\n",
       "    'acc_stderr': 0.03586594738573975,\n",
       "    'acc_norm': 0.8446601941747572,\n",
       "    'acc_norm_stderr': 0.03586594738573975},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8931623931623932,\n",
       "    'acc_stderr': 0.02023714900899093,\n",
       "    'acc_norm': 0.8931623931623932,\n",
       "    'acc_norm_stderr': 0.02023714900899093},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932261,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932261},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8671775223499362,\n",
       "    'acc_stderr': 0.012136303209884562,\n",
       "    'acc_norm': 0.8671775223499362,\n",
       "    'acc_norm_stderr': 0.012136303209884562},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7745664739884393,\n",
       "    'acc_stderr': 0.022497230190967558,\n",
       "    'acc_norm': 0.7745664739884393,\n",
       "    'acc_norm_stderr': 0.022497230190967558},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.4659217877094972,\n",
       "    'acc_stderr': 0.016683615837486863,\n",
       "    'acc_norm': 0.4659217877094972,\n",
       "    'acc_norm_stderr': 0.016683615837486863},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7287581699346405,\n",
       "    'acc_stderr': 0.02545775669666788,\n",
       "    'acc_norm': 0.7287581699346405,\n",
       "    'acc_norm_stderr': 0.02545775669666788},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7877813504823151,\n",
       "    'acc_stderr': 0.02322275679743512,\n",
       "    'acc_norm': 0.7877813504823151,\n",
       "    'acc_norm_stderr': 0.02322275679743512},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7870370370370371,\n",
       "    'acc_stderr': 0.022779719088733403,\n",
       "    'acc_norm': 0.7870370370370371,\n",
       "    'acc_norm_stderr': 0.022779719088733403},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5460992907801419,\n",
       "    'acc_stderr': 0.02970045324729146,\n",
       "    'acc_norm': 0.5460992907801419,\n",
       "    'acc_norm_stderr': 0.02970045324729146},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5345501955671447,\n",
       "    'acc_stderr': 0.012739711554045713,\n",
       "    'acc_norm': 0.5345501955671447,\n",
       "    'acc_norm_stderr': 0.012739711554045713},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6875,\n",
       "    'acc_stderr': 0.02815637344037142,\n",
       "    'acc_norm': 0.6875,\n",
       "    'acc_norm_stderr': 0.02815637344037142},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.7271241830065359,\n",
       "    'acc_stderr': 0.018020474148393577,\n",
       "    'acc_norm': 0.7271241830065359,\n",
       "    'acc_norm_stderr': 0.018020474148393577},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.763265306122449,\n",
       "    'acc_stderr': 0.027212835884073156,\n",
       "    'acc_norm': 0.763265306122449,\n",
       "    'acc_norm_stderr': 0.027212835884073156},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8258706467661692,\n",
       "    'acc_stderr': 0.026814951200421603,\n",
       "    'acc_norm': 0.8258706467661692,\n",
       "    'acc_norm_stderr': 0.026814951200421603},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.89,\n",
       "    'acc_stderr': 0.03144660377352203,\n",
       "    'acc_norm': 0.89,\n",
       "    'acc_norm_stderr': 0.03144660377352203},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5240963855421686,\n",
       "    'acc_stderr': 0.03887971849597264,\n",
       "    'acc_norm': 0.5240963855421686,\n",
       "    'acc_norm_stderr': 0.03887971849597264},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8421052631578947,\n",
       "    'acc_stderr': 0.02796678585916089,\n",
       "    'acc_norm': 0.8421052631578947,\n",
       "    'acc_norm_stderr': 0.02796678585916089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.37576499388004897,\n",
       "    'mc1_stderr': 0.016954584060214297,\n",
       "    'mc2': 0.5369457047083463,\n",
       "    'mc2_stderr': 0.014662579093423517},\n",
       "   'all': {'acc': 0.6793461177088693,\n",
       "    'acc_stderr': 0.031533098229447415,\n",
       "    'acc_norm': 0.6830709633208555,\n",
       "    'acc_norm_stderr': 0.03150582985173109,\n",
       "    'mc1': 0.37576499388004897,\n",
       "    'mc1_stderr': 0.016954584060214297,\n",
       "    'mc2': 0.5369457047083463,\n",
       "    'mc2_stderr': 0.014662579093423517}}},\n",
       " 'guanaco-33b': {'key': 'guanaco-33b',\n",
       "  'Model': 'Guanaco-33B',\n",
       "  'MT-bench (score)': '6.53',\n",
       "  'MMLU': '0.576',\n",
       "  'License': 'Non-commercial',\n",
       "  'Organization': 'UW',\n",
       "  'Link': 'https://huggingface.co/timdettmers/guanaco-33b-merged',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5870307167235495,\n",
       "    'acc_stderr': 0.014388344935398326,\n",
       "    'acc_norm': 0.6245733788395904,\n",
       "    'acc_norm_stderr': 0.014150631435111726},\n",
       "   'harness|hellaswag|10': {'acc': 0.6446922923720374,\n",
       "    'acc_stderr': 0.004776283203468098,\n",
       "    'acc_norm': 0.8447520414260108,\n",
       "    'acc_norm_stderr': 0.003614007841341989},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.047609522856952365,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.047609522856952365},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5657894736842105,\n",
       "    'acc_stderr': 0.04033565667848319,\n",
       "    'acc_norm': 0.5657894736842105,\n",
       "    'acc_norm_stderr': 0.04033565667848319},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620332,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620332},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5622641509433962,\n",
       "    'acc_stderr': 0.030533338430467516,\n",
       "    'acc_norm': 0.5622641509433962,\n",
       "    'acc_norm_stderr': 0.030533338430467516},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5625,\n",
       "    'acc_stderr': 0.04148415739394154,\n",
       "    'acc_norm': 0.5625,\n",
       "    'acc_norm_stderr': 0.04148415739394154},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3431372549019608,\n",
       "    'acc_stderr': 0.04724007352383889,\n",
       "    'acc_norm': 0.3431372549019608,\n",
       "    'acc_norm_stderr': 0.04724007352383889},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4595744680851064,\n",
       "    'acc_stderr': 0.03257901482099835,\n",
       "    'acc_norm': 0.4595744680851064,\n",
       "    'acc_norm_stderr': 0.03257901482099835},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3684210526315789,\n",
       "    'acc_stderr': 0.04537815354939391,\n",
       "    'acc_norm': 0.3684210526315789,\n",
       "    'acc_norm_stderr': 0.04537815354939391},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4413793103448276,\n",
       "    'acc_stderr': 0.04137931034482758,\n",
       "    'acc_norm': 0.4413793103448276,\n",
       "    'acc_norm_stderr': 0.04137931034482758},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.31216931216931215,\n",
       "    'acc_stderr': 0.0238652068369726,\n",
       "    'acc_norm': 0.31216931216931215,\n",
       "    'acc_norm_stderr': 0.0238652068369726},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.29365079365079366,\n",
       "    'acc_stderr': 0.04073524322147126,\n",
       "    'acc_norm': 0.29365079365079366,\n",
       "    'acc_norm_stderr': 0.04073524322147126},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6290322580645161,\n",
       "    'acc_stderr': 0.027480541887953593,\n",
       "    'acc_norm': 0.6290322580645161,\n",
       "    'acc_norm_stderr': 0.027480541887953593},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3793103448275862,\n",
       "    'acc_stderr': 0.03413963805906235,\n",
       "    'acc_norm': 0.3793103448275862,\n",
       "    'acc_norm_stderr': 0.03413963805906235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7212121212121212,\n",
       "    'acc_stderr': 0.035014387062967806,\n",
       "    'acc_norm': 0.7212121212121212,\n",
       "    'acc_norm_stderr': 0.035014387062967806},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.03191178226713547,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.03191178226713547},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7150259067357513,\n",
       "    'acc_stderr': 0.032577140777096614,\n",
       "    'acc_norm': 0.7150259067357513,\n",
       "    'acc_norm_stderr': 0.032577140777096614},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.49230769230769234,\n",
       "    'acc_stderr': 0.025348006031534778,\n",
       "    'acc_norm': 0.49230769230769234,\n",
       "    'acc_norm_stderr': 0.025348006031534778},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2777777777777778,\n",
       "    'acc_stderr': 0.02730914058823019,\n",
       "    'acc_norm': 0.2777777777777778,\n",
       "    'acc_norm_stderr': 0.02730914058823019},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03242225027115006,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03242225027115006},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.03822746937658752,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.03822746937658752},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7302752293577982,\n",
       "    'acc_stderr': 0.019028486711115438,\n",
       "    'acc_norm': 0.7302752293577982,\n",
       "    'acc_norm_stderr': 0.019028486711115438},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.375,\n",
       "    'acc_stderr': 0.033016908987210894,\n",
       "    'acc_norm': 0.375,\n",
       "    'acc_norm_stderr': 0.033016908987210894},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.02910225438967409,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.02910225438967409},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7679324894514767,\n",
       "    'acc_stderr': 0.02747974455080851,\n",
       "    'acc_norm': 0.7679324894514767,\n",
       "    'acc_norm_stderr': 0.02747974455080851},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5874439461883408,\n",
       "    'acc_stderr': 0.03304062175449297,\n",
       "    'acc_norm': 0.5874439461883408,\n",
       "    'acc_norm_stderr': 0.03304062175449297},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6412213740458015,\n",
       "    'acc_stderr': 0.04206739313864908,\n",
       "    'acc_norm': 0.6412213740458015,\n",
       "    'acc_norm_stderr': 0.04206739313864908},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7272727272727273,\n",
       "    'acc_stderr': 0.04065578140908705,\n",
       "    'acc_norm': 0.7272727272727273,\n",
       "    'acc_norm_stderr': 0.04065578140908705},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6111111111111112,\n",
       "    'acc_stderr': 0.04712821257426769,\n",
       "    'acc_norm': 0.6111111111111112,\n",
       "    'acc_norm_stderr': 0.04712821257426769},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6380368098159509,\n",
       "    'acc_stderr': 0.037757007291414416,\n",
       "    'acc_norm': 0.6380368098159509,\n",
       "    'acc_norm_stderr': 0.037757007291414416},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.38392857142857145,\n",
       "    'acc_stderr': 0.04616143075028546,\n",
       "    'acc_norm': 0.38392857142857145,\n",
       "    'acc_norm_stderr': 0.04616143075028546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6310679611650486,\n",
       "    'acc_stderr': 0.0477761518115674,\n",
       "    'acc_norm': 0.6310679611650486,\n",
       "    'acc_norm_stderr': 0.0477761518115674},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7863247863247863,\n",
       "    'acc_stderr': 0.026853450377009154,\n",
       "    'acc_norm': 0.7863247863247863,\n",
       "    'acc_norm_stderr': 0.026853450377009154},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6756066411238825,\n",
       "    'acc_stderr': 0.0167409290471627,\n",
       "    'acc_norm': 0.6756066411238825,\n",
       "    'acc_norm_stderr': 0.0167409290471627},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5491329479768786,\n",
       "    'acc_stderr': 0.026788811931562757,\n",
       "    'acc_norm': 0.5491329479768786,\n",
       "    'acc_norm_stderr': 0.026788811931562757},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2569832402234637,\n",
       "    'acc_stderr': 0.01461446582196632,\n",
       "    'acc_norm': 0.2569832402234637,\n",
       "    'acc_norm_stderr': 0.01461446582196632},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5718954248366013,\n",
       "    'acc_stderr': 0.028332397483664278,\n",
       "    'acc_norm': 0.5718954248366013,\n",
       "    'acc_norm_stderr': 0.028332397483664278},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6012861736334405,\n",
       "    'acc_stderr': 0.027809322585774496,\n",
       "    'acc_norm': 0.6012861736334405,\n",
       "    'acc_norm_stderr': 0.027809322585774496},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6172839506172839,\n",
       "    'acc_stderr': 0.027044538138402595,\n",
       "    'acc_norm': 0.6172839506172839,\n",
       "    'acc_norm_stderr': 0.027044538138402595},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.42907801418439717,\n",
       "    'acc_stderr': 0.029525914302558555,\n",
       "    'acc_norm': 0.42907801418439717,\n",
       "    'acc_norm_stderr': 0.029525914302558555},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.424380704041721,\n",
       "    'acc_stderr': 0.01262334375743002,\n",
       "    'acc_norm': 0.424380704041721,\n",
       "    'acc_norm_stderr': 0.01262334375743002},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5808823529411765,\n",
       "    'acc_stderr': 0.02997280717046462,\n",
       "    'acc_norm': 0.5808823529411765,\n",
       "    'acc_norm_stderr': 0.02997280717046462},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5212418300653595,\n",
       "    'acc_stderr': 0.020209572388600248,\n",
       "    'acc_norm': 0.5212418300653595,\n",
       "    'acc_norm_stderr': 0.020209572388600248},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.04607582090719976,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.04607582090719976},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5918367346938775,\n",
       "    'acc_stderr': 0.03146465712827424,\n",
       "    'acc_norm': 0.5918367346938775,\n",
       "    'acc_norm_stderr': 0.03146465712827424},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6915422885572139,\n",
       "    'acc_stderr': 0.032658195885126966,\n",
       "    'acc_norm': 0.6915422885572139,\n",
       "    'acc_norm_stderr': 0.032658195885126966},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.041633319989322626,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.041633319989322626},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4578313253012048,\n",
       "    'acc_stderr': 0.038786267710023595,\n",
       "    'acc_norm': 0.4578313253012048,\n",
       "    'acc_norm_stderr': 0.038786267710023595},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.34761321909424725,\n",
       "    'mc1_stderr': 0.016670769188897306,\n",
       "    'mc2': 0.5121992740888713,\n",
       "    'mc2_stderr': 0.014650490351006002},\n",
       "   'all': {'acc': 0.5404553018205109,\n",
       "    'acc_stderr': 0.03488622237927161,\n",
       "    'acc_norm': 0.5444824613318672,\n",
       "    'acc_norm_stderr': 0.03486249375448495,\n",
       "    'mc1': 0.34761321909424725,\n",
       "    'mc1_stderr': 0.016670769188897306,\n",
       "    'mc2': 0.5121992740888713,\n",
       "    'mc2_stderr': 0.014650490351006002}}},\n",
       " 'llama-2-13b-chat': {'key': 'llama-2-13b-chat',\n",
       "  'Model': 'Llama-2-13b-chat',\n",
       "  'MT-bench (score)': '6.65',\n",
       "  'MMLU': '0.536',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-13b-chat-hf',\n",
       "  'results': {'harness|drop|3': {'em': 0.1782718120805369,\n",
       "    'em_stderr': 0.003919630092588375,\n",
       "    'f1': 0.2387195889261742,\n",
       "    'f1_stderr': 0.003944947017182046},\n",
       "   'harness|gsm8k|5': {'acc': 0.15238817285822592,\n",
       "    'acc_stderr': 0.009899572254794204},\n",
       "   'harness|winogrande|5': {'acc': 0.745067087608524,\n",
       "    'acc_stderr': 0.012248806969376422},\n",
       "   'all': {'acc': 0.5479380524707899,\n",
       "    'acc_stderr': 0.03451142729909022,\n",
       "    'acc_norm': 0.5517368945804153,\n",
       "    'acc_norm_stderr': 0.03449229816957583,\n",
       "    'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.4411794590119937,\n",
       "    'mc2_stderr': 0.015755921757439843},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5563139931740614,\n",
       "    'acc_stderr': 0.014518421825670449,\n",
       "    'acc_norm': 0.590443686006826,\n",
       "    'acc_norm_stderr': 0.01437035863247244},\n",
       "   'harness|hellaswag|10': {'acc': 0.6293567018522207,\n",
       "    'acc_stderr': 0.004819899945342489,\n",
       "    'acc_norm': 0.8193586934873531,\n",
       "    'acc_norm_stderr': 0.0038393444971919545},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.046482319871173156,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.046482319871173156},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4740740740740741,\n",
       "    'acc_stderr': 0.04313531696750574,\n",
       "    'acc_norm': 0.4740740740740741,\n",
       "    'acc_norm_stderr': 0.04313531696750574},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5460526315789473,\n",
       "    'acc_stderr': 0.04051646342874142,\n",
       "    'acc_norm': 0.5460526315789473,\n",
       "    'acc_norm_stderr': 0.04051646342874142},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5849056603773585,\n",
       "    'acc_stderr': 0.03032594578928611,\n",
       "    'acc_norm': 0.5849056603773585,\n",
       "    'acc_norm_stderr': 0.03032594578928611},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04122728707651282,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04122728707651282},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4624277456647399,\n",
       "    'acc_stderr': 0.0380168510452446,\n",
       "    'acc_norm': 0.4624277456647399,\n",
       "    'acc_norm_stderr': 0.0380168510452446},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3137254901960784,\n",
       "    'acc_stderr': 0.04617034827006717,\n",
       "    'acc_norm': 0.3137254901960784,\n",
       "    'acc_norm_stderr': 0.04617034827006717},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.03202563076101735,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.03202563076101735},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.043727482902780064,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.043727482902780064},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.503448275862069,\n",
       "    'acc_stderr': 0.0416656757710158,\n",
       "    'acc_norm': 0.503448275862069,\n",
       "    'acc_norm_stderr': 0.0416656757710158},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.024419234966819064,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.024419234966819064},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.30952380952380953,\n",
       "    'acc_stderr': 0.04134913018303316,\n",
       "    'acc_norm': 0.30952380952380953,\n",
       "    'acc_norm_stderr': 0.04134913018303316},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6419354838709678,\n",
       "    'acc_stderr': 0.02727389059430064,\n",
       "    'acc_norm': 0.6419354838709678,\n",
       "    'acc_norm_stderr': 0.02727389059430064},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4630541871921182,\n",
       "    'acc_stderr': 0.035083705204426656,\n",
       "    'acc_norm': 0.4630541871921182,\n",
       "    'acc_norm_stderr': 0.035083705204426656},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.03663974994391244,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.03663974994391244},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.702020202020202,\n",
       "    'acc_stderr': 0.03258630383836556,\n",
       "    'acc_norm': 0.702020202020202,\n",
       "    'acc_norm_stderr': 0.03258630383836556},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7875647668393783,\n",
       "    'acc_stderr': 0.029519282616817234,\n",
       "    'acc_norm': 0.7875647668393783,\n",
       "    'acc_norm_stderr': 0.029519282616817234},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.49230769230769234,\n",
       "    'acc_stderr': 0.025348006031534788,\n",
       "    'acc_norm': 0.49230769230769234,\n",
       "    'acc_norm_stderr': 0.025348006031534788},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3111111111111111,\n",
       "    'acc_stderr': 0.028226446749683522,\n",
       "    'acc_norm': 0.3111111111111111,\n",
       "    'acc_norm_stderr': 0.028226446749683522},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03242225027115007,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03242225027115007},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.33774834437086093,\n",
       "    'acc_stderr': 0.038615575462551684,\n",
       "    'acc_norm': 0.33774834437086093,\n",
       "    'acc_norm_stderr': 0.038615575462551684},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7321100917431193,\n",
       "    'acc_stderr': 0.018987462257978652,\n",
       "    'acc_norm': 0.7321100917431193,\n",
       "    'acc_norm_stderr': 0.018987462257978652},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.03324708911809117,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.03324708911809117},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03039153369274154,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03039153369274154},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7172995780590717,\n",
       "    'acc_stderr': 0.02931281415395592,\n",
       "    'acc_norm': 0.7172995780590717,\n",
       "    'acc_norm_stderr': 0.02931281415395592},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6457399103139013,\n",
       "    'acc_stderr': 0.032100621541349864,\n",
       "    'acc_norm': 0.6457399103139013,\n",
       "    'acc_norm_stderr': 0.032100621541349864},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6335877862595419,\n",
       "    'acc_stderr': 0.04225875451969637,\n",
       "    'acc_norm': 0.6335877862595419,\n",
       "    'acc_norm_stderr': 0.04225875451969637},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.768595041322314,\n",
       "    'acc_stderr': 0.03849856098794089,\n",
       "    'acc_norm': 0.768595041322314,\n",
       "    'acc_norm_stderr': 0.03849856098794089},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6944444444444444,\n",
       "    'acc_stderr': 0.044531975073749834,\n",
       "    'acc_norm': 0.6944444444444444,\n",
       "    'acc_norm_stderr': 0.044531975073749834},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6503067484662577,\n",
       "    'acc_stderr': 0.037466683254700206,\n",
       "    'acc_norm': 0.6503067484662577,\n",
       "    'acc_norm_stderr': 0.037466683254700206},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.35714285714285715,\n",
       "    'acc_stderr': 0.04547960999764376,\n",
       "    'acc_norm': 0.35714285714285715,\n",
       "    'acc_norm_stderr': 0.04547960999764376},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.04354631077260595,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.04354631077260595},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7863247863247863,\n",
       "    'acc_stderr': 0.026853450377009175,\n",
       "    'acc_norm': 0.7863247863247863,\n",
       "    'acc_norm_stderr': 0.026853450377009175},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7471264367816092,\n",
       "    'acc_stderr': 0.015543377313719681,\n",
       "    'acc_norm': 0.7471264367816092,\n",
       "    'acc_norm_stderr': 0.015543377313719681},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6127167630057804,\n",
       "    'acc_stderr': 0.026226158605124655,\n",
       "    'acc_norm': 0.6127167630057804,\n",
       "    'acc_norm_stderr': 0.026226158605124655},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30502793296089387,\n",
       "    'acc_stderr': 0.015398723510916716,\n",
       "    'acc_norm': 0.30502793296089387,\n",
       "    'acc_norm_stderr': 0.015398723510916716},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5947712418300654,\n",
       "    'acc_stderr': 0.028110928492809068,\n",
       "    'acc_norm': 0.5947712418300654,\n",
       "    'acc_norm_stderr': 0.028110928492809068},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5884244372990354,\n",
       "    'acc_stderr': 0.02795048149440127,\n",
       "    'acc_norm': 0.5884244372990354,\n",
       "    'acc_norm_stderr': 0.02795048149440127},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6111111111111112,\n",
       "    'acc_stderr': 0.02712511551316687,\n",
       "    'acc_norm': 0.6111111111111112,\n",
       "    'acc_norm_stderr': 0.02712511551316687},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.38652482269503546,\n",
       "    'acc_stderr': 0.029049190342543454,\n",
       "    'acc_norm': 0.38652482269503546,\n",
       "    'acc_norm_stderr': 0.029049190342543454},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.39113428943937417,\n",
       "    'acc_stderr': 0.012463861839982058,\n",
       "    'acc_norm': 0.39113428943937417,\n",
       "    'acc_norm_stderr': 0.012463861839982058},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.030372836961539352,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.030372836961539352},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5424836601307189,\n",
       "    'acc_stderr': 0.020154685712590888,\n",
       "    'acc_norm': 0.5424836601307189,\n",
       "    'acc_norm_stderr': 0.020154685712590888},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302505,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302505},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6408163265306123,\n",
       "    'acc_stderr': 0.030713560455108493,\n",
       "    'acc_norm': 0.6408163265306123,\n",
       "    'acc_norm_stderr': 0.030713560455108493},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7512437810945274,\n",
       "    'acc_stderr': 0.030567675938916714,\n",
       "    'acc_norm': 0.7512437810945274,\n",
       "    'acc_norm_stderr': 0.030567675938916714},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4819277108433735,\n",
       "    'acc_stderr': 0.038899512528272166,\n",
       "    'acc_norm': 0.4819277108433735,\n",
       "    'acc_norm_stderr': 0.038899512528272166},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7309941520467836,\n",
       "    'acc_stderr': 0.03401052620104089,\n",
       "    'acc_norm': 0.7309941520467836,\n",
       "    'acc_norm_stderr': 0.03401052620104089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.4411794590119937,\n",
       "    'mc2_stderr': 0.015755921757439843}}},\n",
       " 'llama-2-70b-chat': {'key': 'llama-2-70b-chat',\n",
       "  'Model': 'Llama-2-70b-chat',\n",
       "  'MT-bench (score)': '6.86',\n",
       "  'MMLU': '0.630',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-70b-chat-hf',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6049488054607508,\n",
       "    'acc_stderr': 0.01428589829293817,\n",
       "    'acc_norm': 0.6459044368600683,\n",
       "    'acc_norm_stderr': 0.013975454122756564},\n",
       "   'harness|hellaswag|10': {'acc': 0.6693885680143398,\n",
       "    'acc_stderr': 0.004694718918225751,\n",
       "    'acc_norm': 0.8587930691097391,\n",
       "    'acc_norm_stderr': 0.003475231889452833},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3561811505507956,\n",
       "    'mc1_stderr': 0.016763790728446335,\n",
       "    'mc2': 0.5280473232260097,\n",
       "    'mc2_stderr': 0.01553022126123046},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.04793724854411021,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.04793724854411021},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5185185185185185,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.5185185185185185,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.7302631578947368,\n",
       "    'acc_stderr': 0.03611780560284898,\n",
       "    'acc_norm': 0.7302631578947368,\n",
       "    'acc_norm_stderr': 0.03611780560284898},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6377358490566037,\n",
       "    'acc_stderr': 0.029582245128384303,\n",
       "    'acc_norm': 0.6377358490566037,\n",
       "    'acc_norm_stderr': 0.029582245128384303},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03621034121889507,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03621034121889507},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237101,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237101},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6011560693641619,\n",
       "    'acc_stderr': 0.0373362665538351,\n",
       "    'acc_norm': 0.6011560693641619,\n",
       "    'acc_norm_stderr': 0.0373362665538351},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.04690650298201943,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.04690650298201943},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5829787234042553,\n",
       "    'acc_stderr': 0.032232762667117124,\n",
       "    'acc_norm': 0.5829787234042553,\n",
       "    'acc_norm_stderr': 0.032232762667117124},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.41228070175438597,\n",
       "    'acc_stderr': 0.04630653203366595,\n",
       "    'acc_norm': 0.41228070175438597,\n",
       "    'acc_norm_stderr': 0.04630653203366595},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5793103448275863,\n",
       "    'acc_stderr': 0.0411391498118926,\n",
       "    'acc_norm': 0.5793103448275863,\n",
       "    'acc_norm_stderr': 0.0411391498118926},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.41005291005291006,\n",
       "    'acc_stderr': 0.02533120243894442,\n",
       "    'acc_norm': 0.41005291005291006,\n",
       "    'acc_norm_stderr': 0.02533120243894442},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4126984126984127,\n",
       "    'acc_stderr': 0.04403438954768176,\n",
       "    'acc_norm': 0.4126984126984127,\n",
       "    'acc_norm_stderr': 0.04403438954768176},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7645161290322581,\n",
       "    'acc_stderr': 0.02413763242933771,\n",
       "    'acc_norm': 0.7645161290322581,\n",
       "    'acc_norm_stderr': 0.02413763242933771},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4630541871921182,\n",
       "    'acc_stderr': 0.035083705204426656,\n",
       "    'acc_norm': 0.4630541871921182,\n",
       "    'acc_norm_stderr': 0.035083705204426656},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03011768892950359,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03011768892950359},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.02805779167298902,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.02805779167298902},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8911917098445595,\n",
       "    'acc_stderr': 0.022473253332768783,\n",
       "    'acc_norm': 0.8911917098445595,\n",
       "    'acc_norm_stderr': 0.022473253332768783},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6410256410256411,\n",
       "    'acc_stderr': 0.02432173848460235,\n",
       "    'acc_norm': 0.6410256410256411,\n",
       "    'acc_norm_stderr': 0.02432173848460235},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.027940457136228416,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.027940457136228416},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6596638655462185,\n",
       "    'acc_stderr': 0.030778057422931673,\n",
       "    'acc_norm': 0.6596638655462185,\n",
       "    'acc_norm_stderr': 0.030778057422931673},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.423841059602649,\n",
       "    'acc_stderr': 0.04034846678603397,\n",
       "    'acc_norm': 0.423841059602649,\n",
       "    'acc_norm_stderr': 0.04034846678603397},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8385321100917431,\n",
       "    'acc_stderr': 0.015776239256163255,\n",
       "    'acc_norm': 0.8385321100917431,\n",
       "    'acc_norm_stderr': 0.015776239256163255},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.03407632093854052,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.03407632093854052},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8578431372549019,\n",
       "    'acc_stderr': 0.024509803921568606,\n",
       "    'acc_norm': 0.8578431372549019,\n",
       "    'acc_norm_stderr': 0.024509803921568606},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8438818565400844,\n",
       "    'acc_stderr': 0.02362715946031867,\n",
       "    'acc_norm': 0.8438818565400844,\n",
       "    'acc_norm_stderr': 0.02362715946031867},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.726457399103139,\n",
       "    'acc_stderr': 0.02991858670779883,\n",
       "    'acc_norm': 0.726457399103139,\n",
       "    'acc_norm_stderr': 0.02991858670779883},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7099236641221374,\n",
       "    'acc_stderr': 0.039800662464677665,\n",
       "    'acc_norm': 0.7099236641221374,\n",
       "    'acc_norm_stderr': 0.039800662464677665},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8016528925619835,\n",
       "    'acc_stderr': 0.03640118271990946,\n",
       "    'acc_norm': 0.8016528925619835,\n",
       "    'acc_norm_stderr': 0.03640118271990946},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8240740740740741,\n",
       "    'acc_stderr': 0.036809181416738807,\n",
       "    'acc_norm': 0.8240740740740741,\n",
       "    'acc_norm_stderr': 0.036809181416738807},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7607361963190185,\n",
       "    'acc_stderr': 0.033519538795212696,\n",
       "    'acc_norm': 0.7607361963190185,\n",
       "    'acc_norm_stderr': 0.033519538795212696},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8058252427184466,\n",
       "    'acc_stderr': 0.03916667762822584,\n",
       "    'acc_norm': 0.8058252427184466,\n",
       "    'acc_norm_stderr': 0.03916667762822584},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8717948717948718,\n",
       "    'acc_stderr': 0.02190190511507332,\n",
       "    'acc_norm': 0.8717948717948718,\n",
       "    'acc_norm_stderr': 0.02190190511507332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8275862068965517,\n",
       "    'acc_stderr': 0.013507943909371798,\n",
       "    'acc_norm': 0.8275862068965517,\n",
       "    'acc_norm_stderr': 0.013507943909371798},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7167630057803468,\n",
       "    'acc_stderr': 0.02425790170532338,\n",
       "    'acc_norm': 0.7167630057803468,\n",
       "    'acc_norm_stderr': 0.02425790170532338},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.39553072625698327,\n",
       "    'acc_stderr': 0.01635341541007577,\n",
       "    'acc_norm': 0.39553072625698327,\n",
       "    'acc_norm_stderr': 0.01635341541007577},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6993464052287581,\n",
       "    'acc_stderr': 0.026256053835718968,\n",
       "    'acc_norm': 0.6993464052287581,\n",
       "    'acc_norm_stderr': 0.026256053835718968},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7041800643086816,\n",
       "    'acc_stderr': 0.02592237178881877,\n",
       "    'acc_norm': 0.7041800643086816,\n",
       "    'acc_norm_stderr': 0.02592237178881877},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7098765432098766,\n",
       "    'acc_stderr': 0.025251173936495036,\n",
       "    'acc_norm': 0.7098765432098766,\n",
       "    'acc_norm_stderr': 0.025251173936495036},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5070921985815603,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.5070921985815603,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4771838331160365,\n",
       "    'acc_stderr': 0.012756933382823694,\n",
       "    'acc_norm': 0.4771838331160365,\n",
       "    'acc_norm_stderr': 0.012756933382823694},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5772058823529411,\n",
       "    'acc_stderr': 0.030008562845003476,\n",
       "    'acc_norm': 0.5772058823529411,\n",
       "    'acc_norm_stderr': 0.030008562845003476},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6699346405228758,\n",
       "    'acc_stderr': 0.019023726160724556,\n",
       "    'acc_norm': 0.6699346405228758,\n",
       "    'acc_norm_stderr': 0.019023726160724556},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7877551020408163,\n",
       "    'acc_stderr': 0.026176967197866767,\n",
       "    'acc_norm': 0.7877551020408163,\n",
       "    'acc_norm_stderr': 0.026176967197866767},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8706467661691543,\n",
       "    'acc_stderr': 0.023729830881018526,\n",
       "    'acc_norm': 0.8706467661691543,\n",
       "    'acc_norm_stderr': 0.023729830881018526},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.03379976689896309,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.03379976689896309},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5120481927710844,\n",
       "    'acc_stderr': 0.03891364495835817,\n",
       "    'acc_norm': 0.5120481927710844,\n",
       "    'acc_norm_stderr': 0.03891364495835817},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8187134502923976,\n",
       "    'acc_stderr': 0.029547741687640038,\n",
       "    'acc_norm': 0.8187134502923976,\n",
       "    'acc_norm_stderr': 0.029547741687640038},\n",
       "   'all': {'em': 0.040373322147651006,\n",
       "    'em_stderr': 0.0020157564185176837,\n",
       "    'f1': 0.1050272651006715,\n",
       "    'f1_stderr': 0.0023756238577676155,\n",
       "    'acc': 0.5359600711595986,\n",
       "    'acc_stderr': 0.011658939983913113},\n",
       "   'harness|drop|3': {'em': 0.040373322147651006,\n",
       "    'em_stderr': 0.0020157564185176837,\n",
       "    'f1': 0.1050272651006715,\n",
       "    'f1_stderr': 0.0023756238577676155},\n",
       "   'harness|gsm8k|5': {'acc': 0.266868840030326,\n",
       "    'acc_stderr': 0.012183780551887957},\n",
       "   'harness|winogrande|5': {'acc': 0.8050513022888713,\n",
       "    'acc_stderr': 0.011134099415938268}}},\n",
       " 'llama-2-7b-chat': {'key': 'llama-2-7b-chat',\n",
       "  'Model': 'Llama-2-7b-chat',\n",
       "  'MT-bench (score)': '6.27',\n",
       "  'MMLU': '0.458',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-7b-chat-hf',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.4948805460750853,\n",
       "    'acc_stderr': 0.01461062489030916,\n",
       "    'acc_norm': 0.5290102389078498,\n",
       "    'acc_norm_stderr': 0.014586776355294323},\n",
       "   'harness|hellaswag|10': {'acc': 0.5978888667596096,\n",
       "    'acc_stderr': 0.004893220635011792,\n",
       "    'acc_norm': 0.7855008962358097,\n",
       "    'acc_norm_stderr': 0.004096355125117511},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542129,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542129},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.42962962962962964,\n",
       "    'acc_stderr': 0.04276349494376599,\n",
       "    'acc_norm': 0.42962962962962964,\n",
       "    'acc_norm_stderr': 0.04276349494376599},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.4868421052631579,\n",
       "    'acc_stderr': 0.04067533136309173,\n",
       "    'acc_norm': 0.4868421052631579,\n",
       "    'acc_norm_stderr': 0.04067533136309173},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5358490566037736,\n",
       "    'acc_stderr': 0.030693675018458003,\n",
       "    'acc_norm': 0.5358490566037736,\n",
       "    'acc_norm_stderr': 0.030693675018458003},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5208333333333334,\n",
       "    'acc_stderr': 0.041775789507399935,\n",
       "    'acc_norm': 0.5208333333333334,\n",
       "    'acc_norm_stderr': 0.041775789507399935},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.04560480215720684,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.04560480215720684},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.3988439306358382,\n",
       "    'acc_stderr': 0.037336266553835096,\n",
       "    'acc_norm': 0.3988439306358382,\n",
       "    'acc_norm_stderr': 0.037336266553835096},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.22549019607843138,\n",
       "    'acc_stderr': 0.041583075330832865,\n",
       "    'acc_norm': 0.22549019607843138,\n",
       "    'acc_norm_stderr': 0.041583075330832865},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.58,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.58,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4085106382978723,\n",
       "    'acc_stderr': 0.03213418026701576,\n",
       "    'acc_norm': 0.4085106382978723,\n",
       "    'acc_norm_stderr': 0.03213418026701576},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.37719298245614036,\n",
       "    'acc_stderr': 0.045595221419582166,\n",
       "    'acc_norm': 0.37719298245614036,\n",
       "    'acc_norm_stderr': 0.045595221419582166},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4896551724137931,\n",
       "    'acc_stderr': 0.04165774775728762,\n",
       "    'acc_norm': 0.4896551724137931,\n",
       "    'acc_norm_stderr': 0.04165774775728762},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.29894179894179895,\n",
       "    'acc_stderr': 0.023577604791655805,\n",
       "    'acc_norm': 0.29894179894179895,\n",
       "    'acc_norm_stderr': 0.023577604791655805},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.25396825396825395,\n",
       "    'acc_stderr': 0.03893259610604675,\n",
       "    'acc_norm': 0.25396825396825395,\n",
       "    'acc_norm_stderr': 0.03893259610604675},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.5225806451612903,\n",
       "    'acc_stderr': 0.02841498501970786,\n",
       "    'acc_norm': 0.5225806451612903,\n",
       "    'acc_norm_stderr': 0.02841498501970786},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3645320197044335,\n",
       "    'acc_stderr': 0.033864057460620905,\n",
       "    'acc_norm': 0.3645320197044335,\n",
       "    'acc_norm_stderr': 0.033864057460620905},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.5878787878787879,\n",
       "    'acc_stderr': 0.03843566993588718,\n",
       "    'acc_norm': 0.5878787878787879,\n",
       "    'acc_norm_stderr': 0.03843566993588718},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6060606060606061,\n",
       "    'acc_stderr': 0.034812853382329624,\n",
       "    'acc_norm': 0.6060606060606061,\n",
       "    'acc_norm_stderr': 0.034812853382329624},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7150259067357513,\n",
       "    'acc_stderr': 0.032577140777096614,\n",
       "    'acc_norm': 0.7150259067357513,\n",
       "    'acc_norm_stderr': 0.032577140777096614},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.4256410256410256,\n",
       "    'acc_stderr': 0.02506909438729654,\n",
       "    'acc_norm': 0.4256410256410256,\n",
       "    'acc_norm_stderr': 0.02506909438729654},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.25555555555555554,\n",
       "    'acc_stderr': 0.02659393910184408,\n",
       "    'acc_norm': 0.25555555555555554,\n",
       "    'acc_norm_stderr': 0.02659393910184408},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.42436974789915966,\n",
       "    'acc_stderr': 0.03210479051015776,\n",
       "    'acc_norm': 0.42436974789915966,\n",
       "    'acc_norm_stderr': 0.03210479051015776},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2913907284768212,\n",
       "    'acc_stderr': 0.03710185726119995,\n",
       "    'acc_norm': 0.2913907284768212,\n",
       "    'acc_norm_stderr': 0.03710185726119995},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.6752293577981652,\n",
       "    'acc_stderr': 0.020077729109310327,\n",
       "    'acc_norm': 0.6752293577981652,\n",
       "    'acc_norm_stderr': 0.020077729109310327},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.0321495214780275,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.0321495214780275},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.0328347205610856,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.0328347205610856},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.03068582059661079,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.03068582059661079},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5605381165919282,\n",
       "    'acc_stderr': 0.03331092511038179,\n",
       "    'acc_norm': 0.5605381165919282,\n",
       "    'acc_norm_stderr': 0.03331092511038179},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5725190839694656,\n",
       "    'acc_stderr': 0.04338920305792401,\n",
       "    'acc_norm': 0.5725190839694656,\n",
       "    'acc_norm_stderr': 0.04338920305792401},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.628099173553719,\n",
       "    'acc_stderr': 0.04412015806624504,\n",
       "    'acc_norm': 0.628099173553719,\n",
       "    'acc_norm_stderr': 0.04412015806624504},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04766075165356461,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04766075165356461},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.5521472392638037,\n",
       "    'acc_stderr': 0.03906947479456606,\n",
       "    'acc_norm': 0.5521472392638037,\n",
       "    'acc_norm_stderr': 0.03906947479456606},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.30357142857142855,\n",
       "    'acc_stderr': 0.04364226155841044,\n",
       "    'acc_norm': 0.30357142857142855,\n",
       "    'acc_norm_stderr': 0.04364226155841044},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.04656147110012351,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.04656147110012351},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7094017094017094,\n",
       "    'acc_stderr': 0.029745048572674074,\n",
       "    'acc_norm': 0.7094017094017094,\n",
       "    'acc_norm_stderr': 0.029745048572674074},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6756066411238825,\n",
       "    'acc_stderr': 0.0167409290471627,\n",
       "    'acc_norm': 0.6756066411238825,\n",
       "    'acc_norm_stderr': 0.0167409290471627},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.026907849856282542,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.026907849856282542},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2201117318435754,\n",
       "    'acc_stderr': 0.013856994024227175,\n",
       "    'acc_norm': 0.2201117318435754,\n",
       "    'acc_norm_stderr': 0.013856994024227175},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5196078431372549,\n",
       "    'acc_stderr': 0.028607893699576066,\n",
       "    'acc_norm': 0.5196078431372549,\n",
       "    'acc_norm_stderr': 0.028607893699576066},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5659163987138264,\n",
       "    'acc_stderr': 0.02815023224453559,\n",
       "    'acc_norm': 0.5659163987138264,\n",
       "    'acc_norm_stderr': 0.02815023224453559},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5679012345679012,\n",
       "    'acc_stderr': 0.027563010971606676,\n",
       "    'acc_norm': 0.5679012345679012,\n",
       "    'acc_norm_stderr': 0.027563010971606676},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3723404255319149,\n",
       "    'acc_stderr': 0.028838921471251458,\n",
       "    'acc_norm': 0.3723404255319149,\n",
       "    'acc_norm_stderr': 0.028838921471251458},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3500651890482399,\n",
       "    'acc_stderr': 0.012182552313215175,\n",
       "    'acc_norm': 0.3500651890482399,\n",
       "    'acc_norm_stderr': 0.012182552313215175},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.45588235294117646,\n",
       "    'acc_stderr': 0.030254372573976684,\n",
       "    'acc_norm': 0.45588235294117646,\n",
       "    'acc_norm_stderr': 0.030254372573976684},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4803921568627451,\n",
       "    'acc_stderr': 0.020212274976302957,\n",
       "    'acc_norm': 0.4803921568627451,\n",
       "    'acc_norm_stderr': 0.020212274976302957},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5272727272727272,\n",
       "    'acc_stderr': 0.04782001791380061,\n",
       "    'acc_norm': 0.5272727272727272,\n",
       "    'acc_norm_stderr': 0.04782001791380061},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5265306122448979,\n",
       "    'acc_stderr': 0.03196412734523272,\n",
       "    'acc_norm': 0.5265306122448979,\n",
       "    'acc_norm_stderr': 0.03196412734523272},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6467661691542289,\n",
       "    'acc_stderr': 0.03379790611796777,\n",
       "    'acc_norm': 0.6467661691542289,\n",
       "    'acc_norm_stderr': 0.03379790611796777},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.43373493975903615,\n",
       "    'acc_stderr': 0.03858158940685517,\n",
       "    'acc_norm': 0.43373493975903615,\n",
       "    'acc_norm_stderr': 0.03858158940685517},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.034240429246915824,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.034240429246915824},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3011015911872705,\n",
       "    'mc1_stderr': 0.016058999026100616,\n",
       "    'mc2': 0.45570370195101134,\n",
       "    'mc2_stderr': 0.015691038880908878},\n",
       "   'all': {'em': 0.06763842281879194,\n",
       "    'em_stderr': 0.0025717489509556085,\n",
       "    'f1': 0.13085570469798627,\n",
       "    'f1_stderr': 0.0028825856446422905,\n",
       "    'acc': 0.39549166962367155,\n",
       "    'acc_stderr': 0.009921949302668327},\n",
       "   'harness|drop|3': {'em': 0.06763842281879194,\n",
       "    'em_stderr': 0.0025717489509556085,\n",
       "    'f1': 0.13085570469798627,\n",
       "    'f1_stderr': 0.0028825856446422905},\n",
       "   'harness|gsm8k|5': {'acc': 0.07354056103108415,\n",
       "    'acc_stderr': 0.0071898357543652685},\n",
       "   'harness|winogrande|5': {'acc': 0.7174427782162589,\n",
       "    'acc_stderr': 0.012654062850971384}}},\n",
       " 'mistral-7b-instruct': {'key': 'mistral-7b-instruct',\n",
       "  'Model': 'Mistral-7B-Instruct-v0.1',\n",
       "  'MT-bench (score)': '6.84',\n",
       "  'MMLU': '0.554',\n",
       "  'License': 'Apache 2.0',\n",
       "  'Organization': 'Mistral',\n",
       "  'Link': 'https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.523037542662116,\n",
       "    'acc_stderr': 0.014595873205358269,\n",
       "    'acc_norm': 0.5452218430034129,\n",
       "    'acc_norm_stderr': 0.014551507060836357},\n",
       "   'harness|hellaswag|10': {'acc': 0.5694084843656642,\n",
       "    'acc_stderr': 0.004941470620074867,\n",
       "    'acc_norm': 0.7563234415455089,\n",
       "    'acc_norm_stderr': 0.0042842240337755385},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.047609522856952365,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.047609522856952365},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4222222222222222,\n",
       "    'acc_stderr': 0.04266763404099582,\n",
       "    'acc_norm': 0.4222222222222222,\n",
       "    'acc_norm_stderr': 0.04266763404099582},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5657894736842105,\n",
       "    'acc_stderr': 0.040335656678483205,\n",
       "    'acc_norm': 0.5657894736842105,\n",
       "    'acc_norm_stderr': 0.040335656678483205},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5849056603773585,\n",
       "    'acc_stderr': 0.030325945789286105,\n",
       "    'acc_norm': 0.5849056603773585,\n",
       "    'acc_norm_stderr': 0.030325945789286105},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5902777777777778,\n",
       "    'acc_stderr': 0.04112490974670787,\n",
       "    'acc_norm': 0.5902777777777778,\n",
       "    'acc_norm_stderr': 0.04112490974670787},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5202312138728323,\n",
       "    'acc_stderr': 0.03809342081273956,\n",
       "    'acc_norm': 0.5202312138728323,\n",
       "    'acc_norm_stderr': 0.03809342081273956},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.30392156862745096,\n",
       "    'acc_stderr': 0.045766654032077636,\n",
       "    'acc_norm': 0.30392156862745096,\n",
       "    'acc_norm_stderr': 0.045766654032077636},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.047258156262526094,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.047258156262526094},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4808510638297872,\n",
       "    'acc_stderr': 0.032662042990646775,\n",
       "    'acc_norm': 0.4808510638297872,\n",
       "    'acc_norm_stderr': 0.032662042990646775},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.37719298245614036,\n",
       "    'acc_stderr': 0.04559522141958216,\n",
       "    'acc_norm': 0.37719298245614036,\n",
       "    'acc_norm_stderr': 0.04559522141958216},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5517241379310345,\n",
       "    'acc_stderr': 0.04144311810878151,\n",
       "    'acc_norm': 0.5517241379310345,\n",
       "    'acc_norm_stderr': 0.04144311810878151},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36507936507936506,\n",
       "    'acc_stderr': 0.024796060602699947,\n",
       "    'acc_norm': 0.36507936507936506,\n",
       "    'acc_norm_stderr': 0.024796060602699947},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.38095238095238093,\n",
       "    'acc_stderr': 0.043435254289490965,\n",
       "    'acc_norm': 0.38095238095238093,\n",
       "    'acc_norm_stderr': 0.043435254289490965},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6419354838709678,\n",
       "    'acc_stderr': 0.027273890594300642,\n",
       "    'acc_norm': 0.6419354838709678,\n",
       "    'acc_norm_stderr': 0.027273890594300642},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.41379310344827586,\n",
       "    'acc_stderr': 0.03465304488406795,\n",
       "    'acc_norm': 0.41379310344827586,\n",
       "    'acc_norm_stderr': 0.03465304488406795},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.036810508691615486,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.036810508691615486},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.031911782267135466,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.031911782267135466},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7253886010362695,\n",
       "    'acc_stderr': 0.03221024508041154,\n",
       "    'acc_norm': 0.7253886010362695,\n",
       "    'acc_norm_stderr': 0.03221024508041154},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5205128205128206,\n",
       "    'acc_stderr': 0.02532966316348994,\n",
       "    'acc_norm': 0.5205128205128206,\n",
       "    'acc_norm_stderr': 0.02532966316348994},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.027940457136228416,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.027940457136228416},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5462184873949579,\n",
       "    'acc_stderr': 0.032339434681820885,\n",
       "    'acc_norm': 0.5462184873949579,\n",
       "    'acc_norm_stderr': 0.032339434681820885},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.03822746937658753,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.03822746937658753},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.710091743119266,\n",
       "    'acc_stderr': 0.019453066609201597,\n",
       "    'acc_norm': 0.710091743119266,\n",
       "    'acc_norm_stderr': 0.019453066609201597},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4537037037037037,\n",
       "    'acc_stderr': 0.03395322726375797,\n",
       "    'acc_norm': 0.4537037037037037,\n",
       "    'acc_norm_stderr': 0.03395322726375797},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7058823529411765,\n",
       "    'acc_stderr': 0.03198001660115072,\n",
       "    'acc_norm': 0.7058823529411765,\n",
       "    'acc_norm_stderr': 0.03198001660115072},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6962025316455697,\n",
       "    'acc_stderr': 0.0299366963871386,\n",
       "    'acc_norm': 0.6962025316455697,\n",
       "    'acc_norm_stderr': 0.0299366963871386},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6547085201793722,\n",
       "    'acc_stderr': 0.03191100192835794,\n",
       "    'acc_norm': 0.6547085201793722,\n",
       "    'acc_norm_stderr': 0.03191100192835794},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6717557251908397,\n",
       "    'acc_stderr': 0.04118438565806299,\n",
       "    'acc_norm': 0.6717557251908397,\n",
       "    'acc_norm_stderr': 0.04118438565806299},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.6776859504132231,\n",
       "    'acc_stderr': 0.042664163633521685,\n",
       "    'acc_norm': 0.6776859504132231,\n",
       "    'acc_norm_stderr': 0.042664163633521685},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6944444444444444,\n",
       "    'acc_stderr': 0.044531975073749834,\n",
       "    'acc_norm': 0.6944444444444444,\n",
       "    'acc_norm_stderr': 0.044531975073749834},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6503067484662577,\n",
       "    'acc_stderr': 0.03746668325470021,\n",
       "    'acc_norm': 0.6503067484662577,\n",
       "    'acc_norm_stderr': 0.03746668325470021},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.4732142857142857,\n",
       "    'acc_stderr': 0.047389751192741546,\n",
       "    'acc_norm': 0.4732142857142857,\n",
       "    'acc_norm_stderr': 0.047389751192741546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6893203883495146,\n",
       "    'acc_stderr': 0.04582124160161551,\n",
       "    'acc_norm': 0.6893203883495146,\n",
       "    'acc_norm_stderr': 0.04582124160161551},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8418803418803419,\n",
       "    'acc_stderr': 0.023902325549560392,\n",
       "    'acc_norm': 0.8418803418803419,\n",
       "    'acc_norm_stderr': 0.023902325549560392},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.047258156262526094,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.047258156262526094},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7432950191570882,\n",
       "    'acc_stderr': 0.015620480263064533,\n",
       "    'acc_norm': 0.7432950191570882,\n",
       "    'acc_norm_stderr': 0.015620480263064533},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5895953757225434,\n",
       "    'acc_stderr': 0.026483392042098174,\n",
       "    'acc_norm': 0.5895953757225434,\n",
       "    'acc_norm_stderr': 0.026483392042098174},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2446927374301676,\n",
       "    'acc_stderr': 0.014378169884098417,\n",
       "    'acc_norm': 0.2446927374301676,\n",
       "    'acc_norm_stderr': 0.014378169884098417},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6143790849673203,\n",
       "    'acc_stderr': 0.02787074527829027,\n",
       "    'acc_norm': 0.6143790849673203,\n",
       "    'acc_norm_stderr': 0.02787074527829027},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6077170418006431,\n",
       "    'acc_stderr': 0.027731258647012,\n",
       "    'acc_norm': 0.6077170418006431,\n",
       "    'acc_norm_stderr': 0.027731258647012},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5771604938271605,\n",
       "    'acc_stderr': 0.027487472980871595,\n",
       "    'acc_norm': 0.5771604938271605,\n",
       "    'acc_norm_stderr': 0.027487472980871595},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3829787234042553,\n",
       "    'acc_stderr': 0.028999080904806185,\n",
       "    'acc_norm': 0.3829787234042553,\n",
       "    'acc_norm_stderr': 0.028999080904806185},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.40091264667535853,\n",
       "    'acc_stderr': 0.012516960350640824,\n",
       "    'acc_norm': 0.40091264667535853,\n",
       "    'acc_norm_stderr': 0.012516960350640824},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5808823529411765,\n",
       "    'acc_stderr': 0.02997280717046462,\n",
       "    'acc_norm': 0.5808823529411765,\n",
       "    'acc_norm_stderr': 0.02997280717046462},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5163398692810458,\n",
       "    'acc_stderr': 0.02021703065318646,\n",
       "    'acc_norm': 0.5163398692810458,\n",
       "    'acc_norm_stderr': 0.02021703065318646},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302505,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302505},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6775510204081633,\n",
       "    'acc_stderr': 0.02992310056368391,\n",
       "    'acc_norm': 0.6775510204081633,\n",
       "    'acc_norm_stderr': 0.02992310056368391},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.746268656716418,\n",
       "    'acc_stderr': 0.03076944496729602,\n",
       "    'acc_norm': 0.746268656716418,\n",
       "    'acc_norm_stderr': 0.03076944496729602},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.46987951807228917,\n",
       "    'acc_stderr': 0.03885425420866766,\n",
       "    'acc_norm': 0.46987951807228917,\n",
       "    'acc_norm_stderr': 0.03885425420866766},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3953488372093023,\n",
       "    'mc1_stderr': 0.017115815632418194,\n",
       "    'mc2': 0.5628382292113293,\n",
       "    'mc2_stderr': 0.015351892312006444},\n",
       "   'all': {'em': 0.37038590604026844,\n",
       "    'em_stderr': 0.00494543044549648,\n",
       "    'f1': 0.43100566275167973,\n",
       "    'f1_stderr': 0.00478990485809286,\n",
       "    'acc': 0.4398533245809979,\n",
       "    'acc_stderr': 0.01100025548646791},\n",
       "   'harness|drop|3': {'em': 0.37038590604026844,\n",
       "    'em_stderr': 0.00494543044549648,\n",
       "    'f1': 0.43100566275167973,\n",
       "    'f1_stderr': 0.00478990485809286},\n",
       "   'harness|gsm8k|5': {'acc': 0.1425322213798332,\n",
       "    'acc_stderr': 0.009629588445673814},\n",
       "   'harness|winogrande|5': {'acc': 0.7371744277821626,\n",
       "    'acc_stderr': 0.012370922527262006}}},\n",
       " 'mpt-30b-chat': {'key': 'mpt-30b-chat',\n",
       "  'Model': 'MPT-30B-chat',\n",
       "  'MT-bench (score)': '6.39',\n",
       "  'MMLU': '0.504',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'MosaicML',\n",
       "  'Link': 'https://huggingface.co/mosaicml/mpt-30b-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5477815699658704,\n",
       "    'acc_stderr': 0.014544519880633829,\n",
       "    'acc_norm': 0.5836177474402731,\n",
       "    'acc_norm_stderr': 0.014405618279436176},\n",
       "   'harness|hellaswag|10': {'acc': 0.6325433180641307,\n",
       "    'acc_stderr': 0.0048112699754506005,\n",
       "    'acc_norm': 0.8241386178052181,\n",
       "    'acc_norm_stderr': 0.0037992414085029525},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.45925925925925926,\n",
       "    'acc_stderr': 0.04304979692464243,\n",
       "    'acc_norm': 0.45925925925925926,\n",
       "    'acc_norm_stderr': 0.04304979692464243},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.04068942293855797,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.04068942293855797},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5509433962264151,\n",
       "    'acc_stderr': 0.030612730713641095,\n",
       "    'acc_norm': 0.5509433962264151,\n",
       "    'acc_norm_stderr': 0.030612730713641095},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5902777777777778,\n",
       "    'acc_stderr': 0.04112490974670788,\n",
       "    'acc_norm': 0.5902777777777778,\n",
       "    'acc_norm_stderr': 0.04112490974670788},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.043898699568087785,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.043898699568087785},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4297872340425532,\n",
       "    'acc_stderr': 0.03236214467715564,\n",
       "    'acc_norm': 0.4297872340425532,\n",
       "    'acc_norm_stderr': 0.03236214467715564},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2807017543859649,\n",
       "    'acc_stderr': 0.042270544512322,\n",
       "    'acc_norm': 0.2807017543859649,\n",
       "    'acc_norm_stderr': 0.042270544512322},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.47586206896551725,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.47586206896551725,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.0242785680243077,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.0242785680243077},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.31746031746031744,\n",
       "    'acc_stderr': 0.04163453031302859,\n",
       "    'acc_norm': 0.31746031746031744,\n",
       "    'acc_norm_stderr': 0.04163453031302859},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6193548387096774,\n",
       "    'acc_stderr': 0.027621717832907046,\n",
       "    'acc_norm': 0.6193548387096774,\n",
       "    'acc_norm_stderr': 0.027621717832907046},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3842364532019704,\n",
       "    'acc_stderr': 0.03422398565657551,\n",
       "    'acc_norm': 0.3842364532019704,\n",
       "    'acc_norm_stderr': 0.03422398565657551},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6303030303030303,\n",
       "    'acc_stderr': 0.03769430314512568,\n",
       "    'acc_norm': 0.6303030303030303,\n",
       "    'acc_norm_stderr': 0.03769430314512568},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6919191919191919,\n",
       "    'acc_stderr': 0.03289477330098616,\n",
       "    'acc_norm': 0.6919191919191919,\n",
       "    'acc_norm_stderr': 0.03289477330098616},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.6476683937823834,\n",
       "    'acc_stderr': 0.03447478286414357,\n",
       "    'acc_norm': 0.6476683937823834,\n",
       "    'acc_norm_stderr': 0.03447478286414357},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.45384615384615384,\n",
       "    'acc_stderr': 0.02524277098712618,\n",
       "    'acc_norm': 0.45384615384615384,\n",
       "    'acc_norm_stderr': 0.02524277098712618},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.28888888888888886,\n",
       "    'acc_stderr': 0.027634907264178544,\n",
       "    'acc_norm': 0.28888888888888886,\n",
       "    'acc_norm_stderr': 0.027634907264178544},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.47058823529411764,\n",
       "    'acc_stderr': 0.03242225027115006,\n",
       "    'acc_norm': 0.47058823529411764,\n",
       "    'acc_norm_stderr': 0.03242225027115006},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3443708609271523,\n",
       "    'acc_stderr': 0.038796870240733264,\n",
       "    'acc_norm': 0.3443708609271523,\n",
       "    'acc_norm_stderr': 0.038796870240733264},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.728440366972477,\n",
       "    'acc_stderr': 0.019069098363191442,\n",
       "    'acc_norm': 0.728440366972477,\n",
       "    'acc_norm_stderr': 0.019069098363191442},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4166666666666667,\n",
       "    'acc_stderr': 0.03362277436608044,\n",
       "    'acc_norm': 0.4166666666666667,\n",
       "    'acc_norm_stderr': 0.03362277436608044},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7598039215686274,\n",
       "    'acc_stderr': 0.02998373305591362,\n",
       "    'acc_norm': 0.7598039215686274,\n",
       "    'acc_norm_stderr': 0.02998373305591362},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7088607594936709,\n",
       "    'acc_stderr': 0.029571601065753374,\n",
       "    'acc_norm': 0.7088607594936709,\n",
       "    'acc_norm_stderr': 0.029571601065753374},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5291479820627802,\n",
       "    'acc_stderr': 0.03350073248773403,\n",
       "    'acc_norm': 0.5291479820627802,\n",
       "    'acc_norm_stderr': 0.03350073248773403},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5725190839694656,\n",
       "    'acc_stderr': 0.04338920305792401,\n",
       "    'acc_norm': 0.5725190839694656,\n",
       "    'acc_norm_stderr': 0.04338920305792401},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.4793388429752066,\n",
       "    'acc_stderr': 0.04560456086387235,\n",
       "    'acc_norm': 0.4793388429752066,\n",
       "    'acc_norm_stderr': 0.04560456086387235},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.04812917324536823,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.04812917324536823},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6012269938650306,\n",
       "    'acc_stderr': 0.03847021420456023,\n",
       "    'acc_norm': 0.6012269938650306,\n",
       "    'acc_norm_stderr': 0.03847021420456023},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.36607142857142855,\n",
       "    'acc_stderr': 0.045723723587374296,\n",
       "    'acc_norm': 0.36607142857142855,\n",
       "    'acc_norm_stderr': 0.045723723587374296},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6990291262135923,\n",
       "    'acc_stderr': 0.045416094465039476,\n",
       "    'acc_norm': 0.6990291262135923,\n",
       "    'acc_norm_stderr': 0.045416094465039476},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7948717948717948,\n",
       "    'acc_stderr': 0.026453508054040332,\n",
       "    'acc_norm': 0.7948717948717948,\n",
       "    'acc_norm_stderr': 0.026453508054040332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.04999999999999999,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.04999999999999999},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.698595146871009,\n",
       "    'acc_stderr': 0.01640909109726878,\n",
       "    'acc_norm': 0.698595146871009,\n",
       "    'acc_norm_stderr': 0.01640909109726878},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5491329479768786,\n",
       "    'acc_stderr': 0.026788811931562757,\n",
       "    'acc_norm': 0.5491329479768786,\n",
       "    'acc_norm_stderr': 0.026788811931562757},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2860335195530726,\n",
       "    'acc_stderr': 0.015113972129062141,\n",
       "    'acc_norm': 0.2860335195530726,\n",
       "    'acc_norm_stderr': 0.015113972129062141},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5032679738562091,\n",
       "    'acc_stderr': 0.02862930519400354,\n",
       "    'acc_norm': 0.5032679738562091,\n",
       "    'acc_norm_stderr': 0.02862930519400354},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5594855305466238,\n",
       "    'acc_stderr': 0.028196400574197422,\n",
       "    'acc_norm': 0.5594855305466238,\n",
       "    'acc_norm_stderr': 0.028196400574197422},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.027431623722415012,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.027431623722415012},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3829787234042553,\n",
       "    'acc_stderr': 0.028999080904806178,\n",
       "    'acc_norm': 0.3829787234042553,\n",
       "    'acc_norm_stderr': 0.028999080904806178},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.37157757496740546,\n",
       "    'acc_stderr': 0.012341828514528298,\n",
       "    'acc_norm': 0.37157757496740546,\n",
       "    'acc_norm_stderr': 0.012341828514528298},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.48161764705882354,\n",
       "    'acc_stderr': 0.03035230339535196,\n",
       "    'acc_norm': 0.48161764705882354,\n",
       "    'acc_norm_stderr': 0.03035230339535196},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.49019607843137253,\n",
       "    'acc_stderr': 0.0202239460050743,\n",
       "    'acc_norm': 0.49019607843137253,\n",
       "    'acc_norm_stderr': 0.0202239460050743},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5727272727272728,\n",
       "    'acc_stderr': 0.04738198703545483,\n",
       "    'acc_norm': 0.5727272727272728,\n",
       "    'acc_norm_stderr': 0.04738198703545483},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5755102040816327,\n",
       "    'acc_stderr': 0.031642094879429414,\n",
       "    'acc_norm': 0.5755102040816327,\n",
       "    'acc_norm_stderr': 0.031642094879429414},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6169154228855721,\n",
       "    'acc_stderr': 0.034375193373382504,\n",
       "    'acc_norm': 0.6169154228855721,\n",
       "    'acc_norm_stderr': 0.034375193373382504},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.77,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.77,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.45180722891566266,\n",
       "    'acc_stderr': 0.038743715565879536,\n",
       "    'acc_norm': 0.45180722891566266,\n",
       "    'acc_norm_stderr': 0.038743715565879536},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3390452876376989,\n",
       "    'mc1_stderr': 0.01657179791062661,\n",
       "    'mc2': 0.5199824927914821,\n",
       "    'mc2_stderr': 0.01582403747940678},\n",
       "   'all': {'em': 0.0017827181208053692,\n",
       "    'em_stderr': 0.00043200973460389824,\n",
       "    'f1': 0.06156250000000011,\n",
       "    'f1_stderr': 0.001404482472524456,\n",
       "    'acc': 0.4371318828152442,\n",
       "    'acc_stderr': 0.010557145720065584},\n",
       "   'harness|drop|3': {'em': 0.0017827181208053692,\n",
       "    'em_stderr': 0.00043200973460389824,\n",
       "    'f1': 0.06156250000000011,\n",
       "    'f1_stderr': 0.001404482472524456},\n",
       "   'harness|gsm8k|5': {'acc': 0.12130401819560273,\n",
       "    'acc_stderr': 0.008992888497275597},\n",
       "   'harness|winogrande|5': {'acc': 0.7529597474348856,\n",
       "    'acc_stderr': 0.01212140294285557}}},\n",
       " 'mpt-7b-chat': {'key': 'mpt-7b-chat',\n",
       "  'Model': 'MPT-7B-Chat',\n",
       "  'MT-bench (score)': '5.42',\n",
       "  'MMLU': '0.320',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'MosaicML',\n",
       "  'Link': 'https://huggingface.co/mosaicml/mpt-7b-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.431740614334471,\n",
       "    'acc_stderr': 0.014474591427196204,\n",
       "    'acc_norm': 0.46501706484641636,\n",
       "    'acc_norm_stderr': 0.014575583922019669},\n",
       "   'harness|hellaswag|10': {'acc': 0.5710017924716192,\n",
       "    'acc_stderr': 0.004939215682191771,\n",
       "    'acc_norm': 0.7551284604660427,\n",
       "    'acc_norm_stderr': 0.00429132188812274},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.3925925925925926,\n",
       "    'acc_stderr': 0.04218506215368879,\n",
       "    'acc_norm': 0.3925925925925926,\n",
       "    'acc_norm_stderr': 0.04218506215368879},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.3684210526315789,\n",
       "    'acc_stderr': 0.03925523381052932,\n",
       "    'acc_norm': 0.3684210526315789,\n",
       "    'acc_norm_stderr': 0.03925523381052932},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.3849056603773585,\n",
       "    'acc_stderr': 0.029946498567699948,\n",
       "    'acc_norm': 0.3849056603773585,\n",
       "    'acc_norm_stderr': 0.029946498567699948},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.04076663253918567,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.04076663253918567},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621504,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621504},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.32947976878612717,\n",
       "    'acc_stderr': 0.03583901754736411,\n",
       "    'acc_norm': 0.32947976878612717,\n",
       "    'acc_norm_stderr': 0.03583901754736411},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.17647058823529413,\n",
       "    'acc_stderr': 0.0379328118530781,\n",
       "    'acc_norm': 0.17647058823529413,\n",
       "    'acc_norm_stderr': 0.0379328118530781},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.049888765156985884,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.049888765156985884},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.3702127659574468,\n",
       "    'acc_stderr': 0.031565646822367836,\n",
       "    'acc_norm': 0.3702127659574468,\n",
       "    'acc_norm_stderr': 0.031565646822367836},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.21929824561403508,\n",
       "    'acc_stderr': 0.03892431106518754,\n",
       "    'acc_norm': 0.21929824561403508,\n",
       "    'acc_norm_stderr': 0.03892431106518754},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.45517241379310347,\n",
       "    'acc_stderr': 0.04149886942192117,\n",
       "    'acc_norm': 0.45517241379310347,\n",
       "    'acc_norm_stderr': 0.04149886942192117},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.24338624338624337,\n",
       "    'acc_stderr': 0.02210112878741543,\n",
       "    'acc_norm': 0.24338624338624337,\n",
       "    'acc_norm_stderr': 0.02210112878741543},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.25396825396825395,\n",
       "    'acc_stderr': 0.038932596106046755,\n",
       "    'acc_norm': 0.25396825396825395,\n",
       "    'acc_norm_stderr': 0.038932596106046755},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768077,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768077},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.4032258064516129,\n",
       "    'acc_stderr': 0.027906150826041143,\n",
       "    'acc_norm': 0.4032258064516129,\n",
       "    'acc_norm_stderr': 0.027906150826041143},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.2413793103448276,\n",
       "    'acc_stderr': 0.030108330718011625,\n",
       "    'acc_norm': 0.2413793103448276,\n",
       "    'acc_norm_stderr': 0.030108330718011625},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.3212121212121212,\n",
       "    'acc_stderr': 0.036462049632538115,\n",
       "    'acc_norm': 0.3212121212121212,\n",
       "    'acc_norm_stderr': 0.036462049632538115},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.36363636363636365,\n",
       "    'acc_stderr': 0.034273086529999365,\n",
       "    'acc_norm': 0.36363636363636365,\n",
       "    'acc_norm_stderr': 0.034273086529999365},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.49222797927461137,\n",
       "    'acc_stderr': 0.03608003225569653,\n",
       "    'acc_norm': 0.49222797927461137,\n",
       "    'acc_norm_stderr': 0.03608003225569653},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.3871794871794872,\n",
       "    'acc_stderr': 0.02469721693087894,\n",
       "    'acc_norm': 0.3871794871794872,\n",
       "    'acc_norm_stderr': 0.02469721693087894},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.21851851851851853,\n",
       "    'acc_stderr': 0.025195752251823786,\n",
       "    'acc_norm': 0.21851851851851853,\n",
       "    'acc_norm_stderr': 0.025195752251823786},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.3907563025210084,\n",
       "    'acc_stderr': 0.031693802357129965,\n",
       "    'acc_norm': 0.3907563025210084,\n",
       "    'acc_norm_stderr': 0.031693802357129965},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2847682119205298,\n",
       "    'acc_stderr': 0.03684881521389024,\n",
       "    'acc_norm': 0.2847682119205298,\n",
       "    'acc_norm_stderr': 0.03684881521389024},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.5100917431192661,\n",
       "    'acc_stderr': 0.02143295620345332,\n",
       "    'acc_norm': 0.5100917431192661,\n",
       "    'acc_norm_stderr': 0.02143295620345332},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.35185185185185186,\n",
       "    'acc_stderr': 0.03256850570293648,\n",
       "    'acc_norm': 0.35185185185185186,\n",
       "    'acc_norm_stderr': 0.03256850570293648},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.3284313725490196,\n",
       "    'acc_stderr': 0.032962451101722294,\n",
       "    'acc_norm': 0.3284313725490196,\n",
       "    'acc_norm_stderr': 0.032962451101722294},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.4092827004219409,\n",
       "    'acc_stderr': 0.032007041833595914,\n",
       "    'acc_norm': 0.4092827004219409,\n",
       "    'acc_norm_stderr': 0.032007041833595914},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.484304932735426,\n",
       "    'acc_stderr': 0.0335412657542081,\n",
       "    'acc_norm': 0.484304932735426,\n",
       "    'acc_norm_stderr': 0.0335412657542081},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.46564885496183206,\n",
       "    'acc_stderr': 0.04374928560599738,\n",
       "    'acc_norm': 0.46564885496183206,\n",
       "    'acc_norm_stderr': 0.04374928560599738},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.4049586776859504,\n",
       "    'acc_stderr': 0.044811377559424694,\n",
       "    'acc_norm': 0.4049586776859504,\n",
       "    'acc_norm_stderr': 0.044811377559424694},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.4074074074074074,\n",
       "    'acc_stderr': 0.047500773411999854,\n",
       "    'acc_norm': 0.4074074074074074,\n",
       "    'acc_norm_stderr': 0.047500773411999854},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2883435582822086,\n",
       "    'acc_stderr': 0.03559039531617342,\n",
       "    'acc_norm': 0.2883435582822086,\n",
       "    'acc_norm_stderr': 0.03559039531617342},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.33035714285714285,\n",
       "    'acc_stderr': 0.04464285714285713,\n",
       "    'acc_norm': 0.33035714285714285,\n",
       "    'acc_norm_stderr': 0.04464285714285713},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.36893203883495146,\n",
       "    'acc_stderr': 0.04777615181156739,\n",
       "    'acc_norm': 0.36893203883495146,\n",
       "    'acc_norm_stderr': 0.04777615181156739},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.5512820512820513,\n",
       "    'acc_stderr': 0.032583346493868806,\n",
       "    'acc_norm': 0.5512820512820513,\n",
       "    'acc_norm_stderr': 0.032583346493868806},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.5325670498084292,\n",
       "    'acc_stderr': 0.01784199575052087,\n",
       "    'acc_norm': 0.5325670498084292,\n",
       "    'acc_norm_stderr': 0.01784199575052087},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.41040462427745666,\n",
       "    'acc_stderr': 0.026483392042098187,\n",
       "    'acc_norm': 0.41040462427745666,\n",
       "    'acc_norm_stderr': 0.026483392042098187},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961443,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961443},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.4019607843137255,\n",
       "    'acc_stderr': 0.02807415894760065,\n",
       "    'acc_norm': 0.4019607843137255,\n",
       "    'acc_norm_stderr': 0.02807415894760065},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.40836012861736337,\n",
       "    'acc_stderr': 0.027917050748484624,\n",
       "    'acc_norm': 0.40836012861736337,\n",
       "    'acc_norm_stderr': 0.027917050748484624},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.36728395061728397,\n",
       "    'acc_stderr': 0.026822801759507894,\n",
       "    'acc_norm': 0.36728395061728397,\n",
       "    'acc_norm_stderr': 0.026822801759507894},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2695035460992908,\n",
       "    'acc_stderr': 0.026469036818590627,\n",
       "    'acc_norm': 0.2695035460992908,\n",
       "    'acc_norm_stderr': 0.026469036818590627},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.2907431551499348,\n",
       "    'acc_stderr': 0.011598062372851988,\n",
       "    'acc_norm': 0.2907431551499348,\n",
       "    'acc_norm_stderr': 0.011598062372851988},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.3897058823529412,\n",
       "    'acc_stderr': 0.029624663581159696,\n",
       "    'acc_norm': 0.3897058823529412,\n",
       "    'acc_norm_stderr': 0.029624663581159696},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.3202614379084967,\n",
       "    'acc_stderr': 0.018875682938069446,\n",
       "    'acc_norm': 0.3202614379084967,\n",
       "    'acc_norm_stderr': 0.018875682938069446},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.4636363636363636,\n",
       "    'acc_stderr': 0.047764491623961985,\n",
       "    'acc_norm': 0.4636363636363636,\n",
       "    'acc_norm_stderr': 0.047764491623961985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.4857142857142857,\n",
       "    'acc_stderr': 0.03199615232806286,\n",
       "    'acc_norm': 0.4857142857142857,\n",
       "    'acc_norm_stderr': 0.03199615232806286},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.48258706467661694,\n",
       "    'acc_stderr': 0.03533389234739245,\n",
       "    'acc_norm': 0.48258706467661694,\n",
       "    'acc_norm_stderr': 0.03533389234739245},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.04975698519562426,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.04975698519562426},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.42771084337349397,\n",
       "    'acc_stderr': 0.038515976837185335,\n",
       "    'acc_norm': 0.42771084337349397,\n",
       "    'acc_norm_stderr': 0.038515976837185335},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.49707602339181284,\n",
       "    'acc_stderr': 0.03834759370936839,\n",
       "    'acc_norm': 0.49707602339181284,\n",
       "    'acc_norm_stderr': 0.03834759370936839},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.27050183598531213,\n",
       "    'mc1_stderr': 0.015550778332842895,\n",
       "    'mc2': 0.40163647231251104,\n",
       "    'mc2_stderr': 0.014753108409806075},\n",
       "   'all': {'em': 0.06952600671140939,\n",
       "    'em_stderr': 0.002604746204517829,\n",
       "    'f1': 0.12196937919463072,\n",
       "    'f1_stderr': 0.002840521979064293,\n",
       "    'acc': 0.3626168565432783,\n",
       "    'acc_stderr': 0.009260585769647573},\n",
       "   'harness|drop|3': {'em': 0.06952600671140939,\n",
       "    'em_stderr': 0.002604746204517829,\n",
       "    'f1': 0.12196937919463072,\n",
       "    'f1_stderr': 0.002840521979064293},\n",
       "   'harness|gsm8k|5': {'acc': 0.04094010614101592,\n",
       "    'acc_stderr': 0.005458076796294338},\n",
       "   'harness|winogrande|5': {'acc': 0.6842936069455406,\n",
       "    'acc_stderr': 0.01306309474300081}}},\n",
       " 'oasst-pythia-12b': {'key': 'oasst-pythia-12b',\n",
       "  'Model': 'OpenAssistant-Pythia-12B',\n",
       "  'MT-bench (score)': '4.32',\n",
       "  'MMLU': '0.270',\n",
       "  'License': 'Apache 2.0',\n",
       "  'Organization': 'OpenAssistant',\n",
       "  'Link': 'https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',\n",
       "  'results': {'harness|drop|3': {'em': 0.001363255033557047,\n",
       "    'em_stderr': 0.00037786091964606887,\n",
       "    'f1': 0.059077181208053976,\n",
       "    'f1_stderr': 0.001394848925611238},\n",
       "   'harness|gsm8k|5': {'acc': 0.030326004548900682,\n",
       "    'acc_stderr': 0.004723487465514761},\n",
       "   'harness|winogrande|5': {'acc': 0.659037095501184,\n",
       "    'acc_stderr': 0.013322681435934807},\n",
       "   'all': {'acc': 0.2748440247593001,\n",
       "    'acc_stderr': 0.03228989271589345,\n",
       "    'acc_norm': 0.2784679867417539,\n",
       "    'acc_norm_stderr': 0.03228683184562667,\n",
       "    'mc1': 0.2386780905752754,\n",
       "    'mc1_stderr': 0.014922629695456418,\n",
       "    'mc2': 0.37807719406726975,\n",
       "    'mc2_stderr': 0.014701655079555453},\n",
       "   'harness|arc:challenge|25': {'acc': 0.4121160409556314,\n",
       "    'acc_stderr': 0.0143839153022254,\n",
       "    'acc_norm': 0.45733788395904434,\n",
       "    'acc_norm_stderr': 0.014558106543924067},\n",
       "   'harness|hellaswag|10': {'acc': 0.5173272256522605,\n",
       "    'acc_stderr': 0.004986784319771785,\n",
       "    'acc_norm': 0.6859191396136228,\n",
       "    'acc_norm_stderr': 0.004632001732332984},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768081,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768081},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.31851851851851853,\n",
       "    'acc_stderr': 0.04024778401977109,\n",
       "    'acc_norm': 0.31851851851851853,\n",
       "    'acc_norm_stderr': 0.04024778401977109},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.23026315789473684,\n",
       "    'acc_stderr': 0.03426059424403165,\n",
       "    'acc_norm': 0.23026315789473684,\n",
       "    'acc_norm_stderr': 0.03426059424403165},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.33584905660377357,\n",
       "    'acc_stderr': 0.029067220146644833,\n",
       "    'acc_norm': 0.33584905660377357,\n",
       "    'acc_norm_stderr': 0.029067220146644833},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2847222222222222,\n",
       "    'acc_stderr': 0.03773809990686936,\n",
       "    'acc_norm': 0.2847222222222222,\n",
       "    'acc_norm_stderr': 0.03773809990686936},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.2,\n",
       "    'acc_stderr': 0.040201512610368445,\n",
       "    'acc_norm': 0.2,\n",
       "    'acc_norm_stderr': 0.040201512610368445},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.24855491329479767,\n",
       "    'acc_stderr': 0.03295304696818317,\n",
       "    'acc_norm': 0.24855491329479767,\n",
       "    'acc_norm_stderr': 0.03295304696818317},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.28431372549019607,\n",
       "    'acc_stderr': 0.04488482852329017,\n",
       "    'acc_norm': 0.28431372549019607,\n",
       "    'acc_norm_stderr': 0.04488482852329017},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768079,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768079},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.30638297872340425,\n",
       "    'acc_stderr': 0.030135906478517563,\n",
       "    'acc_norm': 0.30638297872340425,\n",
       "    'acc_norm_stderr': 0.030135906478517563},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.22807017543859648,\n",
       "    'acc_stderr': 0.03947152782669416,\n",
       "    'acc_norm': 0.22807017543859648,\n",
       "    'acc_norm_stderr': 0.03947152782669416},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.25517241379310346,\n",
       "    'acc_stderr': 0.03632984052707841,\n",
       "    'acc_norm': 0.25517241379310346,\n",
       "    'acc_norm_stderr': 0.03632984052707841},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.25132275132275134,\n",
       "    'acc_stderr': 0.022340482339643895,\n",
       "    'acc_norm': 0.25132275132275134,\n",
       "    'acc_norm_stderr': 0.022340482339643895},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.1746031746031746,\n",
       "    'acc_stderr': 0.033954900208561116,\n",
       "    'acc_norm': 0.1746031746031746,\n",
       "    'acc_norm_stderr': 0.033954900208561116},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.04560480215720684,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.04560480215720684},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.267741935483871,\n",
       "    'acc_stderr': 0.025189006660212385,\n",
       "    'acc_norm': 0.267741935483871,\n",
       "    'acc_norm_stderr': 0.025189006660212385},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.21674876847290642,\n",
       "    'acc_stderr': 0.028990331252516235,\n",
       "    'acc_norm': 0.21674876847290642,\n",
       "    'acc_norm_stderr': 0.028990331252516235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.28484848484848485,\n",
       "    'acc_stderr': 0.03524390844511784,\n",
       "    'acc_norm': 0.28484848484848485,\n",
       "    'acc_norm_stderr': 0.03524390844511784},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.25757575757575757,\n",
       "    'acc_stderr': 0.031156269519646836,\n",
       "    'acc_norm': 0.25757575757575757,\n",
       "    'acc_norm_stderr': 0.031156269519646836},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.23834196891191708,\n",
       "    'acc_stderr': 0.03074890536390989,\n",
       "    'acc_norm': 0.23834196891191708,\n",
       "    'acc_norm_stderr': 0.03074890536390989},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.2641025641025641,\n",
       "    'acc_stderr': 0.022352193737453282,\n",
       "    'acc_norm': 0.2641025641025641,\n",
       "    'acc_norm_stderr': 0.022352193737453282},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2518518518518518,\n",
       "    'acc_stderr': 0.02646611753895991,\n",
       "    'acc_norm': 0.2518518518518518,\n",
       "    'acc_norm_stderr': 0.02646611753895991},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.24369747899159663,\n",
       "    'acc_stderr': 0.02788682807838058,\n",
       "    'acc_norm': 0.24369747899159663,\n",
       "    'acc_norm_stderr': 0.02788682807838058},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.26490066225165565,\n",
       "    'acc_stderr': 0.03603038545360384,\n",
       "    'acc_norm': 0.26490066225165565,\n",
       "    'acc_norm_stderr': 0.03603038545360384},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.24587155963302754,\n",
       "    'acc_stderr': 0.018461940968708446,\n",
       "    'acc_norm': 0.24587155963302754,\n",
       "    'acc_norm_stderr': 0.018461940968708446},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.24074074074074073,\n",
       "    'acc_stderr': 0.02915752218460561,\n",
       "    'acc_norm': 0.24074074074074073,\n",
       "    'acc_norm_stderr': 0.02915752218460561},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.29411764705882354,\n",
       "    'acc_stderr': 0.03198001660115071,\n",
       "    'acc_norm': 0.29411764705882354,\n",
       "    'acc_norm_stderr': 0.03198001660115071},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.23628691983122363,\n",
       "    'acc_stderr': 0.027652153144159263,\n",
       "    'acc_norm': 0.23628691983122363,\n",
       "    'acc_norm_stderr': 0.027652153144159263},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.3004484304932735,\n",
       "    'acc_stderr': 0.030769352008229136,\n",
       "    'acc_norm': 0.3004484304932735,\n",
       "    'acc_norm_stderr': 0.030769352008229136},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.22137404580152673,\n",
       "    'acc_stderr': 0.036412970813137276,\n",
       "    'acc_norm': 0.22137404580152673,\n",
       "    'acc_norm_stderr': 0.036412970813137276},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.3305785123966942,\n",
       "    'acc_stderr': 0.04294340845212094,\n",
       "    'acc_norm': 0.3305785123966942,\n",
       "    'acc_norm_stderr': 0.04294340845212094},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.28703703703703703,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.28703703703703703,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2822085889570552,\n",
       "    'acc_stderr': 0.03536117886664743,\n",
       "    'acc_norm': 0.2822085889570552,\n",
       "    'acc_norm_stderr': 0.03536117886664743},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.26785714285714285,\n",
       "    'acc_stderr': 0.04203277291467764,\n",
       "    'acc_norm': 0.26785714285714285,\n",
       "    'acc_norm_stderr': 0.04203277291467764},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.18446601941747573,\n",
       "    'acc_stderr': 0.03840423627288276,\n",
       "    'acc_norm': 0.18446601941747573,\n",
       "    'acc_norm_stderr': 0.03840423627288276},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.29914529914529914,\n",
       "    'acc_stderr': 0.029996951858349497,\n",
       "    'acc_norm': 0.29914529914529914,\n",
       "    'acc_norm_stderr': 0.029996951858349497},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.28607918263090676,\n",
       "    'acc_stderr': 0.01616087140512754,\n",
       "    'acc_norm': 0.28607918263090676,\n",
       "    'acc_norm_stderr': 0.01616087140512754},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.25722543352601157,\n",
       "    'acc_stderr': 0.02353292543104429,\n",
       "    'acc_norm': 0.25722543352601157,\n",
       "    'acc_norm_stderr': 0.02353292543104429},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2424581005586592,\n",
       "    'acc_stderr': 0.014333522059217889,\n",
       "    'acc_norm': 0.2424581005586592,\n",
       "    'acc_norm_stderr': 0.014333522059217889},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.27124183006535946,\n",
       "    'acc_stderr': 0.025457756696667864,\n",
       "    'acc_norm': 0.27124183006535946,\n",
       "    'acc_norm_stderr': 0.025457756696667864},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.2861736334405145,\n",
       "    'acc_stderr': 0.02567025924218895,\n",
       "    'acc_norm': 0.2861736334405145,\n",
       "    'acc_norm_stderr': 0.02567025924218895},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.2716049382716049,\n",
       "    'acc_stderr': 0.02474862449053737,\n",
       "    'acc_norm': 0.2716049382716049,\n",
       "    'acc_norm_stderr': 0.02474862449053737},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2730496453900709,\n",
       "    'acc_stderr': 0.026577860943307857,\n",
       "    'acc_norm': 0.2730496453900709,\n",
       "    'acc_norm_stderr': 0.026577860943307857},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.27835723598435463,\n",
       "    'acc_stderr': 0.011446990197380985,\n",
       "    'acc_norm': 0.27835723598435463,\n",
       "    'acc_norm_stderr': 0.011446990197380985},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.23529411764705882,\n",
       "    'acc_stderr': 0.025767252010855973,\n",
       "    'acc_norm': 0.23529411764705882,\n",
       "    'acc_norm_stderr': 0.025767252010855973},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.2777777777777778,\n",
       "    'acc_stderr': 0.018120224251484584,\n",
       "    'acc_norm': 0.2777777777777778,\n",
       "    'acc_norm_stderr': 0.018120224251484584},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.32727272727272727,\n",
       "    'acc_stderr': 0.04494290866252089,\n",
       "    'acc_norm': 0.32727272727272727,\n",
       "    'acc_norm_stderr': 0.04494290866252089},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.2571428571428571,\n",
       "    'acc_stderr': 0.027979823538744543,\n",
       "    'acc_norm': 0.2571428571428571,\n",
       "    'acc_norm_stderr': 0.027979823538744543},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.27860696517412936,\n",
       "    'acc_stderr': 0.031700561834973086,\n",
       "    'acc_norm': 0.27860696517412936,\n",
       "    'acc_norm_stderr': 0.031700561834973086},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.3373493975903614,\n",
       "    'acc_stderr': 0.0368078369072758,\n",
       "    'acc_norm': 0.3373493975903614,\n",
       "    'acc_norm_stderr': 0.0368078369072758},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.26900584795321636,\n",
       "    'acc_stderr': 0.03401052620104089,\n",
       "    'acc_norm': 0.26900584795321636,\n",
       "    'acc_norm_stderr': 0.03401052620104089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.2386780905752754,\n",
       "    'mc1_stderr': 0.014922629695456418,\n",
       "    'mc2': 0.37807719406726975,\n",
       "    'mc2_stderr': 0.014701655079555453}}},\n",
       " 'openchat-3.5': {'key': 'openchat-3.5',\n",
       "  'Model': 'OpenChat-3.5',\n",
       "  'MT-bench (score)': '7.81',\n",
       "  'MMLU': '0.643',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'OpenChat',\n",
       "  'Link': 'https://huggingface.co/openchat/openchat_3.5',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5972696245733788,\n",
       "    'acc_stderr': 0.014332236306790147,\n",
       "    'acc_norm': 0.6382252559726962,\n",
       "    'acc_norm_stderr': 0.014041957945038078},\n",
       "   'harness|hellaswag|10': {'acc': 0.6579366660027883,\n",
       "    'acc_stderr': 0.004734311435009196,\n",
       "    'acc_norm': 0.8480382393945429,\n",
       "    'acc_norm_stderr': 0.003582501596564539},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5703703703703704,\n",
       "    'acc_stderr': 0.042763494943765995,\n",
       "    'acc_norm': 0.5703703703703704,\n",
       "    'acc_norm_stderr': 0.042763494943765995},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6973684210526315,\n",
       "    'acc_stderr': 0.037385206761196686,\n",
       "    'acc_norm': 0.6973684210526315,\n",
       "    'acc_norm_stderr': 0.037385206761196686},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.028544793319055326,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.028544793319055326},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7708333333333334,\n",
       "    'acc_stderr': 0.035146974678623884,\n",
       "    'acc_norm': 0.7708333333333334,\n",
       "    'acc_norm_stderr': 0.035146974678623884},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.653179190751445,\n",
       "    'acc_stderr': 0.036291466701596636,\n",
       "    'acc_norm': 0.653179190751445,\n",
       "    'acc_norm_stderr': 0.036291466701596636},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4117647058823529,\n",
       "    'acc_stderr': 0.04897104952726366,\n",
       "    'acc_norm': 0.4117647058823529,\n",
       "    'acc_norm_stderr': 0.04897104952726366},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.574468085106383,\n",
       "    'acc_stderr': 0.032321469162244675,\n",
       "    'acc_norm': 0.574468085106383,\n",
       "    'acc_norm_stderr': 0.032321469162244675},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5175438596491229,\n",
       "    'acc_stderr': 0.04700708033551038,\n",
       "    'acc_norm': 0.5175438596491229,\n",
       "    'acc_norm_stderr': 0.04700708033551038},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370332,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370332},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42328042328042326,\n",
       "    'acc_stderr': 0.02544636563440679,\n",
       "    'acc_norm': 0.42328042328042326,\n",
       "    'acc_norm_stderr': 0.02544636563440679},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5158730158730159,\n",
       "    'acc_stderr': 0.044698818540726076,\n",
       "    'acc_norm': 0.5158730158730159,\n",
       "    'acc_norm_stderr': 0.044698818540726076},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7967741935483871,\n",
       "    'acc_stderr': 0.02289168798455496,\n",
       "    'acc_norm': 0.7967741935483871,\n",
       "    'acc_norm_stderr': 0.02289168798455496},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5073891625615764,\n",
       "    'acc_stderr': 0.035176035403610105,\n",
       "    'acc_norm': 0.5073891625615764,\n",
       "    'acc_norm_stderr': 0.035176035403610105},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7757575757575758,\n",
       "    'acc_stderr': 0.032568666616811015,\n",
       "    'acc_norm': 0.7757575757575758,\n",
       "    'acc_norm_stderr': 0.032568666616811015},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.803030303030303,\n",
       "    'acc_stderr': 0.028335609732463362,\n",
       "    'acc_norm': 0.803030303030303,\n",
       "    'acc_norm_stderr': 0.028335609732463362},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9067357512953368,\n",
       "    'acc_stderr': 0.02098685459328974,\n",
       "    'acc_norm': 0.9067357512953368,\n",
       "    'acc_norm_stderr': 0.02098685459328974},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.023901157979402538,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.023901157979402538},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.36666666666666664,\n",
       "    'acc_stderr': 0.02938162072646507,\n",
       "    'acc_norm': 0.36666666666666664,\n",
       "    'acc_norm_stderr': 0.02938162072646507},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6680672268907563,\n",
       "    'acc_stderr': 0.03058869701378364,\n",
       "    'acc_norm': 0.6680672268907563,\n",
       "    'acc_norm_stderr': 0.03058869701378364},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.038227469376587525,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.038227469376587525},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8550458715596331,\n",
       "    'acc_stderr': 0.015094215699700469,\n",
       "    'acc_norm': 0.8550458715596331,\n",
       "    'acc_norm_stderr': 0.015094215699700469},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.03395322726375798,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.03395322726375798},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8333333333333334,\n",
       "    'acc_stderr': 0.026156867523931045,\n",
       "    'acc_norm': 0.8333333333333334,\n",
       "    'acc_norm_stderr': 0.026156867523931045},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8227848101265823,\n",
       "    'acc_stderr': 0.024856364184503234,\n",
       "    'acc_norm': 0.8227848101265823,\n",
       "    'acc_norm_stderr': 0.024856364184503234},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.726457399103139,\n",
       "    'acc_stderr': 0.029918586707798827,\n",
       "    'acc_norm': 0.726457399103139,\n",
       "    'acc_norm_stderr': 0.029918586707798827},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7786259541984732,\n",
       "    'acc_stderr': 0.03641297081313729,\n",
       "    'acc_norm': 0.7786259541984732,\n",
       "    'acc_norm_stderr': 0.03641297081313729},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03520893951097653,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03520893951097653},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.0401910747255735,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.0401910747255735},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7791411042944786,\n",
       "    'acc_stderr': 0.03259177392742178,\n",
       "    'acc_norm': 0.7791411042944786,\n",
       "    'acc_norm_stderr': 0.03259177392742178},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.44642857142857145,\n",
       "    'acc_stderr': 0.04718471485219588,\n",
       "    'acc_norm': 0.44642857142857145,\n",
       "    'acc_norm_stderr': 0.04718471485219588},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8252427184466019,\n",
       "    'acc_stderr': 0.03760178006026621,\n",
       "    'acc_norm': 0.8252427184466019,\n",
       "    'acc_norm_stderr': 0.03760178006026621},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8760683760683761,\n",
       "    'acc_stderr': 0.02158649400128138,\n",
       "    'acc_norm': 0.8760683760683761,\n",
       "    'acc_norm_stderr': 0.02158649400128138},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8212005108556832,\n",
       "    'acc_stderr': 0.013702643715368982,\n",
       "    'acc_norm': 0.8212005108556832,\n",
       "    'acc_norm_stderr': 0.013702643715368982},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7456647398843931,\n",
       "    'acc_stderr': 0.023445826276545546,\n",
       "    'acc_norm': 0.7456647398843931,\n",
       "    'acc_norm_stderr': 0.023445826276545546},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.42681564245810055,\n",
       "    'acc_stderr': 0.016542401954631906,\n",
       "    'acc_norm': 0.42681564245810055,\n",
       "    'acc_norm_stderr': 0.016542401954631906},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7189542483660131,\n",
       "    'acc_stderr': 0.025738854797818733,\n",
       "    'acc_norm': 0.7189542483660131,\n",
       "    'acc_norm_stderr': 0.025738854797818733},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7202572347266881,\n",
       "    'acc_stderr': 0.025494259350694912,\n",
       "    'acc_norm': 0.7202572347266881,\n",
       "    'acc_norm_stderr': 0.025494259350694912},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7314814814814815,\n",
       "    'acc_stderr': 0.02465968518596728,\n",
       "    'acc_norm': 0.7314814814814815,\n",
       "    'acc_norm_stderr': 0.02465968518596728},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.48226950354609927,\n",
       "    'acc_stderr': 0.02980873964223777,\n",
       "    'acc_norm': 0.48226950354609927,\n",
       "    'acc_norm_stderr': 0.02980873964223777},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.47392438070404175,\n",
       "    'acc_stderr': 0.012752858346533134,\n",
       "    'acc_norm': 0.47392438070404175,\n",
       "    'acc_norm_stderr': 0.012752858346533134},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6801470588235294,\n",
       "    'acc_stderr': 0.02833295951403121,\n",
       "    'acc_norm': 0.6801470588235294,\n",
       "    'acc_norm_stderr': 0.02833295951403121},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6748366013071896,\n",
       "    'acc_stderr': 0.01895088677080631,\n",
       "    'acc_norm': 0.6748366013071896,\n",
       "    'acc_norm_stderr': 0.01895088677080631},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.0449429086625209,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.0449429086625209},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.028263889943784603,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.028263889943784603},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.835820895522388,\n",
       "    'acc_stderr': 0.026193923544454125,\n",
       "    'acc_norm': 0.835820895522388,\n",
       "    'acc_norm_stderr': 0.026193923544454125},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.88,\n",
       "    'acc_stderr': 0.03265986323710906,\n",
       "    'acc_norm': 0.88,\n",
       "    'acc_norm_stderr': 0.03265986323710906},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.536144578313253,\n",
       "    'acc_stderr': 0.038823108508905954,\n",
       "    'acc_norm': 0.536144578313253,\n",
       "    'acc_norm_stderr': 0.038823108508905954},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.30966952264381886,\n",
       "    'mc1_stderr': 0.0161857443551449,\n",
       "    'mc2': 0.46392847173087287,\n",
       "    'mc2_stderr': 0.015046687408684912},\n",
       "   'harness|winogrande|5': {'acc': 0.8074191002367798,\n",
       "    'acc_stderr': 0.011082538847491902},\n",
       "   'harness|drop|3': {'em': 0.0019924496644295304,\n",
       "    'em_stderr': 0.0004566676462666931,\n",
       "    'f1': 0.07226510067114073,\n",
       "    'f1_stderr': 0.0014840662284336832},\n",
       "   'harness|gsm8k|5': {'acc': 0.26611068991660347,\n",
       "    'acc_stderr': 0.012172750939040322},\n",
       "   'all': {'acc': 0.6453244910928128,\n",
       "    'acc_stderr': 0.031930893551459144,\n",
       "    'acc_norm': 0.6529207012084622,\n",
       "    'acc_norm_stderr': 0.03259469709580134,\n",
       "    'mc1': 0.30966952264381886,\n",
       "    'mc1_stderr': 0.0161857443551449,\n",
       "    'mc2': 0.46392847173087287,\n",
       "    'mc2_stderr': 0.015046687408684912,\n",
       "    'em': 0.0019924496644295304,\n",
       "    'em_stderr': 0.0004566676462666931,\n",
       "    'f1': 0.07226510067114073,\n",
       "    'f1_stderr': 0.0014840662284336832}}},\n",
       " 'openhermes-2.5-mistral-7b': {'key': 'openhermes-2.5-mistral-7b',\n",
       "  'Model': 'OpenHermes-2.5-Mistral-7b',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '-',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'NousResearch',\n",
       "  'Link': 'https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6126279863481229,\n",
       "    'acc_stderr': 0.014235872487909869,\n",
       "    'acc_norm': 0.6493174061433447,\n",
       "    'acc_norm_stderr': 0.013944635930726099},\n",
       "   'harness|hellaswag|10': {'acc': 0.6519617606054571,\n",
       "    'acc_stderr': 0.004753746951620152,\n",
       "    'acc_norm': 0.8429595698068114,\n",
       "    'acc_norm_stderr': 0.003630952999843739},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695236,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695236},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.0421850621536888,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.0421850621536888},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6973684210526315,\n",
       "    'acc_stderr': 0.037385206761196686,\n",
       "    'acc_norm': 0.6973684210526315,\n",
       "    'acc_norm_stderr': 0.037385206761196686},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.02854479331905533,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.02854479331905533},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7569444444444444,\n",
       "    'acc_stderr': 0.03586879280080341,\n",
       "    'acc_norm': 0.7569444444444444,\n",
       "    'acc_norm_stderr': 0.03586879280080341},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.46,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.46,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6242774566473989,\n",
       "    'acc_stderr': 0.036928207672648664,\n",
       "    'acc_norm': 0.6242774566473989,\n",
       "    'acc_norm_stderr': 0.036928207672648664},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107223,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107223},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5659574468085107,\n",
       "    'acc_stderr': 0.03240038086792747,\n",
       "    'acc_norm': 0.5659574468085107,\n",
       "    'acc_norm_stderr': 0.03240038086792747},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.49122807017543857,\n",
       "    'acc_stderr': 0.04702880432049615,\n",
       "    'acc_norm': 0.49122807017543857,\n",
       "    'acc_norm_stderr': 0.04702880432049615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5241379310344828,\n",
       "    'acc_stderr': 0.0416180850350153,\n",
       "    'acc_norm': 0.5241379310344828,\n",
       "    'acc_norm_stderr': 0.0416180850350153},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42592592592592593,\n",
       "    'acc_stderr': 0.02546714904546955,\n",
       "    'acc_norm': 0.42592592592592593,\n",
       "    'acc_norm_stderr': 0.02546714904546955},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.46825396825396826,\n",
       "    'acc_stderr': 0.04463112720677172,\n",
       "    'acc_norm': 0.46825396825396826,\n",
       "    'acc_norm_stderr': 0.04463112720677172},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7935483870967742,\n",
       "    'acc_stderr': 0.02302589961718871,\n",
       "    'acc_norm': 0.7935483870967742,\n",
       "    'acc_norm_stderr': 0.02302589961718871},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.035158955511656986,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.035158955511656986},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7818181818181819,\n",
       "    'acc_stderr': 0.032250781083062896,\n",
       "    'acc_norm': 0.7818181818181819,\n",
       "    'acc_norm_stderr': 0.032250781083062896},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.028057791672989017,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.028057791672989017},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8911917098445595,\n",
       "    'acc_stderr': 0.022473253332768776,\n",
       "    'acc_norm': 0.8911917098445595,\n",
       "    'acc_norm_stderr': 0.022473253332768776},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6128205128205129,\n",
       "    'acc_stderr': 0.024697216930878937,\n",
       "    'acc_norm': 0.6128205128205129,\n",
       "    'acc_norm_stderr': 0.024697216930878937},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3037037037037037,\n",
       "    'acc_stderr': 0.02803792996911499,\n",
       "    'acc_norm': 0.3037037037037037,\n",
       "    'acc_norm_stderr': 0.02803792996911499},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.680672268907563,\n",
       "    'acc_stderr': 0.030283995525884396,\n",
       "    'acc_norm': 0.680672268907563,\n",
       "    'acc_norm_stderr': 0.030283995525884396},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.31788079470198677,\n",
       "    'acc_stderr': 0.038020397601079024,\n",
       "    'acc_norm': 0.31788079470198677,\n",
       "    'acc_norm_stderr': 0.038020397601079024},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8330275229357799,\n",
       "    'acc_stderr': 0.01599015488507338,\n",
       "    'acc_norm': 0.8330275229357799,\n",
       "    'acc_norm_stderr': 0.01599015488507338},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5092592592592593,\n",
       "    'acc_stderr': 0.034093869469927006,\n",
       "    'acc_norm': 0.5092592592592593,\n",
       "    'acc_norm_stderr': 0.034093869469927006},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7990196078431373,\n",
       "    'acc_stderr': 0.02812597226565437,\n",
       "    'acc_norm': 0.7990196078431373,\n",
       "    'acc_norm_stderr': 0.02812597226565437},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8143459915611815,\n",
       "    'acc_stderr': 0.025310495376944856,\n",
       "    'acc_norm': 0.8143459915611815,\n",
       "    'acc_norm_stderr': 0.025310495376944856},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7040358744394619,\n",
       "    'acc_stderr': 0.030636591348699803,\n",
       "    'acc_norm': 0.7040358744394619,\n",
       "    'acc_norm_stderr': 0.030636591348699803},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7938931297709924,\n",
       "    'acc_stderr': 0.035477710041594654,\n",
       "    'acc_norm': 0.7938931297709924,\n",
       "    'acc_norm_stderr': 0.035477710041594654},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7603305785123967,\n",
       "    'acc_stderr': 0.03896878985070416,\n",
       "    'acc_norm': 0.7603305785123967,\n",
       "    'acc_norm_stderr': 0.03896878985070416},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7870370370370371,\n",
       "    'acc_stderr': 0.039578354719809805,\n",
       "    'acc_norm': 0.7870370370370371,\n",
       "    'acc_norm_stderr': 0.039578354719809805},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7852760736196319,\n",
       "    'acc_stderr': 0.032262193772867744,\n",
       "    'acc_norm': 0.7852760736196319,\n",
       "    'acc_norm_stderr': 0.032262193772867744},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.5089285714285714,\n",
       "    'acc_stderr': 0.04745033255489123,\n",
       "    'acc_norm': 0.5089285714285714,\n",
       "    'acc_norm_stderr': 0.04745033255489123},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7766990291262136,\n",
       "    'acc_stderr': 0.04123553189891431,\n",
       "    'acc_norm': 0.7766990291262136,\n",
       "    'acc_norm_stderr': 0.04123553189891431},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.022801382534597528,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.022801382534597528},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8301404853128991,\n",
       "    'acc_stderr': 0.013428186370608306,\n",
       "    'acc_norm': 0.8301404853128991,\n",
       "    'acc_norm_stderr': 0.013428186370608306},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7167630057803468,\n",
       "    'acc_stderr': 0.02425790170532338,\n",
       "    'acc_norm': 0.7167630057803468,\n",
       "    'acc_norm_stderr': 0.02425790170532338},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30837988826815643,\n",
       "    'acc_stderr': 0.01544571691099888,\n",
       "    'acc_norm': 0.30837988826815643,\n",
       "    'acc_norm_stderr': 0.01544571691099888},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7549019607843137,\n",
       "    'acc_stderr': 0.024630048979824782,\n",
       "    'acc_norm': 0.7549019607843137,\n",
       "    'acc_norm_stderr': 0.024630048979824782},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.684887459807074,\n",
       "    'acc_stderr': 0.026385273703464485,\n",
       "    'acc_norm': 0.684887459807074,\n",
       "    'acc_norm_stderr': 0.026385273703464485},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7530864197530864,\n",
       "    'acc_stderr': 0.02399350170904211,\n",
       "    'acc_norm': 0.7530864197530864,\n",
       "    'acc_norm_stderr': 0.02399350170904211},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5070921985815603,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.5070921985815603,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.46936114732724904,\n",
       "    'acc_stderr': 0.012746237711716634,\n",
       "    'acc_norm': 0.46936114732724904,\n",
       "    'acc_norm_stderr': 0.012746237711716634},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.028418208619406762,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.028418208619406762},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.673202614379085,\n",
       "    'acc_stderr': 0.018975427920507215,\n",
       "    'acc_norm': 0.673202614379085,\n",
       "    'acc_norm_stderr': 0.018975427920507215},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.028263889943784596,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.028263889943784596},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8159203980099502,\n",
       "    'acc_stderr': 0.027403859410786845,\n",
       "    'acc_norm': 0.8159203980099502,\n",
       "    'acc_norm_stderr': 0.027403859410786845},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.033799766898963086,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.033799766898963086},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5542168674698795,\n",
       "    'acc_stderr': 0.03869543323472101,\n",
       "    'acc_norm': 0.5542168674698795,\n",
       "    'acc_norm_stderr': 0.03869543323472101},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3598531211750306,\n",
       "    'mc1_stderr': 0.01680186046667716,\n",
       "    'mc2': 0.5230564118100686,\n",
       "    'mc2_stderr': 0.015250230546286025},\n",
       "   'harness|winogrande|5': {'acc': 0.7790055248618785,\n",
       "    'acc_stderr': 0.011661223637643412},\n",
       "   'harness|drop|3': {'em': 0.30830536912751677,\n",
       "    'em_stderr': 0.004729196914949925,\n",
       "    'f1': 0.35989723154362524,\n",
       "    'f1_stderr': 0.004629324720589026},\n",
       "   'harness|gsm8k|5': {'acc': 0.25473843821076575,\n",
       "    'acc_stderr': 0.012001731232879136},\n",
       "   'all': {'acc': 0.6340440103659418,\n",
       "    'acc_stderr': 0.03220763540877311,\n",
       "    'acc_norm': 0.6418750491228201,\n",
       "    'acc_norm_stderr': 0.032874386009418256,\n",
       "    'mc1': 0.3598531211750306,\n",
       "    'mc1_stderr': 0.01680186046667716,\n",
       "    'mc2': 0.5230564118100686,\n",
       "    'mc2_stderr': 0.015250230546286025,\n",
       "    'em': 0.30830536912751677,\n",
       "    'em_stderr': 0.004729196914949925,\n",
       "    'f1': 0.35989723154362524,\n",
       "    'f1_stderr': 0.004629324720589026}}},\n",
       " 'solar-10.7b-instruct-v1.0': {'key': 'solar-10.7b-instruct-v1.0',\n",
       "  'Model': 'SOLAR-10.7B-Instruct-v1.0',\n",
       "  'MT-bench (score)': '7.58',\n",
       "  'MMLU': '0.662',\n",
       "  'License': 'CC-BY-NC-4.0',\n",
       "  'Organization': 'Upstage AI',\n",
       "  'Link': 'https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6808873720136519,\n",
       "    'acc_stderr': 0.013621696119173307,\n",
       "    'acc_norm': 0.7107508532423208,\n",
       "    'acc_norm_stderr': 0.01325001257939344},\n",
       "   'harness|hellaswag|10': {'acc': 0.7070304720175263,\n",
       "    'acc_stderr': 0.004541944342035901,\n",
       "    'acc_norm': 0.8815972913762199,\n",
       "    'acc_norm_stderr': 0.003224240722351317},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.7368421052631579,\n",
       "    'acc_stderr': 0.03583496176361072,\n",
       "    'acc_norm': 0.7368421052631579,\n",
       "    'acc_norm_stderr': 0.03583496176361072},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.02872750295788027,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.02872750295788027},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7638888888888888,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.7638888888888888,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6647398843930635,\n",
       "    'acc_stderr': 0.03599586301247077,\n",
       "    'acc_norm': 0.6647398843930635,\n",
       "    'acc_norm_stderr': 0.03599586301247077},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107223,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107223},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.6297872340425532,\n",
       "    'acc_stderr': 0.03156564682236785,\n",
       "    'acc_norm': 0.6297872340425532,\n",
       "    'acc_norm_stderr': 0.03156564682236785},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.047036043419179864,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.047036043419179864},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.6413793103448275,\n",
       "    'acc_stderr': 0.039966295748767186,\n",
       "    'acc_norm': 0.6413793103448275,\n",
       "    'acc_norm_stderr': 0.039966295748767186},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.47883597883597884,\n",
       "    'acc_stderr': 0.025728230952130726,\n",
       "    'acc_norm': 0.47883597883597884,\n",
       "    'acc_norm_stderr': 0.025728230952130726},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.044444444444444495,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.044444444444444495},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.8032258064516129,\n",
       "    'acc_stderr': 0.022616409420742025,\n",
       "    'acc_norm': 0.8032258064516129,\n",
       "    'acc_norm_stderr': 0.022616409420742025},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.03515895551165698,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.03515895551165698},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.031234752377721175,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.031234752377721175},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8737373737373737,\n",
       "    'acc_stderr': 0.02366435940288023,\n",
       "    'acc_norm': 0.8737373737373737,\n",
       "    'acc_norm_stderr': 0.02366435940288023},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9067357512953368,\n",
       "    'acc_stderr': 0.02098685459328973,\n",
       "    'acc_norm': 0.9067357512953368,\n",
       "    'acc_norm_stderr': 0.02098685459328973},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6615384615384615,\n",
       "    'acc_stderr': 0.023991500500313036,\n",
       "    'acc_norm': 0.6615384615384615,\n",
       "    'acc_norm_stderr': 0.023991500500313036},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3814814814814815,\n",
       "    'acc_stderr': 0.029616718927497593,\n",
       "    'acc_norm': 0.3814814814814815,\n",
       "    'acc_norm_stderr': 0.029616718927497593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7184873949579832,\n",
       "    'acc_stderr': 0.02921354941437217,\n",
       "    'acc_norm': 0.7184873949579832,\n",
       "    'acc_norm_stderr': 0.02921354941437217},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3708609271523179,\n",
       "    'acc_stderr': 0.03943966699183629,\n",
       "    'acc_norm': 0.3708609271523179,\n",
       "    'acc_norm_stderr': 0.03943966699183629},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.03388857118502325,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.03388857118502325},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8480392156862745,\n",
       "    'acc_stderr': 0.0251956584289318,\n",
       "    'acc_norm': 0.8480392156862745,\n",
       "    'acc_norm_stderr': 0.0251956584289318},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8565400843881856,\n",
       "    'acc_stderr': 0.022818291821017012,\n",
       "    'acc_norm': 0.8565400843881856,\n",
       "    'acc_norm_stderr': 0.022818291821017012},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6816143497757847,\n",
       "    'acc_stderr': 0.03126580522513713,\n",
       "    'acc_norm': 0.6816143497757847,\n",
       "    'acc_norm_stderr': 0.03126580522513713},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7480916030534351,\n",
       "    'acc_stderr': 0.03807387116306086,\n",
       "    'acc_norm': 0.7480916030534351,\n",
       "    'acc_norm_stderr': 0.03807387116306086},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7768595041322314,\n",
       "    'acc_stderr': 0.03800754475228733,\n",
       "    'acc_norm': 0.7768595041322314,\n",
       "    'acc_norm_stderr': 0.03800754475228733},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8055555555555556,\n",
       "    'acc_stderr': 0.038260763248848646,\n",
       "    'acc_norm': 0.8055555555555556,\n",
       "    'acc_norm_stderr': 0.038260763248848646},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.754601226993865,\n",
       "    'acc_stderr': 0.03380939813943354,\n",
       "    'acc_norm': 0.754601226993865,\n",
       "    'acc_norm_stderr': 0.03380939813943354},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.44642857142857145,\n",
       "    'acc_stderr': 0.047184714852195886,\n",
       "    'acc_norm': 0.44642857142857145,\n",
       "    'acc_norm_stderr': 0.047184714852195886},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8252427184466019,\n",
       "    'acc_stderr': 0.03760178006026621,\n",
       "    'acc_norm': 0.8252427184466019,\n",
       "    'acc_norm_stderr': 0.03760178006026621},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.02280138253459753,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.02280138253459753},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8033205619412516,\n",
       "    'acc_stderr': 0.014214138556913917,\n",
       "    'acc_norm': 0.8033205619412516,\n",
       "    'acc_norm_stderr': 0.014214138556913917},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7601156069364162,\n",
       "    'acc_stderr': 0.022989592543123567,\n",
       "    'acc_norm': 0.7601156069364162,\n",
       "    'acc_norm_stderr': 0.022989592543123567},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.39329608938547483,\n",
       "    'acc_stderr': 0.016337268694270112,\n",
       "    'acc_norm': 0.39329608938547483,\n",
       "    'acc_norm_stderr': 0.016337268694270112},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7581699346405228,\n",
       "    'acc_stderr': 0.024518195641879334,\n",
       "    'acc_norm': 0.7581699346405228,\n",
       "    'acc_norm_stderr': 0.024518195641879334},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.729903536977492,\n",
       "    'acc_stderr': 0.02521804037341062,\n",
       "    'acc_norm': 0.729903536977492,\n",
       "    'acc_norm_stderr': 0.02521804037341062},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7901234567901234,\n",
       "    'acc_stderr': 0.02265834408598137,\n",
       "    'acc_norm': 0.7901234567901234,\n",
       "    'acc_norm_stderr': 0.02265834408598137},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.49645390070921985,\n",
       "    'acc_stderr': 0.02982674915328092,\n",
       "    'acc_norm': 0.49645390070921985,\n",
       "    'acc_norm_stderr': 0.02982674915328092},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4934810951760104,\n",
       "    'acc_stderr': 0.012769150688867503,\n",
       "    'acc_norm': 0.4934810951760104,\n",
       "    'acc_norm_stderr': 0.012769150688867503},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.7389705882352942,\n",
       "    'acc_stderr': 0.026679252270103135,\n",
       "    'acc_norm': 0.7389705882352942,\n",
       "    'acc_norm_stderr': 0.026679252270103135},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.018690850273595294,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.018690850273595294},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.0282638899437846,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.0282638899437846},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8407960199004975,\n",
       "    'acc_stderr': 0.02587064676616913,\n",
       "    'acc_norm': 0.8407960199004975,\n",
       "    'acc_norm_stderr': 0.02587064676616913},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.9,\n",
       "    'acc_stderr': 0.030151134457776334,\n",
       "    'acc_norm': 0.9,\n",
       "    'acc_norm_stderr': 0.030151134457776334},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5843373493975904,\n",
       "    'acc_stderr': 0.03836722176598052,\n",
       "    'acc_norm': 0.5843373493975904,\n",
       "    'acc_norm_stderr': 0.03836722176598052},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7894736842105263,\n",
       "    'acc_stderr': 0.03126781714663179,\n",
       "    'acc_norm': 0.7894736842105263,\n",
       "    'acc_norm_stderr': 0.03126781714663179},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.5667074663402693,\n",
       "    'mc1_stderr': 0.017347024450107485,\n",
       "    'mc2': 0.7142943510205136,\n",
       "    'mc2_stderr': 0.015024530295000761},\n",
       "   'harness|winogrande|5': {'acc': 0.8358326756116812,\n",
       "    'acc_stderr': 0.01041084977522279},\n",
       "   'harness|gsm8k|5': {'acc': 0.6474601971190296,\n",
       "    'acc_stderr': 0.013159909755930337},\n",
       "   'all': {'acc': 0.6657586984797939,\n",
       "    'acc_stderr': 0.03165995758526614,\n",
       "    'acc_norm': 0.6666511531376961,\n",
       "    'acc_norm_stderr': 0.0323050384069596,\n",
       "    'mc1': 0.5667074663402693,\n",
       "    'mc1_stderr': 0.017347024450107485,\n",
       "    'mc2': 0.7142943510205136,\n",
       "    'mc2_stderr': 0.015024530295000761}}},\n",
       " 'stablelm-tuned-alpha-7b': {'key': 'stablelm-tuned-alpha-7b',\n",
       "  'Model': 'StableLM-Tuned-Alpha-7B',\n",
       "  'MT-bench (score)': '2.75',\n",
       "  'MMLU': '0.244',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'Stability AI',\n",
       "  'Link': 'https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b',\n",
       "  'results': {'harness|drop|3': {'em': 0.0041946308724832215,\n",
       "    'em_stderr': 0.0006618716168266466,\n",
       "    'f1': 0.05621224832214779,\n",
       "    'f1_stderr': 0.0014117433231649174},\n",
       "   'harness|gsm8k|5': {'acc': 0.008339651250947688,\n",
       "    'acc_stderr': 0.002504942226860537},\n",
       "   'harness|winogrande|5': {'acc': 0.5311760063141279,\n",
       "    'acc_stderr': 0.014025142640639513},\n",
       "   'all': {'acc': 0.2479374638777667,\n",
       "    'acc_stderr': 0.03127884661282089,\n",
       "    'acc_norm': 0.2503417754412081,\n",
       "    'acc_norm_stderr': 0.031283748741553784,\n",
       "    'mc1': 0.23990208078335373,\n",
       "    'mc1_stderr': 0.014948812679062137,\n",
       "    'mc2': 0.4037161827493261,\n",
       "    'mc2_stderr': 0.014483731026265276},\n",
       "   'harness|arc:challenge|25': {'acc': 0.3003412969283277,\n",
       "    'acc_stderr': 0.013395909309956997,\n",
       "    'acc_norm': 0.3191126279863481,\n",
       "    'acc_norm_stderr': 0.0136216961191733},\n",
       "   'harness|hellaswag|10': {'acc': 0.41286596295558653,\n",
       "    'acc_stderr': 0.004913429010559072,\n",
       "    'acc_norm': 0.5359490141406095,\n",
       "    'acc_norm_stderr': 0.004976867796583554},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.26666666666666666,\n",
       "    'acc_stderr': 0.03820169914517904,\n",
       "    'acc_norm': 0.26666666666666666,\n",
       "    'acc_norm_stderr': 0.03820169914517904},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.17105263157894737,\n",
       "    'acc_stderr': 0.030643607071677088,\n",
       "    'acc_norm': 0.17105263157894737,\n",
       "    'acc_norm_stderr': 0.030643607071677088},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.24,\n",
       "    'acc_stderr': 0.04292346959909281,\n",
       "    'acc_norm': 0.24,\n",
       "    'acc_norm_stderr': 0.04292346959909281},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.27547169811320754,\n",
       "    'acc_stderr': 0.027495663683724064,\n",
       "    'acc_norm': 0.27547169811320754,\n",
       "    'acc_norm_stderr': 0.027495663683724064},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2361111111111111,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.2361111111111111,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.041633319989322695,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.041633319989322695},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.23121387283236994,\n",
       "    'acc_stderr': 0.0321473730202947,\n",
       "    'acc_norm': 0.23121387283236994,\n",
       "    'acc_norm_stderr': 0.0321473730202947},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2549019607843137,\n",
       "    'acc_stderr': 0.04336432707993176,\n",
       "    'acc_norm': 0.2549019607843137,\n",
       "    'acc_norm_stderr': 0.04336432707993176},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.3148936170212766,\n",
       "    'acc_stderr': 0.03036358219723816,\n",
       "    'acc_norm': 0.3148936170212766,\n",
       "    'acc_norm_stderr': 0.03036358219723816},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.21052631578947367,\n",
       "    'acc_stderr': 0.038351539543994194,\n",
       "    'acc_norm': 0.21052631578947367,\n",
       "    'acc_norm_stderr': 0.038351539543994194},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.27586206896551724,\n",
       "    'acc_stderr': 0.037245636197746325,\n",
       "    'acc_norm': 0.27586206896551724,\n",
       "    'acc_norm_stderr': 0.037245636197746325},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.24338624338624337,\n",
       "    'acc_stderr': 0.022101128787415426,\n",
       "    'acc_norm': 0.24338624338624337,\n",
       "    'acc_norm_stderr': 0.022101128787415426},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.24603174603174602,\n",
       "    'acc_stderr': 0.03852273364924316,\n",
       "    'acc_norm': 0.24603174603174602,\n",
       "    'acc_norm_stderr': 0.03852273364924316},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.19,\n",
       "    'acc_stderr': 0.03942772444036624,\n",
       "    'acc_norm': 0.19,\n",
       "    'acc_norm_stderr': 0.03942772444036624},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.2129032258064516,\n",
       "    'acc_stderr': 0.023287665127268525,\n",
       "    'acc_norm': 0.2129032258064516,\n",
       "    'acc_norm_stderr': 0.023287665127268525},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.18226600985221675,\n",
       "    'acc_stderr': 0.02716334085964515,\n",
       "    'acc_norm': 0.18226600985221675,\n",
       "    'acc_norm_stderr': 0.02716334085964515},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.04163331998932269,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.04163331998932269},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.22424242424242424,\n",
       "    'acc_stderr': 0.03256866661681102,\n",
       "    'acc_norm': 0.22424242424242424,\n",
       "    'acc_norm_stderr': 0.03256866661681102},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.18686868686868688,\n",
       "    'acc_stderr': 0.027772533334218977,\n",
       "    'acc_norm': 0.18686868686868688,\n",
       "    'acc_norm_stderr': 0.027772533334218977},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.21243523316062177,\n",
       "    'acc_stderr': 0.029519282616817254,\n",
       "    'acc_norm': 0.21243523316062177,\n",
       "    'acc_norm_stderr': 0.029519282616817254},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.2743589743589744,\n",
       "    'acc_stderr': 0.022622765767493218,\n",
       "    'acc_norm': 0.2743589743589744,\n",
       "    'acc_norm_stderr': 0.022622765767493218},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.25925925925925924,\n",
       "    'acc_stderr': 0.026719240783712163,\n",
       "    'acc_norm': 0.25925925925925924,\n",
       "    'acc_norm_stderr': 0.026719240783712163},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.19747899159663865,\n",
       "    'acc_stderr': 0.02585916412205145,\n",
       "    'acc_norm': 0.19747899159663865,\n",
       "    'acc_norm_stderr': 0.02585916412205145},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2185430463576159,\n",
       "    'acc_stderr': 0.03374235550425694,\n",
       "    'acc_norm': 0.2185430463576159,\n",
       "    'acc_norm_stderr': 0.03374235550425694},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.22935779816513763,\n",
       "    'acc_stderr': 0.018025349724618684,\n",
       "    'acc_norm': 0.22935779816513763,\n",
       "    'acc_norm_stderr': 0.018025349724618684},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.35185185185185186,\n",
       "    'acc_stderr': 0.03256850570293647,\n",
       "    'acc_norm': 0.35185185185185186,\n",
       "    'acc_norm_stderr': 0.03256850570293647},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.24019607843137256,\n",
       "    'acc_stderr': 0.02998373305591361,\n",
       "    'acc_norm': 0.24019607843137256,\n",
       "    'acc_norm_stderr': 0.02998373305591361},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.2869198312236287,\n",
       "    'acc_stderr': 0.02944377302259469,\n",
       "    'acc_norm': 0.2869198312236287,\n",
       "    'acc_norm_stderr': 0.02944377302259469},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.3542600896860987,\n",
       "    'acc_stderr': 0.03210062154134987,\n",
       "    'acc_norm': 0.3542600896860987,\n",
       "    'acc_norm_stderr': 0.03210062154134987},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.2900763358778626,\n",
       "    'acc_stderr': 0.03980066246467765,\n",
       "    'acc_norm': 0.2900763358778626,\n",
       "    'acc_norm_stderr': 0.03980066246467765},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.2396694214876033,\n",
       "    'acc_stderr': 0.03896878985070417,\n",
       "    'acc_norm': 0.2396694214876033,\n",
       "    'acc_norm_stderr': 0.03896878985070417},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.28703703703703703,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.28703703703703703,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.27607361963190186,\n",
       "    'acc_stderr': 0.0351238528370505,\n",
       "    'acc_norm': 0.27607361963190186,\n",
       "    'acc_norm_stderr': 0.0351238528370505},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.24107142857142858,\n",
       "    'acc_stderr': 0.04059867246952687,\n",
       "    'acc_norm': 0.24107142857142858,\n",
       "    'acc_norm_stderr': 0.04059867246952687},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.1650485436893204,\n",
       "    'acc_stderr': 0.036756688322331886,\n",
       "    'acc_norm': 0.1650485436893204,\n",
       "    'acc_norm_stderr': 0.036756688322331886},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.24786324786324787,\n",
       "    'acc_stderr': 0.028286324075564397,\n",
       "    'acc_norm': 0.24786324786324787,\n",
       "    'acc_norm_stderr': 0.028286324075564397},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.23499361430395913,\n",
       "    'acc_stderr': 0.015162024152278434,\n",
       "    'acc_norm': 0.23499361430395913,\n",
       "    'acc_norm_stderr': 0.015162024152278434},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.2254335260115607,\n",
       "    'acc_stderr': 0.02249723019096756,\n",
       "    'acc_norm': 0.2254335260115607,\n",
       "    'acc_norm_stderr': 0.02249723019096756},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961459,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961459},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.25163398692810457,\n",
       "    'acc_stderr': 0.024848018263875195,\n",
       "    'acc_norm': 0.25163398692810457,\n",
       "    'acc_norm_stderr': 0.024848018263875195},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.21864951768488747,\n",
       "    'acc_stderr': 0.023475581417861106,\n",
       "    'acc_norm': 0.21864951768488747,\n",
       "    'acc_norm_stderr': 0.023475581417861106},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.2222222222222222,\n",
       "    'acc_stderr': 0.023132376234543325,\n",
       "    'acc_norm': 0.2222222222222222,\n",
       "    'acc_norm_stderr': 0.023132376234543325},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.24822695035460993,\n",
       "    'acc_stderr': 0.025770015644290403,\n",
       "    'acc_norm': 0.24822695035460993,\n",
       "    'acc_norm_stderr': 0.025770015644290403},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.22946544980443284,\n",
       "    'acc_stderr': 0.010739489382279503,\n",
       "    'acc_norm': 0.22946544980443284,\n",
       "    'acc_norm_stderr': 0.010739489382279503},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.16544117647058823,\n",
       "    'acc_stderr': 0.022571771025494767,\n",
       "    'acc_norm': 0.16544117647058823,\n",
       "    'acc_norm_stderr': 0.022571771025494767},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.25163398692810457,\n",
       "    'acc_stderr': 0.017555818091322263,\n",
       "    'acc_norm': 0.25163398692810457,\n",
       "    'acc_norm_stderr': 0.017555818091322263},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.23636363636363636,\n",
       "    'acc_stderr': 0.04069306319721378,\n",
       "    'acc_norm': 0.23636363636363636,\n",
       "    'acc_norm_stderr': 0.04069306319721378},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.1673469387755102,\n",
       "    'acc_stderr': 0.023897144768914524,\n",
       "    'acc_norm': 0.1673469387755102,\n",
       "    'acc_norm_stderr': 0.023897144768914524},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.263681592039801,\n",
       "    'acc_stderr': 0.03115715086935558,\n",
       "    'acc_norm': 0.263681592039801,\n",
       "    'acc_norm_stderr': 0.03115715086935558},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.24,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.24,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.2891566265060241,\n",
       "    'acc_stderr': 0.03529486801511115,\n",
       "    'acc_norm': 0.2891566265060241,\n",
       "    'acc_norm_stderr': 0.03529486801511115},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.24561403508771928,\n",
       "    'acc_stderr': 0.03301405946987249,\n",
       "    'acc_norm': 0.24561403508771928,\n",
       "    'acc_norm_stderr': 0.03301405946987249},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.23990208078335373,\n",
       "    'mc1_stderr': 0.014948812679062137,\n",
       "    'mc2': 0.4037161827493261,\n",
       "    'mc2_stderr': 0.014483731026265276}}},\n",
       " 'starling-lm-7b-alpha': {'key': 'starling-lm-7b-alpha',\n",
       "  'Model': 'Starling-LM-7B-alpha',\n",
       "  'MT-bench (score)': '8.09',\n",
       "  'MMLU': '0.639',\n",
       "  'License': 'CC-BY-NC-4.0',\n",
       "  'Organization': 'UC Berkeley',\n",
       "  'Link': 'https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5998293515358362,\n",
       "    'acc_stderr': 0.014317197787809172,\n",
       "    'acc_norm': 0.6382252559726962,\n",
       "    'acc_norm_stderr': 0.01404195794503808},\n",
       "   'harness|hellaswag|10': {'acc': 0.665803624775941,\n",
       "    'acc_stderr': 0.004707447244200621,\n",
       "    'acc_norm': 0.8490340569607648,\n",
       "    'acc_norm_stderr': 0.0035728399695219874},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6907894736842105,\n",
       "    'acc_stderr': 0.037610708698674805,\n",
       "    'acc_norm': 0.6907894736842105,\n",
       "    'acc_norm_stderr': 0.037610708698674805},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.63,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.63,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.028544793319055326,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.028544793319055326},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7708333333333334,\n",
       "    'acc_stderr': 0.035146974678623884,\n",
       "    'acc_norm': 0.7708333333333334,\n",
       "    'acc_norm_stderr': 0.035146974678623884},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6589595375722543,\n",
       "    'acc_stderr': 0.03614665424180826,\n",
       "    'acc_norm': 0.6589595375722543,\n",
       "    'acc_norm_stderr': 0.03614665424180826},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4411764705882353,\n",
       "    'acc_stderr': 0.049406356306056595,\n",
       "    'acc_norm': 0.4411764705882353,\n",
       "    'acc_norm_stderr': 0.049406356306056595},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768079,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768079},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5531914893617021,\n",
       "    'acc_stderr': 0.032500536843658404,\n",
       "    'acc_norm': 0.5531914893617021,\n",
       "    'acc_norm_stderr': 0.032500536843658404},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.49122807017543857,\n",
       "    'acc_stderr': 0.04702880432049615,\n",
       "    'acc_norm': 0.49122807017543857,\n",
       "    'acc_norm_stderr': 0.04702880432049615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370332,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370332},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.41005291005291006,\n",
       "    'acc_stderr': 0.02533120243894444,\n",
       "    'acc_norm': 0.41005291005291006,\n",
       "    'acc_norm_stderr': 0.02533120243894444},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5238095238095238,\n",
       "    'acc_stderr': 0.04467062628403273,\n",
       "    'acc_norm': 0.5238095238095238,\n",
       "    'acc_norm_stderr': 0.04467062628403273},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.04163331998932269,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.04163331998932269},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7903225806451613,\n",
       "    'acc_stderr': 0.023157879349083525,\n",
       "    'acc_norm': 0.7903225806451613,\n",
       "    'acc_norm_stderr': 0.023157879349083525},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.46798029556650245,\n",
       "    'acc_stderr': 0.035107665979592154,\n",
       "    'acc_norm': 0.46798029556650245,\n",
       "    'acc_norm_stderr': 0.035107665979592154},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7636363636363637,\n",
       "    'acc_stderr': 0.03317505930009181,\n",
       "    'acc_norm': 0.7636363636363637,\n",
       "    'acc_norm_stderr': 0.03317505930009181},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.797979797979798,\n",
       "    'acc_stderr': 0.028606204289229865,\n",
       "    'acc_norm': 0.797979797979798,\n",
       "    'acc_norm_stderr': 0.028606204289229865},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9119170984455959,\n",
       "    'acc_stderr': 0.02045374660160103,\n",
       "    'acc_norm': 0.9119170984455959,\n",
       "    'acc_norm_stderr': 0.02045374660160103},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.676923076923077,\n",
       "    'acc_stderr': 0.02371088850197057,\n",
       "    'acc_norm': 0.676923076923077,\n",
       "    'acc_norm_stderr': 0.02371088850197057},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028593,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.680672268907563,\n",
       "    'acc_stderr': 0.0302839955258844,\n",
       "    'acc_norm': 0.680672268907563,\n",
       "    'acc_norm_stderr': 0.0302839955258844},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.37748344370860926,\n",
       "    'acc_stderr': 0.03958027231121569,\n",
       "    'acc_norm': 0.37748344370860926,\n",
       "    'acc_norm_stderr': 0.03958027231121569},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.034099716973523674,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.034099716973523674},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8235294117647058,\n",
       "    'acc_stderr': 0.026756401538078966,\n",
       "    'acc_norm': 0.8235294117647058,\n",
       "    'acc_norm_stderr': 0.026756401538078966},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8270042194092827,\n",
       "    'acc_stderr': 0.024621562866768434,\n",
       "    'acc_norm': 0.8270042194092827,\n",
       "    'acc_norm_stderr': 0.024621562866768434},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7130044843049327,\n",
       "    'acc_stderr': 0.030360379710291947,\n",
       "    'acc_norm': 0.7130044843049327,\n",
       "    'acc_norm_stderr': 0.030360379710291947},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7862595419847328,\n",
       "    'acc_stderr': 0.0359546161177469,\n",
       "    'acc_norm': 0.7862595419847328,\n",
       "    'acc_norm_stderr': 0.0359546161177469},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8264462809917356,\n",
       "    'acc_stderr': 0.0345727283691767,\n",
       "    'acc_norm': 0.8264462809917356,\n",
       "    'acc_norm_stderr': 0.0345727283691767},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7407407407407407,\n",
       "    'acc_stderr': 0.042365112580946336,\n",
       "    'acc_norm': 0.7407407407407407,\n",
       "    'acc_norm_stderr': 0.042365112580946336},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7730061349693251,\n",
       "    'acc_stderr': 0.03291099578615769,\n",
       "    'acc_norm': 0.7730061349693251,\n",
       "    'acc_norm_stderr': 0.03291099578615769},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.41964285714285715,\n",
       "    'acc_stderr': 0.04684099321077106,\n",
       "    'acc_norm': 0.41964285714285715,\n",
       "    'acc_norm_stderr': 0.04684099321077106},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8446601941747572,\n",
       "    'acc_stderr': 0.03586594738573973,\n",
       "    'acc_norm': 0.8446601941747572,\n",
       "    'acc_norm_stderr': 0.03586594738573973},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8760683760683761,\n",
       "    'acc_stderr': 0.02158649400128138,\n",
       "    'acc_norm': 0.8760683760683761,\n",
       "    'acc_norm_stderr': 0.02158649400128138},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.0446196043338474,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.0446196043338474},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8263090676883781,\n",
       "    'acc_stderr': 0.01354741565866226,\n",
       "    'acc_norm': 0.8263090676883781,\n",
       "    'acc_norm_stderr': 0.01354741565866226},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7369942196531792,\n",
       "    'acc_stderr': 0.023703099525258176,\n",
       "    'acc_norm': 0.7369942196531792,\n",
       "    'acc_norm_stderr': 0.023703099525258176},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.47039106145251397,\n",
       "    'acc_stderr': 0.016693154927383557,\n",
       "    'acc_norm': 0.47039106145251397,\n",
       "    'acc_norm_stderr': 0.016693154927383557},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7352941176470589,\n",
       "    'acc_stderr': 0.025261691219729484,\n",
       "    'acc_norm': 0.7352941176470589,\n",
       "    'acc_norm_stderr': 0.025261691219729484},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6977491961414791,\n",
       "    'acc_stderr': 0.02608270069539966,\n",
       "    'acc_norm': 0.6977491961414791,\n",
       "    'acc_norm_stderr': 0.02608270069539966},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7345679012345679,\n",
       "    'acc_stderr': 0.024569223600460845,\n",
       "    'acc_norm': 0.7345679012345679,\n",
       "    'acc_norm_stderr': 0.024569223600460845},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4645390070921986,\n",
       "    'acc_stderr': 0.029752389657427047,\n",
       "    'acc_norm': 0.4645390070921986,\n",
       "    'acc_norm_stderr': 0.029752389657427047},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4641460234680574,\n",
       "    'acc_stderr': 0.012737361318730581,\n",
       "    'acc_norm': 0.4641460234680574,\n",
       "    'acc_norm_stderr': 0.012737361318730581},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.028064998167040094,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.028064998167040094},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6633986928104575,\n",
       "    'acc_stderr': 0.019117213911495144,\n",
       "    'acc_norm': 0.6633986928104575,\n",
       "    'acc_norm_stderr': 0.019117213911495144},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.726530612244898,\n",
       "    'acc_stderr': 0.028535560337128448,\n",
       "    'acc_norm': 0.726530612244898,\n",
       "    'acc_norm_stderr': 0.028535560337128448},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.845771144278607,\n",
       "    'acc_stderr': 0.025538433368578334,\n",
       "    'acc_norm': 0.845771144278607,\n",
       "    'acc_norm_stderr': 0.025538433368578334},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.88,\n",
       "    'acc_stderr': 0.032659863237109066,\n",
       "    'acc_norm': 0.88,\n",
       "    'acc_norm_stderr': 0.032659863237109066},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5301204819277109,\n",
       "    'acc_stderr': 0.03885425420866767,\n",
       "    'acc_norm': 0.5301204819277109,\n",
       "    'acc_norm_stderr': 0.03885425420866767},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.847953216374269,\n",
       "    'acc_stderr': 0.027539122889061456,\n",
       "    'acc_norm': 0.847953216374269,\n",
       "    'acc_norm_stderr': 0.027539122889061456},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3047735618115055,\n",
       "    'mc1_stderr': 0.01611412415688245,\n",
       "    'mc2': 0.463936332301049,\n",
       "    'mc2_stderr': 0.015153266555511496},\n",
       "   'harness|winogrande|5': {'acc': 0.8058405682715075,\n",
       "    'acc_stderr': 0.01111698339239267},\n",
       "   'harness|drop|3': {'em': 0.0012583892617449664,\n",
       "    'em_stderr': 0.0003630560893118993,\n",
       "    'f1': 0.07203124999999957,\n",
       "    'f1_stderr': 0.0014672437300568956},\n",
       "   'harness|gsm8k|5': {'acc': 0.623199393479909,\n",
       "    'acc_stderr': 0.013347858757829158},\n",
       "   'all': {'acc': 0.623199393479909, 'acc_stderr': 0.013347858757829158}}},\n",
       " 'vicuna-13b': {'key': 'vicuna-13b',\n",
       "  'Model': 'Vicuna-13B',\n",
       "  'MT-bench (score)': '6.57',\n",
       "  'MMLU': '0.558',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-13b-v1.5',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5349829351535836,\n",
       "    'acc_stderr': 0.014575583922019672,\n",
       "    'acc_norm': 0.5656996587030717,\n",
       "    'acc_norm_stderr': 0.014484703048857357},\n",
       "   'harness|hellaswag|10': {'acc': 0.6115315674168492,\n",
       "    'acc_stderr': 0.004864058877626272,\n",
       "    'acc_norm': 0.8108942441744672,\n",
       "    'acc_norm_stderr': 0.0039079230108406094},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542129,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542129},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4888888888888889,\n",
       "    'acc_stderr': 0.04318275491977976,\n",
       "    'acc_norm': 0.4888888888888889,\n",
       "    'acc_norm_stderr': 0.04318275491977976},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5855263157894737,\n",
       "    'acc_stderr': 0.04008973785779205,\n",
       "    'acc_norm': 0.5855263157894737,\n",
       "    'acc_norm_stderr': 0.04008973785779205},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.630188679245283,\n",
       "    'acc_stderr': 0.02971142188010793,\n",
       "    'acc_norm': 0.630188679245283,\n",
       "    'acc_norm_stderr': 0.02971142188010793},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5972222222222222,\n",
       "    'acc_stderr': 0.04101405519842426,\n",
       "    'acc_norm': 0.5972222222222222,\n",
       "    'acc_norm_stderr': 0.04101405519842426},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5433526011560693,\n",
       "    'acc_stderr': 0.03798106566014498,\n",
       "    'acc_norm': 0.5433526011560693,\n",
       "    'acc_norm_stderr': 0.03798106566014498},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3235294117647059,\n",
       "    'acc_stderr': 0.046550104113196156,\n",
       "    'acc_norm': 0.3235294117647059,\n",
       "    'acc_norm_stderr': 0.046550104113196156},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.451063829787234,\n",
       "    'acc_stderr': 0.032529096196131965,\n",
       "    'acc_norm': 0.451063829787234,\n",
       "    'acc_norm_stderr': 0.032529096196131965},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2894736842105263,\n",
       "    'acc_stderr': 0.04266339443159394,\n",
       "    'acc_norm': 0.2894736842105263,\n",
       "    'acc_norm_stderr': 0.04266339443159394},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.47586206896551725,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.47586206896551725,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3306878306878307,\n",
       "    'acc_stderr': 0.024229965298425075,\n",
       "    'acc_norm': 0.3306878306878307,\n",
       "    'acc_norm_stderr': 0.024229965298425075},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.04240799327574924,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.04240799327574924},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001974,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001974},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6774193548387096,\n",
       "    'acc_stderr': 0.02659308451657227,\n",
       "    'acc_norm': 0.6774193548387096,\n",
       "    'acc_norm_stderr': 0.02659308451657227},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4482758620689655,\n",
       "    'acc_stderr': 0.034991131376767445,\n",
       "    'acc_norm': 0.4482758620689655,\n",
       "    'acc_norm_stderr': 0.034991131376767445},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.696969696969697,\n",
       "    'acc_stderr': 0.03588624800091707,\n",
       "    'acc_norm': 0.696969696969697,\n",
       "    'acc_norm_stderr': 0.03588624800091707},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7070707070707071,\n",
       "    'acc_stderr': 0.03242497958178815,\n",
       "    'acc_norm': 0.7070707070707071,\n",
       "    'acc_norm_stderr': 0.03242497958178815},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8134715025906736,\n",
       "    'acc_stderr': 0.028112091210117467,\n",
       "    'acc_norm': 0.8134715025906736,\n",
       "    'acc_norm_stderr': 0.028112091210117467},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5487179487179488,\n",
       "    'acc_stderr': 0.025230381238934833,\n",
       "    'acc_norm': 0.5487179487179488,\n",
       "    'acc_norm_stderr': 0.025230381238934833},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.0284934650910286,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.0284934650910286},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5798319327731093,\n",
       "    'acc_stderr': 0.03206183783236153,\n",
       "    'acc_norm': 0.5798319327731093,\n",
       "    'acc_norm_stderr': 0.03206183783236153},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.33112582781456956,\n",
       "    'acc_stderr': 0.038425817186598696,\n",
       "    'acc_norm': 0.33112582781456956,\n",
       "    'acc_norm_stderr': 0.038425817186598696},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7541284403669725,\n",
       "    'acc_stderr': 0.01846194096870842,\n",
       "    'acc_norm': 0.7541284403669725,\n",
       "    'acc_norm_stderr': 0.01846194096870842},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4583333333333333,\n",
       "    'acc_stderr': 0.03398110890294636,\n",
       "    'acc_norm': 0.4583333333333333,\n",
       "    'acc_norm_stderr': 0.03398110890294636},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7450980392156863,\n",
       "    'acc_stderr': 0.03058759135160426,\n",
       "    'acc_norm': 0.7450980392156863,\n",
       "    'acc_norm_stderr': 0.03058759135160426},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7637130801687764,\n",
       "    'acc_stderr': 0.027652153144159256,\n",
       "    'acc_norm': 0.7637130801687764,\n",
       "    'acc_norm_stderr': 0.027652153144159256},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6502242152466368,\n",
       "    'acc_stderr': 0.03200736719484503,\n",
       "    'acc_norm': 0.6502242152466368,\n",
       "    'acc_norm_stderr': 0.03200736719484503},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6412213740458015,\n",
       "    'acc_stderr': 0.04206739313864908,\n",
       "    'acc_norm': 0.6412213740458015,\n",
       "    'acc_norm_stderr': 0.04206739313864908},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7768595041322314,\n",
       "    'acc_stderr': 0.03800754475228732,\n",
       "    'acc_norm': 0.7768595041322314,\n",
       "    'acc_norm_stderr': 0.03800754475228732},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.043300437496507416,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.043300437496507416},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6932515337423313,\n",
       "    'acc_stderr': 0.03623089915724147,\n",
       "    'acc_norm': 0.6932515337423313,\n",
       "    'acc_norm_stderr': 0.03623089915724147},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.42857142857142855,\n",
       "    'acc_stderr': 0.04697113923010212,\n",
       "    'acc_norm': 0.42857142857142855,\n",
       "    'acc_norm_stderr': 0.04697113923010212},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.043546310772605956,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.043546310772605956},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8461538461538461,\n",
       "    'acc_stderr': 0.023636873317489288,\n",
       "    'acc_norm': 0.8461538461538461,\n",
       "    'acc_norm_stderr': 0.023636873317489288},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.58,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.58,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.756066411238825,\n",
       "    'acc_stderr': 0.015357212665829461,\n",
       "    'acc_norm': 0.756066411238825,\n",
       "    'acc_norm_stderr': 0.015357212665829461},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6069364161849711,\n",
       "    'acc_stderr': 0.026296227915613663,\n",
       "    'acc_norm': 0.6069364161849711,\n",
       "    'acc_norm_stderr': 0.026296227915613663},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2782122905027933,\n",
       "    'acc_stderr': 0.01498732543996355,\n",
       "    'acc_norm': 0.2782122905027933,\n",
       "    'acc_norm_stderr': 0.01498732543996355},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6372549019607843,\n",
       "    'acc_stderr': 0.027530078447110307,\n",
       "    'acc_norm': 0.6372549019607843,\n",
       "    'acc_norm_stderr': 0.027530078447110307},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6366559485530546,\n",
       "    'acc_stderr': 0.027316847674192717,\n",
       "    'acc_norm': 0.6366559485530546,\n",
       "    'acc_norm_stderr': 0.027316847674192717},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6388888888888888,\n",
       "    'acc_stderr': 0.026725868809100793,\n",
       "    'acc_norm': 0.6388888888888888,\n",
       "    'acc_norm_stderr': 0.026725868809100793},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.41134751773049644,\n",
       "    'acc_stderr': 0.02935491115994098,\n",
       "    'acc_norm': 0.41134751773049644,\n",
       "    'acc_norm_stderr': 0.02935491115994098},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.43285528031290743,\n",
       "    'acc_stderr': 0.012654565234622862,\n",
       "    'acc_norm': 0.43285528031290743,\n",
       "    'acc_norm_stderr': 0.012654565234622862},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5367647058823529,\n",
       "    'acc_stderr': 0.03029061918048569,\n",
       "    'acc_norm': 0.5367647058823529,\n",
       "    'acc_norm_stderr': 0.03029061918048569},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.019944914136873583,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.019944914136873583},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6122448979591837,\n",
       "    'acc_stderr': 0.031192230726795656,\n",
       "    'acc_norm': 0.6122448979591837,\n",
       "    'acc_norm_stderr': 0.031192230726795656},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7860696517412935,\n",
       "    'acc_stderr': 0.02899690969332891,\n",
       "    'acc_norm': 0.7860696517412935,\n",
       "    'acc_norm_stderr': 0.02899690969332891},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.82,\n",
       "    'acc_stderr': 0.038612291966536934,\n",
       "    'acc_norm': 0.82,\n",
       "    'acc_norm_stderr': 0.038612291966536934},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.45180722891566266,\n",
       "    'acc_stderr': 0.03874371556587953,\n",
       "    'acc_norm': 0.45180722891566266,\n",
       "    'acc_norm_stderr': 0.03874371556587953},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.783625730994152,\n",
       "    'acc_stderr': 0.03158149539338734,\n",
       "    'acc_norm': 0.783625730994152,\n",
       "    'acc_norm_stderr': 0.03158149539338734},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.34516523867809057,\n",
       "    'mc1_stderr': 0.01664310331927494,\n",
       "    'mc2': 0.5107449529759277,\n",
       "    'mc2_stderr': 0.015464470932268133},\n",
       "   'all': {'em': 0.21403104026845637,\n",
       "    'em_stderr': 0.004200304057589016,\n",
       "    'f1': 0.2773447986577177,\n",
       "    'f1_stderr': 0.004194161726605588,\n",
       "    'acc': 0.4298049932592257,\n",
       "    'acc_stderr': 0.010471546731533343},\n",
       "   'harness|drop|3': {'em': 0.21403104026845637,\n",
       "    'em_stderr': 0.004200304057589016,\n",
       "    'f1': 0.2773447986577177,\n",
       "    'f1_stderr': 0.004194161726605588},\n",
       "   'harness|gsm8k|5': {'acc': 0.11296436694465505,\n",
       "    'acc_stderr': 0.008719339028833057},\n",
       "   'harness|winogrande|5': {'acc': 0.7466456195737964,\n",
       "    'acc_stderr': 0.01222375443423363}}},\n",
       " 'vicuna-33b': {'key': 'vicuna-33b',\n",
       "  'Model': 'Vicuna-33B',\n",
       "  'MT-bench (score)': '7.12',\n",
       "  'MMLU': '0.592',\n",
       "  'License': 'Non-commercial',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-33b-v1.3',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5921501706484642,\n",
       "    'acc_stderr': 0.014361097288449707,\n",
       "    'acc_norm': 0.6194539249146758,\n",
       "    'acc_norm_stderr': 0.014188277712349807},\n",
       "   'harness|hellaswag|10': {'acc': 0.6249751045608445,\n",
       "    'acc_stderr': 0.00483139921850023,\n",
       "    'acc_norm': 0.82194781915953,\n",
       "    'acc_norm_stderr': 0.003817748269107777},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.27,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.27,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5333333333333333,\n",
       "    'acc_stderr': 0.043097329010363554,\n",
       "    'acc_norm': 0.5333333333333333,\n",
       "    'acc_norm_stderr': 0.043097329010363554},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.625,\n",
       "    'acc_stderr': 0.039397364351956274,\n",
       "    'acc_norm': 0.625,\n",
       "    'acc_norm_stderr': 0.039397364351956274},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6037735849056604,\n",
       "    'acc_stderr': 0.030102793781791197,\n",
       "    'acc_norm': 0.6037735849056604,\n",
       "    'acc_norm_stderr': 0.030102793781791197},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.6319444444444444,\n",
       "    'acc_stderr': 0.040329990539607195,\n",
       "    'acc_norm': 0.6319444444444444,\n",
       "    'acc_norm_stderr': 0.040329990539607195},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.49,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.49,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.04725815626252604,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.04725815626252604},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.27450980392156865,\n",
       "    'acc_stderr': 0.04440521906179326,\n",
       "    'acc_norm': 0.27450980392156865,\n",
       "    'acc_norm_stderr': 0.04440521906179326},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.46808510638297873,\n",
       "    'acc_stderr': 0.03261936918467382,\n",
       "    'acc_norm': 0.46808510638297873,\n",
       "    'acc_norm_stderr': 0.03261936918467382},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.34210526315789475,\n",
       "    'acc_stderr': 0.04462917535336936,\n",
       "    'acc_norm': 0.34210526315789475,\n",
       "    'acc_norm_stderr': 0.04462917535336936},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.04164188720169375,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.04164188720169375},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36507936507936506,\n",
       "    'acc_stderr': 0.024796060602699958,\n",
       "    'acc_norm': 0.36507936507936506,\n",
       "    'acc_norm_stderr': 0.024796060602699958},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.04240799327574924,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.04240799327574924},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6870967741935484,\n",
       "    'acc_stderr': 0.02637756702864586,\n",
       "    'acc_norm': 0.6870967741935484,\n",
       "    'acc_norm_stderr': 0.02637756702864586},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.39901477832512317,\n",
       "    'acc_stderr': 0.03445487686264716,\n",
       "    'acc_norm': 0.39901477832512317,\n",
       "    'acc_norm_stderr': 0.03445487686264716},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7151515151515152,\n",
       "    'acc_stderr': 0.03524390844511781,\n",
       "    'acc_norm': 0.7151515151515152,\n",
       "    'acc_norm_stderr': 0.03524390844511781},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7727272727272727,\n",
       "    'acc_stderr': 0.02985751567338642,\n",
       "    'acc_norm': 0.7727272727272727,\n",
       "    'acc_norm_stderr': 0.02985751567338642},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8393782383419689,\n",
       "    'acc_stderr': 0.02649905770139744,\n",
       "    'acc_norm': 0.8393782383419689,\n",
       "    'acc_norm_stderr': 0.02649905770139744},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5871794871794872,\n",
       "    'acc_stderr': 0.024962683564331806,\n",
       "    'acc_norm': 0.5871794871794872,\n",
       "    'acc_norm_stderr': 0.024962683564331806},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2740740740740741,\n",
       "    'acc_stderr': 0.027195934804085626,\n",
       "    'acc_norm': 0.2740740740740741,\n",
       "    'acc_norm_stderr': 0.027195934804085626},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5840336134453782,\n",
       "    'acc_stderr': 0.03201650100739611,\n",
       "    'acc_norm': 0.5840336134453782,\n",
       "    'acc_norm_stderr': 0.03201650100739611},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.40397350993377484,\n",
       "    'acc_stderr': 0.04006485685365342,\n",
       "    'acc_norm': 0.40397350993377484,\n",
       "    'acc_norm_stderr': 0.04006485685365342},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7761467889908257,\n",
       "    'acc_stderr': 0.017871217767790232,\n",
       "    'acc_norm': 0.7761467889908257,\n",
       "    'acc_norm_stderr': 0.017871217767790232},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4027777777777778,\n",
       "    'acc_stderr': 0.033448873829978666,\n",
       "    'acc_norm': 0.4027777777777778,\n",
       "    'acc_norm_stderr': 0.033448873829978666},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7745098039215687,\n",
       "    'acc_stderr': 0.029331162294251735,\n",
       "    'acc_norm': 0.7745098039215687,\n",
       "    'acc_norm_stderr': 0.029331162294251735},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8143459915611815,\n",
       "    'acc_stderr': 0.025310495376944863,\n",
       "    'acc_norm': 0.8143459915611815,\n",
       "    'acc_norm_stderr': 0.025310495376944863},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6771300448430493,\n",
       "    'acc_stderr': 0.03138147637575498,\n",
       "    'acc_norm': 0.6771300448430493,\n",
       "    'acc_norm_stderr': 0.03138147637575498},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6946564885496184,\n",
       "    'acc_stderr': 0.040393149787245626,\n",
       "    'acc_norm': 0.6946564885496184,\n",
       "    'acc_norm_stderr': 0.040393149787245626},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7520661157024794,\n",
       "    'acc_stderr': 0.03941897526516303,\n",
       "    'acc_norm': 0.7520661157024794,\n",
       "    'acc_norm_stderr': 0.03941897526516303},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7129629629629629,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.7129629629629629,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6993865030674846,\n",
       "    'acc_stderr': 0.03602511318806771,\n",
       "    'acc_norm': 0.6993865030674846,\n",
       "    'acc_norm_stderr': 0.03602511318806771},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.45535714285714285,\n",
       "    'acc_stderr': 0.04726835553719099,\n",
       "    'acc_norm': 0.45535714285714285,\n",
       "    'acc_norm_stderr': 0.04726835553719099},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7184466019417476,\n",
       "    'acc_stderr': 0.044532548363264673,\n",
       "    'acc_norm': 0.7184466019417476,\n",
       "    'acc_norm_stderr': 0.044532548363264673},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8803418803418803,\n",
       "    'acc_stderr': 0.021262719400406978,\n",
       "    'acc_norm': 0.8803418803418803,\n",
       "    'acc_norm_stderr': 0.021262719400406978},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7662835249042146,\n",
       "    'acc_stderr': 0.015133383278988829,\n",
       "    'acc_norm': 0.7662835249042146,\n",
       "    'acc_norm_stderr': 0.015133383278988829},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6705202312138728,\n",
       "    'acc_stderr': 0.0253052581318797,\n",
       "    'acc_norm': 0.6705202312138728,\n",
       "    'acc_norm_stderr': 0.0253052581318797},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.43798882681564244,\n",
       "    'acc_stderr': 0.016593394227564843,\n",
       "    'acc_norm': 0.43798882681564244,\n",
       "    'acc_norm_stderr': 0.016593394227564843},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6601307189542484,\n",
       "    'acc_stderr': 0.027121956071388856,\n",
       "    'acc_norm': 0.6601307189542484,\n",
       "    'acc_norm_stderr': 0.027121956071388856},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6688102893890675,\n",
       "    'acc_stderr': 0.026730620728004906,\n",
       "    'acc_norm': 0.6688102893890675,\n",
       "    'acc_norm_stderr': 0.026730620728004906},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6851851851851852,\n",
       "    'acc_stderr': 0.02584224870090217,\n",
       "    'acc_norm': 0.6851851851851852,\n",
       "    'acc_norm_stderr': 0.02584224870090217},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.425531914893617,\n",
       "    'acc_stderr': 0.02949482760014438,\n",
       "    'acc_norm': 0.425531914893617,\n",
       "    'acc_norm_stderr': 0.02949482760014438},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.46740547588005216,\n",
       "    'acc_stderr': 0.012743072942653352,\n",
       "    'acc_norm': 0.46740547588005216,\n",
       "    'acc_norm_stderr': 0.012743072942653352},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5772058823529411,\n",
       "    'acc_stderr': 0.03000856284500348,\n",
       "    'acc_norm': 0.5772058823529411,\n",
       "    'acc_norm_stderr': 0.03000856284500348},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6372549019607843,\n",
       "    'acc_stderr': 0.01945076843250551,\n",
       "    'acc_norm': 0.6372549019607843,\n",
       "    'acc_norm_stderr': 0.01945076843250551},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.046075820907199756,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.046075820907199756},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.689795918367347,\n",
       "    'acc_stderr': 0.029613459872484375,\n",
       "    'acc_norm': 0.689795918367347,\n",
       "    'acc_norm_stderr': 0.029613459872484375},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8109452736318408,\n",
       "    'acc_stderr': 0.027686913588013024,\n",
       "    'acc_norm': 0.8109452736318408,\n",
       "    'acc_norm_stderr': 0.027686913588013024},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.84,\n",
       "    'acc_stderr': 0.03684529491774708,\n",
       "    'acc_norm': 0.84,\n",
       "    'acc_norm_stderr': 0.03684529491774708},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4939759036144578,\n",
       "    'acc_stderr': 0.03892212195333045,\n",
       "    'acc_norm': 0.4939759036144578,\n",
       "    'acc_norm_stderr': 0.03892212195333045},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7719298245614035,\n",
       "    'acc_stderr': 0.03218093795602357,\n",
       "    'acc_norm': 0.7719298245614035,\n",
       "    'acc_norm_stderr': 0.03218093795602357},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3525091799265606,\n",
       "    'mc1_stderr': 0.016724646380756544,\n",
       "    'mc2': 0.5480260004479156,\n",
       "    'mc2_stderr': 0.01562805066884067},\n",
       "   'all': {'acc': 0.5856610701363549,\n",
       "    'acc_stderr': 0.0338972114882748,\n",
       "    'acc_norm': 0.5894623662188109,\n",
       "    'acc_norm_stderr': 0.0338771018183512,\n",
       "    'mc1': 0.3525091799265606,\n",
       "    'mc1_stderr': 0.016724646380756544,\n",
       "    'mc2': 0.5480260004479156,\n",
       "    'mc2_stderr': 0.01562805066884067},\n",
       "   'harness|drop|3': {'em': 0.24611996644295303,\n",
       "    'em_stderr': 0.004411275638567265,\n",
       "    'f1': 0.3191652684563765,\n",
       "    'f1_stderr': 0.004369271114420946},\n",
       "   'harness|gsm8k|5': {'acc': 0.1372251705837756,\n",
       "    'acc_stderr': 0.00947780824460041},\n",
       "   'harness|winogrande|5': {'acc': 0.7703235990528808,\n",
       "    'acc_stderr': 0.011821645601838243}}},\n",
       " 'vicuna-7b': {'key': 'vicuna-7b',\n",
       "  'Model': 'Vicuna-7B',\n",
       "  'MT-bench (score)': '6.17',\n",
       "  'MMLU': '0.498',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-7b-v1.5',\n",
       "  'results': {'harness|drop|3': {'em': 0.017932046979865772,\n",
       "    'em_stderr': 0.0013590184569504276,\n",
       "    'f1': 0.08961094798657747,\n",
       "    'f1_stderr': 0.002014243406072028},\n",
       "   'harness|gsm8k|5': {'acc': 0.08188021228203184,\n",
       "    'acc_stderr': 0.007552338527716956},\n",
       "   'harness|winogrande|5': {'acc': 0.7213891081294396,\n",
       "    'acc_stderr': 0.012599896649493878},\n",
       "   'all': {'acc': 0.509411874714016,\n",
       "    'acc_stderr': 0.03492615918680034,\n",
       "    'acc_norm': 0.5131451594579193,\n",
       "    'acc_norm_stderr': 0.03491300641703413,\n",
       "    'mc1': 0.33047735618115054,\n",
       "    'mc1_stderr': 0.016466769613698303,\n",
       "    'mc2': 0.5032970216179807,\n",
       "    'mc2_stderr': 0.015653721119290503},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5025597269624573,\n",
       "    'acc_stderr': 0.014611199329843784,\n",
       "    'acc_norm': 0.5324232081911263,\n",
       "    'acc_norm_stderr': 0.01458063756999542},\n",
       "   'harness|hellaswag|10': {'acc': 0.5835490938060147,\n",
       "    'acc_stderr': 0.0049196263806455115,\n",
       "    'acc_norm': 0.7739494124676359,\n",
       "    'acc_norm_stderr': 0.004174174724288079},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5037037037037037,\n",
       "    'acc_stderr': 0.04319223625811331,\n",
       "    'acc_norm': 0.5037037037037037,\n",
       "    'acc_norm_stderr': 0.04319223625811331},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.04063302731486671,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.04063302731486671},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5433962264150943,\n",
       "    'acc_stderr': 0.030656748696739428,\n",
       "    'acc_norm': 0.5433962264150943,\n",
       "    'acc_norm_stderr': 0.030656748696739428},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.04179596617581,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.04179596617581},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4682080924855491,\n",
       "    'acc_stderr': 0.03804749744364763,\n",
       "    'acc_norm': 0.4682080924855491,\n",
       "    'acc_norm_stderr': 0.03804749744364763},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.17647058823529413,\n",
       "    'acc_stderr': 0.0379328118530781,\n",
       "    'acc_norm': 0.17647058823529413,\n",
       "    'acc_norm_stderr': 0.0379328118530781},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4553191489361702,\n",
       "    'acc_stderr': 0.03255525359340355,\n",
       "    'acc_norm': 0.4553191489361702,\n",
       "    'acc_norm_stderr': 0.03255525359340355},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.04372748290278006,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.04372748290278006},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.42758620689655175,\n",
       "    'acc_stderr': 0.04122737111370331,\n",
       "    'acc_norm': 0.42758620689655175,\n",
       "    'acc_norm_stderr': 0.04122737111370331},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.30687830687830686,\n",
       "    'acc_stderr': 0.023752928712112143,\n",
       "    'acc_norm': 0.30687830687830686,\n",
       "    'acc_norm_stderr': 0.023752928712112143},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.373015873015873,\n",
       "    'acc_stderr': 0.04325506042017086,\n",
       "    'acc_norm': 0.373015873015873,\n",
       "    'acc_norm_stderr': 0.04325506042017086},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.535483870967742,\n",
       "    'acc_stderr': 0.028372287797962935,\n",
       "    'acc_norm': 0.535483870967742,\n",
       "    'acc_norm_stderr': 0.028372287797962935},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4039408866995074,\n",
       "    'acc_stderr': 0.0345245390382204,\n",
       "    'acc_norm': 0.4039408866995074,\n",
       "    'acc_norm_stderr': 0.0345245390382204},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6303030303030303,\n",
       "    'acc_stderr': 0.037694303145125674,\n",
       "    'acc_norm': 0.6303030303030303,\n",
       "    'acc_norm_stderr': 0.037694303145125674},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6161616161616161,\n",
       "    'acc_stderr': 0.034648816750163396,\n",
       "    'acc_norm': 0.6161616161616161,\n",
       "    'acc_norm_stderr': 0.034648816750163396},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7357512953367875,\n",
       "    'acc_stderr': 0.03182155050916645,\n",
       "    'acc_norm': 0.7357512953367875,\n",
       "    'acc_norm_stderr': 0.03182155050916645},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.4794871794871795,\n",
       "    'acc_stderr': 0.025329663163489943,\n",
       "    'acc_norm': 0.4794871794871795,\n",
       "    'acc_norm_stderr': 0.025329663163489943},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.24444444444444444,\n",
       "    'acc_stderr': 0.02620276653465215,\n",
       "    'acc_norm': 0.24444444444444444,\n",
       "    'acc_norm_stderr': 0.02620276653465215},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.453781512605042,\n",
       "    'acc_stderr': 0.03233943468182088,\n",
       "    'acc_norm': 0.453781512605042,\n",
       "    'acc_norm_stderr': 0.03233943468182088},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.271523178807947,\n",
       "    'acc_stderr': 0.03631329803969653,\n",
       "    'acc_norm': 0.271523178807947,\n",
       "    'acc_norm_stderr': 0.03631329803969653},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.6972477064220184,\n",
       "    'acc_stderr': 0.019698711434756336,\n",
       "    'acc_norm': 0.6972477064220184,\n",
       "    'acc_norm_stderr': 0.019698711434756336},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.03324708911809117,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.03324708911809117},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7156862745098039,\n",
       "    'acc_stderr': 0.031660096793998116,\n",
       "    'acc_norm': 0.7156862745098039,\n",
       "    'acc_norm_stderr': 0.031660096793998116},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7215189873417721,\n",
       "    'acc_stderr': 0.02917868230484253,\n",
       "    'acc_norm': 0.7215189873417721,\n",
       "    'acc_norm_stderr': 0.02917868230484253},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6188340807174888,\n",
       "    'acc_stderr': 0.03259625118416827,\n",
       "    'acc_norm': 0.6188340807174888,\n",
       "    'acc_norm_stderr': 0.03259625118416827},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6335877862595419,\n",
       "    'acc_stderr': 0.04225875451969637,\n",
       "    'acc_norm': 0.6335877862595419,\n",
       "    'acc_norm_stderr': 0.04225875451969637},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.5950413223140496,\n",
       "    'acc_stderr': 0.04481137755942469,\n",
       "    'acc_norm': 0.5950413223140496,\n",
       "    'acc_norm_stderr': 0.04481137755942469},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5648148148148148,\n",
       "    'acc_stderr': 0.04792898170907061,\n",
       "    'acc_norm': 0.5648148148148148,\n",
       "    'acc_norm_stderr': 0.04792898170907061},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.5337423312883436,\n",
       "    'acc_stderr': 0.039194155450484096,\n",
       "    'acc_norm': 0.5337423312883436,\n",
       "    'acc_norm_stderr': 0.039194155450484096},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.42857142857142855,\n",
       "    'acc_stderr': 0.04697113923010212,\n",
       "    'acc_norm': 0.42857142857142855,\n",
       "    'acc_norm_stderr': 0.04697113923010212},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.04656147110012349,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.04656147110012349},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7692307692307693,\n",
       "    'acc_stderr': 0.027601921381417593,\n",
       "    'acc_norm': 0.7692307692307693,\n",
       "    'acc_norm_stderr': 0.027601921381417593},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6883780332056194,\n",
       "    'acc_stderr': 0.016562433867284176,\n",
       "    'acc_norm': 0.6883780332056194,\n",
       "    'acc_norm_stderr': 0.016562433867284176},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5578034682080925,\n",
       "    'acc_stderr': 0.026738603643807403,\n",
       "    'acc_norm': 0.5578034682080925,\n",
       "    'acc_norm_stderr': 0.026738603643807403},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24022346368715083,\n",
       "    'acc_stderr': 0.014288343803925296,\n",
       "    'acc_norm': 0.24022346368715083,\n",
       "    'acc_norm_stderr': 0.014288343803925296},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5751633986928104,\n",
       "    'acc_stderr': 0.02830457667314112,\n",
       "    'acc_norm': 0.5751633986928104,\n",
       "    'acc_norm_stderr': 0.02830457667314112},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5852090032154341,\n",
       "    'acc_stderr': 0.027982680459759567,\n",
       "    'acc_norm': 0.5852090032154341,\n",
       "    'acc_norm_stderr': 0.027982680459759567},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.027648477877413324,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.027648477877413324},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.36524822695035464,\n",
       "    'acc_stderr': 0.028723863853281285,\n",
       "    'acc_norm': 0.36524822695035464,\n",
       "    'acc_norm_stderr': 0.028723863853281285},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.37353324641460234,\n",
       "    'acc_stderr': 0.012354994823515271,\n",
       "    'acc_norm': 0.37353324641460234,\n",
       "    'acc_norm_stderr': 0.012354994823515271},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03032024326500413,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03032024326500413},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4852941176470588,\n",
       "    'acc_stderr': 0.020219083895133924,\n",
       "    'acc_norm': 0.4852941176470588,\n",
       "    'acc_norm_stderr': 0.020219083895133924},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6272727272727273,\n",
       "    'acc_stderr': 0.04631381319425465,\n",
       "    'acc_norm': 0.6272727272727273,\n",
       "    'acc_norm_stderr': 0.04631381319425465},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6244897959183674,\n",
       "    'acc_stderr': 0.03100120903989484,\n",
       "    'acc_norm': 0.6244897959183674,\n",
       "    'acc_norm_stderr': 0.03100120903989484},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.03333333333333335,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.03333333333333335},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.04292346959909282,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.04292346959909282},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4397590361445783,\n",
       "    'acc_stderr': 0.03864139923699121,\n",
       "    'acc_norm': 0.4397590361445783,\n",
       "    'acc_norm_stderr': 0.03864139923699121},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7192982456140351,\n",
       "    'acc_stderr': 0.034462962170884265,\n",
       "    'acc_norm': 0.7192982456140351,\n",
       "    'acc_norm_stderr': 0.034462962170884265},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.33047735618115054,\n",
       "    'mc1_stderr': 0.016466769613698303,\n",
       "    'mc2': 0.5032970216179807,\n",
       "    'mc2_stderr': 0.015653721119290503}}},\n",
       " 'wizardlm-13b': {'key': 'wizardlm-13b',\n",
       "  'Model': 'WizardLM-13b-v1.2',\n",
       "  'MT-bench (score)': '7.20',\n",
       "  'MMLU': '0.527',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Microsoft',\n",
       "  'Link': 'https://huggingface.co/WizardLM/WizardLM-13B-V1.2',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5443686006825939,\n",
       "    'acc_stderr': 0.014553749939306861,\n",
       "    'acc_norm': 0.590443686006826,\n",
       "    'acc_norm_stderr': 0.014370358632472439},\n",
       "   'harness|hellaswag|10': {'acc': 0.6294562836088429,\n",
       "    'acc_stderr': 0.00481963366883254,\n",
       "    'acc_norm': 0.8221469826727743,\n",
       "    'acc_norm_stderr': 0.0038160747120605347},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5111111111111111,\n",
       "    'acc_stderr': 0.04318275491977976,\n",
       "    'acc_norm': 0.5111111111111111,\n",
       "    'acc_norm_stderr': 0.04318275491977976},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5394736842105263,\n",
       "    'acc_stderr': 0.04056242252249034,\n",
       "    'acc_norm': 0.5394736842105263,\n",
       "    'acc_norm_stderr': 0.04056242252249034},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.04999999999999999,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.04999999999999999},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6113207547169811,\n",
       "    'acc_stderr': 0.030000485448675986,\n",
       "    'acc_norm': 0.6113207547169811,\n",
       "    'acc_norm_stderr': 0.030000485448675986},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5694444444444444,\n",
       "    'acc_stderr': 0.04140685639111503,\n",
       "    'acc_norm': 0.5694444444444444,\n",
       "    'acc_norm_stderr': 0.04140685639111503},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4913294797687861,\n",
       "    'acc_stderr': 0.03811890988940412,\n",
       "    'acc_norm': 0.4913294797687861,\n",
       "    'acc_norm_stderr': 0.03811890988940412},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.04389869956808777,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.04389869956808777},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.42127659574468085,\n",
       "    'acc_stderr': 0.03227834510146268,\n",
       "    'acc_norm': 0.42127659574468085,\n",
       "    'acc_norm_stderr': 0.03227834510146268},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2894736842105263,\n",
       "    'acc_stderr': 0.04266339443159394,\n",
       "    'acc_norm': 0.2894736842105263,\n",
       "    'acc_norm_stderr': 0.04266339443159394},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4482758620689655,\n",
       "    'acc_stderr': 0.04144311810878151,\n",
       "    'acc_norm': 0.4482758620689655,\n",
       "    'acc_norm_stderr': 0.04144311810878151},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.02441923496681907,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.02441923496681907},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3492063492063492,\n",
       "    'acc_stderr': 0.04263906892795132,\n",
       "    'acc_norm': 0.3492063492063492,\n",
       "    'acc_norm_stderr': 0.04263906892795132},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.632258064516129,\n",
       "    'acc_stderr': 0.02743086657997347,\n",
       "    'acc_norm': 0.632258064516129,\n",
       "    'acc_norm_stderr': 0.02743086657997347},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.43842364532019706,\n",
       "    'acc_stderr': 0.03491207857486518,\n",
       "    'acc_norm': 0.43842364532019706,\n",
       "    'acc_norm_stderr': 0.03491207857486518},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.049236596391733084,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.049236596391733084},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6787878787878788,\n",
       "    'acc_stderr': 0.036462049632538115,\n",
       "    'acc_norm': 0.6787878787878788,\n",
       "    'acc_norm_stderr': 0.036462049632538115},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6919191919191919,\n",
       "    'acc_stderr': 0.03289477330098616,\n",
       "    'acc_norm': 0.6919191919191919,\n",
       "    'acc_norm_stderr': 0.03289477330098616},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8031088082901554,\n",
       "    'acc_stderr': 0.028697873971860695,\n",
       "    'acc_norm': 0.8031088082901554,\n",
       "    'acc_norm_stderr': 0.028697873971860695},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5230769230769231,\n",
       "    'acc_stderr': 0.025323990861736236,\n",
       "    'acc_norm': 0.5230769230769231,\n",
       "    'acc_norm_stderr': 0.025323990861736236},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028604,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028604},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5798319327731093,\n",
       "    'acc_stderr': 0.03206183783236152,\n",
       "    'acc_norm': 0.5798319327731093,\n",
       "    'acc_norm_stderr': 0.03206183783236152},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.304635761589404,\n",
       "    'acc_stderr': 0.03757949922943343,\n",
       "    'acc_norm': 0.304635761589404,\n",
       "    'acc_norm_stderr': 0.03757949922943343},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7321100917431193,\n",
       "    'acc_stderr': 0.018987462257978652,\n",
       "    'acc_norm': 0.7321100917431193,\n",
       "    'acc_norm_stderr': 0.018987462257978652},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4027777777777778,\n",
       "    'acc_stderr': 0.033448873829978666,\n",
       "    'acc_norm': 0.4027777777777778,\n",
       "    'acc_norm_stderr': 0.033448873829978666},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03039153369274154,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03039153369274154},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7257383966244726,\n",
       "    'acc_stderr': 0.029041333510598028,\n",
       "    'acc_norm': 0.7257383966244726,\n",
       "    'acc_norm_stderr': 0.029041333510598028},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6322869955156951,\n",
       "    'acc_stderr': 0.03236198350928276,\n",
       "    'acc_norm': 0.6322869955156951,\n",
       "    'acc_norm_stderr': 0.03236198350928276},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5954198473282443,\n",
       "    'acc_stderr': 0.043046937953806645,\n",
       "    'acc_norm': 0.5954198473282443,\n",
       "    'acc_norm_stderr': 0.043046937953806645},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7520661157024794,\n",
       "    'acc_stderr': 0.03941897526516303,\n",
       "    'acc_norm': 0.7520661157024794,\n",
       "    'acc_norm_stderr': 0.03941897526516303},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7314814814814815,\n",
       "    'acc_stderr': 0.042844679680521934,\n",
       "    'acc_norm': 0.7314814814814815,\n",
       "    'acc_norm_stderr': 0.042844679680521934},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6441717791411042,\n",
       "    'acc_stderr': 0.03761521380046734,\n",
       "    'acc_norm': 0.6441717791411042,\n",
       "    'acc_norm_stderr': 0.03761521380046734},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.33035714285714285,\n",
       "    'acc_stderr': 0.04464285714285712,\n",
       "    'acc_norm': 0.33035714285714285,\n",
       "    'acc_norm_stderr': 0.04464285714285712},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6601941747572816,\n",
       "    'acc_stderr': 0.046897659372781335,\n",
       "    'acc_norm': 0.6601941747572816,\n",
       "    'acc_norm_stderr': 0.046897659372781335},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8162393162393162,\n",
       "    'acc_stderr': 0.02537213967172293,\n",
       "    'acc_norm': 0.8162393162393162,\n",
       "    'acc_norm_stderr': 0.02537213967172293},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7279693486590039,\n",
       "    'acc_stderr': 0.015913367447500503,\n",
       "    'acc_norm': 0.7279693486590039,\n",
       "    'acc_norm_stderr': 0.015913367447500503},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5982658959537572,\n",
       "    'acc_stderr': 0.026394104177643634,\n",
       "    'acc_norm': 0.5982658959537572,\n",
       "    'acc_norm_stderr': 0.026394104177643634},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30502793296089387,\n",
       "    'acc_stderr': 0.015398723510916713,\n",
       "    'acc_norm': 0.30502793296089387,\n",
       "    'acc_norm_stderr': 0.015398723510916713},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6013071895424836,\n",
       "    'acc_stderr': 0.028036092273891776,\n",
       "    'acc_norm': 0.6013071895424836,\n",
       "    'acc_norm_stderr': 0.028036092273891776},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5980707395498392,\n",
       "    'acc_stderr': 0.02784647600593047,\n",
       "    'acc_norm': 0.5980707395498392,\n",
       "    'acc_norm_stderr': 0.02784647600593047},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5895061728395061,\n",
       "    'acc_stderr': 0.027371350925124764,\n",
       "    'acc_norm': 0.5895061728395061,\n",
       "    'acc_norm_stderr': 0.027371350925124764},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4219858156028369,\n",
       "    'acc_stderr': 0.029462189233370597,\n",
       "    'acc_norm': 0.4219858156028369,\n",
       "    'acc_norm_stderr': 0.029462189233370597},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4165580182529335,\n",
       "    'acc_stderr': 0.012591153245057388,\n",
       "    'acc_norm': 0.4165580182529335,\n",
       "    'acc_norm_stderr': 0.012591153245057388},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5625,\n",
       "    'acc_stderr': 0.030134614954403924,\n",
       "    'acc_norm': 0.5625,\n",
       "    'acc_norm_stderr': 0.030134614954403924},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5310457516339869,\n",
       "    'acc_stderr': 0.02018880445636189,\n",
       "    'acc_norm': 0.5310457516339869,\n",
       "    'acc_norm_stderr': 0.02018880445636189},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.0449429086625209,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.0449429086625209},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6693877551020408,\n",
       "    'acc_stderr': 0.0301164262965406,\n",
       "    'acc_norm': 0.6693877551020408,\n",
       "    'acc_norm_stderr': 0.0301164262965406},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6865671641791045,\n",
       "    'acc_stderr': 0.032801882053486435,\n",
       "    'acc_norm': 0.6865671641791045,\n",
       "    'acc_norm_stderr': 0.032801882053486435},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.463855421686747,\n",
       "    'acc_stderr': 0.03882310850890593,\n",
       "    'acc_norm': 0.463855421686747,\n",
       "    'acc_norm_stderr': 0.03882310850890593},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7543859649122807,\n",
       "    'acc_stderr': 0.0330140594698725,\n",
       "    'acc_norm': 0.7543859649122807,\n",
       "    'acc_norm_stderr': 0.0330140594698725},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.32558139534883723,\n",
       "    'mc1_stderr': 0.01640398946990783,\n",
       "    'mc2': 0.47267763319871686,\n",
       "    'mc2_stderr': 0.01512716043041388},\n",
       "   'all': {'em': 0.09133808724832215,\n",
       "    'em_stderr': 0.002950304012601038,\n",
       "    'f1': 0.1617292365771806,\n",
       "    'f1_stderr': 0.0032231699829319426,\n",
       "    'acc': 0.4269860152120696,\n",
       "    'acc_stderr': 0.011021928189223498},\n",
       "   'harness|drop|3': {'em': 0.09133808724832215,\n",
       "    'em_stderr': 0.002950304012601038,\n",
       "    'f1': 0.1617292365771806,\n",
       "    'f1_stderr': 0.0032231699829319426},\n",
       "   'harness|gsm8k|5': {'acc': 0.13495072024260804,\n",
       "    'acc_stderr': 0.009411315282571171},\n",
       "   'harness|winogrande|5': {'acc': 0.7190213101815311,\n",
       "    'acc_stderr': 0.012632541095875825}}},\n",
       " 'wizardlm-70b': {'key': 'wizardlm-70b',\n",
       "  'Model': 'WizardLM-70B-v1.0',\n",
       "  'MT-bench (score)': '7.71',\n",
       "  'MMLU': '0.637',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Microsoft',\n",
       "  'Link': 'https://huggingface.co/WizardLM/WizardLM-70B-V1.0',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.606655290102389,\n",
       "    'acc_stderr': 0.014275101465693024,\n",
       "    'acc_norm': 0.6407849829351536,\n",
       "    'acc_norm_stderr': 0.014020224155839157},\n",
       "   'harness|hellaswag|10': {'acc': 0.6654052977494523,\n",
       "    'acc_stderr': 0.0047088426001774385,\n",
       "    'acc_norm': 0.854511053574985,\n",
       "    'acc_norm_stderr': 0.003518725257365601},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5185185185185185,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.5185185185185185,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03523807393012047,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03523807393012047},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621503,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621503},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.02872750295788027,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.02872750295788027},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7638888888888888,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.7638888888888888,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6473988439306358,\n",
       "    'acc_stderr': 0.036430371689585475,\n",
       "    'acc_norm': 0.6473988439306358,\n",
       "    'acc_norm_stderr': 0.036430371689585475},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107224,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107224},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.04461960433384739,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.04461960433384739},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5957446808510638,\n",
       "    'acc_stderr': 0.03208115750788684,\n",
       "    'acc_norm': 0.5957446808510638,\n",
       "    'acc_norm_stderr': 0.03208115750788684},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.046970851366478626,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.046970851366478626},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5448275862068965,\n",
       "    'acc_stderr': 0.04149886942192117,\n",
       "    'acc_norm': 0.5448275862068965,\n",
       "    'acc_norm_stderr': 0.04149886942192117},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42328042328042326,\n",
       "    'acc_stderr': 0.02544636563440676,\n",
       "    'acc_norm': 0.42328042328042326,\n",
       "    'acc_norm_stderr': 0.02544636563440676},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.04444444444444449,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.04444444444444449},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7645161290322581,\n",
       "    'acc_stderr': 0.02413763242933771,\n",
       "    'acc_norm': 0.7645161290322581,\n",
       "    'acc_norm_stderr': 0.02413763242933771},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.49261083743842365,\n",
       "    'acc_stderr': 0.03517603540361008,\n",
       "    'acc_norm': 0.49261083743842365,\n",
       "    'acc_norm_stderr': 0.03517603540361008},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8121212121212121,\n",
       "    'acc_stderr': 0.03050193405942914,\n",
       "    'acc_norm': 0.8121212121212121,\n",
       "    'acc_norm_stderr': 0.03050193405942914},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.02805779167298902,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.02805779167298902},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9119170984455959,\n",
       "    'acc_stderr': 0.02045374660160103,\n",
       "    'acc_norm': 0.9119170984455959,\n",
       "    'acc_norm_stderr': 0.02045374660160103},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6461538461538462,\n",
       "    'acc_stderr': 0.024243783994062153,\n",
       "    'acc_norm': 0.6461538461538462,\n",
       "    'acc_norm_stderr': 0.024243783994062153},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3111111111111111,\n",
       "    'acc_stderr': 0.028226446749683515,\n",
       "    'acc_norm': 0.3111111111111111,\n",
       "    'acc_norm_stderr': 0.028226446749683515},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7142857142857143,\n",
       "    'acc_stderr': 0.029344572500634342,\n",
       "    'acc_norm': 0.7142857142857143,\n",
       "    'acc_norm_stderr': 0.029344572500634342},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.4105960264900662,\n",
       "    'acc_stderr': 0.04016689594849929,\n",
       "    'acc_norm': 0.4105960264900662,\n",
       "    'acc_norm_stderr': 0.04016689594849929},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.03408655867977749,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.03408655867977749},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8480392156862745,\n",
       "    'acc_stderr': 0.025195658428931792,\n",
       "    'acc_norm': 0.8480392156862745,\n",
       "    'acc_norm_stderr': 0.025195658428931792},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8396624472573839,\n",
       "    'acc_stderr': 0.02388438092596567,\n",
       "    'acc_norm': 0.8396624472573839,\n",
       "    'acc_norm_stderr': 0.02388438092596567},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7040358744394619,\n",
       "    'acc_stderr': 0.03063659134869981,\n",
       "    'acc_norm': 0.7040358744394619,\n",
       "    'acc_norm_stderr': 0.03063659134869981},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8015267175572519,\n",
       "    'acc_stderr': 0.034981493854624714,\n",
       "    'acc_norm': 0.8015267175572519,\n",
       "    'acc_norm_stderr': 0.034981493854624714},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8264462809917356,\n",
       "    'acc_stderr': 0.0345727283691767,\n",
       "    'acc_norm': 0.8264462809917356,\n",
       "    'acc_norm_stderr': 0.0345727283691767},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8240740740740741,\n",
       "    'acc_stderr': 0.036809181416738807,\n",
       "    'acc_norm': 0.8240740740740741,\n",
       "    'acc_norm_stderr': 0.036809181416738807},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7791411042944786,\n",
       "    'acc_stderr': 0.03259177392742178,\n",
       "    'acc_norm': 0.7791411042944786,\n",
       "    'acc_norm_stderr': 0.03259177392742178},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7961165048543689,\n",
       "    'acc_stderr': 0.0398913985953177,\n",
       "    'acc_norm': 0.7961165048543689,\n",
       "    'acc_norm_stderr': 0.0398913985953177},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8717948717948718,\n",
       "    'acc_stderr': 0.02190190511507332,\n",
       "    'acc_norm': 0.8717948717948718,\n",
       "    'acc_norm_stderr': 0.02190190511507332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8288633461047255,\n",
       "    'acc_stderr': 0.013468201614066307,\n",
       "    'acc_norm': 0.8288633461047255,\n",
       "    'acc_norm_stderr': 0.013468201614066307},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7283236994219653,\n",
       "    'acc_stderr': 0.023948512905468365,\n",
       "    'acc_norm': 0.7283236994219653,\n",
       "    'acc_norm_stderr': 0.023948512905468365},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.29832402234636873,\n",
       "    'acc_stderr': 0.01530184004512928,\n",
       "    'acc_norm': 0.29832402234636873,\n",
       "    'acc_norm_stderr': 0.01530184004512928},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6797385620915033,\n",
       "    'acc_stderr': 0.026716118380156847,\n",
       "    'acc_norm': 0.6797385620915033,\n",
       "    'acc_norm_stderr': 0.026716118380156847},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6881028938906752,\n",
       "    'acc_stderr': 0.02631185807185416,\n",
       "    'acc_norm': 0.6881028938906752,\n",
       "    'acc_norm_stderr': 0.02631185807185416},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7067901234567902,\n",
       "    'acc_stderr': 0.02532988817190092,\n",
       "    'acc_norm': 0.7067901234567902,\n",
       "    'acc_norm_stderr': 0.02532988817190092},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5425531914893617,\n",
       "    'acc_stderr': 0.029719281272236837,\n",
       "    'acc_norm': 0.5425531914893617,\n",
       "    'acc_norm_stderr': 0.029719281272236837},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5189048239895697,\n",
       "    'acc_stderr': 0.012761104871472652,\n",
       "    'acc_norm': 0.5189048239895697,\n",
       "    'acc_norm_stderr': 0.012761104871472652},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6544117647058824,\n",
       "    'acc_stderr': 0.028888193103988626,\n",
       "    'acc_norm': 0.6544117647058824,\n",
       "    'acc_norm_stderr': 0.028888193103988626},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.01869085027359529,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.01869085027359529},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.7090909090909091,\n",
       "    'acc_stderr': 0.04350271442923243,\n",
       "    'acc_norm': 0.7090909090909091,\n",
       "    'acc_norm_stderr': 0.04350271442923243},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7714285714285715,\n",
       "    'acc_stderr': 0.026882144922307744,\n",
       "    'acc_norm': 0.7714285714285715,\n",
       "    'acc_norm_stderr': 0.026882144922307744},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8557213930348259,\n",
       "    'acc_stderr': 0.024845753212306042,\n",
       "    'acc_norm': 0.8557213930348259,\n",
       "    'acc_norm_stderr': 0.024845753212306042},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.83,\n",
       "    'acc_stderr': 0.03775251680686371,\n",
       "    'acc_norm': 0.83,\n",
       "    'acc_norm_stderr': 0.03775251680686371},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.536144578313253,\n",
       "    'acc_stderr': 0.03882310850890594,\n",
       "    'acc_norm': 0.536144578313253,\n",
       "    'acc_norm_stderr': 0.03882310850890594},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8538011695906432,\n",
       "    'acc_stderr': 0.027097290118070792,\n",
       "    'acc_norm': 0.8538011695906432,\n",
       "    'acc_norm_stderr': 0.027097290118070792},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.386780905752754,\n",
       "    'mc1_stderr': 0.01704885701051511,\n",
       "    'mc2': 0.5476558772806095,\n",
       "    'mc2_stderr': 0.015451111828036913},\n",
       "   'all': {'acc': 0.6477083045829946,\n",
       "    'acc_stderr': 0.032583732941869364,\n",
       "    'acc_norm': 0.6514919562551693,\n",
       "    'acc_norm_stderr': 0.03255924150707842,\n",
       "    'mc1': 0.386780905752754,\n",
       "    'mc1_stderr': 0.01704885701051511,\n",
       "    'mc2': 0.5476558772806095,\n",
       "    'mc2_stderr': 0.015451111828036913},\n",
       "   'harness|drop|3': {'em': 0.26541526845637586,\n",
       "    'em_stderr': 0.004521927044730418,\n",
       "    'f1': 0.3270648070469802,\n",
       "    'f1_stderr': 0.004444377320494032},\n",
       "   'harness|gsm8k|5': {'acc': 0.17968157695223655,\n",
       "    'acc_stderr': 0.010575119964242244},\n",
       "   'harness|winogrande|5': {'acc': 0.8082083662194159,\n",
       "    'acc_stderr': 0.011065209664659527}}},\n",
       " 'yi-34b-chat': {'key': 'yi-34b-chat',\n",
       "  'Model': 'Yi-34B-Chat',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.735',\n",
       "  'License': 'Yi License',\n",
       "  'Organization': '01 AI',\n",
       "  'Link': 'https://huggingface.co/01-ai/Yi-34B-Chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6373720136518771,\n",
       "    'acc_stderr': 0.014049106564955012,\n",
       "    'acc_norm': 0.6544368600682594,\n",
       "    'acc_norm_stderr': 0.013896938461145678},\n",
       "   'harness|hellaswag|10': {'acc': 0.6536546504680343,\n",
       "    'acc_stderr': 0.004748324319714274,\n",
       "    'acc_norm': 0.8415654252141008,\n",
       "    'acc_norm_stderr': 0.003644017383711605},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.7111111111111111,\n",
       "    'acc_stderr': 0.03915450630414251,\n",
       "    'acc_norm': 0.7111111111111111,\n",
       "    'acc_norm_stderr': 0.03915450630414251},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.8552631578947368,\n",
       "    'acc_stderr': 0.028631951845930387,\n",
       "    'acc_norm': 0.8552631578947368,\n",
       "    'acc_norm_stderr': 0.028631951845930387},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.7924528301886793,\n",
       "    'acc_stderr': 0.02495991802891127,\n",
       "    'acc_norm': 0.7924528301886793,\n",
       "    'acc_norm_stderr': 0.02495991802891127},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.8472222222222222,\n",
       "    'acc_stderr': 0.030085743248565666,\n",
       "    'acc_norm': 0.8472222222222222,\n",
       "    'acc_norm_stderr': 0.030085743248565666},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237101,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237101},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6994219653179191,\n",
       "    'acc_stderr': 0.034961014811911786,\n",
       "    'acc_norm': 0.6994219653179191,\n",
       "    'acc_norm_stderr': 0.034961014811911786},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.46078431372549017,\n",
       "    'acc_stderr': 0.049598599663841815,\n",
       "    'acc_norm': 0.46078431372549017,\n",
       "    'acc_norm_stderr': 0.049598599663841815},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.82,\n",
       "    'acc_stderr': 0.03861229196653695,\n",
       "    'acc_norm': 0.82,\n",
       "    'acc_norm_stderr': 0.03861229196653695},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.7659574468085106,\n",
       "    'acc_stderr': 0.02767845257821239,\n",
       "    'acc_norm': 0.7659574468085106,\n",
       "    'acc_norm_stderr': 0.02767845257821239},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5526315789473685,\n",
       "    'acc_stderr': 0.046774730044911984,\n",
       "    'acc_norm': 0.5526315789473685,\n",
       "    'acc_norm_stderr': 0.046774730044911984},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.0333333333333333,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.0333333333333333},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.6349206349206349,\n",
       "    'acc_stderr': 0.024796060602699965,\n",
       "    'acc_norm': 0.6349206349206349,\n",
       "    'acc_norm_stderr': 0.024796060602699965},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5396825396825397,\n",
       "    'acc_stderr': 0.04458029125470973,\n",
       "    'acc_norm': 0.5396825396825397,\n",
       "    'acc_norm_stderr': 0.04458029125470973},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.049888765156985884,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.049888765156985884},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.8709677419354839,\n",
       "    'acc_stderr': 0.019070889254792767,\n",
       "    'acc_norm': 0.8709677419354839,\n",
       "    'acc_norm_stderr': 0.019070889254792767},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.6206896551724138,\n",
       "    'acc_stderr': 0.03413963805906235,\n",
       "    'acc_norm': 0.6206896551724138,\n",
       "    'acc_norm_stderr': 0.03413963805906235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8545454545454545,\n",
       "    'acc_stderr': 0.027530196355066573,\n",
       "    'acc_norm': 0.8545454545454545,\n",
       "    'acc_norm_stderr': 0.027530196355066573},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.898989898989899,\n",
       "    'acc_stderr': 0.021469735576055343,\n",
       "    'acc_norm': 0.898989898989899,\n",
       "    'acc_norm_stderr': 0.021469735576055343},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9533678756476683,\n",
       "    'acc_stderr': 0.015216761819262585,\n",
       "    'acc_norm': 0.9533678756476683,\n",
       "    'acc_norm_stderr': 0.015216761819262585},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.7846153846153846,\n",
       "    'acc_stderr': 0.020843034557462878,\n",
       "    'acc_norm': 0.7846153846153846,\n",
       "    'acc_norm_stderr': 0.020843034557462878},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.35555555555555557,\n",
       "    'acc_stderr': 0.029185714949857403,\n",
       "    'acc_norm': 0.35555555555555557,\n",
       "    'acc_norm_stderr': 0.029185714949857403},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.8361344537815126,\n",
       "    'acc_stderr': 0.024044054940440488,\n",
       "    'acc_norm': 0.8361344537815126,\n",
       "    'acc_norm_stderr': 0.024044054940440488},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.5033112582781457,\n",
       "    'acc_stderr': 0.04082393379449654,\n",
       "    'acc_norm': 0.5033112582781457,\n",
       "    'acc_norm_stderr': 0.04082393379449654},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.908256880733945,\n",
       "    'acc_stderr': 0.012376323409137123,\n",
       "    'acc_norm': 0.908256880733945,\n",
       "    'acc_norm_stderr': 0.012376323409137123},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.6342592592592593,\n",
       "    'acc_stderr': 0.03284738857647206,\n",
       "    'acc_norm': 0.6342592592592593,\n",
       "    'acc_norm_stderr': 0.03284738857647206},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.9068627450980392,\n",
       "    'acc_stderr': 0.020397853969426998,\n",
       "    'acc_norm': 0.9068627450980392,\n",
       "    'acc_norm_stderr': 0.020397853969426998},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.9029535864978903,\n",
       "    'acc_stderr': 0.019269323025640255,\n",
       "    'acc_norm': 0.9029535864978903,\n",
       "    'acc_norm_stderr': 0.019269323025640255},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.8116591928251121,\n",
       "    'acc_stderr': 0.026241132996407256,\n",
       "    'acc_norm': 0.8116591928251121,\n",
       "    'acc_norm_stderr': 0.026241132996407256},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8931297709923665,\n",
       "    'acc_stderr': 0.027096548624883733,\n",
       "    'acc_norm': 0.8931297709923665,\n",
       "    'acc_norm_stderr': 0.027096548624883733},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8925619834710744,\n",
       "    'acc_stderr': 0.028268812192540627,\n",
       "    'acc_norm': 0.8925619834710744,\n",
       "    'acc_norm_stderr': 0.028268812192540627},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8888888888888888,\n",
       "    'acc_stderr': 0.03038159675665167,\n",
       "    'acc_norm': 0.8888888888888888,\n",
       "    'acc_norm_stderr': 0.03038159675665167},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.852760736196319,\n",
       "    'acc_stderr': 0.027839915278339657,\n",
       "    'acc_norm': 0.852760736196319,\n",
       "    'acc_norm_stderr': 0.027839915278339657},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.6160714285714286,\n",
       "    'acc_stderr': 0.04616143075028546,\n",
       "    'acc_norm': 0.6160714285714286,\n",
       "    'acc_norm_stderr': 0.04616143075028546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8737864077669902,\n",
       "    'acc_stderr': 0.0328818027880863,\n",
       "    'acc_norm': 0.8737864077669902,\n",
       "    'acc_norm_stderr': 0.0328818027880863},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.9145299145299145,\n",
       "    'acc_stderr': 0.01831589168562586,\n",
       "    'acc_norm': 0.9145299145299145,\n",
       "    'acc_norm_stderr': 0.01831589168562586},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.033799766898963086,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.033799766898963086},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8991060025542784,\n",
       "    'acc_stderr': 0.01077047201488672,\n",
       "    'acc_norm': 0.8991060025542784,\n",
       "    'acc_norm_stderr': 0.01077047201488672},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.8092485549132948,\n",
       "    'acc_stderr': 0.021152676966575277,\n",
       "    'acc_norm': 0.8092485549132948,\n",
       "    'acc_norm_stderr': 0.021152676966575277},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.7016759776536313,\n",
       "    'acc_stderr': 0.01530184004512928,\n",
       "    'acc_norm': 0.7016759776536313,\n",
       "    'acc_norm_stderr': 0.01530184004512928},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.8300653594771242,\n",
       "    'acc_stderr': 0.02150538312123137,\n",
       "    'acc_norm': 0.8300653594771242,\n",
       "    'acc_norm_stderr': 0.02150538312123137},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.8102893890675241,\n",
       "    'acc_stderr': 0.02226819625878322,\n",
       "    'acc_norm': 0.8102893890675241,\n",
       "    'acc_norm_stderr': 0.02226819625878322},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.8703703703703703,\n",
       "    'acc_stderr': 0.018689725721062072,\n",
       "    'acc_norm': 0.8703703703703703,\n",
       "    'acc_norm_stderr': 0.018689725721062072},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.6170212765957447,\n",
       "    'acc_stderr': 0.02899908090480618,\n",
       "    'acc_norm': 0.6170212765957447,\n",
       "    'acc_norm_stderr': 0.02899908090480618},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5495436766623207,\n",
       "    'acc_stderr': 0.012707390438502348,\n",
       "    'acc_norm': 0.5495436766623207,\n",
       "    'acc_norm_stderr': 0.012707390438502348},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.7830882352941176,\n",
       "    'acc_stderr': 0.025035845227711274,\n",
       "    'acc_norm': 0.7830882352941176,\n",
       "    'acc_norm_stderr': 0.025035845227711274},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.826797385620915,\n",
       "    'acc_stderr': 0.015309329266969138,\n",
       "    'acc_norm': 0.826797385620915,\n",
       "    'acc_norm_stderr': 0.015309329266969138},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.7272727272727273,\n",
       "    'acc_stderr': 0.04265792110940589,\n",
       "    'acc_norm': 0.7272727272727273,\n",
       "    'acc_norm_stderr': 0.04265792110940589},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.8367346938775511,\n",
       "    'acc_stderr': 0.02366169917709861,\n",
       "    'acc_norm': 0.8367346938775511,\n",
       "    'acc_norm_stderr': 0.02366169917709861},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8855721393034826,\n",
       "    'acc_stderr': 0.022509345325101706,\n",
       "    'acc_norm': 0.8855721393034826,\n",
       "    'acc_norm_stderr': 0.022509345325101706},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.89,\n",
       "    'acc_stderr': 0.03144660377352203,\n",
       "    'acc_norm': 0.89,\n",
       "    'acc_norm_stderr': 0.03144660377352203},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5843373493975904,\n",
       "    'acc_stderr': 0.03836722176598053,\n",
       "    'acc_norm': 0.5843373493975904,\n",
       "    'acc_norm_stderr': 0.03836722176598053},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8771929824561403,\n",
       "    'acc_stderr': 0.02517298435015577,\n",
       "    'acc_norm': 0.8771929824561403,\n",
       "    'acc_norm_stderr': 0.02517298435015577},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3843329253365973,\n",
       "    'mc1_stderr': 0.017028707301245203,\n",
       "    'mc2': 0.5536831362008046,\n",
       "    'mc2_stderr': 0.015524186394858242},\n",
       "   'harness|winogrande|5': {'acc': 0.8011049723756906,\n",
       "    'acc_stderr': 0.01121862997251531},\n",
       "   'harness|drop|3': {'em': 0.005138422818791947,\n",
       "    'em_stderr': 0.0007322104102794241,\n",
       "    'f1': 0.08032508389261797,\n",
       "    'f1_stderr': 0.001571649833831937},\n",
       "   'harness|gsm8k|5': {'acc': 0.3191811978771797,\n",
       "    'acc_stderr': 0.012840345676251648},\n",
       "   'all': {'acc': 0.7393930299846158,\n",
       "    'acc_stderr': 0.028807135333088364,\n",
       "    'acc_norm': 0.7489434623723922,\n",
       "    'acc_norm_stderr': 0.02935457295982731,\n",
       "    'mc1': 0.3843329253365973,\n",
       "    'mc1_stderr': 0.017028707301245203,\n",
       "    'mc2': 0.5536831362008046,\n",
       "    'mc2_stderr': 0.015524186394858242}}},\n",
       " 'zephyr-7b-alpha': {'key': 'zephyr-7b-alpha',\n",
       "  'Model': 'Zephyr-7b-alpha',\n",
       "  'MT-bench (score)': '6.88',\n",
       "  'MMLU': '-',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'HuggingFace',\n",
       "  'Link': 'https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha',\n",
       "  'results': {'harness|drop|3': {'em': 0.010171979865771811,\n",
       "    'em_stderr': 0.001027595678252703,\n",
       "    'f1': 0.0982225251677855,\n",
       "    'f1_stderr': 0.00197861649884212},\n",
       "   'harness|gsm8k|5': {'acc': 0.14025777103866566,\n",
       "    'acc_stderr': 0.00956510828142867},\n",
       "   'harness|winogrande|5': {'acc': 0.7861089187056038,\n",
       "    'acc_stderr': 0.011524466954090248},\n",
       "   'all': {'acc': 0.6137978230566867,\n",
       "    'acc_stderr': 0.03380754595328641,\n",
       "    'acc_norm': 0.6176702382672306,\n",
       "    'acc_norm_stderr': 0.03378555360789072,\n",
       "    'mc1': 0.42717258261933905,\n",
       "    'mc1_stderr': 0.017316834410963926,\n",
       "    'mc2': 0.5790339154881958,\n",
       "    'mc2_stderr': 0.015362629183533977},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5810580204778157,\n",
       "    'acc_stderr': 0.014418106953639011,\n",
       "    'acc_norm': 0.6100682593856656,\n",
       "    'acc_norm_stderr': 0.01425295984889289},\n",
       "   'harness|hellaswag|10': {'acc': 0.6409081856203943,\n",
       "    'acc_stderr': 0.004787537385153006,\n",
       "    'acc_norm': 0.8403704441346346,\n",
       "    'acc_norm_stderr': 0.0036551361115537096},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6447368421052632,\n",
       "    'acc_stderr': 0.038947344870133176,\n",
       "    'acc_norm': 0.6447368421052632,\n",
       "    'acc_norm_stderr': 0.038947344870133176},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620332,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620332},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6679245283018868,\n",
       "    'acc_stderr': 0.02898545565233439,\n",
       "    'acc_norm': 0.6679245283018868,\n",
       "    'acc_norm_stderr': 0.02898545565233439},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7013888888888888,\n",
       "    'acc_stderr': 0.03827052357950756,\n",
       "    'acc_norm': 0.7013888888888888,\n",
       "    'acc_norm_stderr': 0.03827052357950756},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.49,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.49,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6184971098265896,\n",
       "    'acc_stderr': 0.03703851193099521,\n",
       "    'acc_norm': 0.6184971098265896,\n",
       "    'acc_norm_stderr': 0.03703851193099521},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4215686274509804,\n",
       "    'acc_stderr': 0.049135952012744975,\n",
       "    'acc_norm': 0.4215686274509804,\n",
       "    'acc_norm_stderr': 0.049135952012744975},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5319148936170213,\n",
       "    'acc_stderr': 0.03261936918467382,\n",
       "    'acc_norm': 0.5319148936170213,\n",
       "    'acc_norm_stderr': 0.03261936918467382},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.43859649122807015,\n",
       "    'acc_stderr': 0.04668000738510455,\n",
       "    'acc_norm': 0.43859649122807015,\n",
       "    'acc_norm_stderr': 0.04668000738510455},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370333,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370333},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.38095238095238093,\n",
       "    'acc_stderr': 0.02501074911613761,\n",
       "    'acc_norm': 0.38095238095238093,\n",
       "    'acc_norm_stderr': 0.02501074911613761},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4126984126984127,\n",
       "    'acc_stderr': 0.04403438954768176,\n",
       "    'acc_norm': 0.4126984126984127,\n",
       "    'acc_norm_stderr': 0.04403438954768176},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.04793724854411019,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.04793724854411019},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7548387096774194,\n",
       "    'acc_stderr': 0.02447224384089553,\n",
       "    'acc_norm': 0.7548387096774194,\n",
       "    'acc_norm_stderr': 0.02447224384089553},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5073891625615764,\n",
       "    'acc_stderr': 0.035176035403610105,\n",
       "    'acc_norm': 0.5073891625615764,\n",
       "    'acc_norm_stderr': 0.035176035403610105},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7515151515151515,\n",
       "    'acc_stderr': 0.033744026441394036,\n",
       "    'acc_norm': 0.7515151515151515,\n",
       "    'acc_norm_stderr': 0.033744026441394036},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7828282828282829,\n",
       "    'acc_stderr': 0.029376616484945633,\n",
       "    'acc_norm': 0.7828282828282829,\n",
       "    'acc_norm_stderr': 0.029376616484945633},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8549222797927462,\n",
       "    'acc_stderr': 0.025416343096306433,\n",
       "    'acc_norm': 0.8549222797927462,\n",
       "    'acc_norm_stderr': 0.025416343096306433},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6153846153846154,\n",
       "    'acc_stderr': 0.024666744915187208,\n",
       "    'acc_norm': 0.6153846153846154,\n",
       "    'acc_norm_stderr': 0.024666744915187208},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.35555555555555557,\n",
       "    'acc_stderr': 0.029185714949857403,\n",
       "    'acc_norm': 0.35555555555555557,\n",
       "    'acc_norm_stderr': 0.029185714949857403},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.634453781512605,\n",
       "    'acc_stderr': 0.03128217706368461,\n",
       "    'acc_norm': 0.634453781512605,\n",
       "    'acc_norm_stderr': 0.03128217706368461},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3708609271523179,\n",
       "    'acc_stderr': 0.03943966699183629,\n",
       "    'acc_norm': 0.3708609271523179,\n",
       "    'acc_norm_stderr': 0.03943966699183629},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8018348623853211,\n",
       "    'acc_stderr': 0.017090573804217902,\n",
       "    'acc_norm': 0.8018348623853211,\n",
       "    'acc_norm_stderr': 0.017090573804217902},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5416666666666666,\n",
       "    'acc_stderr': 0.033981108902946366,\n",
       "    'acc_norm': 0.5416666666666666,\n",
       "    'acc_norm_stderr': 0.033981108902946366},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7598039215686274,\n",
       "    'acc_stderr': 0.02998373305591361,\n",
       "    'acc_norm': 0.7598039215686274,\n",
       "    'acc_norm_stderr': 0.02998373305591361},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7426160337552743,\n",
       "    'acc_stderr': 0.028458820991460285,\n",
       "    'acc_norm': 0.7426160337552743,\n",
       "    'acc_norm_stderr': 0.028458820991460285},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6367713004484304,\n",
       "    'acc_stderr': 0.032277904428505,\n",
       "    'acc_norm': 0.6367713004484304,\n",
       "    'acc_norm_stderr': 0.032277904428505},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6793893129770993,\n",
       "    'acc_stderr': 0.04093329229834278,\n",
       "    'acc_norm': 0.6793893129770993,\n",
       "    'acc_norm_stderr': 0.04093329229834278},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.743801652892562,\n",
       "    'acc_stderr': 0.03984979653302871,\n",
       "    'acc_norm': 0.743801652892562,\n",
       "    'acc_norm_stderr': 0.03984979653302871},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.043300437496507416,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.043300437496507416},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7361963190184049,\n",
       "    'acc_stderr': 0.03462419931615623,\n",
       "    'acc_norm': 0.7361963190184049,\n",
       "    'acc_norm_stderr': 0.03462419931615623},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.39285714285714285,\n",
       "    'acc_stderr': 0.04635550135609976,\n",
       "    'acc_norm': 0.39285714285714285,\n",
       "    'acc_norm_stderr': 0.04635550135609976},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7281553398058253,\n",
       "    'acc_stderr': 0.044052680241409216,\n",
       "    'acc_norm': 0.7281553398058253,\n",
       "    'acc_norm_stderr': 0.044052680241409216},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.02280138253459753,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.02280138253459753},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768078,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768078},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7905491698595147,\n",
       "    'acc_stderr': 0.014551310568143704,\n",
       "    'acc_norm': 0.7905491698595147,\n",
       "    'acc_norm_stderr': 0.014551310568143704},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6994219653179191,\n",
       "    'acc_stderr': 0.0246853168672578,\n",
       "    'acc_norm': 0.6994219653179191,\n",
       "    'acc_norm_stderr': 0.0246853168672578},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.37094972067039106,\n",
       "    'acc_stderr': 0.01615591072134177,\n",
       "    'acc_norm': 0.37094972067039106,\n",
       "    'acc_norm_stderr': 0.01615591072134177},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6862745098039216,\n",
       "    'acc_stderr': 0.02656892101545715,\n",
       "    'acc_norm': 0.6862745098039216,\n",
       "    'acc_norm_stderr': 0.02656892101545715},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7041800643086816,\n",
       "    'acc_stderr': 0.025922371788818774,\n",
       "    'acc_norm': 0.7041800643086816,\n",
       "    'acc_norm_stderr': 0.025922371788818774},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6790123456790124,\n",
       "    'acc_stderr': 0.02597656601086274,\n",
       "    'acc_norm': 0.6790123456790124,\n",
       "    'acc_norm_stderr': 0.02597656601086274},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.46099290780141844,\n",
       "    'acc_stderr': 0.029736592526424438,\n",
       "    'acc_norm': 0.46099290780141844,\n",
       "    'acc_norm_stderr': 0.029736592526424438},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.41003911342894395,\n",
       "    'acc_stderr': 0.012561837621962044,\n",
       "    'acc_norm': 0.41003911342894395,\n",
       "    'acc_norm_stderr': 0.012561837621962044},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6507352941176471,\n",
       "    'acc_stderr': 0.028959755196824866,\n",
       "    'acc_norm': 0.6507352941176471,\n",
       "    'acc_norm_stderr': 0.028959755196824866},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6290849673202614,\n",
       "    'acc_stderr': 0.019542101564854128,\n",
       "    'acc_norm': 0.6290849673202614,\n",
       "    'acc_norm_stderr': 0.019542101564854128},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.04607582090719976,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.04607582090719976},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6816326530612244,\n",
       "    'acc_stderr': 0.029822533793982066,\n",
       "    'acc_norm': 0.6816326530612244,\n",
       "    'acc_norm_stderr': 0.029822533793982066},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8009950248756219,\n",
       "    'acc_stderr': 0.028231365092758406,\n",
       "    'acc_norm': 0.8009950248756219,\n",
       "    'acc_norm_stderr': 0.028231365092758406},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5180722891566265,\n",
       "    'acc_stderr': 0.03889951252827216,\n",
       "    'acc_norm': 0.5180722891566265,\n",
       "    'acc_norm_stderr': 0.03889951252827216},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.42717258261933905,\n",
       "    'mc1_stderr': 0.017316834410963926,\n",
       "    'mc2': 0.5790339154881958,\n",
       "    'mc2_stderr': 0.015362629183533977}}},\n",
       " 'zephyr-7b-beta': {'key': 'zephyr-7b-beta',\n",
       "  'Model': 'Zephyr-7b-beta',\n",
       "  'MT-bench (score)': '7.34',\n",
       "  'MMLU': '0.614',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'HuggingFace',\n",
       "  'Link': 'https://huggingface.co/HuggingFaceH4/zephyr-7b-beta',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.590443686006826,\n",
       "    'acc_stderr': 0.014370358632472437,\n",
       "    'acc_norm': 0.6203071672354948,\n",
       "    'acc_norm_stderr': 0.01418211986697487},\n",
       "   'harness|hellaswag|10': {'acc': 0.6491734714200359,\n",
       "    'acc_stderr': 0.004762534245488399,\n",
       "    'acc_norm': 0.8435570603465445,\n",
       "    'acc_norm_stderr': 0.003625323221166244},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.04218506215368881,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.04218506215368881},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6052631578947368,\n",
       "    'acc_stderr': 0.039777499346220734,\n",
       "    'acc_norm': 0.6052631578947368,\n",
       "    'acc_norm_stderr': 0.039777499346220734},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.660377358490566,\n",
       "    'acc_stderr': 0.02914690474779833,\n",
       "    'acc_norm': 0.660377358490566,\n",
       "    'acc_norm_stderr': 0.02914690474779833},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7013888888888888,\n",
       "    'acc_stderr': 0.03827052357950756,\n",
       "    'acc_norm': 0.7013888888888888,\n",
       "    'acc_norm_stderr': 0.03827052357950756},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.04878317312145633,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.04878317312145633},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6416184971098265,\n",
       "    'acc_stderr': 0.03656343653353159,\n",
       "    'acc_norm': 0.6416184971098265,\n",
       "    'acc_norm_stderr': 0.03656343653353159},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4215686274509804,\n",
       "    'acc_stderr': 0.049135952012744975,\n",
       "    'acc_norm': 0.4215686274509804,\n",
       "    'acc_norm_stderr': 0.049135952012744975},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5191489361702127,\n",
       "    'acc_stderr': 0.032662042990646775,\n",
       "    'acc_norm': 0.5191489361702127,\n",
       "    'acc_norm_stderr': 0.032662042990646775},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.42105263157894735,\n",
       "    'acc_stderr': 0.046446020912223177,\n",
       "    'acc_norm': 0.42105263157894735,\n",
       "    'acc_norm_stderr': 0.046446020912223177},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5379310344827586,\n",
       "    'acc_stderr': 0.04154659671707548,\n",
       "    'acc_norm': 0.5379310344827586,\n",
       "    'acc_norm_stderr': 0.04154659671707548},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36772486772486773,\n",
       "    'acc_stderr': 0.02483383982556242,\n",
       "    'acc_norm': 0.36772486772486773,\n",
       "    'acc_norm_stderr': 0.02483383982556242},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.044444444444444495,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.044444444444444495},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.049236596391733084,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.049236596391733084},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7483870967741936,\n",
       "    'acc_stderr': 0.024685979286239963,\n",
       "    'acc_norm': 0.7483870967741936,\n",
       "    'acc_norm_stderr': 0.024685979286239963},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.035158955511656986,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.035158955511656986},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.04725815626252609,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.04725815626252609},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7575757575757576,\n",
       "    'acc_stderr': 0.03346409881055953,\n",
       "    'acc_norm': 0.7575757575757576,\n",
       "    'acc_norm_stderr': 0.03346409881055953},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7323232323232324,\n",
       "    'acc_stderr': 0.031544498882702866,\n",
       "    'acc_norm': 0.7323232323232324,\n",
       "    'acc_norm_stderr': 0.031544498882702866},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8238341968911918,\n",
       "    'acc_stderr': 0.02749350424454805,\n",
       "    'acc_norm': 0.8238341968911918,\n",
       "    'acc_norm_stderr': 0.02749350424454805},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6282051282051282,\n",
       "    'acc_stderr': 0.024503472557110936,\n",
       "    'acc_norm': 0.6282051282051282,\n",
       "    'acc_norm_stderr': 0.024503472557110936},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.34444444444444444,\n",
       "    'acc_stderr': 0.028972648884844267,\n",
       "    'acc_norm': 0.34444444444444444,\n",
       "    'acc_norm_stderr': 0.028972648884844267},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.0303883535518868,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.0303883535518868},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2980132450331126,\n",
       "    'acc_stderr': 0.037345356767871984,\n",
       "    'acc_norm': 0.2980132450331126,\n",
       "    'acc_norm_stderr': 0.037345356767871984},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8091743119266055,\n",
       "    'acc_stderr': 0.01684767640009109,\n",
       "    'acc_norm': 0.8091743119266055,\n",
       "    'acc_norm_stderr': 0.01684767640009109},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.033953227263757976,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.033953227263757976},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.02910225438967407,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.02910225438967407},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7341772151898734,\n",
       "    'acc_stderr': 0.028756799629658346,\n",
       "    'acc_norm': 0.7341772151898734,\n",
       "    'acc_norm_stderr': 0.028756799629658346},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6367713004484304,\n",
       "    'acc_stderr': 0.032277904428505,\n",
       "    'acc_norm': 0.6367713004484304,\n",
       "    'acc_norm_stderr': 0.032277904428505},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6717557251908397,\n",
       "    'acc_stderr': 0.04118438565806298,\n",
       "    'acc_norm': 0.6717557251908397,\n",
       "    'acc_norm_stderr': 0.04118438565806298},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7107438016528925,\n",
       "    'acc_stderr': 0.04139112727635463,\n",
       "    'acc_norm': 0.7107438016528925,\n",
       "    'acc_norm_stderr': 0.04139112727635463},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.040191074725573483,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.040191074725573483},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7055214723926381,\n",
       "    'acc_stderr': 0.03581165790474082,\n",
       "    'acc_norm': 0.7055214723926381,\n",
       "    'acc_norm_stderr': 0.03581165790474082},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.3482142857142857,\n",
       "    'acc_stderr': 0.04521829902833585,\n",
       "    'acc_norm': 0.3482142857142857,\n",
       "    'acc_norm_stderr': 0.04521829902833585},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.043546310772605956,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.043546310772605956},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8803418803418803,\n",
       "    'acc_stderr': 0.02126271940040697,\n",
       "    'acc_norm': 0.8803418803418803,\n",
       "    'acc_norm_stderr': 0.02126271940040697},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.014866821664709588,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.014866821664709588},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6705202312138728,\n",
       "    'acc_stderr': 0.02530525813187972,\n",
       "    'acc_norm': 0.6705202312138728,\n",
       "    'acc_norm_stderr': 0.02530525813187972},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.3418994413407821,\n",
       "    'acc_stderr': 0.015864506461604637,\n",
       "    'acc_norm': 0.3418994413407821,\n",
       "    'acc_norm_stderr': 0.015864506461604637},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6830065359477124,\n",
       "    'acc_stderr': 0.026643278474508755,\n",
       "    'acc_norm': 0.6830065359477124,\n",
       "    'acc_norm_stderr': 0.026643278474508755},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6688102893890675,\n",
       "    'acc_stderr': 0.02673062072800491,\n",
       "    'acc_norm': 0.6688102893890675,\n",
       "    'acc_norm_stderr': 0.02673062072800491},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.02622964917882117,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.02622964917882117},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4929078014184397,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.4929078014184397,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4276401564537158,\n",
       "    'acc_stderr': 0.012635799922765844,\n",
       "    'acc_norm': 0.4276401564537158,\n",
       "    'acc_norm_stderr': 0.012635799922765844},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6470588235294118,\n",
       "    'acc_stderr': 0.029029422815681397,\n",
       "    'acc_norm': 0.6470588235294118,\n",
       "    'acc_norm_stderr': 0.029029422815681397},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.619281045751634,\n",
       "    'acc_stderr': 0.019643801557924803,\n",
       "    'acc_norm': 0.619281045751634,\n",
       "    'acc_norm_stderr': 0.019643801557924803},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302506,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302506},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6816326530612244,\n",
       "    'acc_stderr': 0.029822533793982066,\n",
       "    'acc_norm': 0.6816326530612244,\n",
       "    'acc_norm_stderr': 0.029822533793982066},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8009950248756219,\n",
       "    'acc_stderr': 0.028231365092758406,\n",
       "    'acc_norm': 0.8009950248756219,\n",
       "    'acc_norm_stderr': 0.028231365092758406},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932262,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932262},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5180722891566265,\n",
       "    'acc_stderr': 0.03889951252827216,\n",
       "    'acc_norm': 0.5180722891566265,\n",
       "    'acc_norm_stderr': 0.03889951252827216},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8070175438596491,\n",
       "    'acc_stderr': 0.030267457554898458,\n",
       "    'acc_norm': 0.8070175438596491,\n",
       "    'acc_norm_stderr': 0.030267457554898458},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.40636474908200737,\n",
       "    'mc1_stderr': 0.017193835812093893,\n",
       "    'mc2': 0.5744916942762855,\n",
       "    'mc2_stderr': 0.015742095840959796},\n",
       "   'harness|winogrande|5': {'acc': 0.7774269928966061,\n",
       "    'acc_stderr': 0.011690933809712667},\n",
       "   'harness|drop|3': {'em': 0.004928691275167785,\n",
       "    'em_stderr': 0.0007171872517059793,\n",
       "    'f1': 0.09662437080536909,\n",
       "    'f1_stderr': 0.0018807376338089597},\n",
       "   'harness|gsm8k|5': {'acc': 0.12736921910538287,\n",
       "    'acc_stderr': 0.009183110326737829},\n",
       "   'all': {'acc': 0.6058001121844437,\n",
       "    'acc_stderr': 0.033164878802299444,\n",
       "    'acc_norm': 0.6148009779899025,\n",
       "    'acc_norm_stderr': 0.033912849373118566,\n",
       "    'mc1': 0.40636474908200737,\n",
       "    'mc1_stderr': 0.017193835812093893,\n",
       "    'mc2': 0.5744916942762855,\n",
       "    'mc2_stderr': 0.015742095840959796,\n",
       "    'em': 0.004928691275167785,\n",
       "    'em_stderr': 0.0007171872517059793,\n",
       "    'f1': 0.09662437080536909,\n",
       "    'f1_stderr': 0.0018807376338089597}}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_hf_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'harness|arc:challenge|25': {'acc': 0.590443686006826,\n",
       "  'acc_stderr': 0.014370358632472437,\n",
       "  'acc_norm': 0.6203071672354948,\n",
       "  'acc_norm_stderr': 0.01418211986697487},\n",
       " 'harness|hellaswag|10': {'acc': 0.6491734714200359,\n",
       "  'acc_stderr': 0.004762534245488399,\n",
       "  'acc_norm': 0.8435570603465445,\n",
       "  'acc_norm_stderr': 0.003625323221166244},\n",
       " 'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.36,\n",
       "  'acc_stderr': 0.04824181513244218,\n",
       "  'acc_norm': 0.36,\n",
       "  'acc_norm_stderr': 0.04824181513244218},\n",
       " 'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "  'acc_stderr': 0.04218506215368881,\n",
       "  'acc_norm': 0.6074074074074074,\n",
       "  'acc_norm_stderr': 0.04218506215368881},\n",
       " 'harness|hendrycksTest-astronomy|5': {'acc': 0.6052631578947368,\n",
       "  'acc_stderr': 0.039777499346220734,\n",
       "  'acc_norm': 0.6052631578947368,\n",
       "  'acc_norm_stderr': 0.039777499346220734},\n",
       " 'harness|hendrycksTest-business_ethics|5': {'acc': 0.56,\n",
       "  'acc_stderr': 0.04988876515698589,\n",
       "  'acc_norm': 0.56,\n",
       "  'acc_norm_stderr': 0.04988876515698589},\n",
       " 'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.660377358490566,\n",
       "  'acc_stderr': 0.02914690474779833,\n",
       "  'acc_norm': 0.660377358490566,\n",
       "  'acc_norm_stderr': 0.02914690474779833},\n",
       " 'harness|hendrycksTest-college_biology|5': {'acc': 0.7013888888888888,\n",
       "  'acc_stderr': 0.03827052357950756,\n",
       "  'acc_norm': 0.7013888888888888,\n",
       "  'acc_norm_stderr': 0.03827052357950756},\n",
       " 'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "  'acc_stderr': 0.050211673156867795,\n",
       "  'acc_norm': 0.48,\n",
       "  'acc_norm_stderr': 0.050211673156867795},\n",
       " 'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "  'acc_stderr': 0.050211673156867795,\n",
       "  'acc_norm': 0.48,\n",
       "  'acc_norm_stderr': 0.050211673156867795},\n",
       " 'harness|hendrycksTest-college_mathematics|5': {'acc': 0.38,\n",
       "  'acc_stderr': 0.04878317312145633,\n",
       "  'acc_norm': 0.38,\n",
       "  'acc_norm_stderr': 0.04878317312145633},\n",
       " 'harness|hendrycksTest-college_medicine|5': {'acc': 0.6416184971098265,\n",
       "  'acc_stderr': 0.03656343653353159,\n",
       "  'acc_norm': 0.6416184971098265,\n",
       "  'acc_norm_stderr': 0.03656343653353159},\n",
       " 'harness|hendrycksTest-college_physics|5': {'acc': 0.4215686274509804,\n",
       "  'acc_stderr': 0.049135952012744975,\n",
       "  'acc_norm': 0.4215686274509804,\n",
       "  'acc_norm_stderr': 0.049135952012744975},\n",
       " 'harness|hendrycksTest-computer_security|5': {'acc': 0.73,\n",
       "  'acc_stderr': 0.044619604333847394,\n",
       "  'acc_norm': 0.73,\n",
       "  'acc_norm_stderr': 0.044619604333847394},\n",
       " 'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5191489361702127,\n",
       "  'acc_stderr': 0.032662042990646775,\n",
       "  'acc_norm': 0.5191489361702127,\n",
       "  'acc_norm_stderr': 0.032662042990646775},\n",
       " 'harness|hendrycksTest-econometrics|5': {'acc': 0.42105263157894735,\n",
       "  'acc_stderr': 0.046446020912223177,\n",
       "  'acc_norm': 0.42105263157894735,\n",
       "  'acc_norm_stderr': 0.046446020912223177},\n",
       " 'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5379310344827586,\n",
       "  'acc_stderr': 0.04154659671707548,\n",
       "  'acc_norm': 0.5379310344827586,\n",
       "  'acc_norm_stderr': 0.04154659671707548},\n",
       " 'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36772486772486773,\n",
       "  'acc_stderr': 0.02483383982556242,\n",
       "  'acc_norm': 0.36772486772486773,\n",
       "  'acc_norm_stderr': 0.02483383982556242},\n",
       " 'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "  'acc_stderr': 0.044444444444444495,\n",
       "  'acc_norm': 0.4444444444444444,\n",
       "  'acc_norm_stderr': 0.044444444444444495},\n",
       " 'harness|hendrycksTest-global_facts|5': {'acc': 0.4,\n",
       "  'acc_stderr': 0.049236596391733084,\n",
       "  'acc_norm': 0.4,\n",
       "  'acc_norm_stderr': 0.049236596391733084},\n",
       " 'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7483870967741936,\n",
       "  'acc_stderr': 0.024685979286239963,\n",
       "  'acc_norm': 0.7483870967741936,\n",
       "  'acc_norm_stderr': 0.024685979286239963},\n",
       " 'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "  'acc_stderr': 0.035158955511656986,\n",
       "  'acc_norm': 0.5172413793103449,\n",
       "  'acc_norm_stderr': 0.035158955511656986},\n",
       " 'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.67,\n",
       "  'acc_stderr': 0.04725815626252609,\n",
       "  'acc_norm': 0.67,\n",
       "  'acc_norm_stderr': 0.04725815626252609},\n",
       " 'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7575757575757576,\n",
       "  'acc_stderr': 0.03346409881055953,\n",
       "  'acc_norm': 0.7575757575757576,\n",
       "  'acc_norm_stderr': 0.03346409881055953},\n",
       " 'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7323232323232324,\n",
       "  'acc_stderr': 0.031544498882702866,\n",
       "  'acc_norm': 0.7323232323232324,\n",
       "  'acc_norm_stderr': 0.031544498882702866},\n",
       " 'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8238341968911918,\n",
       "  'acc_stderr': 0.02749350424454805,\n",
       "  'acc_norm': 0.8238341968911918,\n",
       "  'acc_norm_stderr': 0.02749350424454805},\n",
       " 'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6282051282051282,\n",
       "  'acc_stderr': 0.024503472557110936,\n",
       "  'acc_norm': 0.6282051282051282,\n",
       "  'acc_norm_stderr': 0.024503472557110936},\n",
       " 'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.34444444444444444,\n",
       "  'acc_stderr': 0.028972648884844267,\n",
       "  'acc_norm': 0.34444444444444444,\n",
       "  'acc_norm_stderr': 0.028972648884844267},\n",
       " 'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6764705882352942,\n",
       "  'acc_stderr': 0.0303883535518868,\n",
       "  'acc_norm': 0.6764705882352942,\n",
       "  'acc_norm_stderr': 0.0303883535518868},\n",
       " 'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2980132450331126,\n",
       "  'acc_stderr': 0.037345356767871984,\n",
       "  'acc_norm': 0.2980132450331126,\n",
       "  'acc_norm_stderr': 0.037345356767871984},\n",
       " 'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8091743119266055,\n",
       "  'acc_stderr': 0.01684767640009109,\n",
       "  'acc_norm': 0.8091743119266055,\n",
       "  'acc_norm_stderr': 0.01684767640009109},\n",
       " 'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5462962962962963,\n",
       "  'acc_stderr': 0.033953227263757976,\n",
       "  'acc_norm': 0.5462962962962963,\n",
       "  'acc_norm_stderr': 0.033953227263757976},\n",
       " 'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "  'acc_stderr': 0.02910225438967407,\n",
       "  'acc_norm': 0.7794117647058824,\n",
       "  'acc_norm_stderr': 0.02910225438967407},\n",
       " 'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7341772151898734,\n",
       "  'acc_stderr': 0.028756799629658346,\n",
       "  'acc_norm': 0.7341772151898734,\n",
       "  'acc_norm_stderr': 0.028756799629658346},\n",
       " 'harness|hendrycksTest-human_aging|5': {'acc': 0.6367713004484304,\n",
       "  'acc_stderr': 0.032277904428505,\n",
       "  'acc_norm': 0.6367713004484304,\n",
       "  'acc_norm_stderr': 0.032277904428505},\n",
       " 'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6717557251908397,\n",
       "  'acc_stderr': 0.04118438565806298,\n",
       "  'acc_norm': 0.6717557251908397,\n",
       "  'acc_norm_stderr': 0.04118438565806298},\n",
       " 'harness|hendrycksTest-international_law|5': {'acc': 0.7107438016528925,\n",
       "  'acc_stderr': 0.04139112727635463,\n",
       "  'acc_norm': 0.7107438016528925,\n",
       "  'acc_norm_stderr': 0.04139112727635463},\n",
       " 'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7777777777777778,\n",
       "  'acc_stderr': 0.040191074725573483,\n",
       "  'acc_norm': 0.7777777777777778,\n",
       "  'acc_norm_stderr': 0.040191074725573483},\n",
       " 'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7055214723926381,\n",
       "  'acc_stderr': 0.03581165790474082,\n",
       "  'acc_norm': 0.7055214723926381,\n",
       "  'acc_norm_stderr': 0.03581165790474082},\n",
       " 'harness|hendrycksTest-machine_learning|5': {'acc': 0.3482142857142857,\n",
       "  'acc_stderr': 0.04521829902833585,\n",
       "  'acc_norm': 0.3482142857142857,\n",
       "  'acc_norm_stderr': 0.04521829902833585},\n",
       " 'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "  'acc_stderr': 0.043546310772605956,\n",
       "  'acc_norm': 0.7378640776699029,\n",
       "  'acc_norm_stderr': 0.043546310772605956},\n",
       " 'harness|hendrycksTest-marketing|5': {'acc': 0.8803418803418803,\n",
       "  'acc_stderr': 0.02126271940040697,\n",
       "  'acc_norm': 0.8803418803418803,\n",
       "  'acc_norm_stderr': 0.02126271940040697},\n",
       " 'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "  'acc_stderr': 0.045604802157206845,\n",
       "  'acc_norm': 0.71,\n",
       "  'acc_norm_stderr': 0.045604802157206845},\n",
       " 'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7777777777777778,\n",
       "  'acc_stderr': 0.014866821664709588,\n",
       "  'acc_norm': 0.7777777777777778,\n",
       "  'acc_norm_stderr': 0.014866821664709588},\n",
       " 'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6705202312138728,\n",
       "  'acc_stderr': 0.02530525813187972,\n",
       "  'acc_norm': 0.6705202312138728,\n",
       "  'acc_norm_stderr': 0.02530525813187972},\n",
       " 'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.3418994413407821,\n",
       "  'acc_stderr': 0.015864506461604637,\n",
       "  'acc_norm': 0.3418994413407821,\n",
       "  'acc_norm_stderr': 0.015864506461604637},\n",
       " 'harness|hendrycksTest-nutrition|5': {'acc': 0.6830065359477124,\n",
       "  'acc_stderr': 0.026643278474508755,\n",
       "  'acc_norm': 0.6830065359477124,\n",
       "  'acc_norm_stderr': 0.026643278474508755},\n",
       " 'harness|hendrycksTest-philosophy|5': {'acc': 0.6688102893890675,\n",
       "  'acc_stderr': 0.02673062072800491,\n",
       "  'acc_norm': 0.6688102893890675,\n",
       "  'acc_norm_stderr': 0.02673062072800491},\n",
       " 'harness|hendrycksTest-prehistory|5': {'acc': 0.6666666666666666,\n",
       "  'acc_stderr': 0.02622964917882117,\n",
       "  'acc_norm': 0.6666666666666666,\n",
       "  'acc_norm_stderr': 0.02622964917882117},\n",
       " 'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4929078014184397,\n",
       "  'acc_stderr': 0.02982449855912901,\n",
       "  'acc_norm': 0.4929078014184397,\n",
       "  'acc_norm_stderr': 0.02982449855912901},\n",
       " 'harness|hendrycksTest-professional_law|5': {'acc': 0.4276401564537158,\n",
       "  'acc_stderr': 0.012635799922765844,\n",
       "  'acc_norm': 0.4276401564537158,\n",
       "  'acc_norm_stderr': 0.012635799922765844},\n",
       " 'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6470588235294118,\n",
       "  'acc_stderr': 0.029029422815681397,\n",
       "  'acc_norm': 0.6470588235294118,\n",
       "  'acc_norm_stderr': 0.029029422815681397},\n",
       " 'harness|hendrycksTest-professional_psychology|5': {'acc': 0.619281045751634,\n",
       "  'acc_stderr': 0.019643801557924803,\n",
       "  'acc_norm': 0.619281045751634,\n",
       "  'acc_norm_stderr': 0.019643801557924803},\n",
       " 'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "  'acc_stderr': 0.04525393596302506,\n",
       "  'acc_norm': 0.6636363636363637,\n",
       "  'acc_norm_stderr': 0.04525393596302506},\n",
       " 'harness|hendrycksTest-security_studies|5': {'acc': 0.6816326530612244,\n",
       "  'acc_stderr': 0.029822533793982066,\n",
       "  'acc_norm': 0.6816326530612244,\n",
       "  'acc_norm_stderr': 0.029822533793982066},\n",
       " 'harness|hendrycksTest-sociology|5': {'acc': 0.8009950248756219,\n",
       "  'acc_stderr': 0.028231365092758406,\n",
       "  'acc_norm': 0.8009950248756219,\n",
       "  'acc_norm_stderr': 0.028231365092758406},\n",
       " 'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "  'acc_stderr': 0.04163331998932262,\n",
       "  'acc_norm': 0.78,\n",
       "  'acc_norm_stderr': 0.04163331998932262},\n",
       " 'harness|hendrycksTest-virology|5': {'acc': 0.5180722891566265,\n",
       "  'acc_stderr': 0.03889951252827216,\n",
       "  'acc_norm': 0.5180722891566265,\n",
       "  'acc_norm_stderr': 0.03889951252827216},\n",
       " 'harness|hendrycksTest-world_religions|5': {'acc': 0.8070175438596491,\n",
       "  'acc_stderr': 0.030267457554898458,\n",
       "  'acc_norm': 0.8070175438596491,\n",
       "  'acc_norm_stderr': 0.030267457554898458},\n",
       " 'harness|truthfulqa:mc|0': {'mc1': 0.40636474908200737,\n",
       "  'mc1_stderr': 0.017193835812093893,\n",
       "  'mc2': 0.5744916942762855,\n",
       "  'mc2_stderr': 0.015742095840959796},\n",
       " 'harness|winogrande|5': {'acc': 0.7774269928966061,\n",
       "  'acc_stderr': 0.011690933809712667},\n",
       " 'harness|drop|3': {'em': 0.004928691275167785,\n",
       "  'em_stderr': 0.0007171872517059793,\n",
       "  'f1': 0.09662437080536909,\n",
       "  'f1_stderr': 0.0018807376338089597},\n",
       " 'harness|gsm8k|5': {'acc': 0.12736921910538287,\n",
       "  'acc_stderr': 0.009183110326737829},\n",
       " 'all': {'acc': 0.6058001121844437,\n",
       "  'acc_stderr': 0.033164878802299444,\n",
       "  'acc_norm': 0.6148009779899025,\n",
       "  'acc_norm_stderr': 0.033912849373118566,\n",
       "  'mc1': 0.40636474908200737,\n",
       "  'mc1_stderr': 0.017193835812093893,\n",
       "  'mc2': 0.5744916942762855,\n",
       "  'mc2_stderr': 0.015742095840959796,\n",
       "  'em': 0.004928691275167785,\n",
       "  'em_stderr': 0.0007171872517059793,\n",
       "  'f1': 0.09662437080536909,\n",
       "  'f1_stderr': 0.0018807376338089597}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = []\n",
    "for model in filtered_hf_models.values():\n",
    "    benchmarks.append(list(model['results'].keys()))\n",
    "    \n",
    "# Intersection of benchmarks\n",
    "common_benchmarks = benchmarks[0]\n",
    "for benchmark in benchmarks:\n",
    "    common_benchmarks = list(set(common_benchmarks) & set(benchmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_benchmarks = [x for x in common_benchmarks if '|' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chatglm2-6b': {'key': 'chatglm2-6b',\n",
       "  'Model': 'ChatGLM2-6B',\n",
       "  'MT-bench (score)': '4.96',\n",
       "  'MMLU': '0.455',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'Tsinghua',\n",
       "  'Link': 'https://huggingface.co/THUDM/chatglm2-6b',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.37457337883959047,\n",
       "    'acc_stderr': 0.014144193471893444,\n",
       "    'acc_norm': 0.38822525597269625,\n",
       "    'acc_norm_stderr': 0.014241614207414044},\n",
       "   'harness|hellaswag|10': {'acc': 0.4596693885680143,\n",
       "    'acc_stderr': 0.004973522582431221,\n",
       "    'acc_norm': 0.5902210714997013,\n",
       "    'acc_norm_stderr': 0.004907877144720029},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4148148148148148,\n",
       "    'acc_stderr': 0.042561937679014075,\n",
       "    'acc_norm': 0.4148148148148148,\n",
       "    'acc_norm_stderr': 0.042561937679014075},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5328947368421053,\n",
       "    'acc_stderr': 0.04060127035236395,\n",
       "    'acc_norm': 0.5328947368421053,\n",
       "    'acc_norm_stderr': 0.04060127035236395},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.61,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.61,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.4867924528301887,\n",
       "    'acc_stderr': 0.030762134874500476,\n",
       "    'acc_norm': 0.4867924528301887,\n",
       "    'acc_norm_stderr': 0.030762134874500476},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.04076663253918567,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.04076663253918567},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.42196531791907516,\n",
       "    'acc_stderr': 0.0376574669386515,\n",
       "    'acc_norm': 0.42196531791907516,\n",
       "    'acc_norm_stderr': 0.0376574669386515},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.21568627450980393,\n",
       "    'acc_stderr': 0.040925639582376556,\n",
       "    'acc_norm': 0.21568627450980393,\n",
       "    'acc_norm_stderr': 0.040925639582376556},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.03202563076101736,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.03202563076101736},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.30701754385964913,\n",
       "    'acc_stderr': 0.043391383225798615,\n",
       "    'acc_norm': 0.30701754385964913,\n",
       "    'acc_norm_stderr': 0.043391383225798615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4689655172413793,\n",
       "    'acc_stderr': 0.04158632762097828,\n",
       "    'acc_norm': 0.4689655172413793,\n",
       "    'acc_norm_stderr': 0.04158632762097828},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.31746031746031744,\n",
       "    'acc_stderr': 0.023973861998992072,\n",
       "    'acc_norm': 0.31746031746031744,\n",
       "    'acc_norm_stderr': 0.023973861998992072},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3492063492063492,\n",
       "    'acc_stderr': 0.04263906892795133,\n",
       "    'acc_norm': 0.3492063492063492,\n",
       "    'acc_norm_stderr': 0.04263906892795133},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.5580645161290323,\n",
       "    'acc_stderr': 0.02825155790684973,\n",
       "    'acc_norm': 0.5580645161290323,\n",
       "    'acc_norm_stderr': 0.02825155790684973},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.45320197044334976,\n",
       "    'acc_stderr': 0.03502544650845872,\n",
       "    'acc_norm': 0.45320197044334976,\n",
       "    'acc_norm_stderr': 0.03502544650845872},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.42,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.42,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.038254602783800246,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.038254602783800246},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.035402943770953675,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.035402943770953675},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.5699481865284974,\n",
       "    'acc_stderr': 0.03572954333144809,\n",
       "    'acc_norm': 0.5699481865284974,\n",
       "    'acc_norm_stderr': 0.03572954333144809},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.44871794871794873,\n",
       "    'acc_stderr': 0.025217315184846482,\n",
       "    'acc_norm': 0.44871794871794873,\n",
       "    'acc_norm_stderr': 0.025217315184846482},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.26296296296296295,\n",
       "    'acc_stderr': 0.026842057873833706,\n",
       "    'acc_norm': 0.26296296296296295,\n",
       "    'acc_norm_stderr': 0.026842057873833706},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.4327731092436975,\n",
       "    'acc_stderr': 0.03218358107742613,\n",
       "    'acc_norm': 0.4327731092436975,\n",
       "    'acc_norm_stderr': 0.03218358107742613},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2913907284768212,\n",
       "    'acc_stderr': 0.03710185726119994,\n",
       "    'acc_norm': 0.2913907284768212,\n",
       "    'acc_norm_stderr': 0.03710185726119994},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.5926605504587156,\n",
       "    'acc_stderr': 0.021065986244412895,\n",
       "    'acc_norm': 0.5926605504587156,\n",
       "    'acc_norm_stderr': 0.021065986244412895},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.33796296296296297,\n",
       "    'acc_stderr': 0.032259413526312945,\n",
       "    'acc_norm': 0.33796296296296297,\n",
       "    'acc_norm_stderr': 0.032259413526312945},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.5882352941176471,\n",
       "    'acc_stderr': 0.03454236585380609,\n",
       "    'acc_norm': 0.5882352941176471,\n",
       "    'acc_norm_stderr': 0.03454236585380609},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6624472573839663,\n",
       "    'acc_stderr': 0.03078154910202621,\n",
       "    'acc_norm': 0.6624472573839663,\n",
       "    'acc_norm_stderr': 0.03078154910202621},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.4663677130044843,\n",
       "    'acc_stderr': 0.033481800170603065,\n",
       "    'acc_norm': 0.4663677130044843,\n",
       "    'acc_norm_stderr': 0.033481800170603065},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.48091603053435117,\n",
       "    'acc_stderr': 0.04382094705550988,\n",
       "    'acc_norm': 0.48091603053435117,\n",
       "    'acc_norm_stderr': 0.04382094705550988},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.6033057851239669,\n",
       "    'acc_stderr': 0.04465869780531009,\n",
       "    'acc_norm': 0.6033057851239669,\n",
       "    'acc_norm_stderr': 0.04465869780531009},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04766075165356461,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04766075165356461},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.49079754601226994,\n",
       "    'acc_stderr': 0.03927705600787443,\n",
       "    'acc_norm': 0.49079754601226994,\n",
       "    'acc_norm_stderr': 0.03927705600787443},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.4375,\n",
       "    'acc_stderr': 0.04708567521880525,\n",
       "    'acc_norm': 0.4375,\n",
       "    'acc_norm_stderr': 0.04708567521880525},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6213592233009708,\n",
       "    'acc_stderr': 0.048026946982589726,\n",
       "    'acc_norm': 0.6213592233009708,\n",
       "    'acc_norm_stderr': 0.048026946982589726},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.6923076923076923,\n",
       "    'acc_stderr': 0.030236389942173102,\n",
       "    'acc_norm': 0.6923076923076923,\n",
       "    'acc_norm_stderr': 0.030236389942173102},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.5964240102171137,\n",
       "    'acc_stderr': 0.017544332237926417,\n",
       "    'acc_norm': 0.5964240102171137,\n",
       "    'acc_norm_stderr': 0.017544332237926417},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5289017341040463,\n",
       "    'acc_stderr': 0.026874085883518348,\n",
       "    'acc_norm': 0.5289017341040463,\n",
       "    'acc_norm_stderr': 0.026874085883518348},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.23575418994413408,\n",
       "    'acc_stderr': 0.014196375686290804,\n",
       "    'acc_norm': 0.23575418994413408,\n",
       "    'acc_norm_stderr': 0.014196375686290804},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5326797385620915,\n",
       "    'acc_stderr': 0.028568699752225868,\n",
       "    'acc_norm': 0.5326797385620915,\n",
       "    'acc_norm_stderr': 0.028568699752225868},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.4983922829581994,\n",
       "    'acc_stderr': 0.02839794490780661,\n",
       "    'acc_norm': 0.4983922829581994,\n",
       "    'acc_norm_stderr': 0.02839794490780661},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.49382716049382713,\n",
       "    'acc_stderr': 0.02781862396258329,\n",
       "    'acc_norm': 0.49382716049382713,\n",
       "    'acc_norm_stderr': 0.02781862396258329},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.02812163604063988,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.02812163604063988},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3559322033898305,\n",
       "    'acc_stderr': 0.012228645537277566,\n",
       "    'acc_norm': 0.3559322033898305,\n",
       "    'acc_norm_stderr': 0.012228645537277566},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.35294117647058826,\n",
       "    'acc_stderr': 0.029029422815681393,\n",
       "    'acc_norm': 0.35294117647058826,\n",
       "    'acc_norm_stderr': 0.029029422815681393},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.020102583895887188,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.020102583895887188},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5818181818181818,\n",
       "    'acc_stderr': 0.04724577405731572,\n",
       "    'acc_norm': 0.5818181818181818,\n",
       "    'acc_norm_stderr': 0.04724577405731572},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5673469387755102,\n",
       "    'acc_stderr': 0.031717528240626645,\n",
       "    'acc_norm': 0.5673469387755102,\n",
       "    'acc_norm_stderr': 0.031717528240626645},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6218905472636815,\n",
       "    'acc_stderr': 0.034288678487786564,\n",
       "    'acc_norm': 0.6218905472636815,\n",
       "    'acc_norm_stderr': 0.034288678487786564},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.41566265060240964,\n",
       "    'acc_stderr': 0.03836722176598052,\n",
       "    'acc_norm': 0.41566265060240964,\n",
       "    'acc_norm_stderr': 0.03836722176598052},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.038295098689947286,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.038295098689947286},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.2998776009791922,\n",
       "    'mc1_stderr': 0.016040352966713616,\n",
       "    'mc2': 0.48081249614578586,\n",
       "    'mc2_stderr': 0.01612488256095027},\n",
       "   'all': {'acc': 0.46492840933020524,\n",
       "    'acc_stderr': 0.03535258865761692,\n",
       "    'acc_norm': 0.46737253746689666,\n",
       "    'acc_norm_stderr': 0.035353127221986566,\n",
       "    'mc1': 0.2998776009791922,\n",
       "    'mc1_stderr': 0.016040352966713616,\n",
       "    'mc2': 0.48081249614578586,\n",
       "    'mc2_stderr': 0.01612488256095027}}},\n",
       " 'codellama-34b-instruct': {'key': 'codellama-34b-instruct',\n",
       "  'Model': 'CodeLlama-34B-instruct',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.537',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf',\n",
       "  'results': {'harness|drop|3': {'em': 0.0014681208053691276,\n",
       "    'em_stderr': 0.00039210421902985756,\n",
       "    'f1': 0.05437604865771826,\n",
       "    'f1_stderr': 0.0011898517432786567},\n",
       "   'harness|gsm8k|5': {'acc': 0.3100833965125095,\n",
       "    'acc_stderr': 0.012740305717376268},\n",
       "   'harness|winogrande|5': {'acc': 0.745067087608524,\n",
       "    'acc_stderr': 0.012248806969376422},\n",
       "   'all': {'em': 0.0014681208053691276,\n",
       "    'em_stderr': 0.00039210421902985756,\n",
       "    'f1': 0.05437604865771826,\n",
       "    'f1_stderr': 0.0011898517432786567,\n",
       "    'acc': 0.5275752420605168,\n",
       "    'acc_stderr': 0.012494556343376345},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5093856655290102,\n",
       "    'acc_stderr': 0.014608816322065,\n",
       "    'acc_norm': 0.5426621160409556,\n",
       "    'acc_norm_stderr': 0.01455810654392406},\n",
       "   'harness|hellaswag|10': {'acc': 0.5637323242381995,\n",
       "    'acc_stderr': 0.004949080334816024,\n",
       "    'acc_norm': 0.7691694881497709,\n",
       "    'acc_norm_stderr': 0.004205030476886528},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.45925925925925926,\n",
       "    'acc_stderr': 0.04304979692464244,\n",
       "    'acc_norm': 0.45925925925925926,\n",
       "    'acc_norm_stderr': 0.04304979692464244},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5921052631578947,\n",
       "    'acc_stderr': 0.03999309712777472,\n",
       "    'acc_norm': 0.5921052631578947,\n",
       "    'acc_norm_stderr': 0.03999309712777472},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.61,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.61,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.49056603773584906,\n",
       "    'acc_stderr': 0.0307673947078081,\n",
       "    'acc_norm': 0.49056603773584906,\n",
       "    'acc_norm_stderr': 0.0307673947078081},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.04179596617581,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.04179596617581},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4508670520231214,\n",
       "    'acc_stderr': 0.03794012674697028,\n",
       "    'acc_norm': 0.4508670520231214,\n",
       "    'acc_norm_stderr': 0.03794012674697028},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3431372549019608,\n",
       "    'acc_stderr': 0.047240073523838876,\n",
       "    'acc_norm': 0.3431372549019608,\n",
       "    'acc_norm_stderr': 0.047240073523838876},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.69,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.69,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.49361702127659574,\n",
       "    'acc_stderr': 0.03268335899936336,\n",
       "    'acc_norm': 0.49361702127659574,\n",
       "    'acc_norm_stderr': 0.03268335899936336},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.42105263157894735,\n",
       "    'acc_stderr': 0.046446020912223177,\n",
       "    'acc_norm': 0.42105263157894735,\n",
       "    'acc_norm_stderr': 0.046446020912223177},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5241379310344828,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.5241379310344828,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3915343915343915,\n",
       "    'acc_stderr': 0.025138091388851102,\n",
       "    'acc_norm': 0.3915343915343915,\n",
       "    'acc_norm_stderr': 0.025138091388851102},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4603174603174603,\n",
       "    'acc_stderr': 0.04458029125470973,\n",
       "    'acc_norm': 0.4603174603174603,\n",
       "    'acc_norm_stderr': 0.04458029125470973},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6096774193548387,\n",
       "    'acc_stderr': 0.027751256636969576,\n",
       "    'acc_norm': 0.6096774193548387,\n",
       "    'acc_norm_stderr': 0.027751256636969576},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3891625615763547,\n",
       "    'acc_stderr': 0.034304624161038716,\n",
       "    'acc_norm': 0.3891625615763547,\n",
       "    'acc_norm_stderr': 0.034304624161038716},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.036639749943912434,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.036639749943912434},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.03191178226713549,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.03191178226713549},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7616580310880829,\n",
       "    'acc_stderr': 0.03074890536390989,\n",
       "    'acc_norm': 0.7616580310880829,\n",
       "    'acc_norm_stderr': 0.03074890536390989},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5153846153846153,\n",
       "    'acc_stderr': 0.02533900301010651,\n",
       "    'acc_norm': 0.5153846153846153,\n",
       "    'acc_norm_stderr': 0.02533900301010651},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.34074074074074073,\n",
       "    'acc_stderr': 0.02889774874113114,\n",
       "    'acc_norm': 0.34074074074074073,\n",
       "    'acc_norm_stderr': 0.02889774874113114},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5168067226890757,\n",
       "    'acc_stderr': 0.03246013680375308,\n",
       "    'acc_norm': 0.5168067226890757,\n",
       "    'acc_norm_stderr': 0.03246013680375308},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.36423841059602646,\n",
       "    'acc_stderr': 0.03929111781242742,\n",
       "    'acc_norm': 0.36423841059602646,\n",
       "    'acc_norm_stderr': 0.03929111781242742},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7064220183486238,\n",
       "    'acc_stderr': 0.019525151122639667,\n",
       "    'acc_norm': 0.7064220183486238,\n",
       "    'acc_norm_stderr': 0.019525151122639667},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.033247089118091176,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.033247089118091176},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.029102254389674082,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.029102254389674082},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7637130801687764,\n",
       "    'acc_stderr': 0.027652153144159256,\n",
       "    'acc_norm': 0.7637130801687764,\n",
       "    'acc_norm_stderr': 0.027652153144159256},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6098654708520179,\n",
       "    'acc_stderr': 0.03273766725459156,\n",
       "    'acc_norm': 0.6098654708520179,\n",
       "    'acc_norm_stderr': 0.03273766725459156},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6259541984732825,\n",
       "    'acc_stderr': 0.042438692422305246,\n",
       "    'acc_norm': 0.6259541984732825,\n",
       "    'acc_norm_stderr': 0.042438692422305246},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7355371900826446,\n",
       "    'acc_stderr': 0.040261875275912046,\n",
       "    'acc_norm': 0.7355371900826446,\n",
       "    'acc_norm_stderr': 0.040261875275912046},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.04557239513497751,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.04557239513497751},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6993865030674846,\n",
       "    'acc_stderr': 0.03602511318806771,\n",
       "    'acc_norm': 0.6993865030674846,\n",
       "    'acc_norm_stderr': 0.03602511318806771},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.38392857142857145,\n",
       "    'acc_stderr': 0.04616143075028547,\n",
       "    'acc_norm': 0.38392857142857145,\n",
       "    'acc_norm_stderr': 0.04616143075028547},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.046561471100123514,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.046561471100123514},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.811965811965812,\n",
       "    'acc_stderr': 0.025598193686652265,\n",
       "    'acc_norm': 0.811965811965812,\n",
       "    'acc_norm_stderr': 0.025598193686652265},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.0498887651569859,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.0498887651569859},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7075351213282248,\n",
       "    'acc_stderr': 0.016267000684598645,\n",
       "    'acc_norm': 0.7075351213282248,\n",
       "    'acc_norm_stderr': 0.016267000684598645},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5780346820809249,\n",
       "    'acc_stderr': 0.02658923114217426,\n",
       "    'acc_norm': 0.5780346820809249,\n",
       "    'acc_norm_stderr': 0.02658923114217426},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.31620111731843575,\n",
       "    'acc_stderr': 0.015551673652172554,\n",
       "    'acc_norm': 0.31620111731843575,\n",
       "    'acc_norm_stderr': 0.015551673652172554},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5915032679738562,\n",
       "    'acc_stderr': 0.028146405993096358,\n",
       "    'acc_norm': 0.5915032679738562,\n",
       "    'acc_norm_stderr': 0.028146405993096358},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.594855305466238,\n",
       "    'acc_stderr': 0.027882383791325963,\n",
       "    'acc_norm': 0.594855305466238,\n",
       "    'acc_norm_stderr': 0.027882383791325963},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6141975308641975,\n",
       "    'acc_stderr': 0.027085401226132143,\n",
       "    'acc_norm': 0.6141975308641975,\n",
       "    'acc_norm_stderr': 0.027085401226132143},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.40425531914893614,\n",
       "    'acc_stderr': 0.029275532159704725,\n",
       "    'acc_norm': 0.40425531914893614,\n",
       "    'acc_norm_stderr': 0.029275532159704725},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3891786179921773,\n",
       "    'acc_stderr': 0.012452613934287012,\n",
       "    'acc_norm': 0.3891786179921773,\n",
       "    'acc_norm_stderr': 0.012452613934287012},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.47794117647058826,\n",
       "    'acc_stderr': 0.030343264224213535,\n",
       "    'acc_norm': 0.47794117647058826,\n",
       "    'acc_norm_stderr': 0.030343264224213535},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.020227834851568375,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.020227834851568375},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6489795918367347,\n",
       "    'acc_stderr': 0.03055531675557364,\n",
       "    'acc_norm': 0.6489795918367347,\n",
       "    'acc_norm_stderr': 0.03055531675557364},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7611940298507462,\n",
       "    'acc_stderr': 0.03014777593540922,\n",
       "    'acc_norm': 0.7611940298507462,\n",
       "    'acc_norm_stderr': 0.03014777593540922},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.041633319989322626,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.041633319989322626},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.42168674698795183,\n",
       "    'acc_stderr': 0.03844453181770917,\n",
       "    'acc_norm': 0.42168674698795183,\n",
       "    'acc_norm_stderr': 0.03844453181770917},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7485380116959064,\n",
       "    'acc_stderr': 0.033275044238468436,\n",
       "    'acc_norm': 0.7485380116959064,\n",
       "    'acc_norm_stderr': 0.033275044238468436},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.44437538633055657,\n",
       "    'mc2_stderr': 0.014550940721814704}}},\n",
       " 'dolly-v2-12b': {'key': 'dolly-v2-12b',\n",
       "  'Model': 'Dolly-V2-12B',\n",
       "  'MT-bench (score)': '3.28',\n",
       "  'MMLU': '0.257',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'Databricks',\n",
       "  'Link': 'https://huggingface.co/databricks/dolly-v2-12b',\n",
       "  'results': {'harness|drop|3': {'em': 0.0016778523489932886,\n",
       "    'em_stderr': 0.0004191330178826844,\n",
       "    'f1': 0.06285968959731549,\n",
       "    'f1_stderr': 0.0014820300080071475},\n",
       "   'harness|gsm8k|5': {'acc': 0.012130401819560273,\n",
       "    'acc_stderr': 0.0030152942428909495},\n",
       "   'harness|winogrande|5': {'acc': 0.6085240726124704,\n",
       "    'acc_stderr': 0.013717487071290854},\n",
       "   'all': {'acc': 0.2661068958576839,\n",
       "    'acc_stderr': 0.03186337801557282,\n",
       "    'acc_norm': 0.26986300658126056,\n",
       "    'acc_norm_stderr': 0.0318588678937482,\n",
       "    'mc1': 0.200734394124847,\n",
       "    'mc1_stderr': 0.014022045717482156,\n",
       "    'mc2': 0.33827081326692277,\n",
       "    'mc2_stderr': 0.014862542606576355},\n",
       "   'harness|arc:challenge|25': {'acc': 0.38139931740614336,\n",
       "    'acc_stderr': 0.01419438908668526,\n",
       "    'acc_norm': 0.42406143344709896,\n",
       "    'acc_norm_stderr': 0.0144418896274644},\n",
       "   'harness|hellaswag|10': {'acc': 0.5463055168293168,\n",
       "    'acc_stderr': 0.0049683371441363675,\n",
       "    'acc_norm': 0.7252539334793866,\n",
       "    'acc_norm_stderr': 0.00445473941570504},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.04072314811876837,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.04072314811876837},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.2631578947368421,\n",
       "    'acc_stderr': 0.03583496176361064,\n",
       "    'acc_norm': 0.2631578947368421,\n",
       "    'acc_norm_stderr': 0.03583496176361064},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816507,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816507},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.22264150943396227,\n",
       "    'acc_stderr': 0.0256042334708991,\n",
       "    'acc_norm': 0.22264150943396227,\n",
       "    'acc_norm_stderr': 0.0256042334708991},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2638888888888889,\n",
       "    'acc_stderr': 0.03685651095897532,\n",
       "    'acc_norm': 0.2638888888888889,\n",
       "    'acc_norm_stderr': 0.03685651095897532},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.18,\n",
       "    'acc_stderr': 0.038612291966536955,\n",
       "    'acc_norm': 0.18,\n",
       "    'acc_norm_stderr': 0.038612291966536955},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.2,\n",
       "    'acc_stderr': 0.04020151261036845,\n",
       "    'acc_norm': 0.2,\n",
       "    'acc_norm_stderr': 0.04020151261036845},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.23699421965317918,\n",
       "    'acc_stderr': 0.03242414757483099,\n",
       "    'acc_norm': 0.23699421965317918,\n",
       "    'acc_norm_stderr': 0.03242414757483099},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.23529411764705882,\n",
       "    'acc_stderr': 0.04220773659171452,\n",
       "    'acc_norm': 0.23529411764705882,\n",
       "    'acc_norm_stderr': 0.04220773659171452},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.23829787234042554,\n",
       "    'acc_stderr': 0.027851252973889788,\n",
       "    'acc_norm': 0.23829787234042554,\n",
       "    'acc_norm_stderr': 0.027851252973889788},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2543859649122807,\n",
       "    'acc_stderr': 0.040969851398436695,\n",
       "    'acc_norm': 0.2543859649122807,\n",
       "    'acc_norm_stderr': 0.040969851398436695},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.2827586206896552,\n",
       "    'acc_stderr': 0.03752833958003337,\n",
       "    'acc_norm': 0.2827586206896552,\n",
       "    'acc_norm_stderr': 0.03752833958003337},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.2698412698412698,\n",
       "    'acc_stderr': 0.02286083830923207,\n",
       "    'acc_norm': 0.2698412698412698,\n",
       "    'acc_norm_stderr': 0.02286083830923207},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.1984126984126984,\n",
       "    'acc_stderr': 0.035670166752768635,\n",
       "    'acc_norm': 0.1984126984126984,\n",
       "    'acc_norm_stderr': 0.035670166752768635},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.26129032258064516,\n",
       "    'acc_stderr': 0.024993053397764822,\n",
       "    'acc_norm': 0.26129032258064516,\n",
       "    'acc_norm_stderr': 0.024993053397764822},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.2512315270935961,\n",
       "    'acc_stderr': 0.030516530732694436,\n",
       "    'acc_norm': 0.2512315270935961,\n",
       "    'acc_norm_stderr': 0.030516530732694436},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.20606060606060606,\n",
       "    'acc_stderr': 0.03158415324047708,\n",
       "    'acc_norm': 0.20606060606060606,\n",
       "    'acc_norm_stderr': 0.03158415324047708},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.20707070707070707,\n",
       "    'acc_stderr': 0.02886977846026705,\n",
       "    'acc_norm': 0.20707070707070707,\n",
       "    'acc_norm_stderr': 0.02886977846026705},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.23316062176165803,\n",
       "    'acc_stderr': 0.03051611137147601,\n",
       "    'acc_norm': 0.23316062176165803,\n",
       "    'acc_norm_stderr': 0.03051611137147601},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.23076923076923078,\n",
       "    'acc_stderr': 0.02136202772522273,\n",
       "    'acc_norm': 0.23076923076923078,\n",
       "    'acc_norm_stderr': 0.02136202772522273},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.26666666666666666,\n",
       "    'acc_stderr': 0.026962424325073835,\n",
       "    'acc_norm': 0.26666666666666666,\n",
       "    'acc_norm_stderr': 0.026962424325073835},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.226890756302521,\n",
       "    'acc_stderr': 0.02720537153827949,\n",
       "    'acc_norm': 0.226890756302521,\n",
       "    'acc_norm_stderr': 0.02720537153827949},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.23178807947019867,\n",
       "    'acc_stderr': 0.03445406271987054,\n",
       "    'acc_norm': 0.23178807947019867,\n",
       "    'acc_norm_stderr': 0.03445406271987054},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.26605504587155965,\n",
       "    'acc_stderr': 0.018946022322225593,\n",
       "    'acc_norm': 0.26605504587155965,\n",
       "    'acc_norm_stderr': 0.018946022322225593},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.20833333333333334,\n",
       "    'acc_stderr': 0.02769691071309394,\n",
       "    'acc_norm': 0.20833333333333334,\n",
       "    'acc_norm_stderr': 0.02769691071309394},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.2696078431372549,\n",
       "    'acc_stderr': 0.031145570659486782,\n",
       "    'acc_norm': 0.2696078431372549,\n",
       "    'acc_norm_stderr': 0.031145570659486782},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.32489451476793246,\n",
       "    'acc_stderr': 0.030486039389105307,\n",
       "    'acc_norm': 0.32489451476793246,\n",
       "    'acc_norm_stderr': 0.030486039389105307},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.29596412556053814,\n",
       "    'acc_stderr': 0.0306365913486998,\n",
       "    'acc_norm': 0.29596412556053814,\n",
       "    'acc_norm_stderr': 0.0306365913486998},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.22137404580152673,\n",
       "    'acc_stderr': 0.03641297081313729,\n",
       "    'acc_norm': 0.22137404580152673,\n",
       "    'acc_norm_stderr': 0.03641297081313729},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.38016528925619836,\n",
       "    'acc_stderr': 0.04431324501968431,\n",
       "    'acc_norm': 0.38016528925619836,\n",
       "    'acc_norm_stderr': 0.04431324501968431},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.2222222222222222,\n",
       "    'acc_stderr': 0.040191074725573483,\n",
       "    'acc_norm': 0.2222222222222222,\n",
       "    'acc_norm_stderr': 0.040191074725573483},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2392638036809816,\n",
       "    'acc_stderr': 0.03351953879521272,\n",
       "    'acc_norm': 0.2392638036809816,\n",
       "    'acc_norm_stderr': 0.03351953879521272},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.3482142857142857,\n",
       "    'acc_stderr': 0.045218299028335865,\n",
       "    'acc_norm': 0.3482142857142857,\n",
       "    'acc_norm_stderr': 0.045218299028335865},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.24271844660194175,\n",
       "    'acc_stderr': 0.04245022486384493,\n",
       "    'acc_norm': 0.24271844660194175,\n",
       "    'acc_norm_stderr': 0.04245022486384493},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.27350427350427353,\n",
       "    'acc_stderr': 0.029202540153431187,\n",
       "    'acc_norm': 0.27350427350427353,\n",
       "    'acc_norm_stderr': 0.029202540153431187},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.041633319989322695,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.041633319989322695},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.29246487867177523,\n",
       "    'acc_stderr': 0.016267000684598645,\n",
       "    'acc_norm': 0.29246487867177523,\n",
       "    'acc_norm_stderr': 0.016267000684598645},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.2630057803468208,\n",
       "    'acc_stderr': 0.023703099525258176,\n",
       "    'acc_norm': 0.2630057803468208,\n",
       "    'acc_norm_stderr': 0.023703099525258176},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961443,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961443},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.24509803921568626,\n",
       "    'acc_stderr': 0.02463004897982478,\n",
       "    'acc_norm': 0.24509803921568626,\n",
       "    'acc_norm_stderr': 0.02463004897982478},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.3054662379421222,\n",
       "    'acc_stderr': 0.026160584450140495,\n",
       "    'acc_norm': 0.3054662379421222,\n",
       "    'acc_norm_stderr': 0.026160584450140495},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.02409347123262133,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.02409347123262133},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2695035460992908,\n",
       "    'acc_stderr': 0.02646903681859062,\n",
       "    'acc_norm': 0.2695035460992908,\n",
       "    'acc_norm_stderr': 0.02646903681859062},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.2692307692307692,\n",
       "    'acc_stderr': 0.011328734403140308,\n",
       "    'acc_norm': 0.2692307692307692,\n",
       "    'acc_norm_stderr': 0.011328734403140308},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.18382352941176472,\n",
       "    'acc_stderr': 0.02352924218519311,\n",
       "    'acc_norm': 0.18382352941176472,\n",
       "    'acc_norm_stderr': 0.02352924218519311},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.01784808957491322,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.01784808957491322},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.2818181818181818,\n",
       "    'acc_stderr': 0.0430911870994646,\n",
       "    'acc_norm': 0.2818181818181818,\n",
       "    'acc_norm_stderr': 0.0430911870994646},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.17551020408163265,\n",
       "    'acc_stderr': 0.024352800722970015,\n",
       "    'acc_norm': 0.17551020408163265,\n",
       "    'acc_norm_stderr': 0.024352800722970015},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.2935323383084577,\n",
       "    'acc_stderr': 0.03220024104534205,\n",
       "    'acc_norm': 0.2935323383084577,\n",
       "    'acc_norm_stderr': 0.03220024104534205},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.044084400227680794,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.044084400227680794},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.26506024096385544,\n",
       "    'acc_stderr': 0.03436024037944967,\n",
       "    'acc_norm': 0.26506024096385544,\n",
       "    'acc_norm_stderr': 0.03436024037944967},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.03565079670708311,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.03565079670708311},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.200734394124847,\n",
       "    'mc1_stderr': 0.014022045717482156,\n",
       "    'mc2': 0.33827081326692277,\n",
       "    'mc2_stderr': 0.014862542606576355}}},\n",
       " 'dolphin-2.2.1-mistral-7b': {'key': 'dolphin-2.2.1-mistral-7b',\n",
       "  'Model': 'Dolphin-2.2.1-Mistral-7B',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '-',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'Cognitive Computations',\n",
       "  'Link': 'https://huggingface.co/ehartford/dolphin-2.2.1-mistral-7b',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6049488054607508,\n",
       "    'acc_stderr': 0.014285898292938167,\n",
       "    'acc_norm': 0.6331058020477816,\n",
       "    'acc_norm_stderr': 0.014084133118104301},\n",
       "   'harness|hellaswag|10': {'acc': 0.6431985660227046,\n",
       "    'acc_stderr': 0.004780764443411322,\n",
       "    'acc_norm': 0.8375821549492133,\n",
       "    'acc_norm_stderr': 0.0036807989505319113},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.04218506215368879,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.04218506215368879},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6776315789473685,\n",
       "    'acc_stderr': 0.03803510248351585,\n",
       "    'acc_norm': 0.6776315789473685,\n",
       "    'acc_norm_stderr': 0.03803510248351585},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.028727502957880267,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.028727502957880267},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03621034121889507,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03621034121889507},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.04878317312145633,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.04878317312145633},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6358381502890174,\n",
       "    'acc_stderr': 0.03669072477416907,\n",
       "    'acc_norm': 0.6358381502890174,\n",
       "    'acc_norm_stderr': 0.03669072477416907},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.35294117647058826,\n",
       "    'acc_stderr': 0.04755129616062947,\n",
       "    'acc_norm': 0.35294117647058826,\n",
       "    'acc_norm_stderr': 0.04755129616062947},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.04093601807403326,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.04093601807403326},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5446808510638298,\n",
       "    'acc_stderr': 0.03255525359340354,\n",
       "    'acc_norm': 0.5446808510638298,\n",
       "    'acc_norm_stderr': 0.03255525359340354},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.45614035087719296,\n",
       "    'acc_stderr': 0.04685473041907789,\n",
       "    'acc_norm': 0.45614035087719296,\n",
       "    'acc_norm_stderr': 0.04685473041907789},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.593103448275862,\n",
       "    'acc_stderr': 0.04093793981266236,\n",
       "    'acc_norm': 0.593103448275862,\n",
       "    'acc_norm_stderr': 0.04093793981266236},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3862433862433862,\n",
       "    'acc_stderr': 0.025075981767601684,\n",
       "    'acc_norm': 0.3862433862433862,\n",
       "    'acc_norm_stderr': 0.025075981767601684},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3968253968253968,\n",
       "    'acc_stderr': 0.04375888492727062,\n",
       "    'acc_norm': 0.3968253968253968,\n",
       "    'acc_norm_stderr': 0.04375888492727062},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7709677419354839,\n",
       "    'acc_stderr': 0.02390491431178265,\n",
       "    'acc_norm': 0.7709677419354839,\n",
       "    'acc_norm_stderr': 0.02390491431178265},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4876847290640394,\n",
       "    'acc_stderr': 0.035169204442208966,\n",
       "    'acc_norm': 0.4876847290640394,\n",
       "    'acc_norm_stderr': 0.035169204442208966},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.69,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.69,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7696969696969697,\n",
       "    'acc_stderr': 0.0328766675860349,\n",
       "    'acc_norm': 0.7696969696969697,\n",
       "    'acc_norm_stderr': 0.0328766675860349},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7929292929292929,\n",
       "    'acc_stderr': 0.028869778460267025,\n",
       "    'acc_norm': 0.7929292929292929,\n",
       "    'acc_norm_stderr': 0.028869778460267025},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8704663212435233,\n",
       "    'acc_stderr': 0.02423353229775873,\n",
       "    'acc_norm': 0.8704663212435233,\n",
       "    'acc_norm_stderr': 0.02423353229775873},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6512820512820513,\n",
       "    'acc_stderr': 0.02416278028401772,\n",
       "    'acc_norm': 0.6512820512820513,\n",
       "    'acc_norm_stderr': 0.02416278028401772},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028593,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.030388353551886783,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.030388353551886783},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.304635761589404,\n",
       "    'acc_stderr': 0.03757949922943343,\n",
       "    'acc_norm': 0.304635761589404,\n",
       "    'acc_norm_stderr': 0.03757949922943343},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8348623853211009,\n",
       "    'acc_stderr': 0.015919557829976044,\n",
       "    'acc_norm': 0.8348623853211009,\n",
       "    'acc_norm_stderr': 0.015919557829976044},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.034076320938540516,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.034076320938540516},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7696078431372549,\n",
       "    'acc_stderr': 0.02955429260569508,\n",
       "    'acc_norm': 0.7696078431372549,\n",
       "    'acc_norm_stderr': 0.02955429260569508},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7763713080168776,\n",
       "    'acc_stderr': 0.027123298205229966,\n",
       "    'acc_norm': 0.7763713080168776,\n",
       "    'acc_norm_stderr': 0.027123298205229966},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6771300448430493,\n",
       "    'acc_stderr': 0.03138147637575499,\n",
       "    'acc_norm': 0.6771300448430493,\n",
       "    'acc_norm_stderr': 0.03138147637575499},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7786259541984732,\n",
       "    'acc_stderr': 0.0364129708131373,\n",
       "    'acc_norm': 0.7786259541984732,\n",
       "    'acc_norm_stderr': 0.0364129708131373},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7933884297520661,\n",
       "    'acc_stderr': 0.03695980128098823,\n",
       "    'acc_norm': 0.7933884297520661,\n",
       "    'acc_norm_stderr': 0.03695980128098823},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8055555555555556,\n",
       "    'acc_stderr': 0.03826076324884866,\n",
       "    'acc_norm': 0.8055555555555556,\n",
       "    'acc_norm_stderr': 0.03826076324884866},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7607361963190185,\n",
       "    'acc_stderr': 0.033519538795212696,\n",
       "    'acc_norm': 0.7607361963190185,\n",
       "    'acc_norm_stderr': 0.033519538795212696},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7669902912621359,\n",
       "    'acc_stderr': 0.04185832598928315,\n",
       "    'acc_norm': 0.7669902912621359,\n",
       "    'acc_norm_stderr': 0.04185832598928315},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8632478632478633,\n",
       "    'acc_stderr': 0.022509033937077805,\n",
       "    'acc_norm': 0.8632478632478633,\n",
       "    'acc_norm_stderr': 0.022509033937077805},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768078,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768078},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8135376756066411,\n",
       "    'acc_stderr': 0.013927751372001506,\n",
       "    'acc_norm': 0.8135376756066411,\n",
       "    'acc_norm_stderr': 0.013927751372001506},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7138728323699421,\n",
       "    'acc_stderr': 0.02433214677913413,\n",
       "    'acc_norm': 0.7138728323699421,\n",
       "    'acc_norm_stderr': 0.02433214677913413},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.37988826815642457,\n",
       "    'acc_stderr': 0.016232826818678502,\n",
       "    'acc_norm': 0.37988826815642457,\n",
       "    'acc_norm_stderr': 0.016232826818678502},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7058823529411765,\n",
       "    'acc_stderr': 0.026090162504279053,\n",
       "    'acc_norm': 0.7058823529411765,\n",
       "    'acc_norm_stderr': 0.026090162504279053},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7202572347266881,\n",
       "    'acc_stderr': 0.025494259350694912,\n",
       "    'acc_norm': 0.7202572347266881,\n",
       "    'acc_norm_stderr': 0.025494259350694912},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7098765432098766,\n",
       "    'acc_stderr': 0.025251173936495026,\n",
       "    'acc_norm': 0.7098765432098766,\n",
       "    'acc_norm_stderr': 0.025251173936495026},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4787234042553192,\n",
       "    'acc_stderr': 0.029800481645628693,\n",
       "    'acc_norm': 0.4787234042553192,\n",
       "    'acc_norm_stderr': 0.029800481645628693},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4471968709256845,\n",
       "    'acc_stderr': 0.012698825252435111,\n",
       "    'acc_norm': 0.4471968709256845,\n",
       "    'acc_norm_stderr': 0.012698825252435111},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6470588235294118,\n",
       "    'acc_stderr': 0.029029422815681397,\n",
       "    'acc_norm': 0.6470588235294118,\n",
       "    'acc_norm_stderr': 0.029029422815681397},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6503267973856209,\n",
       "    'acc_stderr': 0.01929196189506638,\n",
       "    'acc_norm': 0.6503267973856209,\n",
       "    'acc_norm_stderr': 0.01929196189506638},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.726530612244898,\n",
       "    'acc_stderr': 0.02853556033712844,\n",
       "    'acc_norm': 0.726530612244898,\n",
       "    'acc_norm_stderr': 0.02853556033712844},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.835820895522388,\n",
       "    'acc_stderr': 0.026193923544454125,\n",
       "    'acc_norm': 0.835820895522388,\n",
       "    'acc_norm_stderr': 0.026193923544454125},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.03379976689896309,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.03379976689896309},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5481927710843374,\n",
       "    'acc_stderr': 0.03874371556587953,\n",
       "    'acc_norm': 0.5481927710843374,\n",
       "    'acc_norm_stderr': 0.03874371556587953},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8304093567251462,\n",
       "    'acc_stderr': 0.02878210810540171,\n",
       "    'acc_norm': 0.8304093567251462,\n",
       "    'acc_norm_stderr': 0.02878210810540171},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3659730722154223,\n",
       "    'mc1_stderr': 0.016862941684088365,\n",
       "    'mc2': 0.5311447373702662,\n",
       "    'mc2_stderr': 0.015062742496541512},\n",
       "   'harness|winogrande|5': {'acc': 0.7813733228097869,\n",
       "    'acc_stderr': 0.01161619821577323},\n",
       "   'harness|drop|3': {'em': 0.004299496644295302,\n",
       "    'em_stderr': 0.0006700586558630193,\n",
       "    'f1': 0.081867659395973,\n",
       "    'f1_stderr': 0.0016972961971096978},\n",
       "   'harness|gsm8k|5': {'acc': 0.4806671721000758,\n",
       "    'acc_stderr': 0.013762185709851354},\n",
       "   'all': {'acc': 0.6314567324183159,\n",
       "    'acc_stderr': 0.032318316802746,\n",
       "    'acc_norm': 0.6352434028495076,\n",
       "    'acc_norm_stderr': 0.032961647633460475,\n",
       "    'mc1': 0.3659730722154223,\n",
       "    'mc1_stderr': 0.016862941684088365,\n",
       "    'mc2': 0.5311447373702662,\n",
       "    'mc2_stderr': 0.015062742496541512}}},\n",
       " 'falcon-180b-chat': {'key': 'falcon-180b-chat',\n",
       "  'Model': 'falcon-180b-chat',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.680',\n",
       "  'License': 'Falcon-180B TII License',\n",
       "  'Organization': 'TII',\n",
       "  'Link': 'https://huggingface.co/tiiuae/falcon-180B-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6143344709897611,\n",
       "    'acc_stderr': 0.014224250973257182,\n",
       "    'acc_norm': 0.64419795221843,\n",
       "    'acc_norm_stderr': 0.01399057113791876},\n",
       "   'harness|hellaswag|10': {'acc': 0.6904999004182434,\n",
       "    'acc_stderr': 0.004613427745209517,\n",
       "    'acc_norm': 0.8804023102967536,\n",
       "    'acc_norm_stderr': 0.0032382732952847414},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.04725815626252606,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.04725815626252606},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6518518518518519,\n",
       "    'acc_stderr': 0.041153246103369526,\n",
       "    'acc_norm': 0.6518518518518519,\n",
       "    'acc_norm_stderr': 0.041153246103369526},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.743421052631579,\n",
       "    'acc_stderr': 0.0355418036802569,\n",
       "    'acc_norm': 0.743421052631579,\n",
       "    'acc_norm_stderr': 0.0355418036802569},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.7018867924528301,\n",
       "    'acc_stderr': 0.028152837942493857,\n",
       "    'acc_norm': 0.7018867924528301,\n",
       "    'acc_norm_stderr': 0.028152837942493857},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.8263888888888888,\n",
       "    'acc_stderr': 0.03167473383795718,\n",
       "    'acc_norm': 0.8263888888888888,\n",
       "    'acc_norm_stderr': 0.03167473383795718},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6473988439306358,\n",
       "    'acc_stderr': 0.036430371689585475,\n",
       "    'acc_norm': 0.6473988439306358,\n",
       "    'acc_norm_stderr': 0.036430371689585475},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3137254901960784,\n",
       "    'acc_stderr': 0.04617034827006717,\n",
       "    'acc_norm': 0.3137254901960784,\n",
       "    'acc_norm_stderr': 0.04617034827006717},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932263,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932263},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.6510638297872341,\n",
       "    'acc_stderr': 0.03115852213135778,\n",
       "    'acc_norm': 0.6510638297872341,\n",
       "    'acc_norm_stderr': 0.03115852213135778},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.4824561403508772,\n",
       "    'acc_stderr': 0.04700708033551038,\n",
       "    'acc_norm': 0.4824561403508772,\n",
       "    'acc_norm_stderr': 0.04700708033551038},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.6137931034482759,\n",
       "    'acc_stderr': 0.04057324734419036,\n",
       "    'acc_norm': 0.6137931034482759,\n",
       "    'acc_norm_stderr': 0.04057324734419036},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.455026455026455,\n",
       "    'acc_stderr': 0.02564692836104939,\n",
       "    'acc_norm': 0.455026455026455,\n",
       "    'acc_norm_stderr': 0.02564692836104939},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4365079365079365,\n",
       "    'acc_stderr': 0.04435932892851466,\n",
       "    'acc_norm': 0.4365079365079365,\n",
       "    'acc_norm_stderr': 0.04435932892851466},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7967741935483871,\n",
       "    'acc_stderr': 0.02289168798455495,\n",
       "    'acc_norm': 0.7967741935483871,\n",
       "    'acc_norm_stderr': 0.02289168798455495},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.541871921182266,\n",
       "    'acc_stderr': 0.03505630140785741,\n",
       "    'acc_norm': 0.541871921182266,\n",
       "    'acc_norm_stderr': 0.03505630140785741},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.03123475237772117,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.03123475237772117},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8484848484848485,\n",
       "    'acc_stderr': 0.025545650426603617,\n",
       "    'acc_norm': 0.8484848484848485,\n",
       "    'acc_norm_stderr': 0.025545650426603617},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9481865284974094,\n",
       "    'acc_stderr': 0.01599622932024412,\n",
       "    'acc_norm': 0.9481865284974094,\n",
       "    'acc_norm_stderr': 0.01599622932024412},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6820512820512821,\n",
       "    'acc_stderr': 0.02361088430892786,\n",
       "    'acc_norm': 0.6820512820512821,\n",
       "    'acc_norm_stderr': 0.02361088430892786},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028597,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028597},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7689075630252101,\n",
       "    'acc_stderr': 0.027381406927868886,\n",
       "    'acc_norm': 0.7689075630252101,\n",
       "    'acc_norm_stderr': 0.027381406927868886},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3973509933774834,\n",
       "    'acc_stderr': 0.039955240076816806,\n",
       "    'acc_norm': 0.3973509933774834,\n",
       "    'acc_norm_stderr': 0.039955240076816806},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8715596330275229,\n",
       "    'acc_stderr': 0.014344977542914318,\n",
       "    'acc_norm': 0.8715596330275229,\n",
       "    'acc_norm_stderr': 0.014344977542914318},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5416666666666666,\n",
       "    'acc_stderr': 0.033981108902946366,\n",
       "    'acc_norm': 0.5416666666666666,\n",
       "    'acc_norm_stderr': 0.033981108902946366},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8578431372549019,\n",
       "    'acc_stderr': 0.02450980392156862,\n",
       "    'acc_norm': 0.8578431372549019,\n",
       "    'acc_norm_stderr': 0.02450980392156862},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8607594936708861,\n",
       "    'acc_stderr': 0.0225355263526927,\n",
       "    'acc_norm': 0.8607594936708861,\n",
       "    'acc_norm_stderr': 0.0225355263526927},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7668161434977578,\n",
       "    'acc_stderr': 0.028380391147094702,\n",
       "    'acc_norm': 0.7668161434977578,\n",
       "    'acc_norm_stderr': 0.028380391147094702},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8396946564885496,\n",
       "    'acc_stderr': 0.03217829420744631,\n",
       "    'acc_norm': 0.8396946564885496,\n",
       "    'acc_norm_stderr': 0.03217829420744631},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03520893951097653,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03520893951097653},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8611111111111112,\n",
       "    'acc_stderr': 0.0334327006286962,\n",
       "    'acc_norm': 0.8611111111111112,\n",
       "    'acc_norm_stderr': 0.0334327006286962},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.8343558282208589,\n",
       "    'acc_stderr': 0.029208296231259104,\n",
       "    'acc_norm': 0.8343558282208589,\n",
       "    'acc_norm_stderr': 0.029208296231259104},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.5446428571428571,\n",
       "    'acc_stderr': 0.04726835553719098,\n",
       "    'acc_norm': 0.5446428571428571,\n",
       "    'acc_norm_stderr': 0.04726835553719098},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8446601941747572,\n",
       "    'acc_stderr': 0.03586594738573975,\n",
       "    'acc_norm': 0.8446601941747572,\n",
       "    'acc_norm_stderr': 0.03586594738573975},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8931623931623932,\n",
       "    'acc_stderr': 0.02023714900899093,\n",
       "    'acc_norm': 0.8931623931623932,\n",
       "    'acc_norm_stderr': 0.02023714900899093},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932261,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932261},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8671775223499362,\n",
       "    'acc_stderr': 0.012136303209884562,\n",
       "    'acc_norm': 0.8671775223499362,\n",
       "    'acc_norm_stderr': 0.012136303209884562},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7745664739884393,\n",
       "    'acc_stderr': 0.022497230190967558,\n",
       "    'acc_norm': 0.7745664739884393,\n",
       "    'acc_norm_stderr': 0.022497230190967558},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.4659217877094972,\n",
       "    'acc_stderr': 0.016683615837486863,\n",
       "    'acc_norm': 0.4659217877094972,\n",
       "    'acc_norm_stderr': 0.016683615837486863},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7287581699346405,\n",
       "    'acc_stderr': 0.02545775669666788,\n",
       "    'acc_norm': 0.7287581699346405,\n",
       "    'acc_norm_stderr': 0.02545775669666788},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7877813504823151,\n",
       "    'acc_stderr': 0.02322275679743512,\n",
       "    'acc_norm': 0.7877813504823151,\n",
       "    'acc_norm_stderr': 0.02322275679743512},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7870370370370371,\n",
       "    'acc_stderr': 0.022779719088733403,\n",
       "    'acc_norm': 0.7870370370370371,\n",
       "    'acc_norm_stderr': 0.022779719088733403},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5460992907801419,\n",
       "    'acc_stderr': 0.02970045324729146,\n",
       "    'acc_norm': 0.5460992907801419,\n",
       "    'acc_norm_stderr': 0.02970045324729146},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5345501955671447,\n",
       "    'acc_stderr': 0.012739711554045713,\n",
       "    'acc_norm': 0.5345501955671447,\n",
       "    'acc_norm_stderr': 0.012739711554045713},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6875,\n",
       "    'acc_stderr': 0.02815637344037142,\n",
       "    'acc_norm': 0.6875,\n",
       "    'acc_norm_stderr': 0.02815637344037142},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.7271241830065359,\n",
       "    'acc_stderr': 0.018020474148393577,\n",
       "    'acc_norm': 0.7271241830065359,\n",
       "    'acc_norm_stderr': 0.018020474148393577},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.763265306122449,\n",
       "    'acc_stderr': 0.027212835884073156,\n",
       "    'acc_norm': 0.763265306122449,\n",
       "    'acc_norm_stderr': 0.027212835884073156},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8258706467661692,\n",
       "    'acc_stderr': 0.026814951200421603,\n",
       "    'acc_norm': 0.8258706467661692,\n",
       "    'acc_norm_stderr': 0.026814951200421603},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.89,\n",
       "    'acc_stderr': 0.03144660377352203,\n",
       "    'acc_norm': 0.89,\n",
       "    'acc_norm_stderr': 0.03144660377352203},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5240963855421686,\n",
       "    'acc_stderr': 0.03887971849597264,\n",
       "    'acc_norm': 0.5240963855421686,\n",
       "    'acc_norm_stderr': 0.03887971849597264},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8421052631578947,\n",
       "    'acc_stderr': 0.02796678585916089,\n",
       "    'acc_norm': 0.8421052631578947,\n",
       "    'acc_norm_stderr': 0.02796678585916089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.37576499388004897,\n",
       "    'mc1_stderr': 0.016954584060214297,\n",
       "    'mc2': 0.5369457047083463,\n",
       "    'mc2_stderr': 0.014662579093423517},\n",
       "   'all': {'acc': 0.6793461177088693,\n",
       "    'acc_stderr': 0.031533098229447415,\n",
       "    'acc_norm': 0.6830709633208555,\n",
       "    'acc_norm_stderr': 0.03150582985173109,\n",
       "    'mc1': 0.37576499388004897,\n",
       "    'mc1_stderr': 0.016954584060214297,\n",
       "    'mc2': 0.5369457047083463,\n",
       "    'mc2_stderr': 0.014662579093423517}}},\n",
       " 'guanaco-33b': {'key': 'guanaco-33b',\n",
       "  'Model': 'Guanaco-33B',\n",
       "  'MT-bench (score)': '6.53',\n",
       "  'MMLU': '0.576',\n",
       "  'License': 'Non-commercial',\n",
       "  'Organization': 'UW',\n",
       "  'Link': 'https://huggingface.co/timdettmers/guanaco-33b-merged',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5870307167235495,\n",
       "    'acc_stderr': 0.014388344935398326,\n",
       "    'acc_norm': 0.6245733788395904,\n",
       "    'acc_norm_stderr': 0.014150631435111726},\n",
       "   'harness|hellaswag|10': {'acc': 0.6446922923720374,\n",
       "    'acc_stderr': 0.004776283203468098,\n",
       "    'acc_norm': 0.8447520414260108,\n",
       "    'acc_norm_stderr': 0.003614007841341989},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.047609522856952365,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.047609522856952365},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5657894736842105,\n",
       "    'acc_stderr': 0.04033565667848319,\n",
       "    'acc_norm': 0.5657894736842105,\n",
       "    'acc_norm_stderr': 0.04033565667848319},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620332,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620332},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5622641509433962,\n",
       "    'acc_stderr': 0.030533338430467516,\n",
       "    'acc_norm': 0.5622641509433962,\n",
       "    'acc_norm_stderr': 0.030533338430467516},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5625,\n",
       "    'acc_stderr': 0.04148415739394154,\n",
       "    'acc_norm': 0.5625,\n",
       "    'acc_norm_stderr': 0.04148415739394154},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3431372549019608,\n",
       "    'acc_stderr': 0.04724007352383889,\n",
       "    'acc_norm': 0.3431372549019608,\n",
       "    'acc_norm_stderr': 0.04724007352383889},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4595744680851064,\n",
       "    'acc_stderr': 0.03257901482099835,\n",
       "    'acc_norm': 0.4595744680851064,\n",
       "    'acc_norm_stderr': 0.03257901482099835},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3684210526315789,\n",
       "    'acc_stderr': 0.04537815354939391,\n",
       "    'acc_norm': 0.3684210526315789,\n",
       "    'acc_norm_stderr': 0.04537815354939391},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4413793103448276,\n",
       "    'acc_stderr': 0.04137931034482758,\n",
       "    'acc_norm': 0.4413793103448276,\n",
       "    'acc_norm_stderr': 0.04137931034482758},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.31216931216931215,\n",
       "    'acc_stderr': 0.0238652068369726,\n",
       "    'acc_norm': 0.31216931216931215,\n",
       "    'acc_norm_stderr': 0.0238652068369726},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.29365079365079366,\n",
       "    'acc_stderr': 0.04073524322147126,\n",
       "    'acc_norm': 0.29365079365079366,\n",
       "    'acc_norm_stderr': 0.04073524322147126},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6290322580645161,\n",
       "    'acc_stderr': 0.027480541887953593,\n",
       "    'acc_norm': 0.6290322580645161,\n",
       "    'acc_norm_stderr': 0.027480541887953593},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3793103448275862,\n",
       "    'acc_stderr': 0.03413963805906235,\n",
       "    'acc_norm': 0.3793103448275862,\n",
       "    'acc_norm_stderr': 0.03413963805906235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7212121212121212,\n",
       "    'acc_stderr': 0.035014387062967806,\n",
       "    'acc_norm': 0.7212121212121212,\n",
       "    'acc_norm_stderr': 0.035014387062967806},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.03191178226713547,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.03191178226713547},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7150259067357513,\n",
       "    'acc_stderr': 0.032577140777096614,\n",
       "    'acc_norm': 0.7150259067357513,\n",
       "    'acc_norm_stderr': 0.032577140777096614},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.49230769230769234,\n",
       "    'acc_stderr': 0.025348006031534778,\n",
       "    'acc_norm': 0.49230769230769234,\n",
       "    'acc_norm_stderr': 0.025348006031534778},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2777777777777778,\n",
       "    'acc_stderr': 0.02730914058823019,\n",
       "    'acc_norm': 0.2777777777777778,\n",
       "    'acc_norm_stderr': 0.02730914058823019},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03242225027115006,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03242225027115006},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.03822746937658752,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.03822746937658752},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7302752293577982,\n",
       "    'acc_stderr': 0.019028486711115438,\n",
       "    'acc_norm': 0.7302752293577982,\n",
       "    'acc_norm_stderr': 0.019028486711115438},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.375,\n",
       "    'acc_stderr': 0.033016908987210894,\n",
       "    'acc_norm': 0.375,\n",
       "    'acc_norm_stderr': 0.033016908987210894},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.02910225438967409,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.02910225438967409},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7679324894514767,\n",
       "    'acc_stderr': 0.02747974455080851,\n",
       "    'acc_norm': 0.7679324894514767,\n",
       "    'acc_norm_stderr': 0.02747974455080851},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5874439461883408,\n",
       "    'acc_stderr': 0.03304062175449297,\n",
       "    'acc_norm': 0.5874439461883408,\n",
       "    'acc_norm_stderr': 0.03304062175449297},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6412213740458015,\n",
       "    'acc_stderr': 0.04206739313864908,\n",
       "    'acc_norm': 0.6412213740458015,\n",
       "    'acc_norm_stderr': 0.04206739313864908},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7272727272727273,\n",
       "    'acc_stderr': 0.04065578140908705,\n",
       "    'acc_norm': 0.7272727272727273,\n",
       "    'acc_norm_stderr': 0.04065578140908705},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6111111111111112,\n",
       "    'acc_stderr': 0.04712821257426769,\n",
       "    'acc_norm': 0.6111111111111112,\n",
       "    'acc_norm_stderr': 0.04712821257426769},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6380368098159509,\n",
       "    'acc_stderr': 0.037757007291414416,\n",
       "    'acc_norm': 0.6380368098159509,\n",
       "    'acc_norm_stderr': 0.037757007291414416},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.38392857142857145,\n",
       "    'acc_stderr': 0.04616143075028546,\n",
       "    'acc_norm': 0.38392857142857145,\n",
       "    'acc_norm_stderr': 0.04616143075028546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6310679611650486,\n",
       "    'acc_stderr': 0.0477761518115674,\n",
       "    'acc_norm': 0.6310679611650486,\n",
       "    'acc_norm_stderr': 0.0477761518115674},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7863247863247863,\n",
       "    'acc_stderr': 0.026853450377009154,\n",
       "    'acc_norm': 0.7863247863247863,\n",
       "    'acc_norm_stderr': 0.026853450377009154},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6756066411238825,\n",
       "    'acc_stderr': 0.0167409290471627,\n",
       "    'acc_norm': 0.6756066411238825,\n",
       "    'acc_norm_stderr': 0.0167409290471627},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5491329479768786,\n",
       "    'acc_stderr': 0.026788811931562757,\n",
       "    'acc_norm': 0.5491329479768786,\n",
       "    'acc_norm_stderr': 0.026788811931562757},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2569832402234637,\n",
       "    'acc_stderr': 0.01461446582196632,\n",
       "    'acc_norm': 0.2569832402234637,\n",
       "    'acc_norm_stderr': 0.01461446582196632},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5718954248366013,\n",
       "    'acc_stderr': 0.028332397483664278,\n",
       "    'acc_norm': 0.5718954248366013,\n",
       "    'acc_norm_stderr': 0.028332397483664278},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6012861736334405,\n",
       "    'acc_stderr': 0.027809322585774496,\n",
       "    'acc_norm': 0.6012861736334405,\n",
       "    'acc_norm_stderr': 0.027809322585774496},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6172839506172839,\n",
       "    'acc_stderr': 0.027044538138402595,\n",
       "    'acc_norm': 0.6172839506172839,\n",
       "    'acc_norm_stderr': 0.027044538138402595},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.42907801418439717,\n",
       "    'acc_stderr': 0.029525914302558555,\n",
       "    'acc_norm': 0.42907801418439717,\n",
       "    'acc_norm_stderr': 0.029525914302558555},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.424380704041721,\n",
       "    'acc_stderr': 0.01262334375743002,\n",
       "    'acc_norm': 0.424380704041721,\n",
       "    'acc_norm_stderr': 0.01262334375743002},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5808823529411765,\n",
       "    'acc_stderr': 0.02997280717046462,\n",
       "    'acc_norm': 0.5808823529411765,\n",
       "    'acc_norm_stderr': 0.02997280717046462},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5212418300653595,\n",
       "    'acc_stderr': 0.020209572388600248,\n",
       "    'acc_norm': 0.5212418300653595,\n",
       "    'acc_norm_stderr': 0.020209572388600248},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.04607582090719976,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.04607582090719976},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5918367346938775,\n",
       "    'acc_stderr': 0.03146465712827424,\n",
       "    'acc_norm': 0.5918367346938775,\n",
       "    'acc_norm_stderr': 0.03146465712827424},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6915422885572139,\n",
       "    'acc_stderr': 0.032658195885126966,\n",
       "    'acc_norm': 0.6915422885572139,\n",
       "    'acc_norm_stderr': 0.032658195885126966},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.041633319989322626,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.041633319989322626},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4578313253012048,\n",
       "    'acc_stderr': 0.038786267710023595,\n",
       "    'acc_norm': 0.4578313253012048,\n",
       "    'acc_norm_stderr': 0.038786267710023595},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.34761321909424725,\n",
       "    'mc1_stderr': 0.016670769188897306,\n",
       "    'mc2': 0.5121992740888713,\n",
       "    'mc2_stderr': 0.014650490351006002},\n",
       "   'all': {'acc': 0.5404553018205109,\n",
       "    'acc_stderr': 0.03488622237927161,\n",
       "    'acc_norm': 0.5444824613318672,\n",
       "    'acc_norm_stderr': 0.03486249375448495,\n",
       "    'mc1': 0.34761321909424725,\n",
       "    'mc1_stderr': 0.016670769188897306,\n",
       "    'mc2': 0.5121992740888713,\n",
       "    'mc2_stderr': 0.014650490351006002}}},\n",
       " 'llama-2-13b-chat': {'key': 'llama-2-13b-chat',\n",
       "  'Model': 'Llama-2-13b-chat',\n",
       "  'MT-bench (score)': '6.65',\n",
       "  'MMLU': '0.536',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-13b-chat-hf',\n",
       "  'results': {'harness|drop|3': {'em': 0.1782718120805369,\n",
       "    'em_stderr': 0.003919630092588375,\n",
       "    'f1': 0.2387195889261742,\n",
       "    'f1_stderr': 0.003944947017182046},\n",
       "   'harness|gsm8k|5': {'acc': 0.15238817285822592,\n",
       "    'acc_stderr': 0.009899572254794204},\n",
       "   'harness|winogrande|5': {'acc': 0.745067087608524,\n",
       "    'acc_stderr': 0.012248806969376422},\n",
       "   'all': {'acc': 0.5479380524707899,\n",
       "    'acc_stderr': 0.03451142729909022,\n",
       "    'acc_norm': 0.5517368945804153,\n",
       "    'acc_norm_stderr': 0.03449229816957583,\n",
       "    'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.4411794590119937,\n",
       "    'mc2_stderr': 0.015755921757439843},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5563139931740614,\n",
       "    'acc_stderr': 0.014518421825670449,\n",
       "    'acc_norm': 0.590443686006826,\n",
       "    'acc_norm_stderr': 0.01437035863247244},\n",
       "   'harness|hellaswag|10': {'acc': 0.6293567018522207,\n",
       "    'acc_stderr': 0.004819899945342489,\n",
       "    'acc_norm': 0.8193586934873531,\n",
       "    'acc_norm_stderr': 0.0038393444971919545},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.046482319871173156,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.046482319871173156},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4740740740740741,\n",
       "    'acc_stderr': 0.04313531696750574,\n",
       "    'acc_norm': 0.4740740740740741,\n",
       "    'acc_norm_stderr': 0.04313531696750574},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5460526315789473,\n",
       "    'acc_stderr': 0.04051646342874142,\n",
       "    'acc_norm': 0.5460526315789473,\n",
       "    'acc_norm_stderr': 0.04051646342874142},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5849056603773585,\n",
       "    'acc_stderr': 0.03032594578928611,\n",
       "    'acc_norm': 0.5849056603773585,\n",
       "    'acc_norm_stderr': 0.03032594578928611},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04122728707651282,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04122728707651282},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4624277456647399,\n",
       "    'acc_stderr': 0.0380168510452446,\n",
       "    'acc_norm': 0.4624277456647399,\n",
       "    'acc_norm_stderr': 0.0380168510452446},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3137254901960784,\n",
       "    'acc_stderr': 0.04617034827006717,\n",
       "    'acc_norm': 0.3137254901960784,\n",
       "    'acc_norm_stderr': 0.04617034827006717},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.03202563076101735,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.03202563076101735},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.043727482902780064,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.043727482902780064},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.503448275862069,\n",
       "    'acc_stderr': 0.0416656757710158,\n",
       "    'acc_norm': 0.503448275862069,\n",
       "    'acc_norm_stderr': 0.0416656757710158},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.024419234966819064,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.024419234966819064},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.30952380952380953,\n",
       "    'acc_stderr': 0.04134913018303316,\n",
       "    'acc_norm': 0.30952380952380953,\n",
       "    'acc_norm_stderr': 0.04134913018303316},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6419354838709678,\n",
       "    'acc_stderr': 0.02727389059430064,\n",
       "    'acc_norm': 0.6419354838709678,\n",
       "    'acc_norm_stderr': 0.02727389059430064},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4630541871921182,\n",
       "    'acc_stderr': 0.035083705204426656,\n",
       "    'acc_norm': 0.4630541871921182,\n",
       "    'acc_norm_stderr': 0.035083705204426656},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.03663974994391244,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.03663974994391244},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.702020202020202,\n",
       "    'acc_stderr': 0.03258630383836556,\n",
       "    'acc_norm': 0.702020202020202,\n",
       "    'acc_norm_stderr': 0.03258630383836556},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7875647668393783,\n",
       "    'acc_stderr': 0.029519282616817234,\n",
       "    'acc_norm': 0.7875647668393783,\n",
       "    'acc_norm_stderr': 0.029519282616817234},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.49230769230769234,\n",
       "    'acc_stderr': 0.025348006031534788,\n",
       "    'acc_norm': 0.49230769230769234,\n",
       "    'acc_norm_stderr': 0.025348006031534788},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3111111111111111,\n",
       "    'acc_stderr': 0.028226446749683522,\n",
       "    'acc_norm': 0.3111111111111111,\n",
       "    'acc_norm_stderr': 0.028226446749683522},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03242225027115007,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03242225027115007},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.33774834437086093,\n",
       "    'acc_stderr': 0.038615575462551684,\n",
       "    'acc_norm': 0.33774834437086093,\n",
       "    'acc_norm_stderr': 0.038615575462551684},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7321100917431193,\n",
       "    'acc_stderr': 0.018987462257978652,\n",
       "    'acc_norm': 0.7321100917431193,\n",
       "    'acc_norm_stderr': 0.018987462257978652},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.03324708911809117,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.03324708911809117},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03039153369274154,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03039153369274154},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7172995780590717,\n",
       "    'acc_stderr': 0.02931281415395592,\n",
       "    'acc_norm': 0.7172995780590717,\n",
       "    'acc_norm_stderr': 0.02931281415395592},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6457399103139013,\n",
       "    'acc_stderr': 0.032100621541349864,\n",
       "    'acc_norm': 0.6457399103139013,\n",
       "    'acc_norm_stderr': 0.032100621541349864},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6335877862595419,\n",
       "    'acc_stderr': 0.04225875451969637,\n",
       "    'acc_norm': 0.6335877862595419,\n",
       "    'acc_norm_stderr': 0.04225875451969637},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.768595041322314,\n",
       "    'acc_stderr': 0.03849856098794089,\n",
       "    'acc_norm': 0.768595041322314,\n",
       "    'acc_norm_stderr': 0.03849856098794089},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6944444444444444,\n",
       "    'acc_stderr': 0.044531975073749834,\n",
       "    'acc_norm': 0.6944444444444444,\n",
       "    'acc_norm_stderr': 0.044531975073749834},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6503067484662577,\n",
       "    'acc_stderr': 0.037466683254700206,\n",
       "    'acc_norm': 0.6503067484662577,\n",
       "    'acc_norm_stderr': 0.037466683254700206},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.35714285714285715,\n",
       "    'acc_stderr': 0.04547960999764376,\n",
       "    'acc_norm': 0.35714285714285715,\n",
       "    'acc_norm_stderr': 0.04547960999764376},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.04354631077260595,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.04354631077260595},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7863247863247863,\n",
       "    'acc_stderr': 0.026853450377009175,\n",
       "    'acc_norm': 0.7863247863247863,\n",
       "    'acc_norm_stderr': 0.026853450377009175},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7471264367816092,\n",
       "    'acc_stderr': 0.015543377313719681,\n",
       "    'acc_norm': 0.7471264367816092,\n",
       "    'acc_norm_stderr': 0.015543377313719681},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6127167630057804,\n",
       "    'acc_stderr': 0.026226158605124655,\n",
       "    'acc_norm': 0.6127167630057804,\n",
       "    'acc_norm_stderr': 0.026226158605124655},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30502793296089387,\n",
       "    'acc_stderr': 0.015398723510916716,\n",
       "    'acc_norm': 0.30502793296089387,\n",
       "    'acc_norm_stderr': 0.015398723510916716},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5947712418300654,\n",
       "    'acc_stderr': 0.028110928492809068,\n",
       "    'acc_norm': 0.5947712418300654,\n",
       "    'acc_norm_stderr': 0.028110928492809068},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5884244372990354,\n",
       "    'acc_stderr': 0.02795048149440127,\n",
       "    'acc_norm': 0.5884244372990354,\n",
       "    'acc_norm_stderr': 0.02795048149440127},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6111111111111112,\n",
       "    'acc_stderr': 0.02712511551316687,\n",
       "    'acc_norm': 0.6111111111111112,\n",
       "    'acc_norm_stderr': 0.02712511551316687},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.38652482269503546,\n",
       "    'acc_stderr': 0.029049190342543454,\n",
       "    'acc_norm': 0.38652482269503546,\n",
       "    'acc_norm_stderr': 0.029049190342543454},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.39113428943937417,\n",
       "    'acc_stderr': 0.012463861839982058,\n",
       "    'acc_norm': 0.39113428943937417,\n",
       "    'acc_norm_stderr': 0.012463861839982058},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.030372836961539352,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.030372836961539352},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5424836601307189,\n",
       "    'acc_stderr': 0.020154685712590888,\n",
       "    'acc_norm': 0.5424836601307189,\n",
       "    'acc_norm_stderr': 0.020154685712590888},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302505,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302505},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6408163265306123,\n",
       "    'acc_stderr': 0.030713560455108493,\n",
       "    'acc_norm': 0.6408163265306123,\n",
       "    'acc_norm_stderr': 0.030713560455108493},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7512437810945274,\n",
       "    'acc_stderr': 0.030567675938916714,\n",
       "    'acc_norm': 0.7512437810945274,\n",
       "    'acc_norm_stderr': 0.030567675938916714},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4819277108433735,\n",
       "    'acc_stderr': 0.038899512528272166,\n",
       "    'acc_norm': 0.4819277108433735,\n",
       "    'acc_norm_stderr': 0.038899512528272166},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7309941520467836,\n",
       "    'acc_stderr': 0.03401052620104089,\n",
       "    'acc_norm': 0.7309941520467836,\n",
       "    'acc_norm_stderr': 0.03401052620104089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.4411794590119937,\n",
       "    'mc2_stderr': 0.015755921757439843}}},\n",
       " 'llama-2-70b-chat': {'key': 'llama-2-70b-chat',\n",
       "  'Model': 'Llama-2-70b-chat',\n",
       "  'MT-bench (score)': '6.86',\n",
       "  'MMLU': '0.630',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-70b-chat-hf',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6049488054607508,\n",
       "    'acc_stderr': 0.01428589829293817,\n",
       "    'acc_norm': 0.6459044368600683,\n",
       "    'acc_norm_stderr': 0.013975454122756564},\n",
       "   'harness|hellaswag|10': {'acc': 0.6693885680143398,\n",
       "    'acc_stderr': 0.004694718918225751,\n",
       "    'acc_norm': 0.8587930691097391,\n",
       "    'acc_norm_stderr': 0.003475231889452833},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3561811505507956,\n",
       "    'mc1_stderr': 0.016763790728446335,\n",
       "    'mc2': 0.5280473232260097,\n",
       "    'mc2_stderr': 0.01553022126123046},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.04793724854411021,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.04793724854411021},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5185185185185185,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.5185185185185185,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.7302631578947368,\n",
       "    'acc_stderr': 0.03611780560284898,\n",
       "    'acc_norm': 0.7302631578947368,\n",
       "    'acc_norm_stderr': 0.03611780560284898},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6377358490566037,\n",
       "    'acc_stderr': 0.029582245128384303,\n",
       "    'acc_norm': 0.6377358490566037,\n",
       "    'acc_norm_stderr': 0.029582245128384303},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03621034121889507,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03621034121889507},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237101,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237101},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6011560693641619,\n",
       "    'acc_stderr': 0.0373362665538351,\n",
       "    'acc_norm': 0.6011560693641619,\n",
       "    'acc_norm_stderr': 0.0373362665538351},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.04690650298201943,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.04690650298201943},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5829787234042553,\n",
       "    'acc_stderr': 0.032232762667117124,\n",
       "    'acc_norm': 0.5829787234042553,\n",
       "    'acc_norm_stderr': 0.032232762667117124},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.41228070175438597,\n",
       "    'acc_stderr': 0.04630653203366595,\n",
       "    'acc_norm': 0.41228070175438597,\n",
       "    'acc_norm_stderr': 0.04630653203366595},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5793103448275863,\n",
       "    'acc_stderr': 0.0411391498118926,\n",
       "    'acc_norm': 0.5793103448275863,\n",
       "    'acc_norm_stderr': 0.0411391498118926},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.41005291005291006,\n",
       "    'acc_stderr': 0.02533120243894442,\n",
       "    'acc_norm': 0.41005291005291006,\n",
       "    'acc_norm_stderr': 0.02533120243894442},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4126984126984127,\n",
       "    'acc_stderr': 0.04403438954768176,\n",
       "    'acc_norm': 0.4126984126984127,\n",
       "    'acc_norm_stderr': 0.04403438954768176},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7645161290322581,\n",
       "    'acc_stderr': 0.02413763242933771,\n",
       "    'acc_norm': 0.7645161290322581,\n",
       "    'acc_norm_stderr': 0.02413763242933771},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4630541871921182,\n",
       "    'acc_stderr': 0.035083705204426656,\n",
       "    'acc_norm': 0.4630541871921182,\n",
       "    'acc_norm_stderr': 0.035083705204426656},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03011768892950359,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03011768892950359},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.02805779167298902,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.02805779167298902},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8911917098445595,\n",
       "    'acc_stderr': 0.022473253332768783,\n",
       "    'acc_norm': 0.8911917098445595,\n",
       "    'acc_norm_stderr': 0.022473253332768783},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6410256410256411,\n",
       "    'acc_stderr': 0.02432173848460235,\n",
       "    'acc_norm': 0.6410256410256411,\n",
       "    'acc_norm_stderr': 0.02432173848460235},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.027940457136228416,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.027940457136228416},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6596638655462185,\n",
       "    'acc_stderr': 0.030778057422931673,\n",
       "    'acc_norm': 0.6596638655462185,\n",
       "    'acc_norm_stderr': 0.030778057422931673},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.423841059602649,\n",
       "    'acc_stderr': 0.04034846678603397,\n",
       "    'acc_norm': 0.423841059602649,\n",
       "    'acc_norm_stderr': 0.04034846678603397},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8385321100917431,\n",
       "    'acc_stderr': 0.015776239256163255,\n",
       "    'acc_norm': 0.8385321100917431,\n",
       "    'acc_norm_stderr': 0.015776239256163255},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.03407632093854052,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.03407632093854052},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8578431372549019,\n",
       "    'acc_stderr': 0.024509803921568606,\n",
       "    'acc_norm': 0.8578431372549019,\n",
       "    'acc_norm_stderr': 0.024509803921568606},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8438818565400844,\n",
       "    'acc_stderr': 0.02362715946031867,\n",
       "    'acc_norm': 0.8438818565400844,\n",
       "    'acc_norm_stderr': 0.02362715946031867},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.726457399103139,\n",
       "    'acc_stderr': 0.02991858670779883,\n",
       "    'acc_norm': 0.726457399103139,\n",
       "    'acc_norm_stderr': 0.02991858670779883},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7099236641221374,\n",
       "    'acc_stderr': 0.039800662464677665,\n",
       "    'acc_norm': 0.7099236641221374,\n",
       "    'acc_norm_stderr': 0.039800662464677665},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8016528925619835,\n",
       "    'acc_stderr': 0.03640118271990946,\n",
       "    'acc_norm': 0.8016528925619835,\n",
       "    'acc_norm_stderr': 0.03640118271990946},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8240740740740741,\n",
       "    'acc_stderr': 0.036809181416738807,\n",
       "    'acc_norm': 0.8240740740740741,\n",
       "    'acc_norm_stderr': 0.036809181416738807},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7607361963190185,\n",
       "    'acc_stderr': 0.033519538795212696,\n",
       "    'acc_norm': 0.7607361963190185,\n",
       "    'acc_norm_stderr': 0.033519538795212696},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8058252427184466,\n",
       "    'acc_stderr': 0.03916667762822584,\n",
       "    'acc_norm': 0.8058252427184466,\n",
       "    'acc_norm_stderr': 0.03916667762822584},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8717948717948718,\n",
       "    'acc_stderr': 0.02190190511507332,\n",
       "    'acc_norm': 0.8717948717948718,\n",
       "    'acc_norm_stderr': 0.02190190511507332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8275862068965517,\n",
       "    'acc_stderr': 0.013507943909371798,\n",
       "    'acc_norm': 0.8275862068965517,\n",
       "    'acc_norm_stderr': 0.013507943909371798},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7167630057803468,\n",
       "    'acc_stderr': 0.02425790170532338,\n",
       "    'acc_norm': 0.7167630057803468,\n",
       "    'acc_norm_stderr': 0.02425790170532338},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.39553072625698327,\n",
       "    'acc_stderr': 0.01635341541007577,\n",
       "    'acc_norm': 0.39553072625698327,\n",
       "    'acc_norm_stderr': 0.01635341541007577},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6993464052287581,\n",
       "    'acc_stderr': 0.026256053835718968,\n",
       "    'acc_norm': 0.6993464052287581,\n",
       "    'acc_norm_stderr': 0.026256053835718968},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7041800643086816,\n",
       "    'acc_stderr': 0.02592237178881877,\n",
       "    'acc_norm': 0.7041800643086816,\n",
       "    'acc_norm_stderr': 0.02592237178881877},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7098765432098766,\n",
       "    'acc_stderr': 0.025251173936495036,\n",
       "    'acc_norm': 0.7098765432098766,\n",
       "    'acc_norm_stderr': 0.025251173936495036},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5070921985815603,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.5070921985815603,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4771838331160365,\n",
       "    'acc_stderr': 0.012756933382823694,\n",
       "    'acc_norm': 0.4771838331160365,\n",
       "    'acc_norm_stderr': 0.012756933382823694},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5772058823529411,\n",
       "    'acc_stderr': 0.030008562845003476,\n",
       "    'acc_norm': 0.5772058823529411,\n",
       "    'acc_norm_stderr': 0.030008562845003476},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6699346405228758,\n",
       "    'acc_stderr': 0.019023726160724556,\n",
       "    'acc_norm': 0.6699346405228758,\n",
       "    'acc_norm_stderr': 0.019023726160724556},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7877551020408163,\n",
       "    'acc_stderr': 0.026176967197866767,\n",
       "    'acc_norm': 0.7877551020408163,\n",
       "    'acc_norm_stderr': 0.026176967197866767},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8706467661691543,\n",
       "    'acc_stderr': 0.023729830881018526,\n",
       "    'acc_norm': 0.8706467661691543,\n",
       "    'acc_norm_stderr': 0.023729830881018526},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.03379976689896309,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.03379976689896309},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5120481927710844,\n",
       "    'acc_stderr': 0.03891364495835817,\n",
       "    'acc_norm': 0.5120481927710844,\n",
       "    'acc_norm_stderr': 0.03891364495835817},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8187134502923976,\n",
       "    'acc_stderr': 0.029547741687640038,\n",
       "    'acc_norm': 0.8187134502923976,\n",
       "    'acc_norm_stderr': 0.029547741687640038},\n",
       "   'all': {'em': 0.040373322147651006,\n",
       "    'em_stderr': 0.0020157564185176837,\n",
       "    'f1': 0.1050272651006715,\n",
       "    'f1_stderr': 0.0023756238577676155,\n",
       "    'acc': 0.5359600711595986,\n",
       "    'acc_stderr': 0.011658939983913113},\n",
       "   'harness|drop|3': {'em': 0.040373322147651006,\n",
       "    'em_stderr': 0.0020157564185176837,\n",
       "    'f1': 0.1050272651006715,\n",
       "    'f1_stderr': 0.0023756238577676155},\n",
       "   'harness|gsm8k|5': {'acc': 0.266868840030326,\n",
       "    'acc_stderr': 0.012183780551887957},\n",
       "   'harness|winogrande|5': {'acc': 0.8050513022888713,\n",
       "    'acc_stderr': 0.011134099415938268}}},\n",
       " 'llama-2-7b-chat': {'key': 'llama-2-7b-chat',\n",
       "  'Model': 'Llama-2-7b-chat',\n",
       "  'MT-bench (score)': '6.27',\n",
       "  'MMLU': '0.458',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-7b-chat-hf',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.4948805460750853,\n",
       "    'acc_stderr': 0.01461062489030916,\n",
       "    'acc_norm': 0.5290102389078498,\n",
       "    'acc_norm_stderr': 0.014586776355294323},\n",
       "   'harness|hellaswag|10': {'acc': 0.5978888667596096,\n",
       "    'acc_stderr': 0.004893220635011792,\n",
       "    'acc_norm': 0.7855008962358097,\n",
       "    'acc_norm_stderr': 0.004096355125117511},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542129,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542129},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.42962962962962964,\n",
       "    'acc_stderr': 0.04276349494376599,\n",
       "    'acc_norm': 0.42962962962962964,\n",
       "    'acc_norm_stderr': 0.04276349494376599},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.4868421052631579,\n",
       "    'acc_stderr': 0.04067533136309173,\n",
       "    'acc_norm': 0.4868421052631579,\n",
       "    'acc_norm_stderr': 0.04067533136309173},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5358490566037736,\n",
       "    'acc_stderr': 0.030693675018458003,\n",
       "    'acc_norm': 0.5358490566037736,\n",
       "    'acc_norm_stderr': 0.030693675018458003},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5208333333333334,\n",
       "    'acc_stderr': 0.041775789507399935,\n",
       "    'acc_norm': 0.5208333333333334,\n",
       "    'acc_norm_stderr': 0.041775789507399935},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.04560480215720684,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.04560480215720684},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.3988439306358382,\n",
       "    'acc_stderr': 0.037336266553835096,\n",
       "    'acc_norm': 0.3988439306358382,\n",
       "    'acc_norm_stderr': 0.037336266553835096},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.22549019607843138,\n",
       "    'acc_stderr': 0.041583075330832865,\n",
       "    'acc_norm': 0.22549019607843138,\n",
       "    'acc_norm_stderr': 0.041583075330832865},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.58,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.58,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4085106382978723,\n",
       "    'acc_stderr': 0.03213418026701576,\n",
       "    'acc_norm': 0.4085106382978723,\n",
       "    'acc_norm_stderr': 0.03213418026701576},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.37719298245614036,\n",
       "    'acc_stderr': 0.045595221419582166,\n",
       "    'acc_norm': 0.37719298245614036,\n",
       "    'acc_norm_stderr': 0.045595221419582166},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4896551724137931,\n",
       "    'acc_stderr': 0.04165774775728762,\n",
       "    'acc_norm': 0.4896551724137931,\n",
       "    'acc_norm_stderr': 0.04165774775728762},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.29894179894179895,\n",
       "    'acc_stderr': 0.023577604791655805,\n",
       "    'acc_norm': 0.29894179894179895,\n",
       "    'acc_norm_stderr': 0.023577604791655805},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.25396825396825395,\n",
       "    'acc_stderr': 0.03893259610604675,\n",
       "    'acc_norm': 0.25396825396825395,\n",
       "    'acc_norm_stderr': 0.03893259610604675},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.5225806451612903,\n",
       "    'acc_stderr': 0.02841498501970786,\n",
       "    'acc_norm': 0.5225806451612903,\n",
       "    'acc_norm_stderr': 0.02841498501970786},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3645320197044335,\n",
       "    'acc_stderr': 0.033864057460620905,\n",
       "    'acc_norm': 0.3645320197044335,\n",
       "    'acc_norm_stderr': 0.033864057460620905},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.5878787878787879,\n",
       "    'acc_stderr': 0.03843566993588718,\n",
       "    'acc_norm': 0.5878787878787879,\n",
       "    'acc_norm_stderr': 0.03843566993588718},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6060606060606061,\n",
       "    'acc_stderr': 0.034812853382329624,\n",
       "    'acc_norm': 0.6060606060606061,\n",
       "    'acc_norm_stderr': 0.034812853382329624},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7150259067357513,\n",
       "    'acc_stderr': 0.032577140777096614,\n",
       "    'acc_norm': 0.7150259067357513,\n",
       "    'acc_norm_stderr': 0.032577140777096614},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.4256410256410256,\n",
       "    'acc_stderr': 0.02506909438729654,\n",
       "    'acc_norm': 0.4256410256410256,\n",
       "    'acc_norm_stderr': 0.02506909438729654},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.25555555555555554,\n",
       "    'acc_stderr': 0.02659393910184408,\n",
       "    'acc_norm': 0.25555555555555554,\n",
       "    'acc_norm_stderr': 0.02659393910184408},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.42436974789915966,\n",
       "    'acc_stderr': 0.03210479051015776,\n",
       "    'acc_norm': 0.42436974789915966,\n",
       "    'acc_norm_stderr': 0.03210479051015776},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2913907284768212,\n",
       "    'acc_stderr': 0.03710185726119995,\n",
       "    'acc_norm': 0.2913907284768212,\n",
       "    'acc_norm_stderr': 0.03710185726119995},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.6752293577981652,\n",
       "    'acc_stderr': 0.020077729109310327,\n",
       "    'acc_norm': 0.6752293577981652,\n",
       "    'acc_norm_stderr': 0.020077729109310327},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.0321495214780275,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.0321495214780275},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.0328347205610856,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.0328347205610856},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.03068582059661079,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.03068582059661079},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5605381165919282,\n",
       "    'acc_stderr': 0.03331092511038179,\n",
       "    'acc_norm': 0.5605381165919282,\n",
       "    'acc_norm_stderr': 0.03331092511038179},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5725190839694656,\n",
       "    'acc_stderr': 0.04338920305792401,\n",
       "    'acc_norm': 0.5725190839694656,\n",
       "    'acc_norm_stderr': 0.04338920305792401},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.628099173553719,\n",
       "    'acc_stderr': 0.04412015806624504,\n",
       "    'acc_norm': 0.628099173553719,\n",
       "    'acc_norm_stderr': 0.04412015806624504},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04766075165356461,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04766075165356461},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.5521472392638037,\n",
       "    'acc_stderr': 0.03906947479456606,\n",
       "    'acc_norm': 0.5521472392638037,\n",
       "    'acc_norm_stderr': 0.03906947479456606},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.30357142857142855,\n",
       "    'acc_stderr': 0.04364226155841044,\n",
       "    'acc_norm': 0.30357142857142855,\n",
       "    'acc_norm_stderr': 0.04364226155841044},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.04656147110012351,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.04656147110012351},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7094017094017094,\n",
       "    'acc_stderr': 0.029745048572674074,\n",
       "    'acc_norm': 0.7094017094017094,\n",
       "    'acc_norm_stderr': 0.029745048572674074},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6756066411238825,\n",
       "    'acc_stderr': 0.0167409290471627,\n",
       "    'acc_norm': 0.6756066411238825,\n",
       "    'acc_norm_stderr': 0.0167409290471627},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.026907849856282542,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.026907849856282542},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2201117318435754,\n",
       "    'acc_stderr': 0.013856994024227175,\n",
       "    'acc_norm': 0.2201117318435754,\n",
       "    'acc_norm_stderr': 0.013856994024227175},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5196078431372549,\n",
       "    'acc_stderr': 0.028607893699576066,\n",
       "    'acc_norm': 0.5196078431372549,\n",
       "    'acc_norm_stderr': 0.028607893699576066},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5659163987138264,\n",
       "    'acc_stderr': 0.02815023224453559,\n",
       "    'acc_norm': 0.5659163987138264,\n",
       "    'acc_norm_stderr': 0.02815023224453559},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5679012345679012,\n",
       "    'acc_stderr': 0.027563010971606676,\n",
       "    'acc_norm': 0.5679012345679012,\n",
       "    'acc_norm_stderr': 0.027563010971606676},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3723404255319149,\n",
       "    'acc_stderr': 0.028838921471251458,\n",
       "    'acc_norm': 0.3723404255319149,\n",
       "    'acc_norm_stderr': 0.028838921471251458},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3500651890482399,\n",
       "    'acc_stderr': 0.012182552313215175,\n",
       "    'acc_norm': 0.3500651890482399,\n",
       "    'acc_norm_stderr': 0.012182552313215175},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.45588235294117646,\n",
       "    'acc_stderr': 0.030254372573976684,\n",
       "    'acc_norm': 0.45588235294117646,\n",
       "    'acc_norm_stderr': 0.030254372573976684},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4803921568627451,\n",
       "    'acc_stderr': 0.020212274976302957,\n",
       "    'acc_norm': 0.4803921568627451,\n",
       "    'acc_norm_stderr': 0.020212274976302957},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5272727272727272,\n",
       "    'acc_stderr': 0.04782001791380061,\n",
       "    'acc_norm': 0.5272727272727272,\n",
       "    'acc_norm_stderr': 0.04782001791380061},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5265306122448979,\n",
       "    'acc_stderr': 0.03196412734523272,\n",
       "    'acc_norm': 0.5265306122448979,\n",
       "    'acc_norm_stderr': 0.03196412734523272},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6467661691542289,\n",
       "    'acc_stderr': 0.03379790611796777,\n",
       "    'acc_norm': 0.6467661691542289,\n",
       "    'acc_norm_stderr': 0.03379790611796777},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.43373493975903615,\n",
       "    'acc_stderr': 0.03858158940685517,\n",
       "    'acc_norm': 0.43373493975903615,\n",
       "    'acc_norm_stderr': 0.03858158940685517},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.034240429246915824,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.034240429246915824},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3011015911872705,\n",
       "    'mc1_stderr': 0.016058999026100616,\n",
       "    'mc2': 0.45570370195101134,\n",
       "    'mc2_stderr': 0.015691038880908878},\n",
       "   'all': {'em': 0.06763842281879194,\n",
       "    'em_stderr': 0.0025717489509556085,\n",
       "    'f1': 0.13085570469798627,\n",
       "    'f1_stderr': 0.0028825856446422905,\n",
       "    'acc': 0.39549166962367155,\n",
       "    'acc_stderr': 0.009921949302668327},\n",
       "   'harness|drop|3': {'em': 0.06763842281879194,\n",
       "    'em_stderr': 0.0025717489509556085,\n",
       "    'f1': 0.13085570469798627,\n",
       "    'f1_stderr': 0.0028825856446422905},\n",
       "   'harness|gsm8k|5': {'acc': 0.07354056103108415,\n",
       "    'acc_stderr': 0.0071898357543652685},\n",
       "   'harness|winogrande|5': {'acc': 0.7174427782162589,\n",
       "    'acc_stderr': 0.012654062850971384}}},\n",
       " 'mistral-7b-instruct': {'key': 'mistral-7b-instruct',\n",
       "  'Model': 'Mistral-7B-Instruct-v0.1',\n",
       "  'MT-bench (score)': '6.84',\n",
       "  'MMLU': '0.554',\n",
       "  'License': 'Apache 2.0',\n",
       "  'Organization': 'Mistral',\n",
       "  'Link': 'https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.523037542662116,\n",
       "    'acc_stderr': 0.014595873205358269,\n",
       "    'acc_norm': 0.5452218430034129,\n",
       "    'acc_norm_stderr': 0.014551507060836357},\n",
       "   'harness|hellaswag|10': {'acc': 0.5694084843656642,\n",
       "    'acc_stderr': 0.004941470620074867,\n",
       "    'acc_norm': 0.7563234415455089,\n",
       "    'acc_norm_stderr': 0.0042842240337755385},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.047609522856952365,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.047609522856952365},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4222222222222222,\n",
       "    'acc_stderr': 0.04266763404099582,\n",
       "    'acc_norm': 0.4222222222222222,\n",
       "    'acc_norm_stderr': 0.04266763404099582},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5657894736842105,\n",
       "    'acc_stderr': 0.040335656678483205,\n",
       "    'acc_norm': 0.5657894736842105,\n",
       "    'acc_norm_stderr': 0.040335656678483205},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5849056603773585,\n",
       "    'acc_stderr': 0.030325945789286105,\n",
       "    'acc_norm': 0.5849056603773585,\n",
       "    'acc_norm_stderr': 0.030325945789286105},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5902777777777778,\n",
       "    'acc_stderr': 0.04112490974670787,\n",
       "    'acc_norm': 0.5902777777777778,\n",
       "    'acc_norm_stderr': 0.04112490974670787},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5202312138728323,\n",
       "    'acc_stderr': 0.03809342081273956,\n",
       "    'acc_norm': 0.5202312138728323,\n",
       "    'acc_norm_stderr': 0.03809342081273956},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.30392156862745096,\n",
       "    'acc_stderr': 0.045766654032077636,\n",
       "    'acc_norm': 0.30392156862745096,\n",
       "    'acc_norm_stderr': 0.045766654032077636},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.047258156262526094,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.047258156262526094},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4808510638297872,\n",
       "    'acc_stderr': 0.032662042990646775,\n",
       "    'acc_norm': 0.4808510638297872,\n",
       "    'acc_norm_stderr': 0.032662042990646775},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.37719298245614036,\n",
       "    'acc_stderr': 0.04559522141958216,\n",
       "    'acc_norm': 0.37719298245614036,\n",
       "    'acc_norm_stderr': 0.04559522141958216},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5517241379310345,\n",
       "    'acc_stderr': 0.04144311810878151,\n",
       "    'acc_norm': 0.5517241379310345,\n",
       "    'acc_norm_stderr': 0.04144311810878151},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36507936507936506,\n",
       "    'acc_stderr': 0.024796060602699947,\n",
       "    'acc_norm': 0.36507936507936506,\n",
       "    'acc_norm_stderr': 0.024796060602699947},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.38095238095238093,\n",
       "    'acc_stderr': 0.043435254289490965,\n",
       "    'acc_norm': 0.38095238095238093,\n",
       "    'acc_norm_stderr': 0.043435254289490965},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6419354838709678,\n",
       "    'acc_stderr': 0.027273890594300642,\n",
       "    'acc_norm': 0.6419354838709678,\n",
       "    'acc_norm_stderr': 0.027273890594300642},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.41379310344827586,\n",
       "    'acc_stderr': 0.03465304488406795,\n",
       "    'acc_norm': 0.41379310344827586,\n",
       "    'acc_norm_stderr': 0.03465304488406795},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.036810508691615486,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.036810508691615486},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.031911782267135466,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.031911782267135466},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7253886010362695,\n",
       "    'acc_stderr': 0.03221024508041154,\n",
       "    'acc_norm': 0.7253886010362695,\n",
       "    'acc_norm_stderr': 0.03221024508041154},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5205128205128206,\n",
       "    'acc_stderr': 0.02532966316348994,\n",
       "    'acc_norm': 0.5205128205128206,\n",
       "    'acc_norm_stderr': 0.02532966316348994},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.027940457136228416,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.027940457136228416},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5462184873949579,\n",
       "    'acc_stderr': 0.032339434681820885,\n",
       "    'acc_norm': 0.5462184873949579,\n",
       "    'acc_norm_stderr': 0.032339434681820885},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.03822746937658753,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.03822746937658753},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.710091743119266,\n",
       "    'acc_stderr': 0.019453066609201597,\n",
       "    'acc_norm': 0.710091743119266,\n",
       "    'acc_norm_stderr': 0.019453066609201597},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4537037037037037,\n",
       "    'acc_stderr': 0.03395322726375797,\n",
       "    'acc_norm': 0.4537037037037037,\n",
       "    'acc_norm_stderr': 0.03395322726375797},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7058823529411765,\n",
       "    'acc_stderr': 0.03198001660115072,\n",
       "    'acc_norm': 0.7058823529411765,\n",
       "    'acc_norm_stderr': 0.03198001660115072},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6962025316455697,\n",
       "    'acc_stderr': 0.0299366963871386,\n",
       "    'acc_norm': 0.6962025316455697,\n",
       "    'acc_norm_stderr': 0.0299366963871386},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6547085201793722,\n",
       "    'acc_stderr': 0.03191100192835794,\n",
       "    'acc_norm': 0.6547085201793722,\n",
       "    'acc_norm_stderr': 0.03191100192835794},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6717557251908397,\n",
       "    'acc_stderr': 0.04118438565806299,\n",
       "    'acc_norm': 0.6717557251908397,\n",
       "    'acc_norm_stderr': 0.04118438565806299},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.6776859504132231,\n",
       "    'acc_stderr': 0.042664163633521685,\n",
       "    'acc_norm': 0.6776859504132231,\n",
       "    'acc_norm_stderr': 0.042664163633521685},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6944444444444444,\n",
       "    'acc_stderr': 0.044531975073749834,\n",
       "    'acc_norm': 0.6944444444444444,\n",
       "    'acc_norm_stderr': 0.044531975073749834},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6503067484662577,\n",
       "    'acc_stderr': 0.03746668325470021,\n",
       "    'acc_norm': 0.6503067484662577,\n",
       "    'acc_norm_stderr': 0.03746668325470021},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.4732142857142857,\n",
       "    'acc_stderr': 0.047389751192741546,\n",
       "    'acc_norm': 0.4732142857142857,\n",
       "    'acc_norm_stderr': 0.047389751192741546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6893203883495146,\n",
       "    'acc_stderr': 0.04582124160161551,\n",
       "    'acc_norm': 0.6893203883495146,\n",
       "    'acc_norm_stderr': 0.04582124160161551},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8418803418803419,\n",
       "    'acc_stderr': 0.023902325549560392,\n",
       "    'acc_norm': 0.8418803418803419,\n",
       "    'acc_norm_stderr': 0.023902325549560392},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.047258156262526094,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.047258156262526094},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7432950191570882,\n",
       "    'acc_stderr': 0.015620480263064533,\n",
       "    'acc_norm': 0.7432950191570882,\n",
       "    'acc_norm_stderr': 0.015620480263064533},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5895953757225434,\n",
       "    'acc_stderr': 0.026483392042098174,\n",
       "    'acc_norm': 0.5895953757225434,\n",
       "    'acc_norm_stderr': 0.026483392042098174},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2446927374301676,\n",
       "    'acc_stderr': 0.014378169884098417,\n",
       "    'acc_norm': 0.2446927374301676,\n",
       "    'acc_norm_stderr': 0.014378169884098417},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6143790849673203,\n",
       "    'acc_stderr': 0.02787074527829027,\n",
       "    'acc_norm': 0.6143790849673203,\n",
       "    'acc_norm_stderr': 0.02787074527829027},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6077170418006431,\n",
       "    'acc_stderr': 0.027731258647012,\n",
       "    'acc_norm': 0.6077170418006431,\n",
       "    'acc_norm_stderr': 0.027731258647012},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5771604938271605,\n",
       "    'acc_stderr': 0.027487472980871595,\n",
       "    'acc_norm': 0.5771604938271605,\n",
       "    'acc_norm_stderr': 0.027487472980871595},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3829787234042553,\n",
       "    'acc_stderr': 0.028999080904806185,\n",
       "    'acc_norm': 0.3829787234042553,\n",
       "    'acc_norm_stderr': 0.028999080904806185},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.40091264667535853,\n",
       "    'acc_stderr': 0.012516960350640824,\n",
       "    'acc_norm': 0.40091264667535853,\n",
       "    'acc_norm_stderr': 0.012516960350640824},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5808823529411765,\n",
       "    'acc_stderr': 0.02997280717046462,\n",
       "    'acc_norm': 0.5808823529411765,\n",
       "    'acc_norm_stderr': 0.02997280717046462},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5163398692810458,\n",
       "    'acc_stderr': 0.02021703065318646,\n",
       "    'acc_norm': 0.5163398692810458,\n",
       "    'acc_norm_stderr': 0.02021703065318646},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302505,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302505},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6775510204081633,\n",
       "    'acc_stderr': 0.02992310056368391,\n",
       "    'acc_norm': 0.6775510204081633,\n",
       "    'acc_norm_stderr': 0.02992310056368391},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.746268656716418,\n",
       "    'acc_stderr': 0.03076944496729602,\n",
       "    'acc_norm': 0.746268656716418,\n",
       "    'acc_norm_stderr': 0.03076944496729602},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.46987951807228917,\n",
       "    'acc_stderr': 0.03885425420866766,\n",
       "    'acc_norm': 0.46987951807228917,\n",
       "    'acc_norm_stderr': 0.03885425420866766},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3953488372093023,\n",
       "    'mc1_stderr': 0.017115815632418194,\n",
       "    'mc2': 0.5628382292113293,\n",
       "    'mc2_stderr': 0.015351892312006444},\n",
       "   'all': {'em': 0.37038590604026844,\n",
       "    'em_stderr': 0.00494543044549648,\n",
       "    'f1': 0.43100566275167973,\n",
       "    'f1_stderr': 0.00478990485809286,\n",
       "    'acc': 0.4398533245809979,\n",
       "    'acc_stderr': 0.01100025548646791},\n",
       "   'harness|drop|3': {'em': 0.37038590604026844,\n",
       "    'em_stderr': 0.00494543044549648,\n",
       "    'f1': 0.43100566275167973,\n",
       "    'f1_stderr': 0.00478990485809286},\n",
       "   'harness|gsm8k|5': {'acc': 0.1425322213798332,\n",
       "    'acc_stderr': 0.009629588445673814},\n",
       "   'harness|winogrande|5': {'acc': 0.7371744277821626,\n",
       "    'acc_stderr': 0.012370922527262006}}},\n",
       " 'mpt-30b-chat': {'key': 'mpt-30b-chat',\n",
       "  'Model': 'MPT-30B-chat',\n",
       "  'MT-bench (score)': '6.39',\n",
       "  'MMLU': '0.504',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'MosaicML',\n",
       "  'Link': 'https://huggingface.co/mosaicml/mpt-30b-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5477815699658704,\n",
       "    'acc_stderr': 0.014544519880633829,\n",
       "    'acc_norm': 0.5836177474402731,\n",
       "    'acc_norm_stderr': 0.014405618279436176},\n",
       "   'harness|hellaswag|10': {'acc': 0.6325433180641307,\n",
       "    'acc_stderr': 0.0048112699754506005,\n",
       "    'acc_norm': 0.8241386178052181,\n",
       "    'acc_norm_stderr': 0.0037992414085029525},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.45925925925925926,\n",
       "    'acc_stderr': 0.04304979692464243,\n",
       "    'acc_norm': 0.45925925925925926,\n",
       "    'acc_norm_stderr': 0.04304979692464243},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.04068942293855797,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.04068942293855797},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5509433962264151,\n",
       "    'acc_stderr': 0.030612730713641095,\n",
       "    'acc_norm': 0.5509433962264151,\n",
       "    'acc_norm_stderr': 0.030612730713641095},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5902777777777778,\n",
       "    'acc_stderr': 0.04112490974670788,\n",
       "    'acc_norm': 0.5902777777777778,\n",
       "    'acc_norm_stderr': 0.04112490974670788},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.043898699568087785,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.043898699568087785},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4297872340425532,\n",
       "    'acc_stderr': 0.03236214467715564,\n",
       "    'acc_norm': 0.4297872340425532,\n",
       "    'acc_norm_stderr': 0.03236214467715564},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2807017543859649,\n",
       "    'acc_stderr': 0.042270544512322,\n",
       "    'acc_norm': 0.2807017543859649,\n",
       "    'acc_norm_stderr': 0.042270544512322},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.47586206896551725,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.47586206896551725,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.0242785680243077,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.0242785680243077},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.31746031746031744,\n",
       "    'acc_stderr': 0.04163453031302859,\n",
       "    'acc_norm': 0.31746031746031744,\n",
       "    'acc_norm_stderr': 0.04163453031302859},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6193548387096774,\n",
       "    'acc_stderr': 0.027621717832907046,\n",
       "    'acc_norm': 0.6193548387096774,\n",
       "    'acc_norm_stderr': 0.027621717832907046},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3842364532019704,\n",
       "    'acc_stderr': 0.03422398565657551,\n",
       "    'acc_norm': 0.3842364532019704,\n",
       "    'acc_norm_stderr': 0.03422398565657551},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6303030303030303,\n",
       "    'acc_stderr': 0.03769430314512568,\n",
       "    'acc_norm': 0.6303030303030303,\n",
       "    'acc_norm_stderr': 0.03769430314512568},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6919191919191919,\n",
       "    'acc_stderr': 0.03289477330098616,\n",
       "    'acc_norm': 0.6919191919191919,\n",
       "    'acc_norm_stderr': 0.03289477330098616},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.6476683937823834,\n",
       "    'acc_stderr': 0.03447478286414357,\n",
       "    'acc_norm': 0.6476683937823834,\n",
       "    'acc_norm_stderr': 0.03447478286414357},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.45384615384615384,\n",
       "    'acc_stderr': 0.02524277098712618,\n",
       "    'acc_norm': 0.45384615384615384,\n",
       "    'acc_norm_stderr': 0.02524277098712618},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.28888888888888886,\n",
       "    'acc_stderr': 0.027634907264178544,\n",
       "    'acc_norm': 0.28888888888888886,\n",
       "    'acc_norm_stderr': 0.027634907264178544},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.47058823529411764,\n",
       "    'acc_stderr': 0.03242225027115006,\n",
       "    'acc_norm': 0.47058823529411764,\n",
       "    'acc_norm_stderr': 0.03242225027115006},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3443708609271523,\n",
       "    'acc_stderr': 0.038796870240733264,\n",
       "    'acc_norm': 0.3443708609271523,\n",
       "    'acc_norm_stderr': 0.038796870240733264},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.728440366972477,\n",
       "    'acc_stderr': 0.019069098363191442,\n",
       "    'acc_norm': 0.728440366972477,\n",
       "    'acc_norm_stderr': 0.019069098363191442},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4166666666666667,\n",
       "    'acc_stderr': 0.03362277436608044,\n",
       "    'acc_norm': 0.4166666666666667,\n",
       "    'acc_norm_stderr': 0.03362277436608044},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7598039215686274,\n",
       "    'acc_stderr': 0.02998373305591362,\n",
       "    'acc_norm': 0.7598039215686274,\n",
       "    'acc_norm_stderr': 0.02998373305591362},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7088607594936709,\n",
       "    'acc_stderr': 0.029571601065753374,\n",
       "    'acc_norm': 0.7088607594936709,\n",
       "    'acc_norm_stderr': 0.029571601065753374},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5291479820627802,\n",
       "    'acc_stderr': 0.03350073248773403,\n",
       "    'acc_norm': 0.5291479820627802,\n",
       "    'acc_norm_stderr': 0.03350073248773403},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5725190839694656,\n",
       "    'acc_stderr': 0.04338920305792401,\n",
       "    'acc_norm': 0.5725190839694656,\n",
       "    'acc_norm_stderr': 0.04338920305792401},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.4793388429752066,\n",
       "    'acc_stderr': 0.04560456086387235,\n",
       "    'acc_norm': 0.4793388429752066,\n",
       "    'acc_norm_stderr': 0.04560456086387235},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.04812917324536823,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.04812917324536823},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6012269938650306,\n",
       "    'acc_stderr': 0.03847021420456023,\n",
       "    'acc_norm': 0.6012269938650306,\n",
       "    'acc_norm_stderr': 0.03847021420456023},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.36607142857142855,\n",
       "    'acc_stderr': 0.045723723587374296,\n",
       "    'acc_norm': 0.36607142857142855,\n",
       "    'acc_norm_stderr': 0.045723723587374296},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6990291262135923,\n",
       "    'acc_stderr': 0.045416094465039476,\n",
       "    'acc_norm': 0.6990291262135923,\n",
       "    'acc_norm_stderr': 0.045416094465039476},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7948717948717948,\n",
       "    'acc_stderr': 0.026453508054040332,\n",
       "    'acc_norm': 0.7948717948717948,\n",
       "    'acc_norm_stderr': 0.026453508054040332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.04999999999999999,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.04999999999999999},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.698595146871009,\n",
       "    'acc_stderr': 0.01640909109726878,\n",
       "    'acc_norm': 0.698595146871009,\n",
       "    'acc_norm_stderr': 0.01640909109726878},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5491329479768786,\n",
       "    'acc_stderr': 0.026788811931562757,\n",
       "    'acc_norm': 0.5491329479768786,\n",
       "    'acc_norm_stderr': 0.026788811931562757},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2860335195530726,\n",
       "    'acc_stderr': 0.015113972129062141,\n",
       "    'acc_norm': 0.2860335195530726,\n",
       "    'acc_norm_stderr': 0.015113972129062141},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5032679738562091,\n",
       "    'acc_stderr': 0.02862930519400354,\n",
       "    'acc_norm': 0.5032679738562091,\n",
       "    'acc_norm_stderr': 0.02862930519400354},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5594855305466238,\n",
       "    'acc_stderr': 0.028196400574197422,\n",
       "    'acc_norm': 0.5594855305466238,\n",
       "    'acc_norm_stderr': 0.028196400574197422},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.027431623722415012,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.027431623722415012},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3829787234042553,\n",
       "    'acc_stderr': 0.028999080904806178,\n",
       "    'acc_norm': 0.3829787234042553,\n",
       "    'acc_norm_stderr': 0.028999080904806178},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.37157757496740546,\n",
       "    'acc_stderr': 0.012341828514528298,\n",
       "    'acc_norm': 0.37157757496740546,\n",
       "    'acc_norm_stderr': 0.012341828514528298},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.48161764705882354,\n",
       "    'acc_stderr': 0.03035230339535196,\n",
       "    'acc_norm': 0.48161764705882354,\n",
       "    'acc_norm_stderr': 0.03035230339535196},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.49019607843137253,\n",
       "    'acc_stderr': 0.0202239460050743,\n",
       "    'acc_norm': 0.49019607843137253,\n",
       "    'acc_norm_stderr': 0.0202239460050743},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5727272727272728,\n",
       "    'acc_stderr': 0.04738198703545483,\n",
       "    'acc_norm': 0.5727272727272728,\n",
       "    'acc_norm_stderr': 0.04738198703545483},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5755102040816327,\n",
       "    'acc_stderr': 0.031642094879429414,\n",
       "    'acc_norm': 0.5755102040816327,\n",
       "    'acc_norm_stderr': 0.031642094879429414},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6169154228855721,\n",
       "    'acc_stderr': 0.034375193373382504,\n",
       "    'acc_norm': 0.6169154228855721,\n",
       "    'acc_norm_stderr': 0.034375193373382504},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.77,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.77,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.45180722891566266,\n",
       "    'acc_stderr': 0.038743715565879536,\n",
       "    'acc_norm': 0.45180722891566266,\n",
       "    'acc_norm_stderr': 0.038743715565879536},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3390452876376989,\n",
       "    'mc1_stderr': 0.01657179791062661,\n",
       "    'mc2': 0.5199824927914821,\n",
       "    'mc2_stderr': 0.01582403747940678},\n",
       "   'all': {'em': 0.0017827181208053692,\n",
       "    'em_stderr': 0.00043200973460389824,\n",
       "    'f1': 0.06156250000000011,\n",
       "    'f1_stderr': 0.001404482472524456,\n",
       "    'acc': 0.4371318828152442,\n",
       "    'acc_stderr': 0.010557145720065584},\n",
       "   'harness|drop|3': {'em': 0.0017827181208053692,\n",
       "    'em_stderr': 0.00043200973460389824,\n",
       "    'f1': 0.06156250000000011,\n",
       "    'f1_stderr': 0.001404482472524456},\n",
       "   'harness|gsm8k|5': {'acc': 0.12130401819560273,\n",
       "    'acc_stderr': 0.008992888497275597},\n",
       "   'harness|winogrande|5': {'acc': 0.7529597474348856,\n",
       "    'acc_stderr': 0.01212140294285557}}},\n",
       " 'mpt-7b-chat': {'key': 'mpt-7b-chat',\n",
       "  'Model': 'MPT-7B-Chat',\n",
       "  'MT-bench (score)': '5.42',\n",
       "  'MMLU': '0.320',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'MosaicML',\n",
       "  'Link': 'https://huggingface.co/mosaicml/mpt-7b-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.431740614334471,\n",
       "    'acc_stderr': 0.014474591427196204,\n",
       "    'acc_norm': 0.46501706484641636,\n",
       "    'acc_norm_stderr': 0.014575583922019669},\n",
       "   'harness|hellaswag|10': {'acc': 0.5710017924716192,\n",
       "    'acc_stderr': 0.004939215682191771,\n",
       "    'acc_norm': 0.7551284604660427,\n",
       "    'acc_norm_stderr': 0.00429132188812274},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.3925925925925926,\n",
       "    'acc_stderr': 0.04218506215368879,\n",
       "    'acc_norm': 0.3925925925925926,\n",
       "    'acc_norm_stderr': 0.04218506215368879},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.3684210526315789,\n",
       "    'acc_stderr': 0.03925523381052932,\n",
       "    'acc_norm': 0.3684210526315789,\n",
       "    'acc_norm_stderr': 0.03925523381052932},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.3849056603773585,\n",
       "    'acc_stderr': 0.029946498567699948,\n",
       "    'acc_norm': 0.3849056603773585,\n",
       "    'acc_norm_stderr': 0.029946498567699948},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.04076663253918567,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.04076663253918567},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621504,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621504},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.32947976878612717,\n",
       "    'acc_stderr': 0.03583901754736411,\n",
       "    'acc_norm': 0.32947976878612717,\n",
       "    'acc_norm_stderr': 0.03583901754736411},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.17647058823529413,\n",
       "    'acc_stderr': 0.0379328118530781,\n",
       "    'acc_norm': 0.17647058823529413,\n",
       "    'acc_norm_stderr': 0.0379328118530781},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.049888765156985884,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.049888765156985884},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.3702127659574468,\n",
       "    'acc_stderr': 0.031565646822367836,\n",
       "    'acc_norm': 0.3702127659574468,\n",
       "    'acc_norm_stderr': 0.031565646822367836},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.21929824561403508,\n",
       "    'acc_stderr': 0.03892431106518754,\n",
       "    'acc_norm': 0.21929824561403508,\n",
       "    'acc_norm_stderr': 0.03892431106518754},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.45517241379310347,\n",
       "    'acc_stderr': 0.04149886942192117,\n",
       "    'acc_norm': 0.45517241379310347,\n",
       "    'acc_norm_stderr': 0.04149886942192117},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.24338624338624337,\n",
       "    'acc_stderr': 0.02210112878741543,\n",
       "    'acc_norm': 0.24338624338624337,\n",
       "    'acc_norm_stderr': 0.02210112878741543},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.25396825396825395,\n",
       "    'acc_stderr': 0.038932596106046755,\n",
       "    'acc_norm': 0.25396825396825395,\n",
       "    'acc_norm_stderr': 0.038932596106046755},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768077,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768077},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.4032258064516129,\n",
       "    'acc_stderr': 0.027906150826041143,\n",
       "    'acc_norm': 0.4032258064516129,\n",
       "    'acc_norm_stderr': 0.027906150826041143},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.2413793103448276,\n",
       "    'acc_stderr': 0.030108330718011625,\n",
       "    'acc_norm': 0.2413793103448276,\n",
       "    'acc_norm_stderr': 0.030108330718011625},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.3212121212121212,\n",
       "    'acc_stderr': 0.036462049632538115,\n",
       "    'acc_norm': 0.3212121212121212,\n",
       "    'acc_norm_stderr': 0.036462049632538115},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.36363636363636365,\n",
       "    'acc_stderr': 0.034273086529999365,\n",
       "    'acc_norm': 0.36363636363636365,\n",
       "    'acc_norm_stderr': 0.034273086529999365},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.49222797927461137,\n",
       "    'acc_stderr': 0.03608003225569653,\n",
       "    'acc_norm': 0.49222797927461137,\n",
       "    'acc_norm_stderr': 0.03608003225569653},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.3871794871794872,\n",
       "    'acc_stderr': 0.02469721693087894,\n",
       "    'acc_norm': 0.3871794871794872,\n",
       "    'acc_norm_stderr': 0.02469721693087894},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.21851851851851853,\n",
       "    'acc_stderr': 0.025195752251823786,\n",
       "    'acc_norm': 0.21851851851851853,\n",
       "    'acc_norm_stderr': 0.025195752251823786},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.3907563025210084,\n",
       "    'acc_stderr': 0.031693802357129965,\n",
       "    'acc_norm': 0.3907563025210084,\n",
       "    'acc_norm_stderr': 0.031693802357129965},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2847682119205298,\n",
       "    'acc_stderr': 0.03684881521389024,\n",
       "    'acc_norm': 0.2847682119205298,\n",
       "    'acc_norm_stderr': 0.03684881521389024},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.5100917431192661,\n",
       "    'acc_stderr': 0.02143295620345332,\n",
       "    'acc_norm': 0.5100917431192661,\n",
       "    'acc_norm_stderr': 0.02143295620345332},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.35185185185185186,\n",
       "    'acc_stderr': 0.03256850570293648,\n",
       "    'acc_norm': 0.35185185185185186,\n",
       "    'acc_norm_stderr': 0.03256850570293648},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.3284313725490196,\n",
       "    'acc_stderr': 0.032962451101722294,\n",
       "    'acc_norm': 0.3284313725490196,\n",
       "    'acc_norm_stderr': 0.032962451101722294},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.4092827004219409,\n",
       "    'acc_stderr': 0.032007041833595914,\n",
       "    'acc_norm': 0.4092827004219409,\n",
       "    'acc_norm_stderr': 0.032007041833595914},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.484304932735426,\n",
       "    'acc_stderr': 0.0335412657542081,\n",
       "    'acc_norm': 0.484304932735426,\n",
       "    'acc_norm_stderr': 0.0335412657542081},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.46564885496183206,\n",
       "    'acc_stderr': 0.04374928560599738,\n",
       "    'acc_norm': 0.46564885496183206,\n",
       "    'acc_norm_stderr': 0.04374928560599738},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.4049586776859504,\n",
       "    'acc_stderr': 0.044811377559424694,\n",
       "    'acc_norm': 0.4049586776859504,\n",
       "    'acc_norm_stderr': 0.044811377559424694},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.4074074074074074,\n",
       "    'acc_stderr': 0.047500773411999854,\n",
       "    'acc_norm': 0.4074074074074074,\n",
       "    'acc_norm_stderr': 0.047500773411999854},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2883435582822086,\n",
       "    'acc_stderr': 0.03559039531617342,\n",
       "    'acc_norm': 0.2883435582822086,\n",
       "    'acc_norm_stderr': 0.03559039531617342},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.33035714285714285,\n",
       "    'acc_stderr': 0.04464285714285713,\n",
       "    'acc_norm': 0.33035714285714285,\n",
       "    'acc_norm_stderr': 0.04464285714285713},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.36893203883495146,\n",
       "    'acc_stderr': 0.04777615181156739,\n",
       "    'acc_norm': 0.36893203883495146,\n",
       "    'acc_norm_stderr': 0.04777615181156739},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.5512820512820513,\n",
       "    'acc_stderr': 0.032583346493868806,\n",
       "    'acc_norm': 0.5512820512820513,\n",
       "    'acc_norm_stderr': 0.032583346493868806},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.5325670498084292,\n",
       "    'acc_stderr': 0.01784199575052087,\n",
       "    'acc_norm': 0.5325670498084292,\n",
       "    'acc_norm_stderr': 0.01784199575052087},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.41040462427745666,\n",
       "    'acc_stderr': 0.026483392042098187,\n",
       "    'acc_norm': 0.41040462427745666,\n",
       "    'acc_norm_stderr': 0.026483392042098187},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961443,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961443},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.4019607843137255,\n",
       "    'acc_stderr': 0.02807415894760065,\n",
       "    'acc_norm': 0.4019607843137255,\n",
       "    'acc_norm_stderr': 0.02807415894760065},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.40836012861736337,\n",
       "    'acc_stderr': 0.027917050748484624,\n",
       "    'acc_norm': 0.40836012861736337,\n",
       "    'acc_norm_stderr': 0.027917050748484624},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.36728395061728397,\n",
       "    'acc_stderr': 0.026822801759507894,\n",
       "    'acc_norm': 0.36728395061728397,\n",
       "    'acc_norm_stderr': 0.026822801759507894},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2695035460992908,\n",
       "    'acc_stderr': 0.026469036818590627,\n",
       "    'acc_norm': 0.2695035460992908,\n",
       "    'acc_norm_stderr': 0.026469036818590627},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.2907431551499348,\n",
       "    'acc_stderr': 0.011598062372851988,\n",
       "    'acc_norm': 0.2907431551499348,\n",
       "    'acc_norm_stderr': 0.011598062372851988},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.3897058823529412,\n",
       "    'acc_stderr': 0.029624663581159696,\n",
       "    'acc_norm': 0.3897058823529412,\n",
       "    'acc_norm_stderr': 0.029624663581159696},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.3202614379084967,\n",
       "    'acc_stderr': 0.018875682938069446,\n",
       "    'acc_norm': 0.3202614379084967,\n",
       "    'acc_norm_stderr': 0.018875682938069446},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.4636363636363636,\n",
       "    'acc_stderr': 0.047764491623961985,\n",
       "    'acc_norm': 0.4636363636363636,\n",
       "    'acc_norm_stderr': 0.047764491623961985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.4857142857142857,\n",
       "    'acc_stderr': 0.03199615232806286,\n",
       "    'acc_norm': 0.4857142857142857,\n",
       "    'acc_norm_stderr': 0.03199615232806286},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.48258706467661694,\n",
       "    'acc_stderr': 0.03533389234739245,\n",
       "    'acc_norm': 0.48258706467661694,\n",
       "    'acc_norm_stderr': 0.03533389234739245},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.04975698519562426,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.04975698519562426},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.42771084337349397,\n",
       "    'acc_stderr': 0.038515976837185335,\n",
       "    'acc_norm': 0.42771084337349397,\n",
       "    'acc_norm_stderr': 0.038515976837185335},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.49707602339181284,\n",
       "    'acc_stderr': 0.03834759370936839,\n",
       "    'acc_norm': 0.49707602339181284,\n",
       "    'acc_norm_stderr': 0.03834759370936839},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.27050183598531213,\n",
       "    'mc1_stderr': 0.015550778332842895,\n",
       "    'mc2': 0.40163647231251104,\n",
       "    'mc2_stderr': 0.014753108409806075},\n",
       "   'all': {'em': 0.06952600671140939,\n",
       "    'em_stderr': 0.002604746204517829,\n",
       "    'f1': 0.12196937919463072,\n",
       "    'f1_stderr': 0.002840521979064293,\n",
       "    'acc': 0.3626168565432783,\n",
       "    'acc_stderr': 0.009260585769647573},\n",
       "   'harness|drop|3': {'em': 0.06952600671140939,\n",
       "    'em_stderr': 0.002604746204517829,\n",
       "    'f1': 0.12196937919463072,\n",
       "    'f1_stderr': 0.002840521979064293},\n",
       "   'harness|gsm8k|5': {'acc': 0.04094010614101592,\n",
       "    'acc_stderr': 0.005458076796294338},\n",
       "   'harness|winogrande|5': {'acc': 0.6842936069455406,\n",
       "    'acc_stderr': 0.01306309474300081}}},\n",
       " 'oasst-pythia-12b': {'key': 'oasst-pythia-12b',\n",
       "  'Model': 'OpenAssistant-Pythia-12B',\n",
       "  'MT-bench (score)': '4.32',\n",
       "  'MMLU': '0.270',\n",
       "  'License': 'Apache 2.0',\n",
       "  'Organization': 'OpenAssistant',\n",
       "  'Link': 'https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',\n",
       "  'results': {'harness|drop|3': {'em': 0.001363255033557047,\n",
       "    'em_stderr': 0.00037786091964606887,\n",
       "    'f1': 0.059077181208053976,\n",
       "    'f1_stderr': 0.001394848925611238},\n",
       "   'harness|gsm8k|5': {'acc': 0.030326004548900682,\n",
       "    'acc_stderr': 0.004723487465514761},\n",
       "   'harness|winogrande|5': {'acc': 0.659037095501184,\n",
       "    'acc_stderr': 0.013322681435934807},\n",
       "   'all': {'acc': 0.2748440247593001,\n",
       "    'acc_stderr': 0.03228989271589345,\n",
       "    'acc_norm': 0.2784679867417539,\n",
       "    'acc_norm_stderr': 0.03228683184562667,\n",
       "    'mc1': 0.2386780905752754,\n",
       "    'mc1_stderr': 0.014922629695456418,\n",
       "    'mc2': 0.37807719406726975,\n",
       "    'mc2_stderr': 0.014701655079555453},\n",
       "   'harness|arc:challenge|25': {'acc': 0.4121160409556314,\n",
       "    'acc_stderr': 0.0143839153022254,\n",
       "    'acc_norm': 0.45733788395904434,\n",
       "    'acc_norm_stderr': 0.014558106543924067},\n",
       "   'harness|hellaswag|10': {'acc': 0.5173272256522605,\n",
       "    'acc_stderr': 0.004986784319771785,\n",
       "    'acc_norm': 0.6859191396136228,\n",
       "    'acc_norm_stderr': 0.004632001732332984},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768081,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768081},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.31851851851851853,\n",
       "    'acc_stderr': 0.04024778401977109,\n",
       "    'acc_norm': 0.31851851851851853,\n",
       "    'acc_norm_stderr': 0.04024778401977109},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.23026315789473684,\n",
       "    'acc_stderr': 0.03426059424403165,\n",
       "    'acc_norm': 0.23026315789473684,\n",
       "    'acc_norm_stderr': 0.03426059424403165},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.33584905660377357,\n",
       "    'acc_stderr': 0.029067220146644833,\n",
       "    'acc_norm': 0.33584905660377357,\n",
       "    'acc_norm_stderr': 0.029067220146644833},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2847222222222222,\n",
       "    'acc_stderr': 0.03773809990686936,\n",
       "    'acc_norm': 0.2847222222222222,\n",
       "    'acc_norm_stderr': 0.03773809990686936},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.2,\n",
       "    'acc_stderr': 0.040201512610368445,\n",
       "    'acc_norm': 0.2,\n",
       "    'acc_norm_stderr': 0.040201512610368445},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.24855491329479767,\n",
       "    'acc_stderr': 0.03295304696818317,\n",
       "    'acc_norm': 0.24855491329479767,\n",
       "    'acc_norm_stderr': 0.03295304696818317},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.28431372549019607,\n",
       "    'acc_stderr': 0.04488482852329017,\n",
       "    'acc_norm': 0.28431372549019607,\n",
       "    'acc_norm_stderr': 0.04488482852329017},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768079,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768079},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.30638297872340425,\n",
       "    'acc_stderr': 0.030135906478517563,\n",
       "    'acc_norm': 0.30638297872340425,\n",
       "    'acc_norm_stderr': 0.030135906478517563},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.22807017543859648,\n",
       "    'acc_stderr': 0.03947152782669416,\n",
       "    'acc_norm': 0.22807017543859648,\n",
       "    'acc_norm_stderr': 0.03947152782669416},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.25517241379310346,\n",
       "    'acc_stderr': 0.03632984052707841,\n",
       "    'acc_norm': 0.25517241379310346,\n",
       "    'acc_norm_stderr': 0.03632984052707841},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.25132275132275134,\n",
       "    'acc_stderr': 0.022340482339643895,\n",
       "    'acc_norm': 0.25132275132275134,\n",
       "    'acc_norm_stderr': 0.022340482339643895},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.1746031746031746,\n",
       "    'acc_stderr': 0.033954900208561116,\n",
       "    'acc_norm': 0.1746031746031746,\n",
       "    'acc_norm_stderr': 0.033954900208561116},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.04560480215720684,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.04560480215720684},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.267741935483871,\n",
       "    'acc_stderr': 0.025189006660212385,\n",
       "    'acc_norm': 0.267741935483871,\n",
       "    'acc_norm_stderr': 0.025189006660212385},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.21674876847290642,\n",
       "    'acc_stderr': 0.028990331252516235,\n",
       "    'acc_norm': 0.21674876847290642,\n",
       "    'acc_norm_stderr': 0.028990331252516235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.28484848484848485,\n",
       "    'acc_stderr': 0.03524390844511784,\n",
       "    'acc_norm': 0.28484848484848485,\n",
       "    'acc_norm_stderr': 0.03524390844511784},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.25757575757575757,\n",
       "    'acc_stderr': 0.031156269519646836,\n",
       "    'acc_norm': 0.25757575757575757,\n",
       "    'acc_norm_stderr': 0.031156269519646836},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.23834196891191708,\n",
       "    'acc_stderr': 0.03074890536390989,\n",
       "    'acc_norm': 0.23834196891191708,\n",
       "    'acc_norm_stderr': 0.03074890536390989},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.2641025641025641,\n",
       "    'acc_stderr': 0.022352193737453282,\n",
       "    'acc_norm': 0.2641025641025641,\n",
       "    'acc_norm_stderr': 0.022352193737453282},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2518518518518518,\n",
       "    'acc_stderr': 0.02646611753895991,\n",
       "    'acc_norm': 0.2518518518518518,\n",
       "    'acc_norm_stderr': 0.02646611753895991},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.24369747899159663,\n",
       "    'acc_stderr': 0.02788682807838058,\n",
       "    'acc_norm': 0.24369747899159663,\n",
       "    'acc_norm_stderr': 0.02788682807838058},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.26490066225165565,\n",
       "    'acc_stderr': 0.03603038545360384,\n",
       "    'acc_norm': 0.26490066225165565,\n",
       "    'acc_norm_stderr': 0.03603038545360384},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.24587155963302754,\n",
       "    'acc_stderr': 0.018461940968708446,\n",
       "    'acc_norm': 0.24587155963302754,\n",
       "    'acc_norm_stderr': 0.018461940968708446},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.24074074074074073,\n",
       "    'acc_stderr': 0.02915752218460561,\n",
       "    'acc_norm': 0.24074074074074073,\n",
       "    'acc_norm_stderr': 0.02915752218460561},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.29411764705882354,\n",
       "    'acc_stderr': 0.03198001660115071,\n",
       "    'acc_norm': 0.29411764705882354,\n",
       "    'acc_norm_stderr': 0.03198001660115071},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.23628691983122363,\n",
       "    'acc_stderr': 0.027652153144159263,\n",
       "    'acc_norm': 0.23628691983122363,\n",
       "    'acc_norm_stderr': 0.027652153144159263},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.3004484304932735,\n",
       "    'acc_stderr': 0.030769352008229136,\n",
       "    'acc_norm': 0.3004484304932735,\n",
       "    'acc_norm_stderr': 0.030769352008229136},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.22137404580152673,\n",
       "    'acc_stderr': 0.036412970813137276,\n",
       "    'acc_norm': 0.22137404580152673,\n",
       "    'acc_norm_stderr': 0.036412970813137276},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.3305785123966942,\n",
       "    'acc_stderr': 0.04294340845212094,\n",
       "    'acc_norm': 0.3305785123966942,\n",
       "    'acc_norm_stderr': 0.04294340845212094},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.28703703703703703,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.28703703703703703,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2822085889570552,\n",
       "    'acc_stderr': 0.03536117886664743,\n",
       "    'acc_norm': 0.2822085889570552,\n",
       "    'acc_norm_stderr': 0.03536117886664743},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.26785714285714285,\n",
       "    'acc_stderr': 0.04203277291467764,\n",
       "    'acc_norm': 0.26785714285714285,\n",
       "    'acc_norm_stderr': 0.04203277291467764},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.18446601941747573,\n",
       "    'acc_stderr': 0.03840423627288276,\n",
       "    'acc_norm': 0.18446601941747573,\n",
       "    'acc_norm_stderr': 0.03840423627288276},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.29914529914529914,\n",
       "    'acc_stderr': 0.029996951858349497,\n",
       "    'acc_norm': 0.29914529914529914,\n",
       "    'acc_norm_stderr': 0.029996951858349497},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.28607918263090676,\n",
       "    'acc_stderr': 0.01616087140512754,\n",
       "    'acc_norm': 0.28607918263090676,\n",
       "    'acc_norm_stderr': 0.01616087140512754},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.25722543352601157,\n",
       "    'acc_stderr': 0.02353292543104429,\n",
       "    'acc_norm': 0.25722543352601157,\n",
       "    'acc_norm_stderr': 0.02353292543104429},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2424581005586592,\n",
       "    'acc_stderr': 0.014333522059217889,\n",
       "    'acc_norm': 0.2424581005586592,\n",
       "    'acc_norm_stderr': 0.014333522059217889},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.27124183006535946,\n",
       "    'acc_stderr': 0.025457756696667864,\n",
       "    'acc_norm': 0.27124183006535946,\n",
       "    'acc_norm_stderr': 0.025457756696667864},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.2861736334405145,\n",
       "    'acc_stderr': 0.02567025924218895,\n",
       "    'acc_norm': 0.2861736334405145,\n",
       "    'acc_norm_stderr': 0.02567025924218895},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.2716049382716049,\n",
       "    'acc_stderr': 0.02474862449053737,\n",
       "    'acc_norm': 0.2716049382716049,\n",
       "    'acc_norm_stderr': 0.02474862449053737},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2730496453900709,\n",
       "    'acc_stderr': 0.026577860943307857,\n",
       "    'acc_norm': 0.2730496453900709,\n",
       "    'acc_norm_stderr': 0.026577860943307857},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.27835723598435463,\n",
       "    'acc_stderr': 0.011446990197380985,\n",
       "    'acc_norm': 0.27835723598435463,\n",
       "    'acc_norm_stderr': 0.011446990197380985},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.23529411764705882,\n",
       "    'acc_stderr': 0.025767252010855973,\n",
       "    'acc_norm': 0.23529411764705882,\n",
       "    'acc_norm_stderr': 0.025767252010855973},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.2777777777777778,\n",
       "    'acc_stderr': 0.018120224251484584,\n",
       "    'acc_norm': 0.2777777777777778,\n",
       "    'acc_norm_stderr': 0.018120224251484584},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.32727272727272727,\n",
       "    'acc_stderr': 0.04494290866252089,\n",
       "    'acc_norm': 0.32727272727272727,\n",
       "    'acc_norm_stderr': 0.04494290866252089},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.2571428571428571,\n",
       "    'acc_stderr': 0.027979823538744543,\n",
       "    'acc_norm': 0.2571428571428571,\n",
       "    'acc_norm_stderr': 0.027979823538744543},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.27860696517412936,\n",
       "    'acc_stderr': 0.031700561834973086,\n",
       "    'acc_norm': 0.27860696517412936,\n",
       "    'acc_norm_stderr': 0.031700561834973086},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.3373493975903614,\n",
       "    'acc_stderr': 0.0368078369072758,\n",
       "    'acc_norm': 0.3373493975903614,\n",
       "    'acc_norm_stderr': 0.0368078369072758},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.26900584795321636,\n",
       "    'acc_stderr': 0.03401052620104089,\n",
       "    'acc_norm': 0.26900584795321636,\n",
       "    'acc_norm_stderr': 0.03401052620104089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.2386780905752754,\n",
       "    'mc1_stderr': 0.014922629695456418,\n",
       "    'mc2': 0.37807719406726975,\n",
       "    'mc2_stderr': 0.014701655079555453}}},\n",
       " 'openchat-3.5': {'key': 'openchat-3.5',\n",
       "  'Model': 'OpenChat-3.5',\n",
       "  'MT-bench (score)': '7.81',\n",
       "  'MMLU': '0.643',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'OpenChat',\n",
       "  'Link': 'https://huggingface.co/openchat/openchat_3.5',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5972696245733788,\n",
       "    'acc_stderr': 0.014332236306790147,\n",
       "    'acc_norm': 0.6382252559726962,\n",
       "    'acc_norm_stderr': 0.014041957945038078},\n",
       "   'harness|hellaswag|10': {'acc': 0.6579366660027883,\n",
       "    'acc_stderr': 0.004734311435009196,\n",
       "    'acc_norm': 0.8480382393945429,\n",
       "    'acc_norm_stderr': 0.003582501596564539},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5703703703703704,\n",
       "    'acc_stderr': 0.042763494943765995,\n",
       "    'acc_norm': 0.5703703703703704,\n",
       "    'acc_norm_stderr': 0.042763494943765995},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6973684210526315,\n",
       "    'acc_stderr': 0.037385206761196686,\n",
       "    'acc_norm': 0.6973684210526315,\n",
       "    'acc_norm_stderr': 0.037385206761196686},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.028544793319055326,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.028544793319055326},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7708333333333334,\n",
       "    'acc_stderr': 0.035146974678623884,\n",
       "    'acc_norm': 0.7708333333333334,\n",
       "    'acc_norm_stderr': 0.035146974678623884},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.653179190751445,\n",
       "    'acc_stderr': 0.036291466701596636,\n",
       "    'acc_norm': 0.653179190751445,\n",
       "    'acc_norm_stderr': 0.036291466701596636},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4117647058823529,\n",
       "    'acc_stderr': 0.04897104952726366,\n",
       "    'acc_norm': 0.4117647058823529,\n",
       "    'acc_norm_stderr': 0.04897104952726366},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.574468085106383,\n",
       "    'acc_stderr': 0.032321469162244675,\n",
       "    'acc_norm': 0.574468085106383,\n",
       "    'acc_norm_stderr': 0.032321469162244675},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5175438596491229,\n",
       "    'acc_stderr': 0.04700708033551038,\n",
       "    'acc_norm': 0.5175438596491229,\n",
       "    'acc_norm_stderr': 0.04700708033551038},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370332,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370332},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42328042328042326,\n",
       "    'acc_stderr': 0.02544636563440679,\n",
       "    'acc_norm': 0.42328042328042326,\n",
       "    'acc_norm_stderr': 0.02544636563440679},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5158730158730159,\n",
       "    'acc_stderr': 0.044698818540726076,\n",
       "    'acc_norm': 0.5158730158730159,\n",
       "    'acc_norm_stderr': 0.044698818540726076},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7967741935483871,\n",
       "    'acc_stderr': 0.02289168798455496,\n",
       "    'acc_norm': 0.7967741935483871,\n",
       "    'acc_norm_stderr': 0.02289168798455496},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5073891625615764,\n",
       "    'acc_stderr': 0.035176035403610105,\n",
       "    'acc_norm': 0.5073891625615764,\n",
       "    'acc_norm_stderr': 0.035176035403610105},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7757575757575758,\n",
       "    'acc_stderr': 0.032568666616811015,\n",
       "    'acc_norm': 0.7757575757575758,\n",
       "    'acc_norm_stderr': 0.032568666616811015},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.803030303030303,\n",
       "    'acc_stderr': 0.028335609732463362,\n",
       "    'acc_norm': 0.803030303030303,\n",
       "    'acc_norm_stderr': 0.028335609732463362},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9067357512953368,\n",
       "    'acc_stderr': 0.02098685459328974,\n",
       "    'acc_norm': 0.9067357512953368,\n",
       "    'acc_norm_stderr': 0.02098685459328974},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.023901157979402538,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.023901157979402538},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.36666666666666664,\n",
       "    'acc_stderr': 0.02938162072646507,\n",
       "    'acc_norm': 0.36666666666666664,\n",
       "    'acc_norm_stderr': 0.02938162072646507},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6680672268907563,\n",
       "    'acc_stderr': 0.03058869701378364,\n",
       "    'acc_norm': 0.6680672268907563,\n",
       "    'acc_norm_stderr': 0.03058869701378364},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.038227469376587525,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.038227469376587525},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8550458715596331,\n",
       "    'acc_stderr': 0.015094215699700469,\n",
       "    'acc_norm': 0.8550458715596331,\n",
       "    'acc_norm_stderr': 0.015094215699700469},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.03395322726375798,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.03395322726375798},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8333333333333334,\n",
       "    'acc_stderr': 0.026156867523931045,\n",
       "    'acc_norm': 0.8333333333333334,\n",
       "    'acc_norm_stderr': 0.026156867523931045},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8227848101265823,\n",
       "    'acc_stderr': 0.024856364184503234,\n",
       "    'acc_norm': 0.8227848101265823,\n",
       "    'acc_norm_stderr': 0.024856364184503234},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.726457399103139,\n",
       "    'acc_stderr': 0.029918586707798827,\n",
       "    'acc_norm': 0.726457399103139,\n",
       "    'acc_norm_stderr': 0.029918586707798827},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7786259541984732,\n",
       "    'acc_stderr': 0.03641297081313729,\n",
       "    'acc_norm': 0.7786259541984732,\n",
       "    'acc_norm_stderr': 0.03641297081313729},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03520893951097653,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03520893951097653},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.0401910747255735,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.0401910747255735},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7791411042944786,\n",
       "    'acc_stderr': 0.03259177392742178,\n",
       "    'acc_norm': 0.7791411042944786,\n",
       "    'acc_norm_stderr': 0.03259177392742178},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.44642857142857145,\n",
       "    'acc_stderr': 0.04718471485219588,\n",
       "    'acc_norm': 0.44642857142857145,\n",
       "    'acc_norm_stderr': 0.04718471485219588},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8252427184466019,\n",
       "    'acc_stderr': 0.03760178006026621,\n",
       "    'acc_norm': 0.8252427184466019,\n",
       "    'acc_norm_stderr': 0.03760178006026621},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8760683760683761,\n",
       "    'acc_stderr': 0.02158649400128138,\n",
       "    'acc_norm': 0.8760683760683761,\n",
       "    'acc_norm_stderr': 0.02158649400128138},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8212005108556832,\n",
       "    'acc_stderr': 0.013702643715368982,\n",
       "    'acc_norm': 0.8212005108556832,\n",
       "    'acc_norm_stderr': 0.013702643715368982},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7456647398843931,\n",
       "    'acc_stderr': 0.023445826276545546,\n",
       "    'acc_norm': 0.7456647398843931,\n",
       "    'acc_norm_stderr': 0.023445826276545546},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.42681564245810055,\n",
       "    'acc_stderr': 0.016542401954631906,\n",
       "    'acc_norm': 0.42681564245810055,\n",
       "    'acc_norm_stderr': 0.016542401954631906},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7189542483660131,\n",
       "    'acc_stderr': 0.025738854797818733,\n",
       "    'acc_norm': 0.7189542483660131,\n",
       "    'acc_norm_stderr': 0.025738854797818733},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7202572347266881,\n",
       "    'acc_stderr': 0.025494259350694912,\n",
       "    'acc_norm': 0.7202572347266881,\n",
       "    'acc_norm_stderr': 0.025494259350694912},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7314814814814815,\n",
       "    'acc_stderr': 0.02465968518596728,\n",
       "    'acc_norm': 0.7314814814814815,\n",
       "    'acc_norm_stderr': 0.02465968518596728},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.48226950354609927,\n",
       "    'acc_stderr': 0.02980873964223777,\n",
       "    'acc_norm': 0.48226950354609927,\n",
       "    'acc_norm_stderr': 0.02980873964223777},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.47392438070404175,\n",
       "    'acc_stderr': 0.012752858346533134,\n",
       "    'acc_norm': 0.47392438070404175,\n",
       "    'acc_norm_stderr': 0.012752858346533134},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6801470588235294,\n",
       "    'acc_stderr': 0.02833295951403121,\n",
       "    'acc_norm': 0.6801470588235294,\n",
       "    'acc_norm_stderr': 0.02833295951403121},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6748366013071896,\n",
       "    'acc_stderr': 0.01895088677080631,\n",
       "    'acc_norm': 0.6748366013071896,\n",
       "    'acc_norm_stderr': 0.01895088677080631},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.0449429086625209,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.0449429086625209},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.028263889943784603,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.028263889943784603},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.835820895522388,\n",
       "    'acc_stderr': 0.026193923544454125,\n",
       "    'acc_norm': 0.835820895522388,\n",
       "    'acc_norm_stderr': 0.026193923544454125},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.88,\n",
       "    'acc_stderr': 0.03265986323710906,\n",
       "    'acc_norm': 0.88,\n",
       "    'acc_norm_stderr': 0.03265986323710906},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.536144578313253,\n",
       "    'acc_stderr': 0.038823108508905954,\n",
       "    'acc_norm': 0.536144578313253,\n",
       "    'acc_norm_stderr': 0.038823108508905954},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.30966952264381886,\n",
       "    'mc1_stderr': 0.0161857443551449,\n",
       "    'mc2': 0.46392847173087287,\n",
       "    'mc2_stderr': 0.015046687408684912},\n",
       "   'harness|winogrande|5': {'acc': 0.8074191002367798,\n",
       "    'acc_stderr': 0.011082538847491902},\n",
       "   'harness|drop|3': {'em': 0.0019924496644295304,\n",
       "    'em_stderr': 0.0004566676462666931,\n",
       "    'f1': 0.07226510067114073,\n",
       "    'f1_stderr': 0.0014840662284336832},\n",
       "   'harness|gsm8k|5': {'acc': 0.26611068991660347,\n",
       "    'acc_stderr': 0.012172750939040322},\n",
       "   'all': {'acc': 0.6453244910928128,\n",
       "    'acc_stderr': 0.031930893551459144,\n",
       "    'acc_norm': 0.6529207012084622,\n",
       "    'acc_norm_stderr': 0.03259469709580134,\n",
       "    'mc1': 0.30966952264381886,\n",
       "    'mc1_stderr': 0.0161857443551449,\n",
       "    'mc2': 0.46392847173087287,\n",
       "    'mc2_stderr': 0.015046687408684912,\n",
       "    'em': 0.0019924496644295304,\n",
       "    'em_stderr': 0.0004566676462666931,\n",
       "    'f1': 0.07226510067114073,\n",
       "    'f1_stderr': 0.0014840662284336832}}},\n",
       " 'openhermes-2.5-mistral-7b': {'key': 'openhermes-2.5-mistral-7b',\n",
       "  'Model': 'OpenHermes-2.5-Mistral-7b',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '-',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'NousResearch',\n",
       "  'Link': 'https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6126279863481229,\n",
       "    'acc_stderr': 0.014235872487909869,\n",
       "    'acc_norm': 0.6493174061433447,\n",
       "    'acc_norm_stderr': 0.013944635930726099},\n",
       "   'harness|hellaswag|10': {'acc': 0.6519617606054571,\n",
       "    'acc_stderr': 0.004753746951620152,\n",
       "    'acc_norm': 0.8429595698068114,\n",
       "    'acc_norm_stderr': 0.003630952999843739},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695236,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695236},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.0421850621536888,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.0421850621536888},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6973684210526315,\n",
       "    'acc_stderr': 0.037385206761196686,\n",
       "    'acc_norm': 0.6973684210526315,\n",
       "    'acc_norm_stderr': 0.037385206761196686},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.02854479331905533,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.02854479331905533},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7569444444444444,\n",
       "    'acc_stderr': 0.03586879280080341,\n",
       "    'acc_norm': 0.7569444444444444,\n",
       "    'acc_norm_stderr': 0.03586879280080341},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.46,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.46,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6242774566473989,\n",
       "    'acc_stderr': 0.036928207672648664,\n",
       "    'acc_norm': 0.6242774566473989,\n",
       "    'acc_norm_stderr': 0.036928207672648664},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107223,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107223},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5659574468085107,\n",
       "    'acc_stderr': 0.03240038086792747,\n",
       "    'acc_norm': 0.5659574468085107,\n",
       "    'acc_norm_stderr': 0.03240038086792747},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.49122807017543857,\n",
       "    'acc_stderr': 0.04702880432049615,\n",
       "    'acc_norm': 0.49122807017543857,\n",
       "    'acc_norm_stderr': 0.04702880432049615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5241379310344828,\n",
       "    'acc_stderr': 0.0416180850350153,\n",
       "    'acc_norm': 0.5241379310344828,\n",
       "    'acc_norm_stderr': 0.0416180850350153},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42592592592592593,\n",
       "    'acc_stderr': 0.02546714904546955,\n",
       "    'acc_norm': 0.42592592592592593,\n",
       "    'acc_norm_stderr': 0.02546714904546955},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.46825396825396826,\n",
       "    'acc_stderr': 0.04463112720677172,\n",
       "    'acc_norm': 0.46825396825396826,\n",
       "    'acc_norm_stderr': 0.04463112720677172},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7935483870967742,\n",
       "    'acc_stderr': 0.02302589961718871,\n",
       "    'acc_norm': 0.7935483870967742,\n",
       "    'acc_norm_stderr': 0.02302589961718871},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.035158955511656986,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.035158955511656986},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7818181818181819,\n",
       "    'acc_stderr': 0.032250781083062896,\n",
       "    'acc_norm': 0.7818181818181819,\n",
       "    'acc_norm_stderr': 0.032250781083062896},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.028057791672989017,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.028057791672989017},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8911917098445595,\n",
       "    'acc_stderr': 0.022473253332768776,\n",
       "    'acc_norm': 0.8911917098445595,\n",
       "    'acc_norm_stderr': 0.022473253332768776},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6128205128205129,\n",
       "    'acc_stderr': 0.024697216930878937,\n",
       "    'acc_norm': 0.6128205128205129,\n",
       "    'acc_norm_stderr': 0.024697216930878937},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3037037037037037,\n",
       "    'acc_stderr': 0.02803792996911499,\n",
       "    'acc_norm': 0.3037037037037037,\n",
       "    'acc_norm_stderr': 0.02803792996911499},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.680672268907563,\n",
       "    'acc_stderr': 0.030283995525884396,\n",
       "    'acc_norm': 0.680672268907563,\n",
       "    'acc_norm_stderr': 0.030283995525884396},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.31788079470198677,\n",
       "    'acc_stderr': 0.038020397601079024,\n",
       "    'acc_norm': 0.31788079470198677,\n",
       "    'acc_norm_stderr': 0.038020397601079024},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8330275229357799,\n",
       "    'acc_stderr': 0.01599015488507338,\n",
       "    'acc_norm': 0.8330275229357799,\n",
       "    'acc_norm_stderr': 0.01599015488507338},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5092592592592593,\n",
       "    'acc_stderr': 0.034093869469927006,\n",
       "    'acc_norm': 0.5092592592592593,\n",
       "    'acc_norm_stderr': 0.034093869469927006},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7990196078431373,\n",
       "    'acc_stderr': 0.02812597226565437,\n",
       "    'acc_norm': 0.7990196078431373,\n",
       "    'acc_norm_stderr': 0.02812597226565437},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8143459915611815,\n",
       "    'acc_stderr': 0.025310495376944856,\n",
       "    'acc_norm': 0.8143459915611815,\n",
       "    'acc_norm_stderr': 0.025310495376944856},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7040358744394619,\n",
       "    'acc_stderr': 0.030636591348699803,\n",
       "    'acc_norm': 0.7040358744394619,\n",
       "    'acc_norm_stderr': 0.030636591348699803},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7938931297709924,\n",
       "    'acc_stderr': 0.035477710041594654,\n",
       "    'acc_norm': 0.7938931297709924,\n",
       "    'acc_norm_stderr': 0.035477710041594654},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7603305785123967,\n",
       "    'acc_stderr': 0.03896878985070416,\n",
       "    'acc_norm': 0.7603305785123967,\n",
       "    'acc_norm_stderr': 0.03896878985070416},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7870370370370371,\n",
       "    'acc_stderr': 0.039578354719809805,\n",
       "    'acc_norm': 0.7870370370370371,\n",
       "    'acc_norm_stderr': 0.039578354719809805},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7852760736196319,\n",
       "    'acc_stderr': 0.032262193772867744,\n",
       "    'acc_norm': 0.7852760736196319,\n",
       "    'acc_norm_stderr': 0.032262193772867744},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.5089285714285714,\n",
       "    'acc_stderr': 0.04745033255489123,\n",
       "    'acc_norm': 0.5089285714285714,\n",
       "    'acc_norm_stderr': 0.04745033255489123},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7766990291262136,\n",
       "    'acc_stderr': 0.04123553189891431,\n",
       "    'acc_norm': 0.7766990291262136,\n",
       "    'acc_norm_stderr': 0.04123553189891431},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.022801382534597528,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.022801382534597528},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8301404853128991,\n",
       "    'acc_stderr': 0.013428186370608306,\n",
       "    'acc_norm': 0.8301404853128991,\n",
       "    'acc_norm_stderr': 0.013428186370608306},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7167630057803468,\n",
       "    'acc_stderr': 0.02425790170532338,\n",
       "    'acc_norm': 0.7167630057803468,\n",
       "    'acc_norm_stderr': 0.02425790170532338},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30837988826815643,\n",
       "    'acc_stderr': 0.01544571691099888,\n",
       "    'acc_norm': 0.30837988826815643,\n",
       "    'acc_norm_stderr': 0.01544571691099888},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7549019607843137,\n",
       "    'acc_stderr': 0.024630048979824782,\n",
       "    'acc_norm': 0.7549019607843137,\n",
       "    'acc_norm_stderr': 0.024630048979824782},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.684887459807074,\n",
       "    'acc_stderr': 0.026385273703464485,\n",
       "    'acc_norm': 0.684887459807074,\n",
       "    'acc_norm_stderr': 0.026385273703464485},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7530864197530864,\n",
       "    'acc_stderr': 0.02399350170904211,\n",
       "    'acc_norm': 0.7530864197530864,\n",
       "    'acc_norm_stderr': 0.02399350170904211},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5070921985815603,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.5070921985815603,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.46936114732724904,\n",
       "    'acc_stderr': 0.012746237711716634,\n",
       "    'acc_norm': 0.46936114732724904,\n",
       "    'acc_norm_stderr': 0.012746237711716634},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.028418208619406762,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.028418208619406762},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.673202614379085,\n",
       "    'acc_stderr': 0.018975427920507215,\n",
       "    'acc_norm': 0.673202614379085,\n",
       "    'acc_norm_stderr': 0.018975427920507215},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.028263889943784596,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.028263889943784596},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8159203980099502,\n",
       "    'acc_stderr': 0.027403859410786845,\n",
       "    'acc_norm': 0.8159203980099502,\n",
       "    'acc_norm_stderr': 0.027403859410786845},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.033799766898963086,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.033799766898963086},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5542168674698795,\n",
       "    'acc_stderr': 0.03869543323472101,\n",
       "    'acc_norm': 0.5542168674698795,\n",
       "    'acc_norm_stderr': 0.03869543323472101},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3598531211750306,\n",
       "    'mc1_stderr': 0.01680186046667716,\n",
       "    'mc2': 0.5230564118100686,\n",
       "    'mc2_stderr': 0.015250230546286025},\n",
       "   'harness|winogrande|5': {'acc': 0.7790055248618785,\n",
       "    'acc_stderr': 0.011661223637643412},\n",
       "   'harness|drop|3': {'em': 0.30830536912751677,\n",
       "    'em_stderr': 0.004729196914949925,\n",
       "    'f1': 0.35989723154362524,\n",
       "    'f1_stderr': 0.004629324720589026},\n",
       "   'harness|gsm8k|5': {'acc': 0.25473843821076575,\n",
       "    'acc_stderr': 0.012001731232879136},\n",
       "   'all': {'acc': 0.6340440103659418,\n",
       "    'acc_stderr': 0.03220763540877311,\n",
       "    'acc_norm': 0.6418750491228201,\n",
       "    'acc_norm_stderr': 0.032874386009418256,\n",
       "    'mc1': 0.3598531211750306,\n",
       "    'mc1_stderr': 0.01680186046667716,\n",
       "    'mc2': 0.5230564118100686,\n",
       "    'mc2_stderr': 0.015250230546286025,\n",
       "    'em': 0.30830536912751677,\n",
       "    'em_stderr': 0.004729196914949925,\n",
       "    'f1': 0.35989723154362524,\n",
       "    'f1_stderr': 0.004629324720589026}}},\n",
       " 'solar-10.7b-instruct-v1.0': {'key': 'solar-10.7b-instruct-v1.0',\n",
       "  'Model': 'SOLAR-10.7B-Instruct-v1.0',\n",
       "  'MT-bench (score)': '7.58',\n",
       "  'MMLU': '0.662',\n",
       "  'License': 'CC-BY-NC-4.0',\n",
       "  'Organization': 'Upstage AI',\n",
       "  'Link': 'https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6808873720136519,\n",
       "    'acc_stderr': 0.013621696119173307,\n",
       "    'acc_norm': 0.7107508532423208,\n",
       "    'acc_norm_stderr': 0.01325001257939344},\n",
       "   'harness|hellaswag|10': {'acc': 0.7070304720175263,\n",
       "    'acc_stderr': 0.004541944342035901,\n",
       "    'acc_norm': 0.8815972913762199,\n",
       "    'acc_norm_stderr': 0.003224240722351317},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.7368421052631579,\n",
       "    'acc_stderr': 0.03583496176361072,\n",
       "    'acc_norm': 0.7368421052631579,\n",
       "    'acc_norm_stderr': 0.03583496176361072},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.02872750295788027,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.02872750295788027},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7638888888888888,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.7638888888888888,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6647398843930635,\n",
       "    'acc_stderr': 0.03599586301247077,\n",
       "    'acc_norm': 0.6647398843930635,\n",
       "    'acc_norm_stderr': 0.03599586301247077},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107223,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107223},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.6297872340425532,\n",
       "    'acc_stderr': 0.03156564682236785,\n",
       "    'acc_norm': 0.6297872340425532,\n",
       "    'acc_norm_stderr': 0.03156564682236785},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.047036043419179864,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.047036043419179864},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.6413793103448275,\n",
       "    'acc_stderr': 0.039966295748767186,\n",
       "    'acc_norm': 0.6413793103448275,\n",
       "    'acc_norm_stderr': 0.039966295748767186},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.47883597883597884,\n",
       "    'acc_stderr': 0.025728230952130726,\n",
       "    'acc_norm': 0.47883597883597884,\n",
       "    'acc_norm_stderr': 0.025728230952130726},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.044444444444444495,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.044444444444444495},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.8032258064516129,\n",
       "    'acc_stderr': 0.022616409420742025,\n",
       "    'acc_norm': 0.8032258064516129,\n",
       "    'acc_norm_stderr': 0.022616409420742025},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.03515895551165698,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.03515895551165698},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.031234752377721175,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.031234752377721175},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8737373737373737,\n",
       "    'acc_stderr': 0.02366435940288023,\n",
       "    'acc_norm': 0.8737373737373737,\n",
       "    'acc_norm_stderr': 0.02366435940288023},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9067357512953368,\n",
       "    'acc_stderr': 0.02098685459328973,\n",
       "    'acc_norm': 0.9067357512953368,\n",
       "    'acc_norm_stderr': 0.02098685459328973},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6615384615384615,\n",
       "    'acc_stderr': 0.023991500500313036,\n",
       "    'acc_norm': 0.6615384615384615,\n",
       "    'acc_norm_stderr': 0.023991500500313036},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3814814814814815,\n",
       "    'acc_stderr': 0.029616718927497593,\n",
       "    'acc_norm': 0.3814814814814815,\n",
       "    'acc_norm_stderr': 0.029616718927497593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7184873949579832,\n",
       "    'acc_stderr': 0.02921354941437217,\n",
       "    'acc_norm': 0.7184873949579832,\n",
       "    'acc_norm_stderr': 0.02921354941437217},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3708609271523179,\n",
       "    'acc_stderr': 0.03943966699183629,\n",
       "    'acc_norm': 0.3708609271523179,\n",
       "    'acc_norm_stderr': 0.03943966699183629},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.03388857118502325,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.03388857118502325},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8480392156862745,\n",
       "    'acc_stderr': 0.0251956584289318,\n",
       "    'acc_norm': 0.8480392156862745,\n",
       "    'acc_norm_stderr': 0.0251956584289318},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8565400843881856,\n",
       "    'acc_stderr': 0.022818291821017012,\n",
       "    'acc_norm': 0.8565400843881856,\n",
       "    'acc_norm_stderr': 0.022818291821017012},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6816143497757847,\n",
       "    'acc_stderr': 0.03126580522513713,\n",
       "    'acc_norm': 0.6816143497757847,\n",
       "    'acc_norm_stderr': 0.03126580522513713},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7480916030534351,\n",
       "    'acc_stderr': 0.03807387116306086,\n",
       "    'acc_norm': 0.7480916030534351,\n",
       "    'acc_norm_stderr': 0.03807387116306086},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7768595041322314,\n",
       "    'acc_stderr': 0.03800754475228733,\n",
       "    'acc_norm': 0.7768595041322314,\n",
       "    'acc_norm_stderr': 0.03800754475228733},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8055555555555556,\n",
       "    'acc_stderr': 0.038260763248848646,\n",
       "    'acc_norm': 0.8055555555555556,\n",
       "    'acc_norm_stderr': 0.038260763248848646},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.754601226993865,\n",
       "    'acc_stderr': 0.03380939813943354,\n",
       "    'acc_norm': 0.754601226993865,\n",
       "    'acc_norm_stderr': 0.03380939813943354},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.44642857142857145,\n",
       "    'acc_stderr': 0.047184714852195886,\n",
       "    'acc_norm': 0.44642857142857145,\n",
       "    'acc_norm_stderr': 0.047184714852195886},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8252427184466019,\n",
       "    'acc_stderr': 0.03760178006026621,\n",
       "    'acc_norm': 0.8252427184466019,\n",
       "    'acc_norm_stderr': 0.03760178006026621},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.02280138253459753,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.02280138253459753},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8033205619412516,\n",
       "    'acc_stderr': 0.014214138556913917,\n",
       "    'acc_norm': 0.8033205619412516,\n",
       "    'acc_norm_stderr': 0.014214138556913917},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7601156069364162,\n",
       "    'acc_stderr': 0.022989592543123567,\n",
       "    'acc_norm': 0.7601156069364162,\n",
       "    'acc_norm_stderr': 0.022989592543123567},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.39329608938547483,\n",
       "    'acc_stderr': 0.016337268694270112,\n",
       "    'acc_norm': 0.39329608938547483,\n",
       "    'acc_norm_stderr': 0.016337268694270112},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7581699346405228,\n",
       "    'acc_stderr': 0.024518195641879334,\n",
       "    'acc_norm': 0.7581699346405228,\n",
       "    'acc_norm_stderr': 0.024518195641879334},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.729903536977492,\n",
       "    'acc_stderr': 0.02521804037341062,\n",
       "    'acc_norm': 0.729903536977492,\n",
       "    'acc_norm_stderr': 0.02521804037341062},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7901234567901234,\n",
       "    'acc_stderr': 0.02265834408598137,\n",
       "    'acc_norm': 0.7901234567901234,\n",
       "    'acc_norm_stderr': 0.02265834408598137},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.49645390070921985,\n",
       "    'acc_stderr': 0.02982674915328092,\n",
       "    'acc_norm': 0.49645390070921985,\n",
       "    'acc_norm_stderr': 0.02982674915328092},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4934810951760104,\n",
       "    'acc_stderr': 0.012769150688867503,\n",
       "    'acc_norm': 0.4934810951760104,\n",
       "    'acc_norm_stderr': 0.012769150688867503},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.7389705882352942,\n",
       "    'acc_stderr': 0.026679252270103135,\n",
       "    'acc_norm': 0.7389705882352942,\n",
       "    'acc_norm_stderr': 0.026679252270103135},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.018690850273595294,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.018690850273595294},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.0282638899437846,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.0282638899437846},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8407960199004975,\n",
       "    'acc_stderr': 0.02587064676616913,\n",
       "    'acc_norm': 0.8407960199004975,\n",
       "    'acc_norm_stderr': 0.02587064676616913},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.9,\n",
       "    'acc_stderr': 0.030151134457776334,\n",
       "    'acc_norm': 0.9,\n",
       "    'acc_norm_stderr': 0.030151134457776334},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5843373493975904,\n",
       "    'acc_stderr': 0.03836722176598052,\n",
       "    'acc_norm': 0.5843373493975904,\n",
       "    'acc_norm_stderr': 0.03836722176598052},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7894736842105263,\n",
       "    'acc_stderr': 0.03126781714663179,\n",
       "    'acc_norm': 0.7894736842105263,\n",
       "    'acc_norm_stderr': 0.03126781714663179},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.5667074663402693,\n",
       "    'mc1_stderr': 0.017347024450107485,\n",
       "    'mc2': 0.7142943510205136,\n",
       "    'mc2_stderr': 0.015024530295000761},\n",
       "   'harness|winogrande|5': {'acc': 0.8358326756116812,\n",
       "    'acc_stderr': 0.01041084977522279},\n",
       "   'harness|gsm8k|5': {'acc': 0.6474601971190296,\n",
       "    'acc_stderr': 0.013159909755930337},\n",
       "   'all': {'acc': 0.6657586984797939,\n",
       "    'acc_stderr': 0.03165995758526614,\n",
       "    'acc_norm': 0.6666511531376961,\n",
       "    'acc_norm_stderr': 0.0323050384069596,\n",
       "    'mc1': 0.5667074663402693,\n",
       "    'mc1_stderr': 0.017347024450107485,\n",
       "    'mc2': 0.7142943510205136,\n",
       "    'mc2_stderr': 0.015024530295000761}}},\n",
       " 'stablelm-tuned-alpha-7b': {'key': 'stablelm-tuned-alpha-7b',\n",
       "  'Model': 'StableLM-Tuned-Alpha-7B',\n",
       "  'MT-bench (score)': '2.75',\n",
       "  'MMLU': '0.244',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'Stability AI',\n",
       "  'Link': 'https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b',\n",
       "  'results': {'harness|drop|3': {'em': 0.0041946308724832215,\n",
       "    'em_stderr': 0.0006618716168266466,\n",
       "    'f1': 0.05621224832214779,\n",
       "    'f1_stderr': 0.0014117433231649174},\n",
       "   'harness|gsm8k|5': {'acc': 0.008339651250947688,\n",
       "    'acc_stderr': 0.002504942226860537},\n",
       "   'harness|winogrande|5': {'acc': 0.5311760063141279,\n",
       "    'acc_stderr': 0.014025142640639513},\n",
       "   'all': {'acc': 0.2479374638777667,\n",
       "    'acc_stderr': 0.03127884661282089,\n",
       "    'acc_norm': 0.2503417754412081,\n",
       "    'acc_norm_stderr': 0.031283748741553784,\n",
       "    'mc1': 0.23990208078335373,\n",
       "    'mc1_stderr': 0.014948812679062137,\n",
       "    'mc2': 0.4037161827493261,\n",
       "    'mc2_stderr': 0.014483731026265276},\n",
       "   'harness|arc:challenge|25': {'acc': 0.3003412969283277,\n",
       "    'acc_stderr': 0.013395909309956997,\n",
       "    'acc_norm': 0.3191126279863481,\n",
       "    'acc_norm_stderr': 0.0136216961191733},\n",
       "   'harness|hellaswag|10': {'acc': 0.41286596295558653,\n",
       "    'acc_stderr': 0.004913429010559072,\n",
       "    'acc_norm': 0.5359490141406095,\n",
       "    'acc_norm_stderr': 0.004976867796583554},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.26666666666666666,\n",
       "    'acc_stderr': 0.03820169914517904,\n",
       "    'acc_norm': 0.26666666666666666,\n",
       "    'acc_norm_stderr': 0.03820169914517904},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.17105263157894737,\n",
       "    'acc_stderr': 0.030643607071677088,\n",
       "    'acc_norm': 0.17105263157894737,\n",
       "    'acc_norm_stderr': 0.030643607071677088},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.24,\n",
       "    'acc_stderr': 0.04292346959909281,\n",
       "    'acc_norm': 0.24,\n",
       "    'acc_norm_stderr': 0.04292346959909281},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.27547169811320754,\n",
       "    'acc_stderr': 0.027495663683724064,\n",
       "    'acc_norm': 0.27547169811320754,\n",
       "    'acc_norm_stderr': 0.027495663683724064},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2361111111111111,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.2361111111111111,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.041633319989322695,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.041633319989322695},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.23121387283236994,\n",
       "    'acc_stderr': 0.0321473730202947,\n",
       "    'acc_norm': 0.23121387283236994,\n",
       "    'acc_norm_stderr': 0.0321473730202947},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2549019607843137,\n",
       "    'acc_stderr': 0.04336432707993176,\n",
       "    'acc_norm': 0.2549019607843137,\n",
       "    'acc_norm_stderr': 0.04336432707993176},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.3148936170212766,\n",
       "    'acc_stderr': 0.03036358219723816,\n",
       "    'acc_norm': 0.3148936170212766,\n",
       "    'acc_norm_stderr': 0.03036358219723816},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.21052631578947367,\n",
       "    'acc_stderr': 0.038351539543994194,\n",
       "    'acc_norm': 0.21052631578947367,\n",
       "    'acc_norm_stderr': 0.038351539543994194},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.27586206896551724,\n",
       "    'acc_stderr': 0.037245636197746325,\n",
       "    'acc_norm': 0.27586206896551724,\n",
       "    'acc_norm_stderr': 0.037245636197746325},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.24338624338624337,\n",
       "    'acc_stderr': 0.022101128787415426,\n",
       "    'acc_norm': 0.24338624338624337,\n",
       "    'acc_norm_stderr': 0.022101128787415426},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.24603174603174602,\n",
       "    'acc_stderr': 0.03852273364924316,\n",
       "    'acc_norm': 0.24603174603174602,\n",
       "    'acc_norm_stderr': 0.03852273364924316},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.19,\n",
       "    'acc_stderr': 0.03942772444036624,\n",
       "    'acc_norm': 0.19,\n",
       "    'acc_norm_stderr': 0.03942772444036624},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.2129032258064516,\n",
       "    'acc_stderr': 0.023287665127268525,\n",
       "    'acc_norm': 0.2129032258064516,\n",
       "    'acc_norm_stderr': 0.023287665127268525},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.18226600985221675,\n",
       "    'acc_stderr': 0.02716334085964515,\n",
       "    'acc_norm': 0.18226600985221675,\n",
       "    'acc_norm_stderr': 0.02716334085964515},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.04163331998932269,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.04163331998932269},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.22424242424242424,\n",
       "    'acc_stderr': 0.03256866661681102,\n",
       "    'acc_norm': 0.22424242424242424,\n",
       "    'acc_norm_stderr': 0.03256866661681102},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.18686868686868688,\n",
       "    'acc_stderr': 0.027772533334218977,\n",
       "    'acc_norm': 0.18686868686868688,\n",
       "    'acc_norm_stderr': 0.027772533334218977},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.21243523316062177,\n",
       "    'acc_stderr': 0.029519282616817254,\n",
       "    'acc_norm': 0.21243523316062177,\n",
       "    'acc_norm_stderr': 0.029519282616817254},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.2743589743589744,\n",
       "    'acc_stderr': 0.022622765767493218,\n",
       "    'acc_norm': 0.2743589743589744,\n",
       "    'acc_norm_stderr': 0.022622765767493218},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.25925925925925924,\n",
       "    'acc_stderr': 0.026719240783712163,\n",
       "    'acc_norm': 0.25925925925925924,\n",
       "    'acc_norm_stderr': 0.026719240783712163},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.19747899159663865,\n",
       "    'acc_stderr': 0.02585916412205145,\n",
       "    'acc_norm': 0.19747899159663865,\n",
       "    'acc_norm_stderr': 0.02585916412205145},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2185430463576159,\n",
       "    'acc_stderr': 0.03374235550425694,\n",
       "    'acc_norm': 0.2185430463576159,\n",
       "    'acc_norm_stderr': 0.03374235550425694},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.22935779816513763,\n",
       "    'acc_stderr': 0.018025349724618684,\n",
       "    'acc_norm': 0.22935779816513763,\n",
       "    'acc_norm_stderr': 0.018025349724618684},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.35185185185185186,\n",
       "    'acc_stderr': 0.03256850570293647,\n",
       "    'acc_norm': 0.35185185185185186,\n",
       "    'acc_norm_stderr': 0.03256850570293647},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.24019607843137256,\n",
       "    'acc_stderr': 0.02998373305591361,\n",
       "    'acc_norm': 0.24019607843137256,\n",
       "    'acc_norm_stderr': 0.02998373305591361},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.2869198312236287,\n",
       "    'acc_stderr': 0.02944377302259469,\n",
       "    'acc_norm': 0.2869198312236287,\n",
       "    'acc_norm_stderr': 0.02944377302259469},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.3542600896860987,\n",
       "    'acc_stderr': 0.03210062154134987,\n",
       "    'acc_norm': 0.3542600896860987,\n",
       "    'acc_norm_stderr': 0.03210062154134987},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.2900763358778626,\n",
       "    'acc_stderr': 0.03980066246467765,\n",
       "    'acc_norm': 0.2900763358778626,\n",
       "    'acc_norm_stderr': 0.03980066246467765},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.2396694214876033,\n",
       "    'acc_stderr': 0.03896878985070417,\n",
       "    'acc_norm': 0.2396694214876033,\n",
       "    'acc_norm_stderr': 0.03896878985070417},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.28703703703703703,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.28703703703703703,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.27607361963190186,\n",
       "    'acc_stderr': 0.0351238528370505,\n",
       "    'acc_norm': 0.27607361963190186,\n",
       "    'acc_norm_stderr': 0.0351238528370505},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.24107142857142858,\n",
       "    'acc_stderr': 0.04059867246952687,\n",
       "    'acc_norm': 0.24107142857142858,\n",
       "    'acc_norm_stderr': 0.04059867246952687},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.1650485436893204,\n",
       "    'acc_stderr': 0.036756688322331886,\n",
       "    'acc_norm': 0.1650485436893204,\n",
       "    'acc_norm_stderr': 0.036756688322331886},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.24786324786324787,\n",
       "    'acc_stderr': 0.028286324075564397,\n",
       "    'acc_norm': 0.24786324786324787,\n",
       "    'acc_norm_stderr': 0.028286324075564397},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.23499361430395913,\n",
       "    'acc_stderr': 0.015162024152278434,\n",
       "    'acc_norm': 0.23499361430395913,\n",
       "    'acc_norm_stderr': 0.015162024152278434},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.2254335260115607,\n",
       "    'acc_stderr': 0.02249723019096756,\n",
       "    'acc_norm': 0.2254335260115607,\n",
       "    'acc_norm_stderr': 0.02249723019096756},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961459,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961459},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.25163398692810457,\n",
       "    'acc_stderr': 0.024848018263875195,\n",
       "    'acc_norm': 0.25163398692810457,\n",
       "    'acc_norm_stderr': 0.024848018263875195},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.21864951768488747,\n",
       "    'acc_stderr': 0.023475581417861106,\n",
       "    'acc_norm': 0.21864951768488747,\n",
       "    'acc_norm_stderr': 0.023475581417861106},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.2222222222222222,\n",
       "    'acc_stderr': 0.023132376234543325,\n",
       "    'acc_norm': 0.2222222222222222,\n",
       "    'acc_norm_stderr': 0.023132376234543325},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.24822695035460993,\n",
       "    'acc_stderr': 0.025770015644290403,\n",
       "    'acc_norm': 0.24822695035460993,\n",
       "    'acc_norm_stderr': 0.025770015644290403},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.22946544980443284,\n",
       "    'acc_stderr': 0.010739489382279503,\n",
       "    'acc_norm': 0.22946544980443284,\n",
       "    'acc_norm_stderr': 0.010739489382279503},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.16544117647058823,\n",
       "    'acc_stderr': 0.022571771025494767,\n",
       "    'acc_norm': 0.16544117647058823,\n",
       "    'acc_norm_stderr': 0.022571771025494767},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.25163398692810457,\n",
       "    'acc_stderr': 0.017555818091322263,\n",
       "    'acc_norm': 0.25163398692810457,\n",
       "    'acc_norm_stderr': 0.017555818091322263},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.23636363636363636,\n",
       "    'acc_stderr': 0.04069306319721378,\n",
       "    'acc_norm': 0.23636363636363636,\n",
       "    'acc_norm_stderr': 0.04069306319721378},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.1673469387755102,\n",
       "    'acc_stderr': 0.023897144768914524,\n",
       "    'acc_norm': 0.1673469387755102,\n",
       "    'acc_norm_stderr': 0.023897144768914524},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.263681592039801,\n",
       "    'acc_stderr': 0.03115715086935558,\n",
       "    'acc_norm': 0.263681592039801,\n",
       "    'acc_norm_stderr': 0.03115715086935558},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.24,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.24,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.2891566265060241,\n",
       "    'acc_stderr': 0.03529486801511115,\n",
       "    'acc_norm': 0.2891566265060241,\n",
       "    'acc_norm_stderr': 0.03529486801511115},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.24561403508771928,\n",
       "    'acc_stderr': 0.03301405946987249,\n",
       "    'acc_norm': 0.24561403508771928,\n",
       "    'acc_norm_stderr': 0.03301405946987249},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.23990208078335373,\n",
       "    'mc1_stderr': 0.014948812679062137,\n",
       "    'mc2': 0.4037161827493261,\n",
       "    'mc2_stderr': 0.014483731026265276}}},\n",
       " 'starling-lm-7b-alpha': {'key': 'starling-lm-7b-alpha',\n",
       "  'Model': 'Starling-LM-7B-alpha',\n",
       "  'MT-bench (score)': '8.09',\n",
       "  'MMLU': '0.639',\n",
       "  'License': 'CC-BY-NC-4.0',\n",
       "  'Organization': 'UC Berkeley',\n",
       "  'Link': 'https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5998293515358362,\n",
       "    'acc_stderr': 0.014317197787809172,\n",
       "    'acc_norm': 0.6382252559726962,\n",
       "    'acc_norm_stderr': 0.01404195794503808},\n",
       "   'harness|hellaswag|10': {'acc': 0.665803624775941,\n",
       "    'acc_stderr': 0.004707447244200621,\n",
       "    'acc_norm': 0.8490340569607648,\n",
       "    'acc_norm_stderr': 0.0035728399695219874},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6907894736842105,\n",
       "    'acc_stderr': 0.037610708698674805,\n",
       "    'acc_norm': 0.6907894736842105,\n",
       "    'acc_norm_stderr': 0.037610708698674805},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.63,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.63,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.028544793319055326,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.028544793319055326},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7708333333333334,\n",
       "    'acc_stderr': 0.035146974678623884,\n",
       "    'acc_norm': 0.7708333333333334,\n",
       "    'acc_norm_stderr': 0.035146974678623884},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6589595375722543,\n",
       "    'acc_stderr': 0.03614665424180826,\n",
       "    'acc_norm': 0.6589595375722543,\n",
       "    'acc_norm_stderr': 0.03614665424180826},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4411764705882353,\n",
       "    'acc_stderr': 0.049406356306056595,\n",
       "    'acc_norm': 0.4411764705882353,\n",
       "    'acc_norm_stderr': 0.049406356306056595},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768079,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768079},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5531914893617021,\n",
       "    'acc_stderr': 0.032500536843658404,\n",
       "    'acc_norm': 0.5531914893617021,\n",
       "    'acc_norm_stderr': 0.032500536843658404},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.49122807017543857,\n",
       "    'acc_stderr': 0.04702880432049615,\n",
       "    'acc_norm': 0.49122807017543857,\n",
       "    'acc_norm_stderr': 0.04702880432049615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370332,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370332},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.41005291005291006,\n",
       "    'acc_stderr': 0.02533120243894444,\n",
       "    'acc_norm': 0.41005291005291006,\n",
       "    'acc_norm_stderr': 0.02533120243894444},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5238095238095238,\n",
       "    'acc_stderr': 0.04467062628403273,\n",
       "    'acc_norm': 0.5238095238095238,\n",
       "    'acc_norm_stderr': 0.04467062628403273},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.04163331998932269,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.04163331998932269},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7903225806451613,\n",
       "    'acc_stderr': 0.023157879349083525,\n",
       "    'acc_norm': 0.7903225806451613,\n",
       "    'acc_norm_stderr': 0.023157879349083525},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.46798029556650245,\n",
       "    'acc_stderr': 0.035107665979592154,\n",
       "    'acc_norm': 0.46798029556650245,\n",
       "    'acc_norm_stderr': 0.035107665979592154},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7636363636363637,\n",
       "    'acc_stderr': 0.03317505930009181,\n",
       "    'acc_norm': 0.7636363636363637,\n",
       "    'acc_norm_stderr': 0.03317505930009181},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.797979797979798,\n",
       "    'acc_stderr': 0.028606204289229865,\n",
       "    'acc_norm': 0.797979797979798,\n",
       "    'acc_norm_stderr': 0.028606204289229865},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9119170984455959,\n",
       "    'acc_stderr': 0.02045374660160103,\n",
       "    'acc_norm': 0.9119170984455959,\n",
       "    'acc_norm_stderr': 0.02045374660160103},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.676923076923077,\n",
       "    'acc_stderr': 0.02371088850197057,\n",
       "    'acc_norm': 0.676923076923077,\n",
       "    'acc_norm_stderr': 0.02371088850197057},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028593,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.680672268907563,\n",
       "    'acc_stderr': 0.0302839955258844,\n",
       "    'acc_norm': 0.680672268907563,\n",
       "    'acc_norm_stderr': 0.0302839955258844},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.37748344370860926,\n",
       "    'acc_stderr': 0.03958027231121569,\n",
       "    'acc_norm': 0.37748344370860926,\n",
       "    'acc_norm_stderr': 0.03958027231121569},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.034099716973523674,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.034099716973523674},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8235294117647058,\n",
       "    'acc_stderr': 0.026756401538078966,\n",
       "    'acc_norm': 0.8235294117647058,\n",
       "    'acc_norm_stderr': 0.026756401538078966},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8270042194092827,\n",
       "    'acc_stderr': 0.024621562866768434,\n",
       "    'acc_norm': 0.8270042194092827,\n",
       "    'acc_norm_stderr': 0.024621562866768434},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7130044843049327,\n",
       "    'acc_stderr': 0.030360379710291947,\n",
       "    'acc_norm': 0.7130044843049327,\n",
       "    'acc_norm_stderr': 0.030360379710291947},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7862595419847328,\n",
       "    'acc_stderr': 0.0359546161177469,\n",
       "    'acc_norm': 0.7862595419847328,\n",
       "    'acc_norm_stderr': 0.0359546161177469},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8264462809917356,\n",
       "    'acc_stderr': 0.0345727283691767,\n",
       "    'acc_norm': 0.8264462809917356,\n",
       "    'acc_norm_stderr': 0.0345727283691767},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7407407407407407,\n",
       "    'acc_stderr': 0.042365112580946336,\n",
       "    'acc_norm': 0.7407407407407407,\n",
       "    'acc_norm_stderr': 0.042365112580946336},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7730061349693251,\n",
       "    'acc_stderr': 0.03291099578615769,\n",
       "    'acc_norm': 0.7730061349693251,\n",
       "    'acc_norm_stderr': 0.03291099578615769},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.41964285714285715,\n",
       "    'acc_stderr': 0.04684099321077106,\n",
       "    'acc_norm': 0.41964285714285715,\n",
       "    'acc_norm_stderr': 0.04684099321077106},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8446601941747572,\n",
       "    'acc_stderr': 0.03586594738573973,\n",
       "    'acc_norm': 0.8446601941747572,\n",
       "    'acc_norm_stderr': 0.03586594738573973},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8760683760683761,\n",
       "    'acc_stderr': 0.02158649400128138,\n",
       "    'acc_norm': 0.8760683760683761,\n",
       "    'acc_norm_stderr': 0.02158649400128138},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.0446196043338474,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.0446196043338474},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8263090676883781,\n",
       "    'acc_stderr': 0.01354741565866226,\n",
       "    'acc_norm': 0.8263090676883781,\n",
       "    'acc_norm_stderr': 0.01354741565866226},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7369942196531792,\n",
       "    'acc_stderr': 0.023703099525258176,\n",
       "    'acc_norm': 0.7369942196531792,\n",
       "    'acc_norm_stderr': 0.023703099525258176},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.47039106145251397,\n",
       "    'acc_stderr': 0.016693154927383557,\n",
       "    'acc_norm': 0.47039106145251397,\n",
       "    'acc_norm_stderr': 0.016693154927383557},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7352941176470589,\n",
       "    'acc_stderr': 0.025261691219729484,\n",
       "    'acc_norm': 0.7352941176470589,\n",
       "    'acc_norm_stderr': 0.025261691219729484},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6977491961414791,\n",
       "    'acc_stderr': 0.02608270069539966,\n",
       "    'acc_norm': 0.6977491961414791,\n",
       "    'acc_norm_stderr': 0.02608270069539966},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7345679012345679,\n",
       "    'acc_stderr': 0.024569223600460845,\n",
       "    'acc_norm': 0.7345679012345679,\n",
       "    'acc_norm_stderr': 0.024569223600460845},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4645390070921986,\n",
       "    'acc_stderr': 0.029752389657427047,\n",
       "    'acc_norm': 0.4645390070921986,\n",
       "    'acc_norm_stderr': 0.029752389657427047},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4641460234680574,\n",
       "    'acc_stderr': 0.012737361318730581,\n",
       "    'acc_norm': 0.4641460234680574,\n",
       "    'acc_norm_stderr': 0.012737361318730581},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.028064998167040094,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.028064998167040094},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6633986928104575,\n",
       "    'acc_stderr': 0.019117213911495144,\n",
       "    'acc_norm': 0.6633986928104575,\n",
       "    'acc_norm_stderr': 0.019117213911495144},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.726530612244898,\n",
       "    'acc_stderr': 0.028535560337128448,\n",
       "    'acc_norm': 0.726530612244898,\n",
       "    'acc_norm_stderr': 0.028535560337128448},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.845771144278607,\n",
       "    'acc_stderr': 0.025538433368578334,\n",
       "    'acc_norm': 0.845771144278607,\n",
       "    'acc_norm_stderr': 0.025538433368578334},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.88,\n",
       "    'acc_stderr': 0.032659863237109066,\n",
       "    'acc_norm': 0.88,\n",
       "    'acc_norm_stderr': 0.032659863237109066},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5301204819277109,\n",
       "    'acc_stderr': 0.03885425420866767,\n",
       "    'acc_norm': 0.5301204819277109,\n",
       "    'acc_norm_stderr': 0.03885425420866767},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.847953216374269,\n",
       "    'acc_stderr': 0.027539122889061456,\n",
       "    'acc_norm': 0.847953216374269,\n",
       "    'acc_norm_stderr': 0.027539122889061456},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3047735618115055,\n",
       "    'mc1_stderr': 0.01611412415688245,\n",
       "    'mc2': 0.463936332301049,\n",
       "    'mc2_stderr': 0.015153266555511496},\n",
       "   'harness|winogrande|5': {'acc': 0.8058405682715075,\n",
       "    'acc_stderr': 0.01111698339239267},\n",
       "   'harness|drop|3': {'em': 0.0012583892617449664,\n",
       "    'em_stderr': 0.0003630560893118993,\n",
       "    'f1': 0.07203124999999957,\n",
       "    'f1_stderr': 0.0014672437300568956},\n",
       "   'harness|gsm8k|5': {'acc': 0.623199393479909,\n",
       "    'acc_stderr': 0.013347858757829158},\n",
       "   'all': {'acc': 0.623199393479909, 'acc_stderr': 0.013347858757829158}}},\n",
       " 'vicuna-13b': {'key': 'vicuna-13b',\n",
       "  'Model': 'Vicuna-13B',\n",
       "  'MT-bench (score)': '6.57',\n",
       "  'MMLU': '0.558',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-13b-v1.5',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5349829351535836,\n",
       "    'acc_stderr': 0.014575583922019672,\n",
       "    'acc_norm': 0.5656996587030717,\n",
       "    'acc_norm_stderr': 0.014484703048857357},\n",
       "   'harness|hellaswag|10': {'acc': 0.6115315674168492,\n",
       "    'acc_stderr': 0.004864058877626272,\n",
       "    'acc_norm': 0.8108942441744672,\n",
       "    'acc_norm_stderr': 0.0039079230108406094},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542129,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542129},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4888888888888889,\n",
       "    'acc_stderr': 0.04318275491977976,\n",
       "    'acc_norm': 0.4888888888888889,\n",
       "    'acc_norm_stderr': 0.04318275491977976},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5855263157894737,\n",
       "    'acc_stderr': 0.04008973785779205,\n",
       "    'acc_norm': 0.5855263157894737,\n",
       "    'acc_norm_stderr': 0.04008973785779205},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.630188679245283,\n",
       "    'acc_stderr': 0.02971142188010793,\n",
       "    'acc_norm': 0.630188679245283,\n",
       "    'acc_norm_stderr': 0.02971142188010793},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5972222222222222,\n",
       "    'acc_stderr': 0.04101405519842426,\n",
       "    'acc_norm': 0.5972222222222222,\n",
       "    'acc_norm_stderr': 0.04101405519842426},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5433526011560693,\n",
       "    'acc_stderr': 0.03798106566014498,\n",
       "    'acc_norm': 0.5433526011560693,\n",
       "    'acc_norm_stderr': 0.03798106566014498},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3235294117647059,\n",
       "    'acc_stderr': 0.046550104113196156,\n",
       "    'acc_norm': 0.3235294117647059,\n",
       "    'acc_norm_stderr': 0.046550104113196156},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.451063829787234,\n",
       "    'acc_stderr': 0.032529096196131965,\n",
       "    'acc_norm': 0.451063829787234,\n",
       "    'acc_norm_stderr': 0.032529096196131965},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2894736842105263,\n",
       "    'acc_stderr': 0.04266339443159394,\n",
       "    'acc_norm': 0.2894736842105263,\n",
       "    'acc_norm_stderr': 0.04266339443159394},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.47586206896551725,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.47586206896551725,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3306878306878307,\n",
       "    'acc_stderr': 0.024229965298425075,\n",
       "    'acc_norm': 0.3306878306878307,\n",
       "    'acc_norm_stderr': 0.024229965298425075},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.04240799327574924,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.04240799327574924},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001974,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001974},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6774193548387096,\n",
       "    'acc_stderr': 0.02659308451657227,\n",
       "    'acc_norm': 0.6774193548387096,\n",
       "    'acc_norm_stderr': 0.02659308451657227},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4482758620689655,\n",
       "    'acc_stderr': 0.034991131376767445,\n",
       "    'acc_norm': 0.4482758620689655,\n",
       "    'acc_norm_stderr': 0.034991131376767445},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.696969696969697,\n",
       "    'acc_stderr': 0.03588624800091707,\n",
       "    'acc_norm': 0.696969696969697,\n",
       "    'acc_norm_stderr': 0.03588624800091707},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7070707070707071,\n",
       "    'acc_stderr': 0.03242497958178815,\n",
       "    'acc_norm': 0.7070707070707071,\n",
       "    'acc_norm_stderr': 0.03242497958178815},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8134715025906736,\n",
       "    'acc_stderr': 0.028112091210117467,\n",
       "    'acc_norm': 0.8134715025906736,\n",
       "    'acc_norm_stderr': 0.028112091210117467},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5487179487179488,\n",
       "    'acc_stderr': 0.025230381238934833,\n",
       "    'acc_norm': 0.5487179487179488,\n",
       "    'acc_norm_stderr': 0.025230381238934833},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.0284934650910286,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.0284934650910286},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5798319327731093,\n",
       "    'acc_stderr': 0.03206183783236153,\n",
       "    'acc_norm': 0.5798319327731093,\n",
       "    'acc_norm_stderr': 0.03206183783236153},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.33112582781456956,\n",
       "    'acc_stderr': 0.038425817186598696,\n",
       "    'acc_norm': 0.33112582781456956,\n",
       "    'acc_norm_stderr': 0.038425817186598696},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7541284403669725,\n",
       "    'acc_stderr': 0.01846194096870842,\n",
       "    'acc_norm': 0.7541284403669725,\n",
       "    'acc_norm_stderr': 0.01846194096870842},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4583333333333333,\n",
       "    'acc_stderr': 0.03398110890294636,\n",
       "    'acc_norm': 0.4583333333333333,\n",
       "    'acc_norm_stderr': 0.03398110890294636},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7450980392156863,\n",
       "    'acc_stderr': 0.03058759135160426,\n",
       "    'acc_norm': 0.7450980392156863,\n",
       "    'acc_norm_stderr': 0.03058759135160426},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7637130801687764,\n",
       "    'acc_stderr': 0.027652153144159256,\n",
       "    'acc_norm': 0.7637130801687764,\n",
       "    'acc_norm_stderr': 0.027652153144159256},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6502242152466368,\n",
       "    'acc_stderr': 0.03200736719484503,\n",
       "    'acc_norm': 0.6502242152466368,\n",
       "    'acc_norm_stderr': 0.03200736719484503},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6412213740458015,\n",
       "    'acc_stderr': 0.04206739313864908,\n",
       "    'acc_norm': 0.6412213740458015,\n",
       "    'acc_norm_stderr': 0.04206739313864908},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7768595041322314,\n",
       "    'acc_stderr': 0.03800754475228732,\n",
       "    'acc_norm': 0.7768595041322314,\n",
       "    'acc_norm_stderr': 0.03800754475228732},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.043300437496507416,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.043300437496507416},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6932515337423313,\n",
       "    'acc_stderr': 0.03623089915724147,\n",
       "    'acc_norm': 0.6932515337423313,\n",
       "    'acc_norm_stderr': 0.03623089915724147},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.42857142857142855,\n",
       "    'acc_stderr': 0.04697113923010212,\n",
       "    'acc_norm': 0.42857142857142855,\n",
       "    'acc_norm_stderr': 0.04697113923010212},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.043546310772605956,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.043546310772605956},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8461538461538461,\n",
       "    'acc_stderr': 0.023636873317489288,\n",
       "    'acc_norm': 0.8461538461538461,\n",
       "    'acc_norm_stderr': 0.023636873317489288},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.58,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.58,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.756066411238825,\n",
       "    'acc_stderr': 0.015357212665829461,\n",
       "    'acc_norm': 0.756066411238825,\n",
       "    'acc_norm_stderr': 0.015357212665829461},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6069364161849711,\n",
       "    'acc_stderr': 0.026296227915613663,\n",
       "    'acc_norm': 0.6069364161849711,\n",
       "    'acc_norm_stderr': 0.026296227915613663},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2782122905027933,\n",
       "    'acc_stderr': 0.01498732543996355,\n",
       "    'acc_norm': 0.2782122905027933,\n",
       "    'acc_norm_stderr': 0.01498732543996355},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6372549019607843,\n",
       "    'acc_stderr': 0.027530078447110307,\n",
       "    'acc_norm': 0.6372549019607843,\n",
       "    'acc_norm_stderr': 0.027530078447110307},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6366559485530546,\n",
       "    'acc_stderr': 0.027316847674192717,\n",
       "    'acc_norm': 0.6366559485530546,\n",
       "    'acc_norm_stderr': 0.027316847674192717},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6388888888888888,\n",
       "    'acc_stderr': 0.026725868809100793,\n",
       "    'acc_norm': 0.6388888888888888,\n",
       "    'acc_norm_stderr': 0.026725868809100793},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.41134751773049644,\n",
       "    'acc_stderr': 0.02935491115994098,\n",
       "    'acc_norm': 0.41134751773049644,\n",
       "    'acc_norm_stderr': 0.02935491115994098},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.43285528031290743,\n",
       "    'acc_stderr': 0.012654565234622862,\n",
       "    'acc_norm': 0.43285528031290743,\n",
       "    'acc_norm_stderr': 0.012654565234622862},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5367647058823529,\n",
       "    'acc_stderr': 0.03029061918048569,\n",
       "    'acc_norm': 0.5367647058823529,\n",
       "    'acc_norm_stderr': 0.03029061918048569},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.019944914136873583,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.019944914136873583},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6122448979591837,\n",
       "    'acc_stderr': 0.031192230726795656,\n",
       "    'acc_norm': 0.6122448979591837,\n",
       "    'acc_norm_stderr': 0.031192230726795656},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7860696517412935,\n",
       "    'acc_stderr': 0.02899690969332891,\n",
       "    'acc_norm': 0.7860696517412935,\n",
       "    'acc_norm_stderr': 0.02899690969332891},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.82,\n",
       "    'acc_stderr': 0.038612291966536934,\n",
       "    'acc_norm': 0.82,\n",
       "    'acc_norm_stderr': 0.038612291966536934},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.45180722891566266,\n",
       "    'acc_stderr': 0.03874371556587953,\n",
       "    'acc_norm': 0.45180722891566266,\n",
       "    'acc_norm_stderr': 0.03874371556587953},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.783625730994152,\n",
       "    'acc_stderr': 0.03158149539338734,\n",
       "    'acc_norm': 0.783625730994152,\n",
       "    'acc_norm_stderr': 0.03158149539338734},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.34516523867809057,\n",
       "    'mc1_stderr': 0.01664310331927494,\n",
       "    'mc2': 0.5107449529759277,\n",
       "    'mc2_stderr': 0.015464470932268133},\n",
       "   'all': {'em': 0.21403104026845637,\n",
       "    'em_stderr': 0.004200304057589016,\n",
       "    'f1': 0.2773447986577177,\n",
       "    'f1_stderr': 0.004194161726605588,\n",
       "    'acc': 0.4298049932592257,\n",
       "    'acc_stderr': 0.010471546731533343},\n",
       "   'harness|drop|3': {'em': 0.21403104026845637,\n",
       "    'em_stderr': 0.004200304057589016,\n",
       "    'f1': 0.2773447986577177,\n",
       "    'f1_stderr': 0.004194161726605588},\n",
       "   'harness|gsm8k|5': {'acc': 0.11296436694465505,\n",
       "    'acc_stderr': 0.008719339028833057},\n",
       "   'harness|winogrande|5': {'acc': 0.7466456195737964,\n",
       "    'acc_stderr': 0.01222375443423363}}},\n",
       " 'vicuna-33b': {'key': 'vicuna-33b',\n",
       "  'Model': 'Vicuna-33B',\n",
       "  'MT-bench (score)': '7.12',\n",
       "  'MMLU': '0.592',\n",
       "  'License': 'Non-commercial',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-33b-v1.3',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5921501706484642,\n",
       "    'acc_stderr': 0.014361097288449707,\n",
       "    'acc_norm': 0.6194539249146758,\n",
       "    'acc_norm_stderr': 0.014188277712349807},\n",
       "   'harness|hellaswag|10': {'acc': 0.6249751045608445,\n",
       "    'acc_stderr': 0.00483139921850023,\n",
       "    'acc_norm': 0.82194781915953,\n",
       "    'acc_norm_stderr': 0.003817748269107777},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.27,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.27,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5333333333333333,\n",
       "    'acc_stderr': 0.043097329010363554,\n",
       "    'acc_norm': 0.5333333333333333,\n",
       "    'acc_norm_stderr': 0.043097329010363554},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.625,\n",
       "    'acc_stderr': 0.039397364351956274,\n",
       "    'acc_norm': 0.625,\n",
       "    'acc_norm_stderr': 0.039397364351956274},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6037735849056604,\n",
       "    'acc_stderr': 0.030102793781791197,\n",
       "    'acc_norm': 0.6037735849056604,\n",
       "    'acc_norm_stderr': 0.030102793781791197},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.6319444444444444,\n",
       "    'acc_stderr': 0.040329990539607195,\n",
       "    'acc_norm': 0.6319444444444444,\n",
       "    'acc_norm_stderr': 0.040329990539607195},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.49,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.49,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.04725815626252604,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.04725815626252604},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.27450980392156865,\n",
       "    'acc_stderr': 0.04440521906179326,\n",
       "    'acc_norm': 0.27450980392156865,\n",
       "    'acc_norm_stderr': 0.04440521906179326},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.46808510638297873,\n",
       "    'acc_stderr': 0.03261936918467382,\n",
       "    'acc_norm': 0.46808510638297873,\n",
       "    'acc_norm_stderr': 0.03261936918467382},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.34210526315789475,\n",
       "    'acc_stderr': 0.04462917535336936,\n",
       "    'acc_norm': 0.34210526315789475,\n",
       "    'acc_norm_stderr': 0.04462917535336936},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.04164188720169375,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.04164188720169375},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36507936507936506,\n",
       "    'acc_stderr': 0.024796060602699958,\n",
       "    'acc_norm': 0.36507936507936506,\n",
       "    'acc_norm_stderr': 0.024796060602699958},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.04240799327574924,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.04240799327574924},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6870967741935484,\n",
       "    'acc_stderr': 0.02637756702864586,\n",
       "    'acc_norm': 0.6870967741935484,\n",
       "    'acc_norm_stderr': 0.02637756702864586},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.39901477832512317,\n",
       "    'acc_stderr': 0.03445487686264716,\n",
       "    'acc_norm': 0.39901477832512317,\n",
       "    'acc_norm_stderr': 0.03445487686264716},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7151515151515152,\n",
       "    'acc_stderr': 0.03524390844511781,\n",
       "    'acc_norm': 0.7151515151515152,\n",
       "    'acc_norm_stderr': 0.03524390844511781},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7727272727272727,\n",
       "    'acc_stderr': 0.02985751567338642,\n",
       "    'acc_norm': 0.7727272727272727,\n",
       "    'acc_norm_stderr': 0.02985751567338642},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8393782383419689,\n",
       "    'acc_stderr': 0.02649905770139744,\n",
       "    'acc_norm': 0.8393782383419689,\n",
       "    'acc_norm_stderr': 0.02649905770139744},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5871794871794872,\n",
       "    'acc_stderr': 0.024962683564331806,\n",
       "    'acc_norm': 0.5871794871794872,\n",
       "    'acc_norm_stderr': 0.024962683564331806},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2740740740740741,\n",
       "    'acc_stderr': 0.027195934804085626,\n",
       "    'acc_norm': 0.2740740740740741,\n",
       "    'acc_norm_stderr': 0.027195934804085626},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5840336134453782,\n",
       "    'acc_stderr': 0.03201650100739611,\n",
       "    'acc_norm': 0.5840336134453782,\n",
       "    'acc_norm_stderr': 0.03201650100739611},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.40397350993377484,\n",
       "    'acc_stderr': 0.04006485685365342,\n",
       "    'acc_norm': 0.40397350993377484,\n",
       "    'acc_norm_stderr': 0.04006485685365342},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7761467889908257,\n",
       "    'acc_stderr': 0.017871217767790232,\n",
       "    'acc_norm': 0.7761467889908257,\n",
       "    'acc_norm_stderr': 0.017871217767790232},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4027777777777778,\n",
       "    'acc_stderr': 0.033448873829978666,\n",
       "    'acc_norm': 0.4027777777777778,\n",
       "    'acc_norm_stderr': 0.033448873829978666},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7745098039215687,\n",
       "    'acc_stderr': 0.029331162294251735,\n",
       "    'acc_norm': 0.7745098039215687,\n",
       "    'acc_norm_stderr': 0.029331162294251735},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8143459915611815,\n",
       "    'acc_stderr': 0.025310495376944863,\n",
       "    'acc_norm': 0.8143459915611815,\n",
       "    'acc_norm_stderr': 0.025310495376944863},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6771300448430493,\n",
       "    'acc_stderr': 0.03138147637575498,\n",
       "    'acc_norm': 0.6771300448430493,\n",
       "    'acc_norm_stderr': 0.03138147637575498},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6946564885496184,\n",
       "    'acc_stderr': 0.040393149787245626,\n",
       "    'acc_norm': 0.6946564885496184,\n",
       "    'acc_norm_stderr': 0.040393149787245626},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7520661157024794,\n",
       "    'acc_stderr': 0.03941897526516303,\n",
       "    'acc_norm': 0.7520661157024794,\n",
       "    'acc_norm_stderr': 0.03941897526516303},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7129629629629629,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.7129629629629629,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6993865030674846,\n",
       "    'acc_stderr': 0.03602511318806771,\n",
       "    'acc_norm': 0.6993865030674846,\n",
       "    'acc_norm_stderr': 0.03602511318806771},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.45535714285714285,\n",
       "    'acc_stderr': 0.04726835553719099,\n",
       "    'acc_norm': 0.45535714285714285,\n",
       "    'acc_norm_stderr': 0.04726835553719099},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7184466019417476,\n",
       "    'acc_stderr': 0.044532548363264673,\n",
       "    'acc_norm': 0.7184466019417476,\n",
       "    'acc_norm_stderr': 0.044532548363264673},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8803418803418803,\n",
       "    'acc_stderr': 0.021262719400406978,\n",
       "    'acc_norm': 0.8803418803418803,\n",
       "    'acc_norm_stderr': 0.021262719400406978},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7662835249042146,\n",
       "    'acc_stderr': 0.015133383278988829,\n",
       "    'acc_norm': 0.7662835249042146,\n",
       "    'acc_norm_stderr': 0.015133383278988829},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6705202312138728,\n",
       "    'acc_stderr': 0.0253052581318797,\n",
       "    'acc_norm': 0.6705202312138728,\n",
       "    'acc_norm_stderr': 0.0253052581318797},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.43798882681564244,\n",
       "    'acc_stderr': 0.016593394227564843,\n",
       "    'acc_norm': 0.43798882681564244,\n",
       "    'acc_norm_stderr': 0.016593394227564843},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6601307189542484,\n",
       "    'acc_stderr': 0.027121956071388856,\n",
       "    'acc_norm': 0.6601307189542484,\n",
       "    'acc_norm_stderr': 0.027121956071388856},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6688102893890675,\n",
       "    'acc_stderr': 0.026730620728004906,\n",
       "    'acc_norm': 0.6688102893890675,\n",
       "    'acc_norm_stderr': 0.026730620728004906},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6851851851851852,\n",
       "    'acc_stderr': 0.02584224870090217,\n",
       "    'acc_norm': 0.6851851851851852,\n",
       "    'acc_norm_stderr': 0.02584224870090217},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.425531914893617,\n",
       "    'acc_stderr': 0.02949482760014438,\n",
       "    'acc_norm': 0.425531914893617,\n",
       "    'acc_norm_stderr': 0.02949482760014438},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.46740547588005216,\n",
       "    'acc_stderr': 0.012743072942653352,\n",
       "    'acc_norm': 0.46740547588005216,\n",
       "    'acc_norm_stderr': 0.012743072942653352},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5772058823529411,\n",
       "    'acc_stderr': 0.03000856284500348,\n",
       "    'acc_norm': 0.5772058823529411,\n",
       "    'acc_norm_stderr': 0.03000856284500348},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6372549019607843,\n",
       "    'acc_stderr': 0.01945076843250551,\n",
       "    'acc_norm': 0.6372549019607843,\n",
       "    'acc_norm_stderr': 0.01945076843250551},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.046075820907199756,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.046075820907199756},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.689795918367347,\n",
       "    'acc_stderr': 0.029613459872484375,\n",
       "    'acc_norm': 0.689795918367347,\n",
       "    'acc_norm_stderr': 0.029613459872484375},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8109452736318408,\n",
       "    'acc_stderr': 0.027686913588013024,\n",
       "    'acc_norm': 0.8109452736318408,\n",
       "    'acc_norm_stderr': 0.027686913588013024},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.84,\n",
       "    'acc_stderr': 0.03684529491774708,\n",
       "    'acc_norm': 0.84,\n",
       "    'acc_norm_stderr': 0.03684529491774708},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4939759036144578,\n",
       "    'acc_stderr': 0.03892212195333045,\n",
       "    'acc_norm': 0.4939759036144578,\n",
       "    'acc_norm_stderr': 0.03892212195333045},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7719298245614035,\n",
       "    'acc_stderr': 0.03218093795602357,\n",
       "    'acc_norm': 0.7719298245614035,\n",
       "    'acc_norm_stderr': 0.03218093795602357},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3525091799265606,\n",
       "    'mc1_stderr': 0.016724646380756544,\n",
       "    'mc2': 0.5480260004479156,\n",
       "    'mc2_stderr': 0.01562805066884067},\n",
       "   'all': {'acc': 0.5856610701363549,\n",
       "    'acc_stderr': 0.0338972114882748,\n",
       "    'acc_norm': 0.5894623662188109,\n",
       "    'acc_norm_stderr': 0.0338771018183512,\n",
       "    'mc1': 0.3525091799265606,\n",
       "    'mc1_stderr': 0.016724646380756544,\n",
       "    'mc2': 0.5480260004479156,\n",
       "    'mc2_stderr': 0.01562805066884067},\n",
       "   'harness|drop|3': {'em': 0.24611996644295303,\n",
       "    'em_stderr': 0.004411275638567265,\n",
       "    'f1': 0.3191652684563765,\n",
       "    'f1_stderr': 0.004369271114420946},\n",
       "   'harness|gsm8k|5': {'acc': 0.1372251705837756,\n",
       "    'acc_stderr': 0.00947780824460041},\n",
       "   'harness|winogrande|5': {'acc': 0.7703235990528808,\n",
       "    'acc_stderr': 0.011821645601838243}}},\n",
       " 'vicuna-7b': {'key': 'vicuna-7b',\n",
       "  'Model': 'Vicuna-7B',\n",
       "  'MT-bench (score)': '6.17',\n",
       "  'MMLU': '0.498',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-7b-v1.5',\n",
       "  'results': {'harness|drop|3': {'em': 0.017932046979865772,\n",
       "    'em_stderr': 0.0013590184569504276,\n",
       "    'f1': 0.08961094798657747,\n",
       "    'f1_stderr': 0.002014243406072028},\n",
       "   'harness|gsm8k|5': {'acc': 0.08188021228203184,\n",
       "    'acc_stderr': 0.007552338527716956},\n",
       "   'harness|winogrande|5': {'acc': 0.7213891081294396,\n",
       "    'acc_stderr': 0.012599896649493878},\n",
       "   'all': {'acc': 0.509411874714016,\n",
       "    'acc_stderr': 0.03492615918680034,\n",
       "    'acc_norm': 0.5131451594579193,\n",
       "    'acc_norm_stderr': 0.03491300641703413,\n",
       "    'mc1': 0.33047735618115054,\n",
       "    'mc1_stderr': 0.016466769613698303,\n",
       "    'mc2': 0.5032970216179807,\n",
       "    'mc2_stderr': 0.015653721119290503},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5025597269624573,\n",
       "    'acc_stderr': 0.014611199329843784,\n",
       "    'acc_norm': 0.5324232081911263,\n",
       "    'acc_norm_stderr': 0.01458063756999542},\n",
       "   'harness|hellaswag|10': {'acc': 0.5835490938060147,\n",
       "    'acc_stderr': 0.0049196263806455115,\n",
       "    'acc_norm': 0.7739494124676359,\n",
       "    'acc_norm_stderr': 0.004174174724288079},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5037037037037037,\n",
       "    'acc_stderr': 0.04319223625811331,\n",
       "    'acc_norm': 0.5037037037037037,\n",
       "    'acc_norm_stderr': 0.04319223625811331},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.04063302731486671,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.04063302731486671},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5433962264150943,\n",
       "    'acc_stderr': 0.030656748696739428,\n",
       "    'acc_norm': 0.5433962264150943,\n",
       "    'acc_norm_stderr': 0.030656748696739428},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.04179596617581,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.04179596617581},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4682080924855491,\n",
       "    'acc_stderr': 0.03804749744364763,\n",
       "    'acc_norm': 0.4682080924855491,\n",
       "    'acc_norm_stderr': 0.03804749744364763},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.17647058823529413,\n",
       "    'acc_stderr': 0.0379328118530781,\n",
       "    'acc_norm': 0.17647058823529413,\n",
       "    'acc_norm_stderr': 0.0379328118530781},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4553191489361702,\n",
       "    'acc_stderr': 0.03255525359340355,\n",
       "    'acc_norm': 0.4553191489361702,\n",
       "    'acc_norm_stderr': 0.03255525359340355},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.04372748290278006,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.04372748290278006},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.42758620689655175,\n",
       "    'acc_stderr': 0.04122737111370331,\n",
       "    'acc_norm': 0.42758620689655175,\n",
       "    'acc_norm_stderr': 0.04122737111370331},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.30687830687830686,\n",
       "    'acc_stderr': 0.023752928712112143,\n",
       "    'acc_norm': 0.30687830687830686,\n",
       "    'acc_norm_stderr': 0.023752928712112143},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.373015873015873,\n",
       "    'acc_stderr': 0.04325506042017086,\n",
       "    'acc_norm': 0.373015873015873,\n",
       "    'acc_norm_stderr': 0.04325506042017086},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.535483870967742,\n",
       "    'acc_stderr': 0.028372287797962935,\n",
       "    'acc_norm': 0.535483870967742,\n",
       "    'acc_norm_stderr': 0.028372287797962935},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4039408866995074,\n",
       "    'acc_stderr': 0.0345245390382204,\n",
       "    'acc_norm': 0.4039408866995074,\n",
       "    'acc_norm_stderr': 0.0345245390382204},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6303030303030303,\n",
       "    'acc_stderr': 0.037694303145125674,\n",
       "    'acc_norm': 0.6303030303030303,\n",
       "    'acc_norm_stderr': 0.037694303145125674},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6161616161616161,\n",
       "    'acc_stderr': 0.034648816750163396,\n",
       "    'acc_norm': 0.6161616161616161,\n",
       "    'acc_norm_stderr': 0.034648816750163396},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7357512953367875,\n",
       "    'acc_stderr': 0.03182155050916645,\n",
       "    'acc_norm': 0.7357512953367875,\n",
       "    'acc_norm_stderr': 0.03182155050916645},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.4794871794871795,\n",
       "    'acc_stderr': 0.025329663163489943,\n",
       "    'acc_norm': 0.4794871794871795,\n",
       "    'acc_norm_stderr': 0.025329663163489943},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.24444444444444444,\n",
       "    'acc_stderr': 0.02620276653465215,\n",
       "    'acc_norm': 0.24444444444444444,\n",
       "    'acc_norm_stderr': 0.02620276653465215},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.453781512605042,\n",
       "    'acc_stderr': 0.03233943468182088,\n",
       "    'acc_norm': 0.453781512605042,\n",
       "    'acc_norm_stderr': 0.03233943468182088},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.271523178807947,\n",
       "    'acc_stderr': 0.03631329803969653,\n",
       "    'acc_norm': 0.271523178807947,\n",
       "    'acc_norm_stderr': 0.03631329803969653},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.6972477064220184,\n",
       "    'acc_stderr': 0.019698711434756336,\n",
       "    'acc_norm': 0.6972477064220184,\n",
       "    'acc_norm_stderr': 0.019698711434756336},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.03324708911809117,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.03324708911809117},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7156862745098039,\n",
       "    'acc_stderr': 0.031660096793998116,\n",
       "    'acc_norm': 0.7156862745098039,\n",
       "    'acc_norm_stderr': 0.031660096793998116},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7215189873417721,\n",
       "    'acc_stderr': 0.02917868230484253,\n",
       "    'acc_norm': 0.7215189873417721,\n",
       "    'acc_norm_stderr': 0.02917868230484253},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6188340807174888,\n",
       "    'acc_stderr': 0.03259625118416827,\n",
       "    'acc_norm': 0.6188340807174888,\n",
       "    'acc_norm_stderr': 0.03259625118416827},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6335877862595419,\n",
       "    'acc_stderr': 0.04225875451969637,\n",
       "    'acc_norm': 0.6335877862595419,\n",
       "    'acc_norm_stderr': 0.04225875451969637},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.5950413223140496,\n",
       "    'acc_stderr': 0.04481137755942469,\n",
       "    'acc_norm': 0.5950413223140496,\n",
       "    'acc_norm_stderr': 0.04481137755942469},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5648148148148148,\n",
       "    'acc_stderr': 0.04792898170907061,\n",
       "    'acc_norm': 0.5648148148148148,\n",
       "    'acc_norm_stderr': 0.04792898170907061},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.5337423312883436,\n",
       "    'acc_stderr': 0.039194155450484096,\n",
       "    'acc_norm': 0.5337423312883436,\n",
       "    'acc_norm_stderr': 0.039194155450484096},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.42857142857142855,\n",
       "    'acc_stderr': 0.04697113923010212,\n",
       "    'acc_norm': 0.42857142857142855,\n",
       "    'acc_norm_stderr': 0.04697113923010212},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.04656147110012349,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.04656147110012349},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7692307692307693,\n",
       "    'acc_stderr': 0.027601921381417593,\n",
       "    'acc_norm': 0.7692307692307693,\n",
       "    'acc_norm_stderr': 0.027601921381417593},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6883780332056194,\n",
       "    'acc_stderr': 0.016562433867284176,\n",
       "    'acc_norm': 0.6883780332056194,\n",
       "    'acc_norm_stderr': 0.016562433867284176},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5578034682080925,\n",
       "    'acc_stderr': 0.026738603643807403,\n",
       "    'acc_norm': 0.5578034682080925,\n",
       "    'acc_norm_stderr': 0.026738603643807403},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24022346368715083,\n",
       "    'acc_stderr': 0.014288343803925296,\n",
       "    'acc_norm': 0.24022346368715083,\n",
       "    'acc_norm_stderr': 0.014288343803925296},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5751633986928104,\n",
       "    'acc_stderr': 0.02830457667314112,\n",
       "    'acc_norm': 0.5751633986928104,\n",
       "    'acc_norm_stderr': 0.02830457667314112},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5852090032154341,\n",
       "    'acc_stderr': 0.027982680459759567,\n",
       "    'acc_norm': 0.5852090032154341,\n",
       "    'acc_norm_stderr': 0.027982680459759567},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.027648477877413324,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.027648477877413324},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.36524822695035464,\n",
       "    'acc_stderr': 0.028723863853281285,\n",
       "    'acc_norm': 0.36524822695035464,\n",
       "    'acc_norm_stderr': 0.028723863853281285},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.37353324641460234,\n",
       "    'acc_stderr': 0.012354994823515271,\n",
       "    'acc_norm': 0.37353324641460234,\n",
       "    'acc_norm_stderr': 0.012354994823515271},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03032024326500413,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03032024326500413},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4852941176470588,\n",
       "    'acc_stderr': 0.020219083895133924,\n",
       "    'acc_norm': 0.4852941176470588,\n",
       "    'acc_norm_stderr': 0.020219083895133924},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6272727272727273,\n",
       "    'acc_stderr': 0.04631381319425465,\n",
       "    'acc_norm': 0.6272727272727273,\n",
       "    'acc_norm_stderr': 0.04631381319425465},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6244897959183674,\n",
       "    'acc_stderr': 0.03100120903989484,\n",
       "    'acc_norm': 0.6244897959183674,\n",
       "    'acc_norm_stderr': 0.03100120903989484},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.03333333333333335,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.03333333333333335},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.04292346959909282,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.04292346959909282},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4397590361445783,\n",
       "    'acc_stderr': 0.03864139923699121,\n",
       "    'acc_norm': 0.4397590361445783,\n",
       "    'acc_norm_stderr': 0.03864139923699121},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7192982456140351,\n",
       "    'acc_stderr': 0.034462962170884265,\n",
       "    'acc_norm': 0.7192982456140351,\n",
       "    'acc_norm_stderr': 0.034462962170884265},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.33047735618115054,\n",
       "    'mc1_stderr': 0.016466769613698303,\n",
       "    'mc2': 0.5032970216179807,\n",
       "    'mc2_stderr': 0.015653721119290503}}},\n",
       " 'wizardlm-13b': {'key': 'wizardlm-13b',\n",
       "  'Model': 'WizardLM-13b-v1.2',\n",
       "  'MT-bench (score)': '7.20',\n",
       "  'MMLU': '0.527',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Microsoft',\n",
       "  'Link': 'https://huggingface.co/WizardLM/WizardLM-13B-V1.2',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5443686006825939,\n",
       "    'acc_stderr': 0.014553749939306861,\n",
       "    'acc_norm': 0.590443686006826,\n",
       "    'acc_norm_stderr': 0.014370358632472439},\n",
       "   'harness|hellaswag|10': {'acc': 0.6294562836088429,\n",
       "    'acc_stderr': 0.00481963366883254,\n",
       "    'acc_norm': 0.8221469826727743,\n",
       "    'acc_norm_stderr': 0.0038160747120605347},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5111111111111111,\n",
       "    'acc_stderr': 0.04318275491977976,\n",
       "    'acc_norm': 0.5111111111111111,\n",
       "    'acc_norm_stderr': 0.04318275491977976},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5394736842105263,\n",
       "    'acc_stderr': 0.04056242252249034,\n",
       "    'acc_norm': 0.5394736842105263,\n",
       "    'acc_norm_stderr': 0.04056242252249034},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.04999999999999999,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.04999999999999999},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6113207547169811,\n",
       "    'acc_stderr': 0.030000485448675986,\n",
       "    'acc_norm': 0.6113207547169811,\n",
       "    'acc_norm_stderr': 0.030000485448675986},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5694444444444444,\n",
       "    'acc_stderr': 0.04140685639111503,\n",
       "    'acc_norm': 0.5694444444444444,\n",
       "    'acc_norm_stderr': 0.04140685639111503},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4913294797687861,\n",
       "    'acc_stderr': 0.03811890988940412,\n",
       "    'acc_norm': 0.4913294797687861,\n",
       "    'acc_norm_stderr': 0.03811890988940412},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.04389869956808777,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.04389869956808777},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.42127659574468085,\n",
       "    'acc_stderr': 0.03227834510146268,\n",
       "    'acc_norm': 0.42127659574468085,\n",
       "    'acc_norm_stderr': 0.03227834510146268},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2894736842105263,\n",
       "    'acc_stderr': 0.04266339443159394,\n",
       "    'acc_norm': 0.2894736842105263,\n",
       "    'acc_norm_stderr': 0.04266339443159394},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4482758620689655,\n",
       "    'acc_stderr': 0.04144311810878151,\n",
       "    'acc_norm': 0.4482758620689655,\n",
       "    'acc_norm_stderr': 0.04144311810878151},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.02441923496681907,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.02441923496681907},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3492063492063492,\n",
       "    'acc_stderr': 0.04263906892795132,\n",
       "    'acc_norm': 0.3492063492063492,\n",
       "    'acc_norm_stderr': 0.04263906892795132},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.632258064516129,\n",
       "    'acc_stderr': 0.02743086657997347,\n",
       "    'acc_norm': 0.632258064516129,\n",
       "    'acc_norm_stderr': 0.02743086657997347},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.43842364532019706,\n",
       "    'acc_stderr': 0.03491207857486518,\n",
       "    'acc_norm': 0.43842364532019706,\n",
       "    'acc_norm_stderr': 0.03491207857486518},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.049236596391733084,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.049236596391733084},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6787878787878788,\n",
       "    'acc_stderr': 0.036462049632538115,\n",
       "    'acc_norm': 0.6787878787878788,\n",
       "    'acc_norm_stderr': 0.036462049632538115},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6919191919191919,\n",
       "    'acc_stderr': 0.03289477330098616,\n",
       "    'acc_norm': 0.6919191919191919,\n",
       "    'acc_norm_stderr': 0.03289477330098616},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8031088082901554,\n",
       "    'acc_stderr': 0.028697873971860695,\n",
       "    'acc_norm': 0.8031088082901554,\n",
       "    'acc_norm_stderr': 0.028697873971860695},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5230769230769231,\n",
       "    'acc_stderr': 0.025323990861736236,\n",
       "    'acc_norm': 0.5230769230769231,\n",
       "    'acc_norm_stderr': 0.025323990861736236},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028604,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028604},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5798319327731093,\n",
       "    'acc_stderr': 0.03206183783236152,\n",
       "    'acc_norm': 0.5798319327731093,\n",
       "    'acc_norm_stderr': 0.03206183783236152},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.304635761589404,\n",
       "    'acc_stderr': 0.03757949922943343,\n",
       "    'acc_norm': 0.304635761589404,\n",
       "    'acc_norm_stderr': 0.03757949922943343},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7321100917431193,\n",
       "    'acc_stderr': 0.018987462257978652,\n",
       "    'acc_norm': 0.7321100917431193,\n",
       "    'acc_norm_stderr': 0.018987462257978652},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4027777777777778,\n",
       "    'acc_stderr': 0.033448873829978666,\n",
       "    'acc_norm': 0.4027777777777778,\n",
       "    'acc_norm_stderr': 0.033448873829978666},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03039153369274154,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03039153369274154},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7257383966244726,\n",
       "    'acc_stderr': 0.029041333510598028,\n",
       "    'acc_norm': 0.7257383966244726,\n",
       "    'acc_norm_stderr': 0.029041333510598028},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6322869955156951,\n",
       "    'acc_stderr': 0.03236198350928276,\n",
       "    'acc_norm': 0.6322869955156951,\n",
       "    'acc_norm_stderr': 0.03236198350928276},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5954198473282443,\n",
       "    'acc_stderr': 0.043046937953806645,\n",
       "    'acc_norm': 0.5954198473282443,\n",
       "    'acc_norm_stderr': 0.043046937953806645},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7520661157024794,\n",
       "    'acc_stderr': 0.03941897526516303,\n",
       "    'acc_norm': 0.7520661157024794,\n",
       "    'acc_norm_stderr': 0.03941897526516303},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7314814814814815,\n",
       "    'acc_stderr': 0.042844679680521934,\n",
       "    'acc_norm': 0.7314814814814815,\n",
       "    'acc_norm_stderr': 0.042844679680521934},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6441717791411042,\n",
       "    'acc_stderr': 0.03761521380046734,\n",
       "    'acc_norm': 0.6441717791411042,\n",
       "    'acc_norm_stderr': 0.03761521380046734},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.33035714285714285,\n",
       "    'acc_stderr': 0.04464285714285712,\n",
       "    'acc_norm': 0.33035714285714285,\n",
       "    'acc_norm_stderr': 0.04464285714285712},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6601941747572816,\n",
       "    'acc_stderr': 0.046897659372781335,\n",
       "    'acc_norm': 0.6601941747572816,\n",
       "    'acc_norm_stderr': 0.046897659372781335},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8162393162393162,\n",
       "    'acc_stderr': 0.02537213967172293,\n",
       "    'acc_norm': 0.8162393162393162,\n",
       "    'acc_norm_stderr': 0.02537213967172293},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7279693486590039,\n",
       "    'acc_stderr': 0.015913367447500503,\n",
       "    'acc_norm': 0.7279693486590039,\n",
       "    'acc_norm_stderr': 0.015913367447500503},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5982658959537572,\n",
       "    'acc_stderr': 0.026394104177643634,\n",
       "    'acc_norm': 0.5982658959537572,\n",
       "    'acc_norm_stderr': 0.026394104177643634},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30502793296089387,\n",
       "    'acc_stderr': 0.015398723510916713,\n",
       "    'acc_norm': 0.30502793296089387,\n",
       "    'acc_norm_stderr': 0.015398723510916713},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6013071895424836,\n",
       "    'acc_stderr': 0.028036092273891776,\n",
       "    'acc_norm': 0.6013071895424836,\n",
       "    'acc_norm_stderr': 0.028036092273891776},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5980707395498392,\n",
       "    'acc_stderr': 0.02784647600593047,\n",
       "    'acc_norm': 0.5980707395498392,\n",
       "    'acc_norm_stderr': 0.02784647600593047},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5895061728395061,\n",
       "    'acc_stderr': 0.027371350925124764,\n",
       "    'acc_norm': 0.5895061728395061,\n",
       "    'acc_norm_stderr': 0.027371350925124764},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4219858156028369,\n",
       "    'acc_stderr': 0.029462189233370597,\n",
       "    'acc_norm': 0.4219858156028369,\n",
       "    'acc_norm_stderr': 0.029462189233370597},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4165580182529335,\n",
       "    'acc_stderr': 0.012591153245057388,\n",
       "    'acc_norm': 0.4165580182529335,\n",
       "    'acc_norm_stderr': 0.012591153245057388},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5625,\n",
       "    'acc_stderr': 0.030134614954403924,\n",
       "    'acc_norm': 0.5625,\n",
       "    'acc_norm_stderr': 0.030134614954403924},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5310457516339869,\n",
       "    'acc_stderr': 0.02018880445636189,\n",
       "    'acc_norm': 0.5310457516339869,\n",
       "    'acc_norm_stderr': 0.02018880445636189},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.0449429086625209,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.0449429086625209},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6693877551020408,\n",
       "    'acc_stderr': 0.0301164262965406,\n",
       "    'acc_norm': 0.6693877551020408,\n",
       "    'acc_norm_stderr': 0.0301164262965406},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6865671641791045,\n",
       "    'acc_stderr': 0.032801882053486435,\n",
       "    'acc_norm': 0.6865671641791045,\n",
       "    'acc_norm_stderr': 0.032801882053486435},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.463855421686747,\n",
       "    'acc_stderr': 0.03882310850890593,\n",
       "    'acc_norm': 0.463855421686747,\n",
       "    'acc_norm_stderr': 0.03882310850890593},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7543859649122807,\n",
       "    'acc_stderr': 0.0330140594698725,\n",
       "    'acc_norm': 0.7543859649122807,\n",
       "    'acc_norm_stderr': 0.0330140594698725},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.32558139534883723,\n",
       "    'mc1_stderr': 0.01640398946990783,\n",
       "    'mc2': 0.47267763319871686,\n",
       "    'mc2_stderr': 0.01512716043041388},\n",
       "   'all': {'em': 0.09133808724832215,\n",
       "    'em_stderr': 0.002950304012601038,\n",
       "    'f1': 0.1617292365771806,\n",
       "    'f1_stderr': 0.0032231699829319426,\n",
       "    'acc': 0.4269860152120696,\n",
       "    'acc_stderr': 0.011021928189223498},\n",
       "   'harness|drop|3': {'em': 0.09133808724832215,\n",
       "    'em_stderr': 0.002950304012601038,\n",
       "    'f1': 0.1617292365771806,\n",
       "    'f1_stderr': 0.0032231699829319426},\n",
       "   'harness|gsm8k|5': {'acc': 0.13495072024260804,\n",
       "    'acc_stderr': 0.009411315282571171},\n",
       "   'harness|winogrande|5': {'acc': 0.7190213101815311,\n",
       "    'acc_stderr': 0.012632541095875825}}},\n",
       " 'wizardlm-70b': {'key': 'wizardlm-70b',\n",
       "  'Model': 'WizardLM-70B-v1.0',\n",
       "  'MT-bench (score)': '7.71',\n",
       "  'MMLU': '0.637',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Microsoft',\n",
       "  'Link': 'https://huggingface.co/WizardLM/WizardLM-70B-V1.0',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.606655290102389,\n",
       "    'acc_stderr': 0.014275101465693024,\n",
       "    'acc_norm': 0.6407849829351536,\n",
       "    'acc_norm_stderr': 0.014020224155839157},\n",
       "   'harness|hellaswag|10': {'acc': 0.6654052977494523,\n",
       "    'acc_stderr': 0.0047088426001774385,\n",
       "    'acc_norm': 0.854511053574985,\n",
       "    'acc_norm_stderr': 0.003518725257365601},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5185185185185185,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.5185185185185185,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03523807393012047,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03523807393012047},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621503,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621503},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.02872750295788027,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.02872750295788027},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7638888888888888,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.7638888888888888,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6473988439306358,\n",
       "    'acc_stderr': 0.036430371689585475,\n",
       "    'acc_norm': 0.6473988439306358,\n",
       "    'acc_norm_stderr': 0.036430371689585475},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107224,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107224},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.04461960433384739,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.04461960433384739},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5957446808510638,\n",
       "    'acc_stderr': 0.03208115750788684,\n",
       "    'acc_norm': 0.5957446808510638,\n",
       "    'acc_norm_stderr': 0.03208115750788684},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.046970851366478626,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.046970851366478626},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5448275862068965,\n",
       "    'acc_stderr': 0.04149886942192117,\n",
       "    'acc_norm': 0.5448275862068965,\n",
       "    'acc_norm_stderr': 0.04149886942192117},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42328042328042326,\n",
       "    'acc_stderr': 0.02544636563440676,\n",
       "    'acc_norm': 0.42328042328042326,\n",
       "    'acc_norm_stderr': 0.02544636563440676},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.04444444444444449,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.04444444444444449},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7645161290322581,\n",
       "    'acc_stderr': 0.02413763242933771,\n",
       "    'acc_norm': 0.7645161290322581,\n",
       "    'acc_norm_stderr': 0.02413763242933771},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.49261083743842365,\n",
       "    'acc_stderr': 0.03517603540361008,\n",
       "    'acc_norm': 0.49261083743842365,\n",
       "    'acc_norm_stderr': 0.03517603540361008},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8121212121212121,\n",
       "    'acc_stderr': 0.03050193405942914,\n",
       "    'acc_norm': 0.8121212121212121,\n",
       "    'acc_norm_stderr': 0.03050193405942914},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.02805779167298902,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.02805779167298902},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9119170984455959,\n",
       "    'acc_stderr': 0.02045374660160103,\n",
       "    'acc_norm': 0.9119170984455959,\n",
       "    'acc_norm_stderr': 0.02045374660160103},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6461538461538462,\n",
       "    'acc_stderr': 0.024243783994062153,\n",
       "    'acc_norm': 0.6461538461538462,\n",
       "    'acc_norm_stderr': 0.024243783994062153},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3111111111111111,\n",
       "    'acc_stderr': 0.028226446749683515,\n",
       "    'acc_norm': 0.3111111111111111,\n",
       "    'acc_norm_stderr': 0.028226446749683515},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7142857142857143,\n",
       "    'acc_stderr': 0.029344572500634342,\n",
       "    'acc_norm': 0.7142857142857143,\n",
       "    'acc_norm_stderr': 0.029344572500634342},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.4105960264900662,\n",
       "    'acc_stderr': 0.04016689594849929,\n",
       "    'acc_norm': 0.4105960264900662,\n",
       "    'acc_norm_stderr': 0.04016689594849929},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.03408655867977749,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.03408655867977749},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8480392156862745,\n",
       "    'acc_stderr': 0.025195658428931792,\n",
       "    'acc_norm': 0.8480392156862745,\n",
       "    'acc_norm_stderr': 0.025195658428931792},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8396624472573839,\n",
       "    'acc_stderr': 0.02388438092596567,\n",
       "    'acc_norm': 0.8396624472573839,\n",
       "    'acc_norm_stderr': 0.02388438092596567},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7040358744394619,\n",
       "    'acc_stderr': 0.03063659134869981,\n",
       "    'acc_norm': 0.7040358744394619,\n",
       "    'acc_norm_stderr': 0.03063659134869981},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8015267175572519,\n",
       "    'acc_stderr': 0.034981493854624714,\n",
       "    'acc_norm': 0.8015267175572519,\n",
       "    'acc_norm_stderr': 0.034981493854624714},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8264462809917356,\n",
       "    'acc_stderr': 0.0345727283691767,\n",
       "    'acc_norm': 0.8264462809917356,\n",
       "    'acc_norm_stderr': 0.0345727283691767},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8240740740740741,\n",
       "    'acc_stderr': 0.036809181416738807,\n",
       "    'acc_norm': 0.8240740740740741,\n",
       "    'acc_norm_stderr': 0.036809181416738807},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7791411042944786,\n",
       "    'acc_stderr': 0.03259177392742178,\n",
       "    'acc_norm': 0.7791411042944786,\n",
       "    'acc_norm_stderr': 0.03259177392742178},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7961165048543689,\n",
       "    'acc_stderr': 0.0398913985953177,\n",
       "    'acc_norm': 0.7961165048543689,\n",
       "    'acc_norm_stderr': 0.0398913985953177},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8717948717948718,\n",
       "    'acc_stderr': 0.02190190511507332,\n",
       "    'acc_norm': 0.8717948717948718,\n",
       "    'acc_norm_stderr': 0.02190190511507332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8288633461047255,\n",
       "    'acc_stderr': 0.013468201614066307,\n",
       "    'acc_norm': 0.8288633461047255,\n",
       "    'acc_norm_stderr': 0.013468201614066307},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7283236994219653,\n",
       "    'acc_stderr': 0.023948512905468365,\n",
       "    'acc_norm': 0.7283236994219653,\n",
       "    'acc_norm_stderr': 0.023948512905468365},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.29832402234636873,\n",
       "    'acc_stderr': 0.01530184004512928,\n",
       "    'acc_norm': 0.29832402234636873,\n",
       "    'acc_norm_stderr': 0.01530184004512928},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6797385620915033,\n",
       "    'acc_stderr': 0.026716118380156847,\n",
       "    'acc_norm': 0.6797385620915033,\n",
       "    'acc_norm_stderr': 0.026716118380156847},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6881028938906752,\n",
       "    'acc_stderr': 0.02631185807185416,\n",
       "    'acc_norm': 0.6881028938906752,\n",
       "    'acc_norm_stderr': 0.02631185807185416},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7067901234567902,\n",
       "    'acc_stderr': 0.02532988817190092,\n",
       "    'acc_norm': 0.7067901234567902,\n",
       "    'acc_norm_stderr': 0.02532988817190092},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5425531914893617,\n",
       "    'acc_stderr': 0.029719281272236837,\n",
       "    'acc_norm': 0.5425531914893617,\n",
       "    'acc_norm_stderr': 0.029719281272236837},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5189048239895697,\n",
       "    'acc_stderr': 0.012761104871472652,\n",
       "    'acc_norm': 0.5189048239895697,\n",
       "    'acc_norm_stderr': 0.012761104871472652},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6544117647058824,\n",
       "    'acc_stderr': 0.028888193103988626,\n",
       "    'acc_norm': 0.6544117647058824,\n",
       "    'acc_norm_stderr': 0.028888193103988626},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.01869085027359529,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.01869085027359529},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.7090909090909091,\n",
       "    'acc_stderr': 0.04350271442923243,\n",
       "    'acc_norm': 0.7090909090909091,\n",
       "    'acc_norm_stderr': 0.04350271442923243},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7714285714285715,\n",
       "    'acc_stderr': 0.026882144922307744,\n",
       "    'acc_norm': 0.7714285714285715,\n",
       "    'acc_norm_stderr': 0.026882144922307744},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8557213930348259,\n",
       "    'acc_stderr': 0.024845753212306042,\n",
       "    'acc_norm': 0.8557213930348259,\n",
       "    'acc_norm_stderr': 0.024845753212306042},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.83,\n",
       "    'acc_stderr': 0.03775251680686371,\n",
       "    'acc_norm': 0.83,\n",
       "    'acc_norm_stderr': 0.03775251680686371},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.536144578313253,\n",
       "    'acc_stderr': 0.03882310850890594,\n",
       "    'acc_norm': 0.536144578313253,\n",
       "    'acc_norm_stderr': 0.03882310850890594},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8538011695906432,\n",
       "    'acc_stderr': 0.027097290118070792,\n",
       "    'acc_norm': 0.8538011695906432,\n",
       "    'acc_norm_stderr': 0.027097290118070792},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.386780905752754,\n",
       "    'mc1_stderr': 0.01704885701051511,\n",
       "    'mc2': 0.5476558772806095,\n",
       "    'mc2_stderr': 0.015451111828036913},\n",
       "   'all': {'acc': 0.6477083045829946,\n",
       "    'acc_stderr': 0.032583732941869364,\n",
       "    'acc_norm': 0.6514919562551693,\n",
       "    'acc_norm_stderr': 0.03255924150707842,\n",
       "    'mc1': 0.386780905752754,\n",
       "    'mc1_stderr': 0.01704885701051511,\n",
       "    'mc2': 0.5476558772806095,\n",
       "    'mc2_stderr': 0.015451111828036913},\n",
       "   'harness|drop|3': {'em': 0.26541526845637586,\n",
       "    'em_stderr': 0.004521927044730418,\n",
       "    'f1': 0.3270648070469802,\n",
       "    'f1_stderr': 0.004444377320494032},\n",
       "   'harness|gsm8k|5': {'acc': 0.17968157695223655,\n",
       "    'acc_stderr': 0.010575119964242244},\n",
       "   'harness|winogrande|5': {'acc': 0.8082083662194159,\n",
       "    'acc_stderr': 0.011065209664659527}}},\n",
       " 'yi-34b-chat': {'key': 'yi-34b-chat',\n",
       "  'Model': 'Yi-34B-Chat',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.735',\n",
       "  'License': 'Yi License',\n",
       "  'Organization': '01 AI',\n",
       "  'Link': 'https://huggingface.co/01-ai/Yi-34B-Chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6373720136518771,\n",
       "    'acc_stderr': 0.014049106564955012,\n",
       "    'acc_norm': 0.6544368600682594,\n",
       "    'acc_norm_stderr': 0.013896938461145678},\n",
       "   'harness|hellaswag|10': {'acc': 0.6536546504680343,\n",
       "    'acc_stderr': 0.004748324319714274,\n",
       "    'acc_norm': 0.8415654252141008,\n",
       "    'acc_norm_stderr': 0.003644017383711605},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.7111111111111111,\n",
       "    'acc_stderr': 0.03915450630414251,\n",
       "    'acc_norm': 0.7111111111111111,\n",
       "    'acc_norm_stderr': 0.03915450630414251},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.8552631578947368,\n",
       "    'acc_stderr': 0.028631951845930387,\n",
       "    'acc_norm': 0.8552631578947368,\n",
       "    'acc_norm_stderr': 0.028631951845930387},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.7924528301886793,\n",
       "    'acc_stderr': 0.02495991802891127,\n",
       "    'acc_norm': 0.7924528301886793,\n",
       "    'acc_norm_stderr': 0.02495991802891127},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.8472222222222222,\n",
       "    'acc_stderr': 0.030085743248565666,\n",
       "    'acc_norm': 0.8472222222222222,\n",
       "    'acc_norm_stderr': 0.030085743248565666},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237101,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237101},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6994219653179191,\n",
       "    'acc_stderr': 0.034961014811911786,\n",
       "    'acc_norm': 0.6994219653179191,\n",
       "    'acc_norm_stderr': 0.034961014811911786},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.46078431372549017,\n",
       "    'acc_stderr': 0.049598599663841815,\n",
       "    'acc_norm': 0.46078431372549017,\n",
       "    'acc_norm_stderr': 0.049598599663841815},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.82,\n",
       "    'acc_stderr': 0.03861229196653695,\n",
       "    'acc_norm': 0.82,\n",
       "    'acc_norm_stderr': 0.03861229196653695},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.7659574468085106,\n",
       "    'acc_stderr': 0.02767845257821239,\n",
       "    'acc_norm': 0.7659574468085106,\n",
       "    'acc_norm_stderr': 0.02767845257821239},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5526315789473685,\n",
       "    'acc_stderr': 0.046774730044911984,\n",
       "    'acc_norm': 0.5526315789473685,\n",
       "    'acc_norm_stderr': 0.046774730044911984},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.0333333333333333,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.0333333333333333},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.6349206349206349,\n",
       "    'acc_stderr': 0.024796060602699965,\n",
       "    'acc_norm': 0.6349206349206349,\n",
       "    'acc_norm_stderr': 0.024796060602699965},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5396825396825397,\n",
       "    'acc_stderr': 0.04458029125470973,\n",
       "    'acc_norm': 0.5396825396825397,\n",
       "    'acc_norm_stderr': 0.04458029125470973},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.049888765156985884,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.049888765156985884},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.8709677419354839,\n",
       "    'acc_stderr': 0.019070889254792767,\n",
       "    'acc_norm': 0.8709677419354839,\n",
       "    'acc_norm_stderr': 0.019070889254792767},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.6206896551724138,\n",
       "    'acc_stderr': 0.03413963805906235,\n",
       "    'acc_norm': 0.6206896551724138,\n",
       "    'acc_norm_stderr': 0.03413963805906235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8545454545454545,\n",
       "    'acc_stderr': 0.027530196355066573,\n",
       "    'acc_norm': 0.8545454545454545,\n",
       "    'acc_norm_stderr': 0.027530196355066573},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.898989898989899,\n",
       "    'acc_stderr': 0.021469735576055343,\n",
       "    'acc_norm': 0.898989898989899,\n",
       "    'acc_norm_stderr': 0.021469735576055343},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9533678756476683,\n",
       "    'acc_stderr': 0.015216761819262585,\n",
       "    'acc_norm': 0.9533678756476683,\n",
       "    'acc_norm_stderr': 0.015216761819262585},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.7846153846153846,\n",
       "    'acc_stderr': 0.020843034557462878,\n",
       "    'acc_norm': 0.7846153846153846,\n",
       "    'acc_norm_stderr': 0.020843034557462878},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.35555555555555557,\n",
       "    'acc_stderr': 0.029185714949857403,\n",
       "    'acc_norm': 0.35555555555555557,\n",
       "    'acc_norm_stderr': 0.029185714949857403},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.8361344537815126,\n",
       "    'acc_stderr': 0.024044054940440488,\n",
       "    'acc_norm': 0.8361344537815126,\n",
       "    'acc_norm_stderr': 0.024044054940440488},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.5033112582781457,\n",
       "    'acc_stderr': 0.04082393379449654,\n",
       "    'acc_norm': 0.5033112582781457,\n",
       "    'acc_norm_stderr': 0.04082393379449654},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.908256880733945,\n",
       "    'acc_stderr': 0.012376323409137123,\n",
       "    'acc_norm': 0.908256880733945,\n",
       "    'acc_norm_stderr': 0.012376323409137123},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.6342592592592593,\n",
       "    'acc_stderr': 0.03284738857647206,\n",
       "    'acc_norm': 0.6342592592592593,\n",
       "    'acc_norm_stderr': 0.03284738857647206},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.9068627450980392,\n",
       "    'acc_stderr': 0.020397853969426998,\n",
       "    'acc_norm': 0.9068627450980392,\n",
       "    'acc_norm_stderr': 0.020397853969426998},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.9029535864978903,\n",
       "    'acc_stderr': 0.019269323025640255,\n",
       "    'acc_norm': 0.9029535864978903,\n",
       "    'acc_norm_stderr': 0.019269323025640255},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.8116591928251121,\n",
       "    'acc_stderr': 0.026241132996407256,\n",
       "    'acc_norm': 0.8116591928251121,\n",
       "    'acc_norm_stderr': 0.026241132996407256},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8931297709923665,\n",
       "    'acc_stderr': 0.027096548624883733,\n",
       "    'acc_norm': 0.8931297709923665,\n",
       "    'acc_norm_stderr': 0.027096548624883733},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8925619834710744,\n",
       "    'acc_stderr': 0.028268812192540627,\n",
       "    'acc_norm': 0.8925619834710744,\n",
       "    'acc_norm_stderr': 0.028268812192540627},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8888888888888888,\n",
       "    'acc_stderr': 0.03038159675665167,\n",
       "    'acc_norm': 0.8888888888888888,\n",
       "    'acc_norm_stderr': 0.03038159675665167},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.852760736196319,\n",
       "    'acc_stderr': 0.027839915278339657,\n",
       "    'acc_norm': 0.852760736196319,\n",
       "    'acc_norm_stderr': 0.027839915278339657},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.6160714285714286,\n",
       "    'acc_stderr': 0.04616143075028546,\n",
       "    'acc_norm': 0.6160714285714286,\n",
       "    'acc_norm_stderr': 0.04616143075028546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8737864077669902,\n",
       "    'acc_stderr': 0.0328818027880863,\n",
       "    'acc_norm': 0.8737864077669902,\n",
       "    'acc_norm_stderr': 0.0328818027880863},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.9145299145299145,\n",
       "    'acc_stderr': 0.01831589168562586,\n",
       "    'acc_norm': 0.9145299145299145,\n",
       "    'acc_norm_stderr': 0.01831589168562586},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.033799766898963086,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.033799766898963086},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8991060025542784,\n",
       "    'acc_stderr': 0.01077047201488672,\n",
       "    'acc_norm': 0.8991060025542784,\n",
       "    'acc_norm_stderr': 0.01077047201488672},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.8092485549132948,\n",
       "    'acc_stderr': 0.021152676966575277,\n",
       "    'acc_norm': 0.8092485549132948,\n",
       "    'acc_norm_stderr': 0.021152676966575277},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.7016759776536313,\n",
       "    'acc_stderr': 0.01530184004512928,\n",
       "    'acc_norm': 0.7016759776536313,\n",
       "    'acc_norm_stderr': 0.01530184004512928},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.8300653594771242,\n",
       "    'acc_stderr': 0.02150538312123137,\n",
       "    'acc_norm': 0.8300653594771242,\n",
       "    'acc_norm_stderr': 0.02150538312123137},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.8102893890675241,\n",
       "    'acc_stderr': 0.02226819625878322,\n",
       "    'acc_norm': 0.8102893890675241,\n",
       "    'acc_norm_stderr': 0.02226819625878322},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.8703703703703703,\n",
       "    'acc_stderr': 0.018689725721062072,\n",
       "    'acc_norm': 0.8703703703703703,\n",
       "    'acc_norm_stderr': 0.018689725721062072},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.6170212765957447,\n",
       "    'acc_stderr': 0.02899908090480618,\n",
       "    'acc_norm': 0.6170212765957447,\n",
       "    'acc_norm_stderr': 0.02899908090480618},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5495436766623207,\n",
       "    'acc_stderr': 0.012707390438502348,\n",
       "    'acc_norm': 0.5495436766623207,\n",
       "    'acc_norm_stderr': 0.012707390438502348},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.7830882352941176,\n",
       "    'acc_stderr': 0.025035845227711274,\n",
       "    'acc_norm': 0.7830882352941176,\n",
       "    'acc_norm_stderr': 0.025035845227711274},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.826797385620915,\n",
       "    'acc_stderr': 0.015309329266969138,\n",
       "    'acc_norm': 0.826797385620915,\n",
       "    'acc_norm_stderr': 0.015309329266969138},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.7272727272727273,\n",
       "    'acc_stderr': 0.04265792110940589,\n",
       "    'acc_norm': 0.7272727272727273,\n",
       "    'acc_norm_stderr': 0.04265792110940589},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.8367346938775511,\n",
       "    'acc_stderr': 0.02366169917709861,\n",
       "    'acc_norm': 0.8367346938775511,\n",
       "    'acc_norm_stderr': 0.02366169917709861},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8855721393034826,\n",
       "    'acc_stderr': 0.022509345325101706,\n",
       "    'acc_norm': 0.8855721393034826,\n",
       "    'acc_norm_stderr': 0.022509345325101706},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.89,\n",
       "    'acc_stderr': 0.03144660377352203,\n",
       "    'acc_norm': 0.89,\n",
       "    'acc_norm_stderr': 0.03144660377352203},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5843373493975904,\n",
       "    'acc_stderr': 0.03836722176598053,\n",
       "    'acc_norm': 0.5843373493975904,\n",
       "    'acc_norm_stderr': 0.03836722176598053},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8771929824561403,\n",
       "    'acc_stderr': 0.02517298435015577,\n",
       "    'acc_norm': 0.8771929824561403,\n",
       "    'acc_norm_stderr': 0.02517298435015577},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3843329253365973,\n",
       "    'mc1_stderr': 0.017028707301245203,\n",
       "    'mc2': 0.5536831362008046,\n",
       "    'mc2_stderr': 0.015524186394858242},\n",
       "   'harness|winogrande|5': {'acc': 0.8011049723756906,\n",
       "    'acc_stderr': 0.01121862997251531},\n",
       "   'harness|drop|3': {'em': 0.005138422818791947,\n",
       "    'em_stderr': 0.0007322104102794241,\n",
       "    'f1': 0.08032508389261797,\n",
       "    'f1_stderr': 0.001571649833831937},\n",
       "   'harness|gsm8k|5': {'acc': 0.3191811978771797,\n",
       "    'acc_stderr': 0.012840345676251648},\n",
       "   'all': {'acc': 0.7393930299846158,\n",
       "    'acc_stderr': 0.028807135333088364,\n",
       "    'acc_norm': 0.7489434623723922,\n",
       "    'acc_norm_stderr': 0.02935457295982731,\n",
       "    'mc1': 0.3843329253365973,\n",
       "    'mc1_stderr': 0.017028707301245203,\n",
       "    'mc2': 0.5536831362008046,\n",
       "    'mc2_stderr': 0.015524186394858242}}},\n",
       " 'zephyr-7b-alpha': {'key': 'zephyr-7b-alpha',\n",
       "  'Model': 'Zephyr-7b-alpha',\n",
       "  'MT-bench (score)': '6.88',\n",
       "  'MMLU': '-',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'HuggingFace',\n",
       "  'Link': 'https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha',\n",
       "  'results': {'harness|drop|3': {'em': 0.010171979865771811,\n",
       "    'em_stderr': 0.001027595678252703,\n",
       "    'f1': 0.0982225251677855,\n",
       "    'f1_stderr': 0.00197861649884212},\n",
       "   'harness|gsm8k|5': {'acc': 0.14025777103866566,\n",
       "    'acc_stderr': 0.00956510828142867},\n",
       "   'harness|winogrande|5': {'acc': 0.7861089187056038,\n",
       "    'acc_stderr': 0.011524466954090248},\n",
       "   'all': {'acc': 0.6137978230566867,\n",
       "    'acc_stderr': 0.03380754595328641,\n",
       "    'acc_norm': 0.6176702382672306,\n",
       "    'acc_norm_stderr': 0.03378555360789072,\n",
       "    'mc1': 0.42717258261933905,\n",
       "    'mc1_stderr': 0.017316834410963926,\n",
       "    'mc2': 0.5790339154881958,\n",
       "    'mc2_stderr': 0.015362629183533977},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5810580204778157,\n",
       "    'acc_stderr': 0.014418106953639011,\n",
       "    'acc_norm': 0.6100682593856656,\n",
       "    'acc_norm_stderr': 0.01425295984889289},\n",
       "   'harness|hellaswag|10': {'acc': 0.6409081856203943,\n",
       "    'acc_stderr': 0.004787537385153006,\n",
       "    'acc_norm': 0.8403704441346346,\n",
       "    'acc_norm_stderr': 0.0036551361115537096},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6447368421052632,\n",
       "    'acc_stderr': 0.038947344870133176,\n",
       "    'acc_norm': 0.6447368421052632,\n",
       "    'acc_norm_stderr': 0.038947344870133176},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620332,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620332},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6679245283018868,\n",
       "    'acc_stderr': 0.02898545565233439,\n",
       "    'acc_norm': 0.6679245283018868,\n",
       "    'acc_norm_stderr': 0.02898545565233439},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7013888888888888,\n",
       "    'acc_stderr': 0.03827052357950756,\n",
       "    'acc_norm': 0.7013888888888888,\n",
       "    'acc_norm_stderr': 0.03827052357950756},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.49,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.49,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6184971098265896,\n",
       "    'acc_stderr': 0.03703851193099521,\n",
       "    'acc_norm': 0.6184971098265896,\n",
       "    'acc_norm_stderr': 0.03703851193099521},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4215686274509804,\n",
       "    'acc_stderr': 0.049135952012744975,\n",
       "    'acc_norm': 0.4215686274509804,\n",
       "    'acc_norm_stderr': 0.049135952012744975},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5319148936170213,\n",
       "    'acc_stderr': 0.03261936918467382,\n",
       "    'acc_norm': 0.5319148936170213,\n",
       "    'acc_norm_stderr': 0.03261936918467382},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.43859649122807015,\n",
       "    'acc_stderr': 0.04668000738510455,\n",
       "    'acc_norm': 0.43859649122807015,\n",
       "    'acc_norm_stderr': 0.04668000738510455},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370333,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370333},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.38095238095238093,\n",
       "    'acc_stderr': 0.02501074911613761,\n",
       "    'acc_norm': 0.38095238095238093,\n",
       "    'acc_norm_stderr': 0.02501074911613761},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4126984126984127,\n",
       "    'acc_stderr': 0.04403438954768176,\n",
       "    'acc_norm': 0.4126984126984127,\n",
       "    'acc_norm_stderr': 0.04403438954768176},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.04793724854411019,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.04793724854411019},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7548387096774194,\n",
       "    'acc_stderr': 0.02447224384089553,\n",
       "    'acc_norm': 0.7548387096774194,\n",
       "    'acc_norm_stderr': 0.02447224384089553},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5073891625615764,\n",
       "    'acc_stderr': 0.035176035403610105,\n",
       "    'acc_norm': 0.5073891625615764,\n",
       "    'acc_norm_stderr': 0.035176035403610105},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7515151515151515,\n",
       "    'acc_stderr': 0.033744026441394036,\n",
       "    'acc_norm': 0.7515151515151515,\n",
       "    'acc_norm_stderr': 0.033744026441394036},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7828282828282829,\n",
       "    'acc_stderr': 0.029376616484945633,\n",
       "    'acc_norm': 0.7828282828282829,\n",
       "    'acc_norm_stderr': 0.029376616484945633},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8549222797927462,\n",
       "    'acc_stderr': 0.025416343096306433,\n",
       "    'acc_norm': 0.8549222797927462,\n",
       "    'acc_norm_stderr': 0.025416343096306433},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6153846153846154,\n",
       "    'acc_stderr': 0.024666744915187208,\n",
       "    'acc_norm': 0.6153846153846154,\n",
       "    'acc_norm_stderr': 0.024666744915187208},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.35555555555555557,\n",
       "    'acc_stderr': 0.029185714949857403,\n",
       "    'acc_norm': 0.35555555555555557,\n",
       "    'acc_norm_stderr': 0.029185714949857403},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.634453781512605,\n",
       "    'acc_stderr': 0.03128217706368461,\n",
       "    'acc_norm': 0.634453781512605,\n",
       "    'acc_norm_stderr': 0.03128217706368461},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3708609271523179,\n",
       "    'acc_stderr': 0.03943966699183629,\n",
       "    'acc_norm': 0.3708609271523179,\n",
       "    'acc_norm_stderr': 0.03943966699183629},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8018348623853211,\n",
       "    'acc_stderr': 0.017090573804217902,\n",
       "    'acc_norm': 0.8018348623853211,\n",
       "    'acc_norm_stderr': 0.017090573804217902},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5416666666666666,\n",
       "    'acc_stderr': 0.033981108902946366,\n",
       "    'acc_norm': 0.5416666666666666,\n",
       "    'acc_norm_stderr': 0.033981108902946366},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7598039215686274,\n",
       "    'acc_stderr': 0.02998373305591361,\n",
       "    'acc_norm': 0.7598039215686274,\n",
       "    'acc_norm_stderr': 0.02998373305591361},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7426160337552743,\n",
       "    'acc_stderr': 0.028458820991460285,\n",
       "    'acc_norm': 0.7426160337552743,\n",
       "    'acc_norm_stderr': 0.028458820991460285},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6367713004484304,\n",
       "    'acc_stderr': 0.032277904428505,\n",
       "    'acc_norm': 0.6367713004484304,\n",
       "    'acc_norm_stderr': 0.032277904428505},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6793893129770993,\n",
       "    'acc_stderr': 0.04093329229834278,\n",
       "    'acc_norm': 0.6793893129770993,\n",
       "    'acc_norm_stderr': 0.04093329229834278},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.743801652892562,\n",
       "    'acc_stderr': 0.03984979653302871,\n",
       "    'acc_norm': 0.743801652892562,\n",
       "    'acc_norm_stderr': 0.03984979653302871},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.043300437496507416,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.043300437496507416},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7361963190184049,\n",
       "    'acc_stderr': 0.03462419931615623,\n",
       "    'acc_norm': 0.7361963190184049,\n",
       "    'acc_norm_stderr': 0.03462419931615623},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.39285714285714285,\n",
       "    'acc_stderr': 0.04635550135609976,\n",
       "    'acc_norm': 0.39285714285714285,\n",
       "    'acc_norm_stderr': 0.04635550135609976},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7281553398058253,\n",
       "    'acc_stderr': 0.044052680241409216,\n",
       "    'acc_norm': 0.7281553398058253,\n",
       "    'acc_norm_stderr': 0.044052680241409216},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.02280138253459753,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.02280138253459753},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768078,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768078},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7905491698595147,\n",
       "    'acc_stderr': 0.014551310568143704,\n",
       "    'acc_norm': 0.7905491698595147,\n",
       "    'acc_norm_stderr': 0.014551310568143704},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6994219653179191,\n",
       "    'acc_stderr': 0.0246853168672578,\n",
       "    'acc_norm': 0.6994219653179191,\n",
       "    'acc_norm_stderr': 0.0246853168672578},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.37094972067039106,\n",
       "    'acc_stderr': 0.01615591072134177,\n",
       "    'acc_norm': 0.37094972067039106,\n",
       "    'acc_norm_stderr': 0.01615591072134177},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6862745098039216,\n",
       "    'acc_stderr': 0.02656892101545715,\n",
       "    'acc_norm': 0.6862745098039216,\n",
       "    'acc_norm_stderr': 0.02656892101545715},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7041800643086816,\n",
       "    'acc_stderr': 0.025922371788818774,\n",
       "    'acc_norm': 0.7041800643086816,\n",
       "    'acc_norm_stderr': 0.025922371788818774},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6790123456790124,\n",
       "    'acc_stderr': 0.02597656601086274,\n",
       "    'acc_norm': 0.6790123456790124,\n",
       "    'acc_norm_stderr': 0.02597656601086274},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.46099290780141844,\n",
       "    'acc_stderr': 0.029736592526424438,\n",
       "    'acc_norm': 0.46099290780141844,\n",
       "    'acc_norm_stderr': 0.029736592526424438},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.41003911342894395,\n",
       "    'acc_stderr': 0.012561837621962044,\n",
       "    'acc_norm': 0.41003911342894395,\n",
       "    'acc_norm_stderr': 0.012561837621962044},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6507352941176471,\n",
       "    'acc_stderr': 0.028959755196824866,\n",
       "    'acc_norm': 0.6507352941176471,\n",
       "    'acc_norm_stderr': 0.028959755196824866},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6290849673202614,\n",
       "    'acc_stderr': 0.019542101564854128,\n",
       "    'acc_norm': 0.6290849673202614,\n",
       "    'acc_norm_stderr': 0.019542101564854128},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.04607582090719976,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.04607582090719976},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6816326530612244,\n",
       "    'acc_stderr': 0.029822533793982066,\n",
       "    'acc_norm': 0.6816326530612244,\n",
       "    'acc_norm_stderr': 0.029822533793982066},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8009950248756219,\n",
       "    'acc_stderr': 0.028231365092758406,\n",
       "    'acc_norm': 0.8009950248756219,\n",
       "    'acc_norm_stderr': 0.028231365092758406},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5180722891566265,\n",
       "    'acc_stderr': 0.03889951252827216,\n",
       "    'acc_norm': 0.5180722891566265,\n",
       "    'acc_norm_stderr': 0.03889951252827216},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.42717258261933905,\n",
       "    'mc1_stderr': 0.017316834410963926,\n",
       "    'mc2': 0.5790339154881958,\n",
       "    'mc2_stderr': 0.015362629183533977}}},\n",
       " 'zephyr-7b-beta': {'key': 'zephyr-7b-beta',\n",
       "  'Model': 'Zephyr-7b-beta',\n",
       "  'MT-bench (score)': '7.34',\n",
       "  'MMLU': '0.614',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'HuggingFace',\n",
       "  'Link': 'https://huggingface.co/HuggingFaceH4/zephyr-7b-beta',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.590443686006826,\n",
       "    'acc_stderr': 0.014370358632472437,\n",
       "    'acc_norm': 0.6203071672354948,\n",
       "    'acc_norm_stderr': 0.01418211986697487},\n",
       "   'harness|hellaswag|10': {'acc': 0.6491734714200359,\n",
       "    'acc_stderr': 0.004762534245488399,\n",
       "    'acc_norm': 0.8435570603465445,\n",
       "    'acc_norm_stderr': 0.003625323221166244},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.04218506215368881,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.04218506215368881},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6052631578947368,\n",
       "    'acc_stderr': 0.039777499346220734,\n",
       "    'acc_norm': 0.6052631578947368,\n",
       "    'acc_norm_stderr': 0.039777499346220734},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.660377358490566,\n",
       "    'acc_stderr': 0.02914690474779833,\n",
       "    'acc_norm': 0.660377358490566,\n",
       "    'acc_norm_stderr': 0.02914690474779833},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7013888888888888,\n",
       "    'acc_stderr': 0.03827052357950756,\n",
       "    'acc_norm': 0.7013888888888888,\n",
       "    'acc_norm_stderr': 0.03827052357950756},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.04878317312145633,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.04878317312145633},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6416184971098265,\n",
       "    'acc_stderr': 0.03656343653353159,\n",
       "    'acc_norm': 0.6416184971098265,\n",
       "    'acc_norm_stderr': 0.03656343653353159},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4215686274509804,\n",
       "    'acc_stderr': 0.049135952012744975,\n",
       "    'acc_norm': 0.4215686274509804,\n",
       "    'acc_norm_stderr': 0.049135952012744975},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5191489361702127,\n",
       "    'acc_stderr': 0.032662042990646775,\n",
       "    'acc_norm': 0.5191489361702127,\n",
       "    'acc_norm_stderr': 0.032662042990646775},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.42105263157894735,\n",
       "    'acc_stderr': 0.046446020912223177,\n",
       "    'acc_norm': 0.42105263157894735,\n",
       "    'acc_norm_stderr': 0.046446020912223177},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5379310344827586,\n",
       "    'acc_stderr': 0.04154659671707548,\n",
       "    'acc_norm': 0.5379310344827586,\n",
       "    'acc_norm_stderr': 0.04154659671707548},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36772486772486773,\n",
       "    'acc_stderr': 0.02483383982556242,\n",
       "    'acc_norm': 0.36772486772486773,\n",
       "    'acc_norm_stderr': 0.02483383982556242},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.044444444444444495,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.044444444444444495},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.049236596391733084,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.049236596391733084},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7483870967741936,\n",
       "    'acc_stderr': 0.024685979286239963,\n",
       "    'acc_norm': 0.7483870967741936,\n",
       "    'acc_norm_stderr': 0.024685979286239963},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.035158955511656986,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.035158955511656986},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.04725815626252609,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.04725815626252609},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7575757575757576,\n",
       "    'acc_stderr': 0.03346409881055953,\n",
       "    'acc_norm': 0.7575757575757576,\n",
       "    'acc_norm_stderr': 0.03346409881055953},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7323232323232324,\n",
       "    'acc_stderr': 0.031544498882702866,\n",
       "    'acc_norm': 0.7323232323232324,\n",
       "    'acc_norm_stderr': 0.031544498882702866},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8238341968911918,\n",
       "    'acc_stderr': 0.02749350424454805,\n",
       "    'acc_norm': 0.8238341968911918,\n",
       "    'acc_norm_stderr': 0.02749350424454805},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6282051282051282,\n",
       "    'acc_stderr': 0.024503472557110936,\n",
       "    'acc_norm': 0.6282051282051282,\n",
       "    'acc_norm_stderr': 0.024503472557110936},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.34444444444444444,\n",
       "    'acc_stderr': 0.028972648884844267,\n",
       "    'acc_norm': 0.34444444444444444,\n",
       "    'acc_norm_stderr': 0.028972648884844267},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.0303883535518868,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.0303883535518868},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2980132450331126,\n",
       "    'acc_stderr': 0.037345356767871984,\n",
       "    'acc_norm': 0.2980132450331126,\n",
       "    'acc_norm_stderr': 0.037345356767871984},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8091743119266055,\n",
       "    'acc_stderr': 0.01684767640009109,\n",
       "    'acc_norm': 0.8091743119266055,\n",
       "    'acc_norm_stderr': 0.01684767640009109},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.033953227263757976,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.033953227263757976},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.02910225438967407,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.02910225438967407},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7341772151898734,\n",
       "    'acc_stderr': 0.028756799629658346,\n",
       "    'acc_norm': 0.7341772151898734,\n",
       "    'acc_norm_stderr': 0.028756799629658346},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6367713004484304,\n",
       "    'acc_stderr': 0.032277904428505,\n",
       "    'acc_norm': 0.6367713004484304,\n",
       "    'acc_norm_stderr': 0.032277904428505},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6717557251908397,\n",
       "    'acc_stderr': 0.04118438565806298,\n",
       "    'acc_norm': 0.6717557251908397,\n",
       "    'acc_norm_stderr': 0.04118438565806298},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7107438016528925,\n",
       "    'acc_stderr': 0.04139112727635463,\n",
       "    'acc_norm': 0.7107438016528925,\n",
       "    'acc_norm_stderr': 0.04139112727635463},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.040191074725573483,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.040191074725573483},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7055214723926381,\n",
       "    'acc_stderr': 0.03581165790474082,\n",
       "    'acc_norm': 0.7055214723926381,\n",
       "    'acc_norm_stderr': 0.03581165790474082},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.3482142857142857,\n",
       "    'acc_stderr': 0.04521829902833585,\n",
       "    'acc_norm': 0.3482142857142857,\n",
       "    'acc_norm_stderr': 0.04521829902833585},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.043546310772605956,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.043546310772605956},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8803418803418803,\n",
       "    'acc_stderr': 0.02126271940040697,\n",
       "    'acc_norm': 0.8803418803418803,\n",
       "    'acc_norm_stderr': 0.02126271940040697},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.014866821664709588,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.014866821664709588},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6705202312138728,\n",
       "    'acc_stderr': 0.02530525813187972,\n",
       "    'acc_norm': 0.6705202312138728,\n",
       "    'acc_norm_stderr': 0.02530525813187972},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.3418994413407821,\n",
       "    'acc_stderr': 0.015864506461604637,\n",
       "    'acc_norm': 0.3418994413407821,\n",
       "    'acc_norm_stderr': 0.015864506461604637},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6830065359477124,\n",
       "    'acc_stderr': 0.026643278474508755,\n",
       "    'acc_norm': 0.6830065359477124,\n",
       "    'acc_norm_stderr': 0.026643278474508755},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6688102893890675,\n",
       "    'acc_stderr': 0.02673062072800491,\n",
       "    'acc_norm': 0.6688102893890675,\n",
       "    'acc_norm_stderr': 0.02673062072800491},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.02622964917882117,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.02622964917882117},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4929078014184397,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.4929078014184397,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4276401564537158,\n",
       "    'acc_stderr': 0.012635799922765844,\n",
       "    'acc_norm': 0.4276401564537158,\n",
       "    'acc_norm_stderr': 0.012635799922765844},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6470588235294118,\n",
       "    'acc_stderr': 0.029029422815681397,\n",
       "    'acc_norm': 0.6470588235294118,\n",
       "    'acc_norm_stderr': 0.029029422815681397},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.619281045751634,\n",
       "    'acc_stderr': 0.019643801557924803,\n",
       "    'acc_norm': 0.619281045751634,\n",
       "    'acc_norm_stderr': 0.019643801557924803},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302506,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302506},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6816326530612244,\n",
       "    'acc_stderr': 0.029822533793982066,\n",
       "    'acc_norm': 0.6816326530612244,\n",
       "    'acc_norm_stderr': 0.029822533793982066},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8009950248756219,\n",
       "    'acc_stderr': 0.028231365092758406,\n",
       "    'acc_norm': 0.8009950248756219,\n",
       "    'acc_norm_stderr': 0.028231365092758406},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932262,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932262},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5180722891566265,\n",
       "    'acc_stderr': 0.03889951252827216,\n",
       "    'acc_norm': 0.5180722891566265,\n",
       "    'acc_norm_stderr': 0.03889951252827216},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8070175438596491,\n",
       "    'acc_stderr': 0.030267457554898458,\n",
       "    'acc_norm': 0.8070175438596491,\n",
       "    'acc_norm_stderr': 0.030267457554898458},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.40636474908200737,\n",
       "    'mc1_stderr': 0.017193835812093893,\n",
       "    'mc2': 0.5744916942762855,\n",
       "    'mc2_stderr': 0.015742095840959796},\n",
       "   'harness|winogrande|5': {'acc': 0.7774269928966061,\n",
       "    'acc_stderr': 0.011690933809712667},\n",
       "   'harness|drop|3': {'em': 0.004928691275167785,\n",
       "    'em_stderr': 0.0007171872517059793,\n",
       "    'f1': 0.09662437080536909,\n",
       "    'f1_stderr': 0.0018807376338089597},\n",
       "   'harness|gsm8k|5': {'acc': 0.12736921910538287,\n",
       "    'acc_stderr': 0.009183110326737829},\n",
       "   'all': {'acc': 0.6058001121844437,\n",
       "    'acc_stderr': 0.033164878802299444,\n",
       "    'acc_norm': 0.6148009779899025,\n",
       "    'acc_norm_stderr': 0.033912849373118566,\n",
       "    'mc1': 0.40636474908200737,\n",
       "    'mc1_stderr': 0.017193835812093893,\n",
       "    'mc2': 0.5744916942762855,\n",
       "    'mc2_stderr': 0.015742095840959796,\n",
       "    'em': 0.004928691275167785,\n",
       "    'em_stderr': 0.0007171872517059793,\n",
       "    'f1': 0.09662437080536909,\n",
       "    'f1_stderr': 0.0018807376338089597}}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_hf_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hendrycksTest-astronomy\n",
      "0.8248974490616756\n",
      "hendrycksTest-electrical_engineering\n",
      "0.690623225047368\n",
      "hendrycksTest-moral_scenarios\n",
      "0.7934990071790254\n",
      "hendrycksTest-logical_fallacies\n",
      "0.8309957470376553\n",
      "hendrycksTest-jurisprudence\n",
      "0.803355017715246\n",
      "hendrycksTest-miscellaneous\n",
      "0.833646789410847\n",
      "hellaswag\n",
      "0.7299145299145299\n",
      "hendrycksTest-high_school_microeconomics\n",
      "0.816284830148969\n",
      "hendrycksTest-anatomy\n",
      "0.753083295828028\n",
      "hendrycksTest-clinical_knowledge\n",
      "0.7933582436134913\n",
      "hendrycksTest-high_school_statistics\n",
      "0.7416547068048434\n",
      "hendrycksTest-moral_disputes\n",
      "0.8127245810661129\n",
      "hendrycksTest-security_studies\n",
      "0.8449696445202932\n",
      "hendrycksTest-high_school_us_history\n",
      "0.8240336380878359\n",
      "hendrycksTest-medical_genetics\n",
      "0.6907544373283913\n",
      "hendrycksTest-nutrition\n",
      "0.79008547008547\n",
      "hendrycksTest-high_school_government_and_politics\n",
      "0.8371538052701287\n",
      "hendrycksTest-machine_learning\n",
      "0.6115398459618884\n",
      "hendrycksTest-elementary_mathematics\n",
      "0.7969207364036006\n",
      "hendrycksTest-professional_accounting\n",
      "0.8195656970683529\n",
      "hendrycksTest-global_facts\n",
      "0.30225294867053554\n",
      "hendrycksTest-high_school_world_history\n",
      "0.8279754246178759\n",
      "hendrycksTest-high_school_chemistry\n",
      "0.6488709785380623\n",
      "hendrycksTest-virology\n",
      "0.8156006014564721\n",
      "hendrycksTest-professional_medicine\n",
      "0.7429451978432647\n",
      "hendrycksTest-college_biology\n",
      "0.8211541063054432\n",
      "hendrycksTest-high_school_geography\n",
      "0.8139026953255367\n",
      "hendrycksTest-college_physics\n",
      "0.703305063609025\n",
      "hendrycksTest-business_ethics\n",
      "0.7525724190297646\n",
      "hendrycksTest-international_law\n",
      "0.8221007740351939\n",
      "hendrycksTest-professional_psychology\n",
      "0.83775005498629\n",
      "hendrycksTest-management\n",
      "0.7924669152096665\n",
      "hendrycksTest-public_relations\n",
      "0.6988576678759426\n",
      "hendrycksTest-high_school_psychology\n",
      "0.8345597578739297\n",
      "hendrycksTest-world_religions\n",
      "0.8464726206755547\n",
      "hendrycksTest-professional_law\n",
      "0.8345299145299144\n",
      "hendrycksTest-high_school_biology\n",
      "0.8038311302632007\n",
      "hendrycksTest-high_school_mathematics\n",
      "0.6143861143657399\n",
      "hendrycksTest-college_chemistry\n",
      "0.700175646624084\n",
      "hendrycksTest-prehistory\n",
      "0.8343306670067543\n",
      "hendrycksTest-us_foreign_policy\n",
      "0.8040441788763094\n",
      "hendrycksTest-college_computer_science\n",
      "0.7356537004948399\n",
      "truthfulqa:mc\n",
      "0.5405128205128205\n",
      "hendrycksTest-econometrics\n",
      "0.7079557765646415\n",
      "hendrycksTest-computer_security\n",
      "0.756507958382953\n",
      "arc:challenge\n",
      "0.8290014164699427\n",
      "hendrycksTest-high_school_physics\n",
      "0.7629647782221062\n",
      "hendrycksTest-sociology\n",
      "0.870041090552613\n",
      "hendrycksTest-formal_logic\n",
      "0.7387887789753013\n",
      "hendrycksTest-philosophy\n",
      "0.7593638762486408\n",
      "hendrycksTest-college_medicine\n",
      "0.7822073007379267\n",
      "hendrycksTest-abstract_algebra\n",
      "0.5774269944978343\n",
      "hendrycksTest-college_mathematics\n",
      "0.032663410259844795\n",
      "hendrycksTest-human_sexuality\n",
      "0.7970572304932209\n",
      "hendrycksTest-high_school_macroeconomics\n",
      "0.8080013795643279\n",
      "hendrycksTest-conceptual_physics\n",
      "0.7686784177996653\n",
      "hendrycksTest-human_aging\n",
      "0.8032844849915253\n",
      "hendrycksTest-marketing\n",
      "0.8356734983682623\n",
      "hendrycksTest-high_school_european_history\n",
      "0.8363264312738408\n",
      "hendrycksTest-high_school_computer_science\n",
      "0.7871949080998777\n"
     ]
    }
   ],
   "source": [
    "# Check Spearman correlation between elo and each benchmark\n",
    "corr_dict = {}\n",
    "\n",
    "for benchmark in common_benchmarks:\n",
    "    name = benchmark.split('|')[1]\n",
    "    print(name)\n",
    "    elo_scores = []\n",
    "    benchmark_scores = []\n",
    "    for model in filtered_hf_models:\n",
    "        elo_scores.append(elo.loc[model]['rating'])\n",
    "        result = filtered_hf_models[model]['results'][benchmark]\n",
    "        if 'acc_norm' in result:\n",
    "            benchmark_scores.append(result['acc_norm'])\n",
    "        elif 'mc2' in result:\n",
    "            benchmark_scores.append(result['mc2'])\n",
    "    corr = spearmanr(elo_scores, benchmark_scores).correlation\n",
    "    corr_dict[name] = corr\n",
    "\n",
    "    print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by correlation\n",
    "corr_dict = {k: v for k, v in sorted(corr_dict.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hendrycksTest-sociology': 0.870041090552613,\n",
       " 'hendrycksTest-world_religions': 0.8464726206755547,\n",
       " 'hendrycksTest-security_studies': 0.8449696445202932,\n",
       " 'hendrycksTest-professional_psychology': 0.83775005498629,\n",
       " 'hendrycksTest-high_school_government_and_politics': 0.8371538052701287,\n",
       " 'hendrycksTest-high_school_european_history': 0.8363264312738408,\n",
       " 'hendrycksTest-marketing': 0.8356734983682623,\n",
       " 'hendrycksTest-high_school_psychology': 0.8345597578739297,\n",
       " 'hendrycksTest-professional_law': 0.8345299145299144,\n",
       " 'hendrycksTest-prehistory': 0.8343306670067543,\n",
       " 'hendrycksTest-miscellaneous': 0.833646789410847,\n",
       " 'hendrycksTest-logical_fallacies': 0.8309957470376553,\n",
       " 'arc:challenge': 0.8290014164699427,\n",
       " 'hendrycksTest-high_school_world_history': 0.8279754246178759,\n",
       " 'hendrycksTest-astronomy': 0.8248974490616756,\n",
       " 'hendrycksTest-high_school_us_history': 0.8240336380878359,\n",
       " 'hendrycksTest-international_law': 0.8221007740351939,\n",
       " 'hendrycksTest-college_biology': 0.8211541063054432,\n",
       " 'hendrycksTest-professional_accounting': 0.8195656970683529,\n",
       " 'hendrycksTest-high_school_microeconomics': 0.816284830148969,\n",
       " 'hendrycksTest-virology': 0.8156006014564721,\n",
       " 'hendrycksTest-high_school_geography': 0.8139026953255367,\n",
       " 'hendrycksTest-moral_disputes': 0.8127245810661129,\n",
       " 'hendrycksTest-high_school_macroeconomics': 0.8080013795643279,\n",
       " 'hendrycksTest-us_foreign_policy': 0.8040441788763094,\n",
       " 'hendrycksTest-high_school_biology': 0.8038311302632007,\n",
       " 'hendrycksTest-jurisprudence': 0.803355017715246,\n",
       " 'hendrycksTest-human_aging': 0.8032844849915253,\n",
       " 'hendrycksTest-human_sexuality': 0.7970572304932209,\n",
       " 'hendrycksTest-elementary_mathematics': 0.7969207364036006,\n",
       " 'hendrycksTest-moral_scenarios': 0.7934990071790254,\n",
       " 'hendrycksTest-clinical_knowledge': 0.7933582436134913,\n",
       " 'hendrycksTest-management': 0.7924669152096665,\n",
       " 'hendrycksTest-nutrition': 0.79008547008547,\n",
       " 'hendrycksTest-high_school_computer_science': 0.7871949080998777,\n",
       " 'hendrycksTest-college_medicine': 0.7822073007379267,\n",
       " 'hendrycksTest-conceptual_physics': 0.7686784177996653,\n",
       " 'hendrycksTest-high_school_physics': 0.7629647782221062,\n",
       " 'hendrycksTest-philosophy': 0.7593638762486408,\n",
       " 'hendrycksTest-computer_security': 0.756507958382953,\n",
       " 'hendrycksTest-anatomy': 0.753083295828028,\n",
       " 'hendrycksTest-business_ethics': 0.7525724190297646,\n",
       " 'hendrycksTest-professional_medicine': 0.7429451978432647,\n",
       " 'hendrycksTest-high_school_statistics': 0.7416547068048434,\n",
       " 'hendrycksTest-formal_logic': 0.7387887789753013,\n",
       " 'hendrycksTest-college_computer_science': 0.7356537004948399,\n",
       " 'hellaswag': 0.7299145299145299,\n",
       " 'hendrycksTest-econometrics': 0.7079557765646415,\n",
       " 'hendrycksTest-college_physics': 0.703305063609025,\n",
       " 'hendrycksTest-college_chemistry': 0.700175646624084,\n",
       " 'hendrycksTest-public_relations': 0.6988576678759426,\n",
       " 'hendrycksTest-medical_genetics': 0.6907544373283913,\n",
       " 'hendrycksTest-electrical_engineering': 0.690623225047368,\n",
       " 'hendrycksTest-high_school_chemistry': 0.6488709785380623,\n",
       " 'hendrycksTest-high_school_mathematics': 0.6143861143657399,\n",
       " 'hendrycksTest-machine_learning': 0.6115398459618884,\n",
       " 'hendrycksTest-abstract_algebra': 0.5774269944978343,\n",
       " 'truthfulqa:mc': 0.5405128205128205,\n",
       " 'hendrycksTest-global_facts': 0.30225294867053554,\n",
       " 'hendrycksTest-college_mathematics': 0.032663410259844795}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot correlation\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.bar(corr_dict.keys(), corr_dict.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Spearman correlation between elo and benchmark scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(10, 10)})\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAMNCAYAAACF+YLFAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAAsTAAALEwEAmpwYAADZfUlEQVR4nOzdZ3hU1f728XsCCRBKKAYE6QgJAqGEXhSBQOgdVKpBOiioh6Ie9Q8KKqBBqijSe0wApUgRUemiAiIghBIglACGQCBMynpe5Jk5EwI4aJIh8P1c17kO2WX2b7un3LNmrbUtxhgjAAAAAJIkN1cXAAAAADxICMgAAACAAwIyAAAA4ICADAAAADggIAMAAAAOCMgAAACAAwIyANxBjx491KhRI1eX4bRGjRqpR48eri7jHwkNDZWPj4927drl6lIAQJKU1dUFAMi8fHx8nN528+bNKlq0aDpW8z+jRo1SWFjYHddNnjxZgYGBGVIHACBzIiAD+Mc++uijFH/v3btXy5YtU9euXeXv759iXf78+TOyNEmp65MkPz+/DK8DAJC5EJAB/GNt27ZN8XdiYqKWLVumKlWqpFrnCg9CDbi769evK1euXK4uAwBSoQ8ygHR348YNTZo0SU2aNFHFihVVr149jRgxQmfPnk2x3a5du+Tj46PQ0FAtWLBAzZo1U6VKldSsWTMtWLDgvo9rjNH169eVlJT0j2s/ffq0Bg4cKH9/f1WrVk2DBw/W6dOn73isxYsXq0OHDqpcubKqVq2qHj16aOfOnSm2O3PmjHx8fDRlyhRt2bJFHTt2VKVKlVS/fn19+OGHSkhISPXYp06d0ujRo/X000+rYsWKql+/vgYOHKjff/891bbh4eHq16+fqlatKn9/f7388suKiopKsc2UKVPk4+OjY8eO6f3331f9+vVVuXJl9erVS8ePH5ckbdiwQe3bt5efn58aNWqkZcuWpTrW2rVrNWDAADVs2FAVK1ZUrVq1NGjQIB0+fDjVtrY+0n/88Yf69Okjf39/tWnT5p7/7WfMmCEfHx+NHTvWfg2///57de/eXbVq1ZKfn58aNmyoIUOG6MSJE/d8LAC4H7QgA0hX8fHx6tOnj3755Rc1a9ZML774ok6dOqUlS5Zo27Zt+uqrr/T444+n2GfhwoWKiopS165dlStXLn3zzTd67733dPXqVQ0ZMsTpY/v7+ys2Nlbu7u6qUaOGhg0bpsqVKzu9/40bN9SjRw/5+fnp1Vdf1alTp7R48WLt27dPYWFh8vb2tm/7n//8R2vWrFGzZs3UoUMHWa1Wff311woKCtKUKVPUuHHjFI+9detWLV68WM8995w6duyozZs368svv5SXl5cGDBhg3+7AgQPq3bu3EhIS1KlTJ5UtW1ZXr17V7t279euvv6pixYr2bS9cuKCePXuqSZMmGjFihA4fPqxly5bp+vXr+vLLL1Od38iRI+Xp6an+/fvrypUrmjNnjl566SW9/PLLmjhxor22kJAQvf322ypTpoyqV69u33/hwoXKmzevunTpIm9vb0VERGj58uV6/vnnFRYWppIlS6Y4XmRkpHr16qXAwEA1bdpUN27cuON/98TERI0ZM0ZLly7Va6+9pn79+kmSdu/erYEDB6ps2bLq37+/cufOrYsXL2rHjh2KiIhQqVKlnL62AHBPBgDSyFdffWXKlStnvvrqK/uyZcuWmXLlypkPP/wwxbZbtmwx5cqVM6+//rp92c6dO025cuVMlSpVzLlz5+zLb926ZTp27GieeuqpFMvvZsKECWbcuHFm1apVZuPGjWbKlCmmevXqpkKFCmbbtm1OnUv37t1NuXLlzHvvvZdi+YYNG0y5cuXMf//731TLli5dmmLb+Ph40759e/Pss8+apKQkY4wxp0+fNuXKlTOVK1c2p0+ftm+blJRkWrZsaerVq5dqWcWKFc2hQ4dS1ZiYmGj/97PPPmvKlStn1qxZk2Kbd99915QrV86Eh4fbl3366aemXLlypn///va6jDFm3rx5ply5cqZq1aomMjLSvvzy5cumYsWKZvjw4SkeOzY2NlVNx44dMxUqVDDvvPNOiuW2+pYvX55qH9vzZufOnebmzZtm0KBBpkKFCiYsLCzFduPGjTPlypUzly5dSvUYAJCW6GIBIF1t3LhRbm5u6t+/f4rlDRs2VPny5bV58+ZUXSBat26dolXZw8PD3or63Xff/e0xX3/9dY0ePVpt2rRRkyZNNGTIEK1YsUJZs2bVu+++e1/121ovbQICAlSqVClt3rzZvmz16tXKmTOnmjRpoitXrtj/FxMTo0aNGuns2bM6efJkisdp3Lhxilk9LBaLatWqpaioKMXGxkqSDh06pKNHj6pDhw7y9fVNVZubW8q38IIFC6pFixYpltWuXVtScjeN2/Xo0UMWi8X+t611uFGjRipcuLB9ef78+VWqVKlU5+Dp6Snpf11Zrly5onz58qlUqVLav39/quPlzZtXHTp0SLXc5urVq3rxxRe1fft2zZgxQ+3atUuxPnfu3JKkb7/99o5dUQAgrdDFAkC6OnPmjAoWLCgvL69U65588kkdOnRIf/31lwoUKGBfXqZMmTtuK+mO/X+dUbJkSTVv3lyhoaE6ceKEUz/H58mTJ0U3Csf6Nm3apBs3bsjT01Ph4eGKjY1V3bp17/pYly9fTnHMYsWKpdomb968kqTo6GjlzJnTHkifeuqpv63Vmcf8u+3z5MkjSXecjs/LyytVn/E//vhDkydP1u7du1N1l7jTYxQrVkxZsmS5a/2jRo3SjRs3tHDhwhRdOWy6deumzZs36//+7/80ceJE+fv7q0GDBmrVqpVLZkkB8PAiIAN4ZDzxxBOSpL/++itN+6saY5Q/f35NmjTprtuULVs2xd/3CorGmH9Ux/0+5u0t0M48jk1kZKS6deumXLlyaeDAgSpdurRy5Mghi8WicePG3bF/cY4cOe75mC1atFBoaKhmzJihadOmKXv27CnW58uXTyEhIfr555+1fft27dmzR+PHj9eUKVM0a9YsVa1a9W/rBgBnEJABpKtixYrpxx9/VExMjL2F0iY8PFy5cuVSvnz5Ui2/3bFjx+yP90/ZWmQfe+wxp7aPiYlRVFRUqlbk8PBwFShQwN7FoESJEjp58qQqV66snDlz/uP6bmcL8YcOHUqzx0wrGzdu1I0bNzRjxgx7Nw6b6OhoeXh43Pdjtm7dWnXq1NGIESPUv39/zZw5M1WozpIli2rVqqVatWpJkg4fPqyOHTtqxowZmjVr1j8/IQBwQB9kAOmqSZMmSkpKShVetm7dqj/++EONGjVK1ZL59ddf6/z58/a/rVar5s6dqyxZsujZZ5+95/Fu3LihW7dupVr+xx9/aP369SpTpoyKFy/udP23171x40adOHFCTZo0sS9r166dkpKS9PHHH9/xMS5duuT08Rz5+vqqbNmy+uqrr3T06NFU6/9pS3NasLUy317D8uXLU00rdz9atmypSZMmae/everbt6+9P7YkXblyJdX2pUuXVrZs2XT16tV/fEwAuB0tyADSVfv27RUWFqbPP/9cZ8+eVfXq1RUREaHFixfrscce06uvvppqn1KlSqlz58567rnnlDNnTn3zzTc6cOCABg0alGLw2J2cOnVKffv2VePGjVWyZEnlyJFDhw8f1ldffaUsWbJozJgxTteeL18+bdy4URcvXlTNmjXt07w99thjKaabCwwMVIcOHbRw4UIdPHhQzz77rPLly6fz58/rt99+06lTp1IM6nOWrbtC79691blzZ/s0bzExMdqzZ48aNGigHj163PfjpoWnn35aOXLk0IgRI9S9e3flyZNHv/zyi3744QcVL15ciYmJ//ixAwMDlTVrVg0bNkx9+vTRF198oVy5cum///2vzp8/r/r166tIkSKKi4vTunXrFBsby01hAKQpAjKAdOXu7q7Zs2drxowZWrt2rTZu3KjcuXMrMDBQw4YNu2Pg7d69u65fv66FCxcqMjJSRYoU0RtvvKFevXr97fEee+wx1alTR7t27dLXX3+tW7duydvbWy1atFC/fv3uOADwbjw9PTVv3jyNGzdOkyZNkjFGDRo00KhRo1SwYMEU244fP161atXS8uXL9dlnnyk+Pl7e3t566qmn9Nprrzl9zNv5+fkpJCRE06dP17p167R06VLlzZtXfn5+qlat2j9+3H+rePHi+vzzz/Xxxx9r5syZypIli6pVq6YFCxZo7NixqQb03a8mTZpo6tSpGjp0qIKCgjR79my1bdtWoaGhCgsL05UrV5QrVy49+eST+vTTT9WsWbM0OjMAkCzGhb/RWa1WTZ48WatWrVJMTIx8fX01fPhw1alT52/3tU0D9OeffyopKUmlS5dWr169Uk1xdO3aNU2fPl2bN2/W+fPn9dhjj6l+/foaPHiwChUqlF6nBuAf2LVrl3r27Knx48ffczowAADSk0tbkEeNGqUNGzaoZ8+eKlGihMLCwtS3b18tWLDgnqORt2zZooEDB6pq1aoaOnSoJGnNmjUaPny4YmNj1blzZ0lSUlKS+vTpo6NHj+r5559XqVKldOLECS1ZskQ7d+7UN998848GkgAAAODh5bKAvH//fq1Zs0ajR49W7969JSUPdGnVqpUmTpyoRYsW3XXfRYsWydvbW/PmzbMH3C5duqhx48ZatWqVPSAfOHBA+/bt09tvv61u3brZ9y9SpIjGjh2rX375JdXoawAAADzaXDaLxfr16+Xu7m4Ps5KULVs2derUSXv37tXFixfvuu/169fl5eWVovXXw8NDXl5eypYtW4rtJKW4AYH0vymebp9jEwAAAHBZC/KhQ4dUqlSpVHOG+vn5yRijQ4cOpRoEY1OzZk199tlnCg4OtvdTDA0N1cmTJzV69Gj7dhUqVJCnp6cmT54sLy8vlS5dWsePH9fkyZNVq1YtVa5cOf1OEMB9q1Wrlo4cOeLqMgAAjziXBeSoqKg7DpKzTch/rxbkAQMGKCIiQjNnztSMGTMkJY82nz59uurVq2ffLm/evPrkk0/01ltv2btxSNKzzz6r4OBgWSyWNDobAAAAPCxcFpDj4uLk7u6earmti8SdJvq38fDwUMmSJRUYGKiAgAAlJiZq+fLlGjZsmObOnSs/Pz/7tvnz51fFihVVtWpVlSlTRocPH9YXX3yhN954466T+gMAAODR5bKAnD17dsXHx6dabgvGjn2Jbzd27FgdOHBAISEh9jtwNW/eXK1atdK4ceO0dOlSSdLp06fVs2dPTZw40X7XqyZNmuiJJ57QqFGj1LFjxxQtzs74669YJSW57u5VD5ICBXLp8uXrri4Df4Pr9ODjGmUOXKfMgeuUzM3Nonz5cv79hrgjlwVkb2/vO3ajsN2i9G79j61Wq0JCQtS/f/8Ut6d1d3dXgwYNtGTJEiUkJChr1qwKDQ2V1WrVM888k+IxGjVqJEn65Zdf7jsgJyUZArID/ltkDlynBx/XKHPgOmUOXCf8Wy6bxcLX11cnTpxQbGxsiuX79u2zr7+T6OhoJSQk3PE2pgkJCUpISJDt3ieXL1+WMUa33wslISEhxf8DAAAANi4LyIGBgYqPj9eKFSvsy6xWq0JDQ1WtWjX7AL7IyEiFh4fbtylQoIDy5MmjjRs3puiiERsbqy1btqhcuXL2vs0lS5ZUUlKS1q1bl+LY33zzjSTpqaeeSrfzAwAAQObksi4WlStXVmBgoCZOnKioqCgVL15cYWFhioyM1Pjx4+3bjRw5Urt377ZP/ZQlSxYFBQUpODhYXbt2VZs2bZSUlKSQkBCdP39eI0eOtO/bvn17ffnll3rzzTf1+++/68knn9TBgwcVEhIiHx8fe1cLAAAAwMalt5r+6KOPFBwcrFWrVunq1avy8fHRrFmz5O/vf8/9Bg4cqKJFi2r+/PmaNm2arFarfHx8NHXqVAUEBNi3y5cvn7766itNnjxZ3333nZYsWaK8efOqU6dOGj58+B1n0QAAAMCjzWJu76CLe7p8+Tqd//8/b+/cioq65uoy8De4Tg8+rlHmwHXKHLhOydzcLCpQIJery8i0XNYHGQAAAHgQEZABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcZHXlwa1WqyZPnqxVq1YpJiZGvr6+Gj58uOrUqfO3+27fvl0zZszQn3/+qaSkJJUuXVq9evVSixYt7NuEhoZq9OjRd32MCRMmqE2bNmlyLgAAAHg4uDQgjxo1Shs2bFDPnj1VokQJhYWFqW/fvlqwYIGqVq161/22bNmigQMHqmrVqho6dKgkac2aNRo+fLhiY2PVuXNnSVKNGjX00Ucfpdp/3rx5Onz4sFNBHAAAAI8WizHGuOLA+/fvV+fOnTV69Gj17t1bknTr1i21atVKBQsW1KJFi+6670svvaQjR45o8+bN8vDwkJTcGt24cWOVKFFCCxcuvOu+cXFxqlu3rqpUqaIvv/zyvuu+fPm6kpJc8p/sgePtnVtRUddcXQb+Btfpwcc1yhy4TpkD1ymZm5tFBQrkcnUZmZbL+iCvX79e7u7u9tZeScqWLZs6deqkvXv36uLFi3fd9/r16/Ly8rKHY0ny8PCQl5eXsmXLds/jfvfdd4qNjVXr1q3//UkAAADgoeOygHzo0CGVKlVKOXPmTLHcz89PxhgdOnTorvvWrFlTR48eVXBwsCIiIhQREaHg4GCdPHlSQUFB9zzu119/rezZsysgICBNzgMAAAAPF5f1QY6KilKhQoVSLff29pake7YgDxgwQBEREZo5c6ZmzJghSfL09NT06dNVr169u+4XHR2tH3/8UU2aNFGuXP/sZ4eM+LnCxMXJkj17uh8nLXh753Z1CX8rM/33TC+Z4To96rhGmQPXKXPgOuHfcllAjouLk7u7e6rlti4St27duuu+Hh4eKlmypAIDAxUQEKDExEQtX75cw4YN09y5c+Xn53fH/b799lvFx8f/q+4VGdEH2ds7t84+USxdj/EoeeLs6Ue6Pxr98R58XKPMgeuUOXCdktEH+d9xWUDOnj274uPjUy23BeN79SUeO3asDhw4oJCQELm5JfcSad68uVq1aqVx48Zp6dKld9zv66+/Vt68efX000+nwRkAAADgYeSyPsje3t537EYRFRUlSSpYsOAd97NarQoJCVHDhg3t4ViS3N3d1aBBAx04cEAJCQmp9ouMjNTPP/+sZs2a3bHlGgAAAJBcGJB9fX114sQJxcbGpli+b98++/o7iY6OVkJCghITE1OtS0hIUEJCgu40c90333wjYww3BgEAAMA9uSwgBwYGKj4+XitWrLAvs1qtCg0NVbVq1ewD+CIjIxUeHm7fpkCBAsqTJ482btyYootGbGystmzZonLlyt2xhfibb75RkSJF5O/vn45nBQAAgMzOZX2QK1eurMDAQE2cOFFRUVEqXry4wsLCFBkZqfHjx9u3GzlypHbv3q0jR45IkrJkyaKgoCAFBwera9euatOmjZKSkhQSEqLz589r5MiRqY71559/6siRI+rXr58sFkuGnSMAAAAyH5feavqjjz5ScHCwVq1apatXr8rHx0ezZs3621begQMHqmjRopo/f76mTZsmq9UqHx8fTZ069Y7zG3/99deSpFatWqXLeQAAAODh4bJbTWdWTPOW+TDNG1MePei4RpkD1ylz4DolY5q3f8dlfZABAACABxEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABwRkAAAAwAEBGQAAAHBAQAYAAAAcEJABAAAABy4NyFarVRMmTFD9+vXl5+enLl26aMeOHU7tu337dvXo0UO1atVSjRo11LVrV61du/aO2168eFFvvvmm6tevr0qVKqlJkyYaP358Wp4KAAAAHhJZ72djY4y2b9+ukydPKjo6WsaYFOstFosGDx7s9OONGjVKGzZsUM+ePVWiRAmFhYWpb9++WrBggapWrXrX/bZs2aKBAweqatWqGjp0qCRpzZo1Gj58uGJjY9W5c2f7tmfPntXzzz+vXLlyqWfPnsqXL5/Onz+vEydO3M+pAwAA4BFhMben3Ls4efKkBg8erOPHj6cKxvYHs1h06NAhpw68f/9+de7cWaNHj1bv3r0lSbdu3VKrVq1UsGBBLVq06K77vvTSSzpy5Ig2b94sDw8PScmt0Y0bN1aJEiW0cOFC+7Z9+vTRtWvXNH/+fGXPnt2p2u7l8uXrSkpy6j/ZP+btnVtnnyiWrsd4lDxx9rSioq65ugyX8fbO/Uiff2bANcocuE6ZA9cpmZubRQUK5HJ1GZmW0y3IY8eOVUREhF5//XXVrl1befPm/VcHXr9+vdzd3VO09mbLlk2dOnXSJ598oosXL6pgwYJ33Pf69evy8vKyh2NJ8vDwkJeXl7Jly2ZfFh4erp9++kmzZs1S9uzZdfPmTbm7uytr1vtqOAcAAMAjxOmkuHfvXvXq1Ut9+vRJkwMfOnRIpUqVUs6cOVMs9/PzkzFGhw4dumtArlmzpj777DMFBwerQ4cOkqTQ0FCdPHlSo0ePtm+3fft2ScnhuUOHDjp48KDc3d3VqFEjvfvuu8qfP3+anAsAAAAeHk4HZA8PDxUtWjTNDhwVFaVChQqlWu7t7S0peWDd3QwYMEARERGaOXOmZsyYIUny9PTU9OnTVa9ePft2p06dkiQNGzZM9evXV//+/XXs2DHNnDlTZ86c0YoVK5QlS5Y0OycAAABkfk4H5Pr16+uXX37Rc889lyYHjouLk7u7e6rlti4St27duuu+Hh4eKlmypAIDAxUQEKDExEQtX75cw4YN09y5c+Xn5ydJunHjhiSpUqVKmjRpkiSpWbNmyps3r8aMGaMtW7aoSZMm91U3/XkyJ2/v3K4uwaUe9fPPDLhGmQPXKXPgOuHfcjogjxo1St27d9eXX36p7t27p+j/+09kz55d8fHxqZbbgrFjX+LbjR07VgcOHFBISIjc3JJnqmvevLlatWqlcePGaenSpfZjSFKrVq1S7N+mTRuNGTNGv/zyy30H5IwapIe09SgP2GDAyoOPa5Q5cJ0yB65TMgbp/TtOB+Tnn39eN2/e1IQJEzRp0iQVLFjQHk5tLBaLNm3a5NTjeXt737EbRVRUlCTdtf+x1WpVSEiI+vfvn+L47u7uatCggZYsWaKEhARlzZrV3l2jQIECKR4jd+7c8vDwUExMjFO1AgAA4NHhdEAuUqRImh7Y19dXCxYsUGxsbIqBevv27bOvv5Po6GglJCQoMTEx1bqEhAQlJCTYp6GrUKGCJOnChQsptrty5YqsViuD9AAAAJCK0wF5wYIFaXrgwMBAffnll1qxYoV9HmSr1arQ0FBVq1bNPoAvMjJSN2/eVJkyZSQltwbnyZNHGzdu1JAhQ+z9mGNjY7VlyxaVK1fOvqxWrVrKly+fQkND1aFDB3uL84oVKyRJderUSdNzAgAAQObnsgmBK1eurMDAQE2cOFFRUVEqXry4wsLCFBkZmeI20CNHjtTu3bt15MgRSVKWLFkUFBSk4OBgde3aVW3atFFSUpJCQkJ0/vx5jRw50r5vtmzZ9Prrr+vNN99Unz591KRJE4WHh2vJkiVq2LAhARkAAACp3HdAjoiI0ObNm3X69GlJUrFixdS4cWMVL178vg/+0UcfKTg4WKtWrdLVq1fl4+OjWbNmyd/f/577DRw4UEWLFtX8+fM1bdo0Wa1W+fj4aOrUqQoICEixbadOneTu7q4vvvhC48ePV968edWrVy8NGzbsvusFAADAw8/pW01LUnBwsD7//PNU/X/d3NzUv39/vfLKK2le4IOGW01nPtxqmhHdDzquUebAdcocuE7JmMXi33G6BTkkJEQzZ85U1apV9dJLL6ls2bKSpKNHj2r27NmaOXOmihUrZr+zHQAAAJAZOR2QFy9erMqVK2vBggXKmvV/uxUvXlzPPPOMunXrpoULFxKQAQAAkKm5/f0mycLDw9WiRYsU4dgma9asatGihcLDw9O0OAAAACCjOR2Q3d3d7bduvpPY2Ng73joaAAAAyEycDsiVKlXSsmXLdOnSpVTrLl++rOXLl6ty5cppWhwAAACQ0Zzugzxo0CD17t1bLVq0UMeOHfXkk09Kko4dO6bQ0FDFxsZq4sSJ6VYoAAAAkBGcDsg1atTQlClTNHbsWM2ZMyfFuiJFiuiDDz5Q9erV07xAAAAAICPd141CGjVqpIYNG+r333/XmTNnJCXfKKRChQr22zgDAAAAmdl930nPzc1Nfn5+8vPzS496AAAAAJei2RcAAABwcNcW5EaNGsnNzU3r1q2Tu7u7Gjdu/LcPZrFYtGnTpjQtEAAAAMhIdw3ITzzxhKTk0CslD8QDAAAAHnZ3DcgLFiy4598AAADAw8jpPsiRkZGKi4u76/q4uDhFRkamSVEAAACAqzgdkBs3bqyNGzfedf13333nVD9lAAAA4EHmdEA2xtxzfVJSkr2/MgAAAJBZ3dc0b/cKwOHh4cqdO/e/LggAAABwpXveKCQsLExhYWH2v2fMmKHly5en2u7q1as6evSomjRpkvYVAgAAABnongE5JibGfktpi8WiK1eu6ObNmym2sVgs8vT0VMeOHTV8+PD0qxQAAADIAPcMyL169VKvXr0kSb6+vnrjjTfUunXrDCkMAAAAcIV7BmRHhw8fTs86AAAAgAfCfQ3SAwAAAB52TrcgS1JERITmzp2rffv2KSYmRklJSSnWWywWbdq0KU0LBAAAADKS0y3IR44cUfv27bVixQrFx8fr9OnT8vT01K1bt3T27FllyZJFhQsXTs9aAQAAgHTndED+9NNP5e7urlWrVmnu3LmSpDfeeEM//fSTxowZo5iYGL3zzjvpVScAAACQIZwOyHv37lXXrl1VunTpVDcM6dKli55++mlNnDgxzQsEAAAAMpLTATk2NlbFihWTJLm7u0uSbty4YV9frVo1/fLLL2lcHgAAAJCxnA7Ijz32mC5duiRJypUrl3LkyKGTJ0/a18fExCgxMTHNCwQAAAAyktOzWPj6+ur333+3/12zZk3Nnz9ffn5+SkpK0sKFC+Xr65suRQIAAAAZxekW5NatW+uvv/5SXFycJOmVV17RtWvX1LNnT/Xu3VvXrl3jVtMAAADI9JxuQW7RooVatGhh//upp57SmjVrtHHjRmXJkkVPP/20vY8yAAAAkFnd141Cble4cGH17NkzrWoBAAAAXI5bTQMAAAAO7tqCPHr06Pt+MIvFonHjxv2rggAAAABXumtADgsLu+8HIyADAAAgs7trQD58+HBG1gEAAAA8EOiDDAAAADj4RwH51KlT2rt3r65du5bW9QAAAAAudV8BecuWLWrSpIkCAwPVvXt3+531Ll++rICAAK1fvz5digQAAAAyitMBedeuXRoyZIi8vLw0ePBgGWPs6woUKKDixYtr7dq16VIkAAAAkFGcDsjTpk2Tj4+PVqxYoW7duqVaX6VKFR08eDBNiwMAAAAymtMB+cCBA2rTpo3c3O68y+OPP65Lly6lWWEAAACAKzgdkI0xcnd3v+v6v/76657rAQAAgMzA6YBcunRp7d27967rt2zZIl9f3zQpCgAAAHAVpwNyp06d9O2332rFihX2AXoWi0U3b97Ue++9p99++01dunRJt0IBAACAjHDXO+nd7oUXXtAvv/yi//73v/rwww9lsVj02muvKTo6WomJierQoYPatGmTnrUCAAAA6c7pgCxJEydOVLNmzbR69WodP35cxhj5+fmpXbt2atasWXrVCAAAAGQYpwJyXFyc1q9fr1KlSikgIEABAQHpXRcAAADgEk71Qfbw8NBbb72lP/74I73rAQAAAFzKqYDs5uamwoUL6/r16+ldDwAAAOBSTs9i0a5dO61evVpWqzU96wEAAABcyulBetWqVdPGjRvVtm1bvfDCCypRooRy5MiRarsaNWqkaYEAAABARnI6IL/44ov2f7///vuyWCwp1htjZLFYdOjQobSrDgAAAMhgTgfk8ePHp2cdAAAAwAPBqYBstVpVtGhReXt7q2TJkulcEgAAAOA6Ts9i0bt3b/3www/pXQ8AAADgUk4F5KxZs+qxxx6TMSa96wEAAABcyulp3gIDA7Vu3TolJSWlZz0AAACASzk9SK9z587atWuXXnzxRfXq1euu07wVKVIkTQsEAAAAMpLTAblVq1ayWCwyxmj37t133Y5p3gAAAJCZOR2QBw8enGruYwAAAOBh43RAHjp0aHrWAQAAADwQnB6kBwAAADwKnG5BlqSkpCSFhYVp48aNOnPmjCSpaNGiatq0qdq1ayc3N/I2AAAAMjenA3JcXJz69u2rn3/+WRaLRd7e3pKkH374QVu3btXKlSv1+eefK1u2bOlWLAAAAJDenG7ynTFjhvbs2aMXX3xRO3bs0NatW7V161bt3LlTQUFB2r17t2bMmJGetQIAAADpzumAvHbtWjVv3lwjRoyQl5eXfXmePHn0n//8R82bN9eaNWvSpUgAAAAgozgdkM+fP6+aNWvedX2NGjV0/vz5NCkKAAAAcBWnA3KePHkUERFx1/URERHKkydPmhQFAAAAuIrTAblu3bpatGiRfvzxx1TrfvrpJy1ZskT169dP0+IAAACAjOb0LBbDhg3TTz/9pH79+ql8+fIqW7asJOno0aM6dOiQ8uXLp5dffjndCgUAAAAygtMB+YknntBXX32lSZMmacuWLfrjjz8kSTlz5lTLli316quvqkiRIulWKAAAAJAR7utGIUWKFNGkSZNkjNGVK1ckSfnz55fFYkmX4gAAAICMdl8B2cZisahAgQJpXQsAAADgck4P0lu0aJF69+591/VBQUFaunRpWtQEAAAAuIzTATk0NFQlSpS46/qSJUvqq6++SpOiAAAAAFdxOiCfOnVK5cqVu+v6J598UqdOnUqTogAAAABXcboPckJCgqxW613XW61W3bp1674ObrVaNXnyZK1atUoxMTHy9fXV8OHDVadOnb/dd/v27ZoxY4b+/PNPJSUlqXTp0urVq5datGiRYjsfH5877v/uu+/q+eefv696AQAA8PBzOiCXLFlS27Zt04svvnjH9T/99JOKFy9+XwcfNWqUNmzYoJ49e6pEiRIKCwtT3759tWDBAlWtWvWu+23ZskUDBw5U1apVNXToUEnSmjVrNHz4cMXGxqpz584ptq9fv77atGmTYlnlypXvq1YAAAA8GpwOyC1bttTHH3+s4OBgDRo0SB4eHpKk+Ph4zZgxQ9u2bdOwYcOcPvD+/fu1Zs0ajR492j74r127dmrVqpUmTpyoRYsW3XXfRYsWydvbW/PmzbPX0aVLFzVu3FirVq1KFZBLly6ttm3bOl0bAAAAHl1OB+TevXvrhx9+0MyZM7VkyRKVLl1aknT8+HFdvXpV1atXv2vr8p2sX79e7u7uKcJstmzZ1KlTJ33yySe6ePGiChYseMd9r1+/Li8vL3s4liQPDw95eXkpW7Zsd9wnLi5OFovlrusBAAAA6T4G6bm7u+vLL7/Ua6+9pscff1yHDh3SoUOHVLhwYf3nP//RnDlzUgTWv3Po0CGVKlVKOXPmTLHcz89PxhgdOnTorvvWrFlTR48eVXBwsCIiIhQREaHg4GCdPHlSQUFBqbYPCQlRlSpV5Ofnp9atW2vjxo1O1wkAAIBHy33dKMTd3V19+/ZV3759//WBo6KiVKhQoVTLvb29JUkXL168674DBgxQRESEZs6cqRkzZkiSPD09NX36dNWrVy/FtlWrVlWLFi1UtGhRnTt3TvPnz9eQIUM0adIktWrV6l+fBwAAAB4u/+hOemkhLi5O7u7uqZbbukDca0YMDw8PlSxZUoGBgQoICFBiYqKWL1+uYcOGae7cufLz87Nve/vNS9q3b69WrVppwoQJatmy5X3fJrtAgVz3tT0eDN7euV1dgks96uefGXCNMgeuU+bAdcK/5bKAnD17dsXHx6dabgvG9+orPHbsWB04cEAhISFyc0vuJdK8eXO1atVK48aNu+cd/Tw9PfXcc89p0qRJOn78uMqUKXNfdV++fF1JSea+9rlfvLDTXlTUNVeX4DLe3rkf6fPPDLhGmQPXKXPgOiVzc7PQqPcvON0HOa15e3vfsRtFVFSUJN11gJ7ValVISIgaNmxoD8dScvePBg0a6MCBA0pISLjnsQsXLixJunr16j8tHwAAAA8plwVkX19fnThxQrGxsSmW79u3z77+TqKjo5WQkKDExMRU6xISEpSQkCBj7t3Ce/r0aUlS/vz5/0npAAAAeIi5LCAHBgYqPj5eK1assC+zWq0KDQ1VtWrV7AP4IiMjFR4ebt+mQIECypMnjzZu3Jiii0ZsbKy2bNmicuXK2fs2X7lyJdVx//rrLy1evFhFixZVyZIl0+nsAAAAkFm5rA9y5cqVFRgYqIkTJyoqKkrFixdXWFiYIiMjNX78ePt2I0eO1O7du3XkyBFJUpYsWRQUFKTg4GB17dpVbdq0UVJSkkJCQnT+/HmNHDnSvu+iRYu0efNmNWzYUEWKFNGFCxe0bNkyXblyRdOmTcvwcwYAAMCDz2UBWZI++ugjBQcHa9WqVbp69ap8fHw0a9Ys+fv733O/gQMHqmjRopo/f76mTZsmq9UqHx8fTZ06VQEBAfbtqlatql9++UUrVqzQ1atX5enpqSpVqqh///5/ewwAAAA8mizm7zrsOoiMjNSyZct08uRJRUdHp+rra7FYNG/evDQv8kGSUbNYnH2iWLoe41HyxNnTj/SIZkZ0P/i4RpkD1ylz4DolYxaLf8fpFuStW7dqyJAhio+Pl6enp/LmzZuOZQEAAACu4XRA/vjjj5UvXz5NmzZNlSpVSs+aAAAAAJdxehaL48ePq1evXoRjAAAAPNScDsj58+e/462hAQAAgIeJ0wG5bdu22rBhQ3rWAgAAALic032Q27dvr127dmngwIHq2bOnihYtqixZsqTarkiRImlaIAAAAJCRnA7IzZs3l8VikTFG33///V23O3ToUFrUBQAAALiE0wF58ODBslgs6VkLAAAA4HJOB+ShQ4emZx0AAADAA8HpQXoAAADAo8DpFmSbxMREHT9+XFevXk11q2lJqlGjRpoUBgAAALjCfQXkWbNm6fPPP9f169fvug2D9AAAAJCZOd3FYsWKFfr444/l6+urYcOGyRijXr16qU+fPvLy8lLFihU1bty49KwVAAAASHdOB+QlS5aoSpUqWrBggbp06SJJeuaZZ/T6669r9erVOnv2rBITE9OtUAAAACAjOB2Qjx8/rsDAQEmyT/eWlJQkSSpYsKC6dOmi+fPnp0OJAAAAQMZxOiC7ubkpR44ckiRPT09JUnR0tH39E088oVOnTqVtdQAAAEAGczogFylSRGfOnJEkeXh4qHDhwvr555/t6w8cOCAvL6+0rxAAAADIQE7PYlG9enV9//33eu211yRJgYGBmjdvnuLi4mSM0erVq9WxY8d0KxQAAADICE4H5J49e8rX11dxcXHKnj27hg4dqhMnTmjlypWSpHr16tnDMwAAAJBZOR2QS5curdKlS9v/9vT01MyZM3Xt2jW5ubkpZ86c6VIgAAAAkJHu+056t8udO3da1AEAAAA8EO47IN+8eVNnz55VdHQ0t5oGAADAQ8fpgHzjxg2NHz9eK1euVEJCQqr1xhhZLBZuNQ0AAIBMzemA/M477+jrr79WQECA/P39mdINAAAADyWnA/LmzZvVqVMnvffee+lZD5Ap5MqTQzmy/esu/BnG2/vBHytw81aCrsfcdHUZAAA4H5Dd3d1VqVKl9KwFyDRyZMuq2u986+oyHio7/6+Zrru6CAAAdB930qtVq5b27duXnrUAAAAALud0QB41apR27typefPmKT4+Pj1rAgAAAFzG6S4WRYoU0fDhwzVy5EhNmDBB3t7ecnNLma8tFos2bdqU5kUCAAAAGcXpgBwaGqo333xT7u7uKlWqlPLkyZOedQEAAAAu4XRAnjlzpsqXL68vvvhC+fPnT8+aAAAAAJdxug/yhQsX1LFjR8IxAAAAHmpOB+RSpUrp6tWr6VkLAAAA4HJOB+T+/ftr8eLFOn/+fHrWAwAAALiU032Qw8PDVahQITVv3lwBAQEqWrToHWexGDx4cJoXCQAAAGQUpwPy1KlT7f9evXr1HbchIAMAACCzczogb968OT3rAAAAAB4ITgXkGzduKCwsTJUrV1aDBg3SuyYASBO583oou3s2V5fhFG/v3K4uwSlx8bd0Ldrq6jIAIF05FZA9PT312Wef6e23307vegAgzWR3z6Y2K1u6uoyHyup2a3RNBGQADzenZ7EoXry4oqKi0rMWAAAAwOWcDsgvvPCCVqxYob/++is96wEAAABcyulBejlz5pSXl5cCAwPVvn17lShRQjly5Ei1Xbt27dKyPgAAACBDOR2QR40aZf/33Llz77iNxWIhIAMAACBTczogz58/Pz3rAAAAAB4ITgfkmjVrpmcdAAAAwAPB6UF6AAAAwKPA6RZkmwMHDmj//v26evWqkpKSUqzjVtMAAADI7JwOyHFxcRoyZIi2bdsmY4wsFouMMZJk/zcBGQAAAJmd010spk2bpm3btmnAgAGaP3++jDH64IMP9Pnnn6t69eqqVKmS1qxZk561AgAAAOnO6YD87bffKjAwUK+88orKli0rSSpUqJAaNGigOXPmKD4+XmFhYelWKAAAAJARnA7I586dU40aNSRJWbJkkSTFx8dLkrJmzaqWLVvSggwAAIBMz+mAnDNnTiUmJtr/7ebmposXL9rX586dW5cuXUr7CgEAAIAM5HRALl68uE6ePCkpuQX5ySef1LfffitJMsZo48aNKly4cLoUCQAAAGQUpwNynTp19O2339pbkbt27aoff/xRTZo0UdOmTbV9+3Z17Ngx3QoFAAAAMoLT07z169dPbdu2tU/t1q1bN1mtVq1evVpubm4aPny4+vbtm26FAgAAABnB6YCcM2dOlS5dOsWyF198US+++GKaFwUAAAC4CreaBgAAABzcV0A+d+6cRo8eraeffloVK1bUjh07JElXrlzR6NGjtX///nQpEgAAAMgoTgfk06dPq2PHjtqwYYPKli1rH6wnSfnz59fvv/+ukJCQdCkSAAAAyChO90EODg6Wm5ubvvnmG2XLlk1169ZNsf6ZZ57Rli1b0rxAAAAAICM53YK8fft2Pf/88ypcuLAsFkuq9UWKFNH58+fTtDgAAAAgozkdkK9fv66CBQvedX18fHyKbhcAAABAZuR0QC5cuLCOHj161/X79u1T8eLF06QoAAAAwFWcDsgBAQH66quv9Oeff9qX2bpafPvtt1q/fr2aN2+e9hUCAAAAGcjpQXoDBw7U999/ry5duqh69eqyWCz6/PPP9cknn2j//v0qX768goKC0rNWAAAAIN053YKcK1cuLVu2TJ06ddLvv/8uY4y2bdumEydO6IUXXtD8+fOVLVu29KwVAAAASHdOtyBLySH5rbfe0ltvvaUrV67IGKP8+fPfcVYLAAAAIDO6r4DsKH/+/GlZBwAAAPBAuO+AvHbtWm3atEmnT5+WJBUrVkxNmjRRixYt0rw4AAAAIKM5HZBv3LihwYMHa+fOnTLGKE+ePJKkAwcOaN26dVq2bJlmzJghT0/PdCsWAAAASG9OD9L75JNPtGPHDnXv3l0//vijdu/erd27d+vHH39U9+7dtWvXLn3yySfpWSsAAACQ7pwOyOvWrVNgYKDefPNNeXt725d7e3vrzTffVNOmTbVu3bp0KRIAAADIKPd1q+latWrddX3t2rV1/fr1NCkKAAAAcBWnA7KPj49OnTp11/WnTp1SuXLl0qQoAAAAwFWcDsjDhg3T8uXL9d1336Vat2nTJq1YsULDhw9P0+IAAACAjOb0LBarV69W0aJFNXjwYJUqVUplypSRJIWHh+vEiRMqV66cVq9erdWrV9v3sVgsGjduXNpXDQAAAKQTpwNyWFiY/d/Hjx/X8ePHU6w/cuSIjhw5kmIZARkAAACZjdMB+fDhw2l+cKvVqsmTJ2vVqlWKiYmRr6+vhg8frjp16vztvtu3b9eMGTP0559/KikpSaVLl1avXr3uecOSffv2qWvXrjLGaM+ePfa5nAEAAAAbp/sgp4dRo0Zp3rx5atOmjd588025ubmpb9+++vXXX++535YtWxQUFKSEhAQNHTpUr7zyitzc3DR8+HCtWLHijvsYY/Tee+8pR44c6XEqAAAAeEj844CckJCgX375RevWrdPRo0fve//9+/drzZo1ev311zVixAh17dpV8+bNU+HChTVx4sR77rto0SJ5e3tr3rx56t69u7p376558+apYMGCWrVq1R33CQsLU0REhDp27HjftQIAAODRcc8uFrt27dLGjRs1cOBAFShQwL789OnTGjx4cIpg3K5dO40fP97pA69fv17u7u7q3LmzfVm2bNnUqVMnffLJJ7p48aIKFix4x32vX78uLy8veXh42Jd5eHjIy8tL2bJlu+P2H3/8sYYMGaLo6GinawQAAMCj554tyGFhYfrxxx9ThGNJGj16tP78809VrVpVvXv31pNPPqmVK1emGMj3dw4dOqRSpUopZ86cKZb7+fnJGKNDhw7ddd+aNWvq6NGjCg4OVkREhCIiIhQcHKyTJ08qKCgo1fbTp09Xrly59PzzzztdHwAAAB5N92xB3r9/v+rXr59iWXh4uH7++WfVqFFDCxYskCS98sorateunVauXKn27ds7deCoqCgVKlQo1XLbbawvXrx4130HDBigiIgIzZw5UzNmzJAkeXp6avr06apXr16KbU+ePKn58+drypQpyprV6TGJd1WgQK5//RjIeN7euV1dApzAdcocHuXr9Cife2bCdcK/dc/EeOnSJZUoUSLFst27d8tisahTp072ZdmzZ1erVq20cOFCpw8cFxcnd3f3VMttXSRu3bp11309PDxUsmRJBQYGKiAgQImJiVq+fLmGDRumuXPnys/Pz77t+PHjVaNGDT377LNO13Yvly9fV1KSSZPHuhte2GkvKupamj4e1yh9cJ0yh7S+TpmFt3fuR/bcMxOuUzI3NwuNev/CPQOy1WpV9uzZUyw7cOCApORuDo4KFy6s69evO33g7NmzKz4+PtVyWzC+U19im7Fjx+rAgQMKCQmRm1tyL5HmzZurVatWGjdunJYuXSpJ+uGHH/Tjjz/eV9cPAAAAPNru2Qe5cOHCqWao2Lt3rwoUKKDChQunWB4XF6fcuZ1vrfH29r5jN4qoqChJuusAPavVqpCQEDVs2NAejiXJ3d1dDRo00IEDB5SQkCBJmjBhgho1aqScOXPqzJkzOnPmjGJiYiRJkZGR9+zGAQAAgEfTPVuQq1evrlWrVqlz584qV66cNm7cqFOnTt2xn/GRI0fu2Kf4bnx9fbVgwQLFxsamGKi3b98++/o7iY6OVkJCghITE1OtS0hIUEJCgoxJ7gJx7tw5/fnnn9q4cWOqbdu2bavKlStr+fLlTtcMAACAh989A3K/fv309ddfq23btsqbN6+io6Pl7u6eaqaIxMREfffdd2rWrJnTBw4MDNSXX36pFStWqHfv3pKSW4dDQ0NVrVo1e9iOjIzUzZs3VaZMGUlSgQIFlCdPHm3cuFFDhgyx92OOjY3Vli1bVK5cOfuyiRMn2luTbdasWaO1a9dqwoQJqVrBAQAAgHsG5GLFimnBggWaNm2aTp06JT8/Pw0cOFBly5ZNsd2uXbuUL18+NW7c2OkDV65cWYGBgZo4caKioqJUvHhxhYWFKTIyMsV8yiNHjtTu3bt15MgRSVKWLFkUFBSk4OBgde3aVW3atFFSUpJCQkJ0/vx5jRw50r5vw4YNUx3XNn1cw4YNudU0AAAAUvnbec8qVaqkmTNn3nObunXr6uuvv77vg3/00UcKDg7WqlWrdPXqVfn4+GjWrFny9/e/534DBw5U0aJFNX/+fE2bNk1Wq1U+Pj6aOnWqAgIC7rsOAAAAwMZibB124ZSMmubt7BPF0vUYj5Inzp5Ol+nDar/zbZo+5qNu5/81S5fr1GZlyzR9zEfd6nZrHtkptJg+LHPgOiVjmrd/556zWAAAAACPGgIyAAAA4ICADAAAADggIAMAAAAOCMgAAACAAwIyAAAA4ICADAAAADggIAMAAAAOCMgAAACAAwIyAAAA4ICADAAAADggIAMAAAAOsrq6AADAo80rd3Z5ZHd3dRlO8fbO7eoS/pY1Ll5Xr8W5ugwgUyMgAwBcyiO7uz5ru9DVZTw0+q/qLhGQgX+FLhYAAACAAwIyAAAA4ICADAAAADigDzIAAPhbBbyyyM3D09VlOCUzDKZMst7Q5auJri4Dd0FABgAAf8vNw1N61+LqMh4abu8aSddcXQbugi4WAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgAMCMgAAAOCAgAwAAAA4ICADAAAADgjIAAAAgIOsrjy41WrV5MmTtWrVKsXExMjX11fDhw9XnTp1/nbf7du3a8aMGfrzzz+VlJSk0qVLq1evXmrRooV9m+joaI0fP1779+/X+fPn5ebmppIlS6pHjx5q27atLBZLep4eAAAAMiGXBuRRo0Zpw4YN6tmzp0qUKKGwsDD17dtXCxYsUNWqVe+635YtWzRw4EBVrVpVQ4cOlSStWbNGw4cPV2xsrDp37ixJun79uk6fPq2AgAAVLlxYSUlJ2r59u0aOHKlTp07plVdeyZDzBAAAQObhsoC8f/9+rVmzRqNHj1bv3r0lSe3atVOrVq00ceJELVq06K77Llq0SN7e3po3b548PDwkSV26dFHjxo21atUqe0AuWrSoFi9enGLfbt26acCAAZo3b55efvllWpEBAACQgsv6IK9fv17u7u72MCtJ2bJlU6dOnbR3715dvHjxrvtev35dXl5e9nAsSR4eHvLy8lK2bNn+9thPPPGEbt68qfj4+H93EgAAAHjouCwgHzp0SKVKlVLOnDlTLPfz85MxRocOHbrrvjVr1tTRo0cVHBysiIgIRUREKDg4WCdPnlRQUFCq7W/duqUrV67ozJkzWrlypUJDQ+Xv758iYAMAAACSC7tYREVFqVChQqmWe3t7S9I9W5AHDBigiIgIzZw5UzNmzJAkeXp6avr06apXr16q7VesWKGxY8fa/65Tp44++OCDf3sKAAAAeAi5LCDHxcXJ3d091XJbF4lbt27ddV8PDw+VLFlSgYGBCggIUGJiopYvX65hw4Zp7ty58vPzS7F9kyZNVLp0af3111/6/vvvFRUVpZs3b/6jugsUyPWP9oNreXvndnUJcALXKXPgOj34uEaZA9fpweWygJw9e/Y79gG2BeN79SUeO3asDhw4oJCQELm5JfcSad68uVq1aqVx48Zp6dKlKbZ//PHH9fjjj0uSWrZsqXfffVcvvvii1q9fr+zZs99X3ZcvX1dSkrmvfe4XL5i0FxV1LU0fj2uUPrhOmQPX6cGX1tdI4jqlh/S4TjZubhYa9f4Fl/VB9vb2vmM3iqioKElSwYIF77if1WpVSEiIGjZsaA/HkuTu7q4GDRrowIEDSkhIuOexmzVrpnPnzmnPnj3/4gwAAADwMHJZQPb19dWJEycUGxubYvm+ffvs6+8kOjpaCQkJSkxMTLUuISFBCQkJMubeLby2Vupr19LvmxsAAAAyJ5cF5MDAQMXHx2vFihX2ZVarVaGhoapWrZp9AF9kZKTCw8Pt2xQoUEB58uTRxo0bU3TRiI2N1ZYtW1SuXDl73+YrV67c8dghISGyWCyqUKFCepwaAAAAMjGX9UGuXLmyAgMDNXHiREVFRal48eIKCwtTZGSkxo8fb99u5MiR2r17t44cOSJJypIli4KCghQcHKyuXbuqTZs2SkpKUkhIiM6fP6+RI0fa9120aJE2bdqkhg0b6oknntDVq1e1ceNG7du3Ty+88IJKlCiR4ecNAACAB5tLbzX90UcfKTg4WKtWrdLVq1fl4+OjWbNmyd/f/577DRw4UEWLFtX8+fM1bdo0Wa1W+fj4aOrUqQoICLBvV6dOHR0+fFgrV67U5cuX5e7uLh8fH73//vvq2LFjep8eAAAAMiGXBuRs2bJp5MiRKVp9b7dgwYI7Lm/durVat259z8evXr26qlev/q9qBAAAwKPFZX2QAQAAgAcRARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMABARkAAABwQEAGAAAAHBCQAQAAAAcEZAAAAMBBVlce3Gq1avLkyVq1apViYmLk6+ur4cOHq06dOn+77/bt2zVjxgz9+eefSkpKUunSpdWrVy+1aNHCvs25c+cUEhKirVu36tSpU3Jzc1O5cuU0aNAgp44BAACAR49LW5BHjRqlefPmqU2bNnrzzTfl5uamvn376tdff73nflu2bFFQUJASEhI0dOhQvfLKK3Jzc9Pw4cO1YsUK+3abN2/WF198oRIlSmjYsGEaNGiQYmNj1bt3b61cuTKdzw4AAACZkctakPfv3681a9Zo9OjR6t27tySpXbt2atWqlSZOnKhFixbddd9FixbJ29tb8+bNk4eHhySpS5cuaty4sVatWqXOnTtLkmrVqqUtW7Yof/789n2ff/55tW3bVp9++qnatWuXbucHAACAzMllLcjr16+Xu7u7PcxKUrZs2dSpUyft3btXFy9evOu+169fl5eXlz0cS5KHh4e8vLyULVs2+7KyZcumCMe27Z555hmdPXtWcXFxaXhGAAAAeBi4rAX50KFDKlWqlHLmzJliuZ+fn4wxOnTokAoWLHjHfWvWrKnPPvtMwcHB6tChgyQpNDRUJ0+e1OjRo//22FFRUfL09EwRpp3l5ma5733+iSxFi2bIcR4V6XHdCufNnuaP+ahLj+tU0PPO7yP459LjOuUqmPPvN4LT0u2zKm+J9HncR1R6ZoqMyisPK4sxxrjiwK1atVKhQoU0e/bsFMuPHTumli1b6r333kvRuuzoxo0beuONN7R+/XrZyvf09NTkyZP19NNP3/O4p06dUuvWrdWyZUuNHz8+bU4GAAAADw2XtSDHxcXJ3d091XJbq+6tW7fuuq+Hh4dKliypwMBABQQEKDExUcuXL9ewYcM0d+5c+fn53XG/mzdv6pVXXlGOHDk0fPjwtDkRAAAAPFRcFpCzZ8+u+Pj4VMttwfhe3R/Gjh2rAwcOKCQkRG5uyd2omzdvrlatWmncuHFaunRpqn0SExM1fPhwhYeHa/bs2XftvgEAAIBHm8sG6Xl7e99xIF5UVJQk3TXAWq1WhYSEqGHDhvZwLEnu7u5q0KCBDhw4oISEhFT7vfXWW9q6das+/PBD1axZM43OAgAAAA8blwVkX19fnThxQrGxsSmW79u3z77+TqKjo5WQkKDExMRU6xISEpSQkKDbu1V/+OGHCg0N1RtvvJHiRiIAAADA7VwWkAMDAxUfH5/ixh5Wq1WhoaGqVq2aChUqJEmKjIxUeHi4fZsCBQooT5482rhxY4ouGrGxsdqyZYvKlSuXom/zF198oS+//FIDBgxQjx49MuDMAAAAkJm5rA9y5cqVFRgYqIkTJyoqKkrFixdXWFiYIiMjU8wuMXLkSO3evVtHjhyRJGXJkkVBQUEKDg5W165d1aZNGyUlJSkkJETnz5/XyJEj7ftu3LhREyZMUMmSJVW6dGmtWrUqRQ0BAQHy9PTMmBMGAABApuCygCxJH330kYKDg7Vq1SpdvXpVPj4+mjVrlvz9/e+538CBA1W0aFHNnz9f06ZNk9VqlY+Pj6ZOnaqAgAD7docPH5YknTx5UiNGjEj1OJs3byYgAwAAIAWXzYMMAAAAPIhc1gcZAAAAeBARkAEAAAAHBGQAAADAAQEZAAAAcEBARqaQlJTk6hIAAMAjgoCMTMHNzU1JSUlav369JKW6WyIePsaYO94xEw+2xMREXp+4K17TSG9JSUlp8h5EQEamkJSUpFmzZmnYsGGKiYmRxWJxdUlIR4mJibJYLMqSJYtiY2N18uRJQtcDzvaFJkuWLLJYLDp58qQiIiIIREghS5YskqRjx465uBI8jBISEuTm5iaLxaKYmBhZrVZJ/6xRjYCMB4ox5o7dKdzc3JQzZ0499thj9hvA4OFl+xCdMWOGOnTooDfeeEMHDhxwcVW4F9sXmvPnz2v48OHq2bOnJk+erKNHj7q6NDxgPvzwQ/Xr109nzpxxdSl4yGTNmlU3btzQBx98oIEDB2ratGmS9I8a1QjIeKBYLBa5ubnJarXav/ElJCRIkmrUqKFLly4pPj5eEt0sHmYHDx5U+/btNW/ePD399NNq0KCBSpQo4eqycBe21+LKlSvVpk0bnTp1Sl27dlVAQIB8fX1dXB1c5fbGDtvf3t7eun79uvLnz++KsvAQW7lypRo3bqzNmzfrySefVM6cOf9xVnDpraYBSfafZW1WrVql6dOn69VXX1WzZs2UNWtWGWP02GOPqXDhwvrpp59Ur149F1aM9LZkyRIlJibq448/VoUKFeTl5eXqknAPFotF169f1/z589WgQQP1799fJUqUULZs2VxdGlzIze1/bXBJSUn2v8uUKaO4uDjt3LlTjRo1kjGGbnO4L7bQ6/i8OXr0qKZPn65nnnlGL7zwgkqXLq1cuXL942PQggyXsbUm2MJxRESEJMnHx0fGGI0dO1aLFy+WlPwicHd3V758+RQZGambN2/yhprJ2X4ZuN25c+f0/fffq0mTJqpbt668vLwUHx+vGzduKC4uLoOrhKN7DZzct2+fzp49q8aNG6tcuXLKli2boqOjdePGDV25coW+yI8AY0yK1rpffvlFr7/+ui5evJgiLBcoUEBZs2bVuXPnJP2zn7/x6EpISJDFYpHFYlF8fLz9OffTTz8pMjJSr776qvz8/JQrVy57H2Sb+2lNpgUZLnF7q/E777yjPXv2aPz48apcubKmTZumOXPmaMyYMYqOjtZzzz2n/Pnzq3Tp0jpy5Ihy5MhBq0MmlzVr8tvP4cOHVahQIeXLl0+SVLhwYeXLl0+//PKL9uzZo5MnT2rXrl2KjIzUiRMn1KFDB3Xs2FGlS5fmOZCBbK/ZLFmy6Nq1azpz5ozy58+vggULymKxqHTp0rJardq2bZty586t7du368SJEwoPD5fValWbNm308ssvy93d3dWngnRiey3Gx8fL3d1d27dv1+bNm3Xy5Em99dZbqlKliiSpYsWKypcvn06cOCEp9ecBYJOQkGD/rLC932fNmlWJiYmaPXu2oqOjVadOHTVo0ECFChWSu7u7Tp8+LU9PT61du1anTp3SyZMn5ebmpmHDhql06dJOH5sWZLiE7UN2/PjxCgkJUXh4uOLi4rRx40ZJUtmyZfXee+/phRde0KJFi/Tuu+9KkmrWrKnTp0/r5MmTslgs9EPORG4fgLl9+3Y1b95cXbp0UevWrbVo0SLduHFDkvTWW2/p+PHj6tGjhz744ANduHBBBQsW1NNPP605c+Zo/vz5kmh5Sg+2D5Xb2QLM1KlT1aRJE/Xp00fNmzfX+PHjFRERocKFC6tv3776+eef1bdvX3333XeyWCwKCAhQ+fLlNWfOHC1ZsiSjTwfp7PZfgpYsWaK+fftKkvr3768lS5bowoULGjlypDZs2CBJunXrlipUqKC9e/fKGEM4Rgpbt25Vnz59JP2vIUX63/v9unXr9PTTT2vJkiU6evSoLl++LCm5646/v7969eql2rVr69NPP9WWLVt04cIF/fzzzxoxYoTOnj3rdB20ICND2b4B/vTTT3r99dfl5eWlv/76SwkJCYqJidGaNWtUr1491alTR25ubho9erSefPJJjRs3TiNGjFCuXLlUqFAhHT16VCVLliQgZSK2n8RiY2N17tw5TZkyReXKlVOzZs30xx9/aPz48cqaNavatWunWrVqae7cufrrr7/0+OOPy83NTYULF5YkHT9+XKdPn1ZsbKxy5szp4rN6uPz666969dVXNXjwYA0YMCBFa298fLzGjBmjzZs3q0uXLipevLiOHDmiNWvW6ODBg1q0aJEGDRqk1q1by2q1KleuXMqRI4fy5MmjmJgYtWzZUpGRkbQWPmRsAWbr1q26ceOG/vzzT+3cuVNbt27VM888I19fXwUHB+vzzz/XiBEjdPPmTbVt21ZFixbVoUOHdO7cORUpUsTFZ4EHye+//65t27bZn0OO9u/fr0mTJqlSpUrq16+fSpYsaR/s6ePjo8mTJ2vlypXKmzevChYsqBIlSqhgwYL64Ycf1K9fP0VGRqpo0aJO1UFARoayBdoVK1bI09NT77//vqpUqSKLxaJvv/1Wr776qlauXKlKlSopV65ccnd31wsvvKCCBQtqypQpio6O1oULF+wzWfBhm7mEhYVpypQpql27tuLj4zVs2DCVKlVKktSnTx999tlnKlGihGrXrp3qp7CEhASFh4frypUrKl++POE4jZw+fVqFChWSh4eHqlatqhdffFHVq1dP1RXi2LFj2rBhg3r27Knnn3/e/qGUlJSkxYsXa+XKlWrXrp2KFSuWYr/Y2Fjt2LFDN2/elK+vL6/Xh0x4eLhGjRqliIgIPf744/blM2bMUM2aNZUjRw75+/urTJkyevvttzVu3DhFRESoTp06+vLLL+2/KtFdCrbnQIcOHVS5cmXVr1/fvs72Wb9+/XpdunRJU6dOTTFDju15lDNnTnXr1i3F4yYkJOjPP/9M0RrtDLpYIN3YBuXcPtXP2bNntW/fPvn7+6t69er2J22LFi3UsWNHff/999q8ebOk/3Wob9KkiUaOHKlKlSpJkn788UdJ4sP2AXX7NbddR19fX124cEFr1qxR9+7dVapUKfu248aN07lz5/TNN98oOjo6xf7Hjx/X+vXrNWHCBCUlJalVq1YZch4Pu59//llt27bVhAkT7MtGjhypOnXqyGq1phgUefjwYVmtVg0ePFj58+fXyZMnNWTIEC1fvlzNmzfXk08+meKxr127pl9//VXLli3TpEmTVL58edWoUSPDzg0ZY86cOYqIiNDYsWM1depULV26VC1bttRvv/2mZcuWSUp+P8ibN68+/PBDde/eXZ999pmCg4OVNWtW7dixQxLdpZDMGKPChQvbw/H27dslyT4g77ffftOTTz5pD8e2zxY3N7cUA0Ftjh8/rm+++UarV69Wo0aN5Ofn53QtBGSkOVswzpIlixITE3Xr1q0U6728vHT16lXlzJlTSUlJKcLUq6++KqvVqpUrVyoyMlIWi8Xex61u3br68MMPVaJECV26dEnXrl3LuJNCKnfq/22b5cD2RnXz5k1J//vwK1++vAYNGqRbt27ZB+i4ubkpISFBhQoV0ksvvaTVq1fb3xSvXr2qvn37qm/fvhozZoxiYmI0ZcoU1axZMyNO8aFXsmRJ1a9fX5s2bdLJkyclJb9+f/vtNz377LP66aef7NuePn1aBQsW1OHDhzVp0iS1atVKZ8+e1ccff6y3335bFStWtG974sQJNWzYUG+88YamTp2qGjVq6LPPPtMTTzyR0aeINHCnmzdJ0uXLl/XNN9+oadOmatq0qYoVK6YcOXJo2LBhatasmWbMmKELFy7Izc1NSUlJypEjhwYPHqz33ntPERERSkhI0JUrVzL4bPAgsH1+REZGatu2bbp69aq9G57Nhx9+qKCgIJ07d05ubm5yd3fXY489Zh+r4virg+MMKklJSfrxxx/Vr18/jRw5Um+99ZZKlCiht99+W9mzZ3e6RgIy/jFbiLmdrVV30aJFeuGFF9S7d28NGjRI+/fvt/dNrF69uvbu3Wu/LWSWLFlktVpVoEABNWrUSHv37tU333wj6X993KxWq3LmzKnWrVvr2LFjypEjR8acKFK5cuWK/vrrL0lKMX2X7W5qFy5c0Ntvv63XX39d7777rtavX2/f5vnnn1ehQoW0f/9+nT592r6fJL322mvKlSuXvvrqK509e1ZeXl5q3bq1mjRpovHjx2v58uWqWLFiqumkcP9sc4u3adNGFotFU6ZMkZT8+i1VqpSMMVq1apUiIyMlJQ+QPXXqlLp3766wsDC9/vrrmjFjhpo0aWKfgeT777/X9evXVapUKQ0dOlRt27ZVSEiI3n//fXl6ejLVWyZja8C4U8uclPzTdbZs2ezzlNu6vhUrVkydO3dWYmKiZs6cKel/cyK7ubmpXbt2GjdunEqUKKHff//dfiw8/MLDw5WUlGR/z9+zZ4/eeecd+wD9/fv3a/z48bp27ZoaN26sIkWKaOzYsZKSn28VKlTQ8ePHtXXrVnsDmi0oWywWXbhwwb4sb968KlOmjObMmaMpU6boscceu6/nGQEZ/8iaNWsUFBSkTZs2pVp35coVDR06VBMmTNDjjz+uXLly6cCBA+rfv79WrVolSWratKmOHj2q5cuXS0r+sPbw8JCUHLiyZs2qH374QQcPHky1PmfOnIqOjrbPm4yMdeLECXXu3FlDhw6V9L8vRLbA+vXXX6tFixbatWuXrly5orVr12rYsGGaNWuWoqOjlT9/fg0YMEA7duzQTz/9ZO9bZpuvctSoUdq2bZvWrFkjY4zatGmj0aNHq3HjxpKSnx+3tzTg/tnCTJMmTdSoUSP99NNP9q5LXl5eGjlypDZu3Khdu3YpPj5e1apVU4MGDZSYmKgxY8aod+/e9gGUN2/e1JIlS/Tee+9pz549kqSePXtqwIABKl26tD1o0SXqwXTx4kX769fxi6ftZ+tDhw7p3Xff1aRJk7Ry5UqdP39eUnKozZcvnw4ePKiYmBi5u7vbvwSVL19ePj4+Wr58uT0E2/aRpHr16qlBgwY6evSooqOj7xrC8fA4fPiwWrZsqWHDhtmXNW/eXLGxsQoJCdGAAQPUpUsXRUZG6saNG/Lz81OHDh303XffaceOHcqaNauqV6+ukiVLaty4cbJarcqaNas9KP/www/6v//7P+3fv19PP/203nrrLX3wwQeqUaNGql83ncEzEv9IxYoVVbt2bU2bNi3VRNx79+7V9u3bNWTIEP3f//2fZs+ereXLlytv3ryaNm2afv75ZwUGBqpBgwaaMmWKfvrppxTfJi9fvqyOHTvqt99+07Zt2+zfDhMTE3Xp0iXt3LlTxYsXV6FChVxx6o+8J554Qv7+/ipevLji4uLsH3gWi0VxcXGaO3euKlWqpMmTJ2vJkiUKCQlRx44dNWXKFK1fv16JiYl6/vnnValSJS1ZskR//vmnJNkHhbVp00YlS5bUX3/9laLF8fYby+Df8fDwkNVq1eHDh1WxYkUVKVLE3tonSa1atZK/v7/mz5+v06dPy8PDQ927d9fNmze1du1a+3RJx48fV1hYmBYuXKhKlSrZu1rYPohsLZAEoAfTkiVL1LFjR/u4D0fx8fGaMGGCOnXqpB07dig0NFSjRo1SUFCQwsPDVbhwYdWtW1d79+7V999/n2LfAgUKqGDBgkpMTLT/OiH973nh6empbNmyKT4+nl8WHnK2bjTe3t7q3LmzChYsKGOMbt68KQ8PDz355JM6ePCg9u3bp7Fjx+qtt97SY489Jg8PDwUEBKhChQr2VmR/f3+98MILunDhgvr06aNNmzZp//79Wr16taZNm6aLFy/af9HKkyePJNlbrO/7s8MA/0BSUpJZv369qVChgpkzZ44xxpjExESTkJBg3n77bVOjRg1z7do1Y4wx8fHxxhhjtm/fbmrUqGFeeeUVk5CQYA4ePGiaN29uKlWqZPr372/eeecd06pVK9OrVy9jjDFt27Y1QUFBJi4uzhhjzK1bt8zEiRNNpUqVzPz58zP8nJF8jY0xJiYm5o7rN23aZHx8fMzChQtT7BMbG2vatWtnWrdubQ4dOmSMMWbbtm3Gx8fHBAcHm9jYWGNM8jU2xpjr16+n52nAGLN69WpTq1Yt06pVK1O7dm1Tr1494+PjY0JCQuzb7N271/j6+ppp06aZGzduGGOMmTlzpqlcubKpWLGiadWqlWnXrp0pX768GT169F2fF3hwHThwwNSqVcuMGDHCXL161RiT/P5ujDG7d+821atXN5MmTTLHjh0zxhizaNEiU69ePdOhQwcTFxdnrl69aqpUqWK6du1qf20bY8wff/xhmjdvbkaPHm18fHzM1q1bjTHGJCQkmKSkJHP16lUzevRo07RpUxMdHZ3BZ42M8Ndff5lGjRqZDz/80Ny8edMYk/q9PTY21jRo0MDUrl3bNGrUyOzYscMY87/noDHGLFu2zPj4+Jh58+YZY4yJjo42q1evNrVr1zY+Pj6mevXqpkKFCubFF180ERERaVY/X+nxj1gsFtWoUUMtWrTQzJkz7bcSzZIli2JjY+Xu7q7Y2FglJSXZ+xDXqVNH9erV086dO3Xw4EE99dRT+uyzzxQYGKhTp05p8+bNKlWqlMaPHy9JqlSpko4cOWL/yc/2bXLLli3q0aOHy879UWZr/cmdO7du3bqlsWPHatGiRfb1tqnXbFO0Wa1Wubm5ydPTUy+//LKOHj2qw4cPS0oedNmmTRstWbLE/rO8rRuNp6enJNGylE6OHTumjz76SH5+fnrnnXc0e/Zsvf7668qaNatmzZqlmJgYSVK1atXUrl07LV68WH/88Yek/938oUePHqpataoqVaqkpUuXaty4ccqdOzd9STMRY4wqVqyo9u3ba9u2bfYbedh+0bN1gXv++edVpkwZSVKnTp302muv6ciRI/ryyy+VJ08ejRo1SseOHdNrr72mlStX6quvvtKsWbNUrFgxderUSU899ZTmzZsnKfkXINvUgKGhoQoMDGQ8yUMqb968KlKkiNavX68DBw5ISv6M+Ouvv9S7d2+tX79enp6eWrdunX0Wo3Xr1ikuLi7FAP2nn35ajRs31uTJkxUfH28fmzJ//nxNnTpVr732mubMmaMvv/xSxYoVS7v3oDSL2nioJSUlpfhGZ7N7925Tu3Zt8+6779qXrVixwpQvX97s2bPHvsxqtRpj/tdquH379hSPc+XKlRTf/C5fvmxatmxpOnbsaG7cuGFvhcaD48SJE6ZNmzamadOm9l8L9uzZY2rUqGHeeustY8z/WgESExPNhQsXTL169czIkSPtj3Hs2DFToUIFs2bNmow/gUfAnV6zxhizcOFC4+PjY/bv359i+RdffGFq1qxpPvjgA/uys2fPGn9/f/PWW2+Zy5cv3/VYiYmJ9l8YkDk4/iLUtGlT07t3b3Pq1Cn7+tdee808++yz9r9t78MXL140gwYNMvXr17evCwsLM02bNjU+Pj6mfPny5tlnnzU7d+40xhgTFBRkWrduneL5s337drN27dp0PT+4TkJCgjHGmMjISOPj42Peeust89dffxljjDl8+LBp1qyZCQgISLHP0KFDTa1atcy6deuMMSnfvzZu3GiqVatmxo4da4wxd32vsR03LdCCjL+VkJBgHxRlbps5oHz58nruuee0bNky+zfEcuXK6YknntCUKVPstxe29S+1tUzdPs9tvnz5VKxYMd28eVNHjhzR559/rvPnz6t169bKkSPHfU/wjbRx+zR8W7du1bZt25SUlKSSJUuqe/fuio2N1bRp0yQlX/uqVatqy5Yt+uOPP2SxWOytyHnz5rU/h+Lj42WMUZkyZbRt2za1aNHCVaf4UHO8Mc/x48clJbcaXrx4UdmyZdNjjz0mSfZxBO3bt1fVqlUVFhamY8eOSZKKFCliH2C7c+fOFI9vHKZVop9x5mEc5o5NTExU7ty51atXLx08eFBr1qyxb+fp6Wm/0Ytte0l67LHHVKtWLUVFRWnr1q2SpHbt2mnZsmVauXKlpk+fru+++061atWSlPx+nyVLFuXKlcv+2HXq1FHz5s0z5HyR8WzTvBYuXFhBQUFavXq1/f3Dx8dH/fr10+nTpzVnzhz7Pv/5z39ktVq1atUqXbhwIcVA7Nq1a6t9+/ZauHChjh8/nuq9xvacTssxKrybwe7ChQuSUk+3Ywunc+fO1Xvvvadp06bpjz/+kDFGuXLlUrNmzVS6dGlNmjRJUvLNIDp37qxdu3Zp+vTpun79uiTp/Pnz+vbbb1W0aFHVrl07xTGSkpK0bds2DR06VKNGjdKCBQvUrVs3ulJkMMcvQLYRv25ubrp8+bJu3LihN954QzNnztSpU6ckJc+AUKtWLa1evVqHDx9Wnjx51LJlSxlj9P777ys+Pt4+GGzDhg2KiYlRjRo15O7ubn/z8/LyUlJSEtO2pZP9+/frv//9r9atW6cbN27IYrEob968io+Ptw+Q9PDwUFJSkvLnz69GjRopOjpaM2bMsD/GCy+8oDx58igiIiLF+4PtGhKMM4fExMS73rHuhRdeUJkyZbRhwwb9/PPPkpJv3nT16lVt3rxZt27dkpubm+Lj42WxWFS8eHFJShF68+bNK19fXzVs2FBS8mfKvHnz9Oeff6pt27b2LlR4NNjeF0aMGKEcOXLoq6++ss8+VbduXTVt2lSTJ09WbGyspOTpAXv06KHt27fbZ8iyWq06efKkPDw81K5dOzVv3lzu7u6pPi/SY1YjmuWg6Oho9enTRwULFlRwcLCyZcuWYv1vv/2mkSNHKjo6Wnnz5tWpU6c0a9Ysde/eXf/5z39UpkwZ9ejRQ++8847Wrl2rFi1aqHXr1oqMjNSUKVO0ceNGPfXUU7pw4YL27NmjQYMGKU+ePCnm17RNAu7t7a3ChQvr008/TXXLWqS9xMRETZs2Td7e3nr++edTTMWVJUsW3bp1S++//7527typsmXLyt3dXQcPHtSOHTtUuHBh5cuXTy1bttSBAwc0depUTZ06VW3atNHp06c1Y8YMdevWTVWrVpWnp6dWrVolHx8fe6uSIwKW8+4WcO42X62fn5/at2+vFStWqE6dOqpWrZpatGihadOmadOmTfLz81O+fPmUkJAgDw8P1axZU7ly5dIPP/ygb7/9Vs2aNVPOnDn19ddf20eHI/NxfG3v27dPR44cUbFixVSuXDkVKFBAkjRo0CC99tprWrt2rfz8/FS7dm0988wzWrlypcqUKaPnn39e7u7uiomJ0aZNm5Q3b14VLFgw1bF+/vlnbdu2TWfOnNG3336revXqcffLR0BCQkKKX3tt/YizZs2q//znP3rzzTf1448/qlu3bnr88cfVrl07bd++XePHj9d7770nSRo6dKjWr1+vefPmyWq16tatW1q5cqV69uypF154QZ988knGnVCaddZAphYUFGSCgoLMxYsXU60bMmSIad68udm+fbu5cuWKOXfunBkyZIjx8fExK1euNMYYc+7cORMUFGSaNm1qH+1ujDGfffaZ6d69u2ndurXp2rWrfSSzI1s/o6SkJGYvyGAnTpwwzzzzjGnQoIF9BgmbGzdumAEDBpiaNWuazz77zISFhZkPPvjA1KxZ07Rp08YcPHjQGJPc5+v99983NWrUMBs3bjTGJI8yXrNmjWnWrJmpV6+eadCggRk1apS9rzL+Hce+3Y597mJiYsz69etTXcszZ86YatWqmf/+97/2foAffvihqVSpkgkLC0vxGvz4449NvXr1TOPGjU2PHj1SHZt+xpnXpUuXzMCBA03FihVNzZo1jY+Pj2ndurXZunWr/TkwfPhw06hRI3s/0NOnT5sWLVoYPz8/8+mnn5qlS5eaTz75xFStWtWMHz/+jn0+V65caZo2bWo6dOhgvvnmmww9R2Q8x77CSUlJ5ttvvzXbtm0z4eHhKbazzWRkG/sQExNjxo8fb3x8fMzhw4ft223evNk0b97c+Pn5mSpVqpgPP/wwxeNk1HsQAfkRZ3tzu1twOXz4sPHx8THTp09PsfzYsWOmd+/epmnTpvZQvWnTJlOpUiUzY8YM+3a2F865c+dS7M+HrOvs2rXL7Nixw35tvvvuO/PHH38YY1K+0R06dMhUqlTJjBs3zj7IMikpyWzevNlUqFDBTJw40T4t1L59+0yHDh1Mp06dUhzrxo0bJjIy0pw/f96+LC0HUTyKIiIizJw5c1KFYGOMmTBhgvHx8TG//fabfZntmk6fPt1UqlTJbNmyxRhjzNWrV027du1MtWrVzKeffmr27NljQkNDTefOnU1YWJjZs2eP/boj84uLizOjRo0y9erVM0uXLjV79uwxYWFh5tlnnzXPPvusfcDcsWPHTL169czQoUPNhQsXjDHJg2//85//GB8fH1OtWjVTs2ZN88knn9zzeLeHIzz8Vq9eberWrWtq1qxpnnrqKVOrVi0ze/Zsc+XKFWNM8rSRPj4+5uOPP7ZP7fnLL7+Ypk2bmueffz7FY124cMF8//339n2NyfjcQECGMeZ/oWXJkiVm8eLF9uW///678fX1tc+NavtQTkpKMhs2bDDly5e3z3l76dIl884775jq1avbZ6S4fRQ94ci1bt26ZerVq2c6d+6car7I24Pshg0bjI+Pj9m1a5cxJuWb09tvv21q1aqVYjaSWbNmmbp165rZs2cbY0yqmUeY5SBtbNu2zdSoUcM+08SqVatMs2bNzK1bt8zZs2dNs2bNzNChQ1N96Y2NjTVNmzY1QUFB5syZM8YYY44fP2769+9vfHx8zFNPPWV8fHxMUFCQvZXZGF6zmc3dXmPh4eHG39/fvPfeeymWHzp0yNSsWdP06NHDREVFGWOMmTRpkqlTp06K+cyNSZ7RZO/evSmeH7ymYUxyq2/dunXNq6++ajZu3GjWrFljBg0aZHx8fMzkyZPtvw4PHTrU1KlTx/z444/GmOTPiTlz5hgfHx/7rw23v+fY5s7OaHT8e0TdPjtBlixZFB8fr6lTpyokJMQ+eCcxMVF58uSxD9qwDeaxWCzy8fFRiRIltH79eknJd05q1aqV4uLiUs2n6XgcuI6Hh4feeecd/fHHH/rpp5/stxs+duyYunXrptGjR9u39fLykpubm31AnuPzpVu3brJarVq9erXOnTsnSQoMDNQTTzyhpUuXKi4uLtXMI8xykDb8/PzUqFEjzZ07Vx06dNCbb76pYsWK6dKlSypSpIiCgoK0ceNG+8wDUnK/ZU9PT7300kvauXOntm/frlu3bqlUqVL6+OOPtWzZMn344YdasGCBZs+erbx589r35TX7YLp48aKee+45+2AmKbkP6N1eYydOnND169dVoUIFSbLfetfX11ddunTR/v377Y81cOBAFShQQBs2bNDRo0clJb/+ixQpomrVqilv3rz2AX+8ph8t5rbBcbbPBdtn/iuvvKImTZqoRYsWmjJlip599lktWLDAPtvJO++8o6tXr+qbb75RVFSUsmbNqgYNGqhs2bKaNWuWpNTvOVmyZEmXQXh/h2f2I8hxdoILFy7o559/1oULF+Tu7q4RI0bo7Nmz+vrrryUlfxiXK1dOO3bssH/g2ibvLl68uCwWi/LkyWNfVqlSJX3zzTfq06ePa04OfysgIEB16tTRggULFB4eLkkqWbKkGjZsqMOHD9tvGevl5aXSpUtr6dKlSkxMVNasWe3X2cvLS3ny5NGWLVv066+/KjExUcWKFdOIESO0fPlyZc+e3VWn91CyfQiZ/z9zzI0bN2SMUWRkpGbPnq0PP/xQRYoUkZQ8s0j16tU1c+ZMRUVFpXicxx9/XElJSVq3bp192jdPT09VrlxZrVq1Uo0aNSRxg5bMIDo6WhcvXtTs2bPtMwVlzZpVV65c0fTp07Vy5Up7Q4eUPAWjxWKxb2ubulOS+vbtq8TERB0/fly3bt1Sjhw51KtXL+3atUtbtmyRlHogratCC1zD9p7geM1tX5Di4+N18OBBPfnkk/bZTWxf1t555x3dvHlTmzdv1pUrV1SgQAENHDhQ69evt2eKMmXKaMKECQoLC8v4E7sHAvIjyNZa/N5776lDhw766KOP9NVXX0mS2rRpo4oVK+q7777Tjz/+KEl69dVXdf78ec2fP1+XL1+Wh4eHjDH6+eefdfbsWZUtW9beWpgtWzaVKFFCUurp4vDgePXVVxUREaG1a9cqJiZGWbNmVbt27fT4449r+vTpkpLnqmzQoIGOHz+uL7/8UtL/pvyzWq3KmzevrFarli5dqosXL0qSqlevbm9dwr93eyud1WrVlStXlDt3btWpU0fR0dG6deuW8ufPb//ykj9/fvXt21dHjhzR6tWr7XelkqRz586pRIkS2r59u9auXWuf/9jGpMNcokgfpUuX1tChQ/Xbb79p5cqVkqSVK1eqUaNGmj17tkaPHq0+ffrot99+k5T83uzv768lS5bYr7vFYrH/Sli2bFlFRETYZzHq1KmT3nvvPb300kuuOD08YGzvCTt27NDKlSt15MgR+/PI3d1dxYsX16VLlyQlv49kzZpViYmJevzxx9WhQwf9/PPP9veoIUOGSJLmzZtnn17W19dXbm5u9m0eBATkR1BkZKT9No/dunVTv3791KtXL/v6l19+WZcvX9a6det0/fp1Va1aVS+++KK2b9+ul19+WcuWLdOyZcv0ySefKH/+/AoMDLzjcfjp7cFgC6vG4aYOjjd4OXTokKTkXwuaNWumU6dOae7cuZKkLl26qFKlSgoODtbatWt17tw5HT16VDNmzJCvr68mTZqkPXv22FuiCVhpxzYtl8Vi0c6dO/Xqq68qODhYSUlJev/99/XKK6+obNmyGjdunKTkLy+2L6W1atVSly5dNHXqVH377be6ePGidu7cqbCwMA0fPlzjxo1Tu3btUs1LS4tg5mALIHXq1FHDhg01d+5cnT59WmvWrFHnzp01Z84cffzxxypQoIDeeOMNRUVFqWDBgmrZsqXCw8M1Z84cWa1WWSwWZcmSRYcPH9bJkydVuHBhSbJ3verUqZPc3Nxo7HhEOTZ0nD9/Xr1791ZQUJDefvttPffcc3rjjTd07do1SVLlypUVHh6uzZs3y2Kx2J9DklS2bFldvHhRV65csS+bNm2aXn/9dRUqVCjFMR+om4JleK9nuNzChQtN5cqVzYoVK1JMq5aYmGjvCP/f//7XNGjQwISFhRljkgf4LFmyxNSrV89UqFDB+Pv7m06dOtmn+sKD5/aBDrZRwzZRUVGmTp065tVXX7WPVo+IiDBBQUGmcePG5tKlS8aY5NuJBwUFGR8fH1O7dm3zzDPPmKpVq5qwsDBz+vRpU6tWLfPf//43Y07qEXPx4kUzcOBAU7lyZfPcc8+ZMWPGmOPHj9vXf/nll8bHx8fMmzfPGGNSzDpx48YN0759e1OhQgXz7LPPGn9/f9OhQ4cUtxJ2xcAX/HNJSUmpXtfff/+9qVq1qmnfvr3p0aOH/bVsTPKMNf7+/mbcuHHGmOSZAUaOHGnKly9vPvjgA3P48GGzc+dOM2rUKFO/fn2zZ8+eDD0fPPiuXbtmDh8+bNasWWPatGljwsLCzNatW824ceOMj4+PeeWVV0xUVJQ5duyY/fbRt8+w8/rrr5t69eqZM2fOZKr3HALyQ8r2Jnr7kzEuLs707dvXtGzZMsVyx3lVjUn+YH722WdNv3797CPejUmeqeL48eNm3759qfbFg+nrr7823bt3N926dTOjR49Oce3mzZtnnnrqKbNmzRr7tf/qq69MvXr1zJgxY4wxydf31q1bZt68eWbMmDHm3Xfftc+AYbVaTe3atc2oUaOYEiyN3bhxw4wYMcK0aNHChIWFpQjGtmsVERFhXnzxRVOlSpUUX4Ac1y9evNgMGzbMfPbZZyken9dt5uIYjG/dumWfceby5cvmgw8+MD4+PqZfv34p9rFarWbSpEmmSpUq9qn/YmJizMsvv2wqVKhgKlasaGrUqGGqVatmVqxYkXEng0zh2rVrpl+/fsbHx8d07NgxxawmiYmJZuHChcbHx8fMnDnTGJM8C1alSpXMSy+9ZLZv326OHz9uQkNDzdNPP23eeecdF53FP0dAfsjc3rpwp/mNmzVrZnr06GFiY2NTTdHj2ELxxRdfpJi2y5nj4cFx9epV8/rrrxs/Pz/Tv39/06dPH9O4cWNTs2ZN8+OPP5qkpCSTmJho2rZta1544QVz4sQJY0zyB+iIESNM3bp1U/1C4Biqrl69ahYsWGAqVKhgnwYQaefUqVOmSpUq9g+fu1m1apWpVKmS+b//+z9jTPJrfseOHSl+HXJ8nfKazdxmz55t2rdvb7p162YiIyONMcb8+uuvplWrVua5556zz01uExERYQICAszgwYNNTEyMMSb5y9fPP/9sli9fbpYsWWJu3rxp354vTnAUGhpqmjZtaipVqmR27txpjEk5tV/Hjh1N06ZNzalTp8zNmzfNihUrTNWqVY2Pj4955plnTIUKFUxQUFCKKUQziweoswfSgq3v58qVK+2zD+TNm1dBQUGqWrWqsmfPrurVq2vr1q2Ki4uTp6enpP/dvtbWJ02S+vTpo2XLlikkJERNmzZV0aJF73o8uFZiYqL9WthuObxz507t2LFDo0ePVqNGjVSwYEFdv35dbdu21cSJE5UnTx75+flp+PDh6t+/v7Zu3apChQopd+7catWqlX755ReNGTNGS5cutR/HNgredqvazz//XH5+fqpZs6arTv2hFR0drWLFitn7fx4+fFh79uxRdHS0YmNjVbNmTTVq1Ej169dXu3bttHjxYiUkJCgpKUkhISEaNGiQXn75ZUnJr1ND//BM7eTJk3rjjTd09OhRtWnTRlmzZpW7u7uk5AFOttf1rl27FBAQYN+vaNGieumllzRmzBjt3LlTjRs3Vo4cOeTv7y9/f3/7drZbAtMP/eF1+vRpJSYmqmTJkn+7rS0TPP300/r111+1YsUK3bx5U1Ly+CLbZ87gwYM1cOBAnT17VsWLF1enTp305JNPKjw8XOfOnVOVKlVUv379FI+ZWRCQHzLXr1/XuHHj9PXXX+uZZ56R1WrVsWPHNHjwYL388svq3bu3nnnmGYWFhWn+/PkaNmyYpP8NzomKitLixYvVuHFjVaxYUWPGjJGnp+cdwzFczxaGbaFnyZIl8vb2VpMmTbRixQqVLl1aHTt2lLu7u27evKnPPvtMZ8+eVUBAgHLlyiVJeuaZZxQQEKAFCxbI399fFStWVIMGDdSoUSNlz57dHqxs4Xjs2LH67bffdPnyZbVs2VJvvvlmqsFe+PeeeuopPfnkk5o7d64WLVqk2NhY5c+fXzdu3FBSUpLmzp2rsLAwlS9fXt26dVN8fLy2bNmibNmy6d1339Vzzz2X4vEy0wfTo87xC69NaGioIiIi9O6776pBgwbKkyePfV327NnVuHFjbdq0SdOnT1eNGjXsc1lbLBYFBAQoNDRU77//vvz9/ZU/f/4Uj23+/6A/PLwuXbqkfv36KUeOHAoNDf3b7W3vFwUKFFBgYKB++OEHLViwQA0bNpT0v0H4hQoVUpYsWXTgwAHVqVNHklSlShVVqVIlxePd6Tn9wHNl8zX+nTv9VLpz505Ts2ZN8/HHH9sHWV2+fNl07tzZ1KpVy2zatMncunXLvPbaa8bPz8+Ehoba9z127JgZP3686dKli/1e6Tb87PZgS0hIMAMHDjT+/v5mwYIF5tatW6ZXr17mP//5jzEmuW9YnTp1TP369c38+fPN2bNnUzx/jh07ZqpUqWLefvttc/nyZWOMueOtjI1J/jk3JCQkxWAvOMfZ7g22nzAvXLhgVq9ebT744APz9ddf2+9cePDgQdOwYUMzYsSIFPsdOnQoxWuVu5xlXgkJCfbuEhcvXjTVqlWzv55tHK9vYmKiWb16tXnqqafMnDlzUj3etm3b7rgcjwar1WqWLl1qKlSoYNasWePUPrb3kps3b5rg4GDj4+NjVq9eneJ9bP369cbHx8esWrXqjvtm5vcgvjJmQrZvYrZvYwcPHlThwoWVP39+7dixQzdu3FCXLl1UoEABJSYmKn/+/BozZoxeeuklLV68WHXr1tUrr7yiCxcuaPTo0Zo9e7YKFy6s8+fP68yZMxo0aJAqVqyY4pi0Pj2YLly4oODgYPn4+CguLk7vvfee6tWrp6xZs6pIkSLavXu3WrdurVOnTqlLly564YUXVKxYMftPs1euXFH+/PlVpkwZtWvXTr/99pv9WttahW2t1DZ3ah2Ac2yvWdvP2Xdj++9dsGBBtW7dWq1bt061zY0bN+wtgbb3BF9f3xR/M9Vi5hQeHq4ePXpo4sSJqlu3rv2mC7abMFitVnl4eNivr+01WqtWLQUEBOjLL79U48aNVaxYMftj1q1bV3Xr1nXJ+cD13N3d1aBBA9WuXVuTJ0/WM888o5w5c95zH4vFImOMsmfPrqZNm+rHH3/U+PHjdfr0aTVr1kyRkZFauHChnnjiCfn5+aXaV8rc071m3sofYbYP2W+//Vbt27fX8OHDFRISIil5/ko3NzdZrVb7PKrGGPn6+iowMFB79+7Vr7/+qmLFimnGjBn2eQjj4uJUrlw5rVy5Un379iUQZxK//vqr1q9fr88//1yenp4KDAxU7ty55ebmpsqVK+vcuXOKj4/X0qVL9dprr6l06dL2cDx9+nS98sor9vkqR48erbCwMOXLly/FMTLzG9yD5vr16+rRo4f9TpX3e0OVmJgY7du3T0uXLpWXl5eaNGki6c63ZsWDz9x2216b7Nmz68qVK4qMjJQk3bx5U7ly5dLvv/+u+Ph4+82apJRfYAsWLKiuXbvq0qVL+vTTT+/rmHh42d7jixQpoh49eujSpUv2ue7/ji0LlC9fXu3bt1diYqKmTp2qsWPH6p133tGpU6f0xhtvONWvObPhky8TiomJ0ahRozRixAh7H9Onn35aklStWjV7v2Pbm6btQ7hz586Ki4tTdHS0JClXrlx66aWX9MUXX2j69OmaNGmSSpQoYb97F1zH2eAUGBio5s2b6/Lly/Z+4nFxcZKSr3elSpV0/fp1RUVFKUeOHJKkixcvKiQkRJs3b1bx4sXtd0OytRg/SHcyethcunRJp06d0u+//y7J+SB7+fJlTZgwQe+8846GDx+ub7/9VoMGDUoxyAqZz+0NEbYBmUlJSSpevLh+//13JSUlqXTp0qpUqZJ+++03+x1Oba9TNzc3XblyxX6L+KeeekpDhgxR48aNnTomHn7u7u4yxmjTpk2KjY1VjRo1NH/+fJ05c+b/tXffYVFcXwPHvwsCKmABBYxgjYANexexECNWolFjVOw9Gnsvid2IvaHYscQusffeK/aGJRZEioAgnfv+4bsTEM3PJDbkfJ4nj3HKnR1ndvfsnXPPfaf99fFAtWrVqFy5MhkyZGDUqFH89ttvHD58+K33WlonAXIadPr0aY4cOcKQIUMYOHAgXbp00R6t5s2bF0dHR2bPnk1sbCxJSUnao9znz5+n+FNPp9Nhbm4O/PVoVj5EP74bN27Qq1cvoqOjMTQ0/J+zV+nXt2rVily5crFr1y5iY2PJmDEjcXFxGBgY0Lt3b6ysrOjTpw+jRo1i0qRJjB07lnHjxmFtbU2PHj1SPWaTwTrvn/4LJm/evGTPnp3Q0NAUM029i6tXrxISEkKDBg04fPgw7u7uKdoWacPr7+ulS5dy6NChFMvs7OywtLQkMDBQ6+jo06cPL168YMmSJTx8+FB7EvTkyROmT5/O3LlziYyMJGvWrHTv3v2tM5yK9OfQoUM4OzszadIkZsyYgb+/P+Hh4SxZsuSd9tfHA3Z2dtSpUwedTseKFSsoW7Ys8OV2qkiAnMYkJSVx+PBhrKyscHV11aZpDA4OJi4ujpw5c/L999/z4MEDpkyZQkhICPCqOsWOHTvIkSOH1tv8JvJo9tM5efIk+/btY8GCBe+0vf6Ls0iRInz33XeEhYWxbNky4K/rWLFiRSZOnIibmxtHjhzh0KFDPH78mNGjRzN37ly++uorCbD+o+fPn2u98MmD3uPHj3P58mWSkpK0qVd1Oh1ly5bFz89PC3D+l6SkJCwtLZkyZQqzZ8+mT58+mJiYaF9K8mM2bUhKSkqVz+/v74+npycDBw7k0KFDKa5lxYoVOX/+vHZv5c+fn969e3Pt2jW6du3KrFmzmDNnDiNGjGDnzp3UqlULExMTbX95XwuA2NhYZs+eTY4cOZgwYQIrVqxg3rx5lCxZkpUrV3L27Nl3akd/P1WsWBF3d3dWrlzJjRs3gC83btApeRelOQsWLGDRokX89NNPWFtbs3v3bp49e8a1a9coVaoUlSpVIkOGDIwfP56CBQvi5OREWFgYx44do2XLlvTp04cMGTJIbulnQv+lGRoaytixYzlz5gw+Pj7ky5fvf5bG0e8bGBhIz549iYiIwNvbGzs7u1QDwRITE3n69Cm5c+dOsexL/XD7UJIHOf7+/qxevZp8+fLRqlUrAB4/fkxYWBi9evUiLi6Onj170qxZM23/xYsXs2DBAry9vSlevPg/Pr5KVnZPpA3J32c3btxg7969ODs7U6JECfz8/Jg/fz6nT5/mp59+om3btgAsWrSIhQsXMnv2bC2VJjExkd27d+Pl5cWff/6JqakpNjY2DBw4UGqRp3Nv+yw/e/YsrVq1YsyYMTRt2lRbfvHiRUaMGIG5uTkrV678R58nZ8+eZejQoVhbW+Pj4/NeXv/nSCKkNKhz586ULFmSWbNm0atXLx4/fky2bNlo1aoVd+/eZfny5dSrV48xY8aQI0cOrl27RmBgIBMnTmTQoEEpRj+LT8/AwICEhAQsLCxwc3PDyMiIWbNmAf/7l7mBgQFKKaytrWncuDHPnz9n0aJFAKmCY0NDQy041uc4S3D8z8yZM4dRo0Zpf8+ePTtnzpxh9erVXL9+nVGjRlG/fn3u3r3LihUrKFSoEL/88gvr1q3Tcv/t7OyIiYkhMjLynY+bvB9DP6GPSDsMDQ0JDw+nX79+tGjRgvXr17NmzRqSkpIoUaIEM2bMoHz58sydO5exY8cCULVq1RRPJxISEjA0NMTNzY3Vq1ezf/9+Fi1axPr16ylfvjxKqf+ZliW+PPprrv8s198v+s8M/Z+WlpYp1hctWpQffviB8+fP4+vr+z+Po17NvAyAk5MTderU4cyZM9y8efM9ns1n5iOWlBPv0YsXL9T169dVYGCgNt2oUkr5+Pio0qVLq7Nnz2rLXp/iMS3XJfwSJa8p+fLlSzV27FhVoUIFdfTo0VTr30RfbzI2NlZ169ZNubi4qBMnTiil5Fq/T1FRUWro0KHKwcFBPXr0SPt337x5sypVqpQqVqyYqlGjhlq+fLl69OiRUkqpR48eqREjRqhSpUqpUaNGKaVeTdFdqlQptWzZMqXUu1+jmJgYdfbsWRUVFfX+T058UAEBAap58+aqQYMGatWqVerKlSva+1r/Z3BwsFq4cKFycHBQw4YNUydOnFD169dXv/32m1IqZS361+vSy/Th6c/r98DOnTtV586d1ZAhQ9TZs2e1OvbHjx9XZcqUUWPGjNG21X/mXLlyRZUqVUrVr19fhYWFvdNx58+frzZs2KCePXumAgMD39PZfJ6kGzGNMjMzw9HRESsrK3LlygW8Gul+/fp1LC0tyZs3r7atPk9Z32sovccf3/Pnz9myZQsRERGp1hkaGhIaGsrgwYOZMWMGt27dwsjISMtFTj5N8JvodDqSkpIwNjamefPmBAUFaWX/5Fq/P5kzZ6Zr165s3ryZ3Llzaz0q69ev5+XLlxgbG9OlSxdat26t9dTnzp2b0aNH07RpU7Zu3crgwYO5c+cORYoU4cKFC8Dbr1Hya75lyxaqVq3KrFmzePHixYc/WfGvvO19eu7cOfz9/enatStNmjShaNGiWo+f/k9LS0s6dOjA4MGDOXv2LEOGDMHQ0JCgoCBiYmJSPDV4/QmCPAlKf5LfA9OmTWPQoEE8fvyYnTt30rFjR5YvXw5ApUqVyJMnD0eOHNE+c/TjF4oWLYqFhQW3b9/Wnjwml/x+PnLkCI0aNWLq1Kk8efIECwsLrKysPuQpfnLy7fkFePToEfv372fGjBns2bMHd3d3smXLlurDWj5EP51ly5YxYMAA/Pz8tGX663Pp0iUaNmyIn58fSimyZs2KoaEhp06d0gLd//XoVB9kubi4MGPGDCZNmvSBziR90l8rOzs7HB0defLkCefPn0en09G9e3f69++PiYkJR48e1fZJSkrSfpT+/PPPTJgwgV27duHt7c29e/cAiIqKSvU+VclyjP38/Pjhhx8YMmQIbm5ujBw5UvvBKz4vCQkJb019efz4McbGxlSvXh1jY2OePn3K4cOH2bdvHwcOHNAeewO0bduW4cOHY2Zmxo0bNwgJCUkx5bsQ8CpVYubMmaxcuZIrV64wbNgwFi5cyMmTJylatCibNm1i3759wKvPnwcPHuDj40NkZCTGxsYkJSWxbds2DA0NqVatGmvWrNE6cJJ/Bj148IDu3bvTqVMn8uTJw4YNG+jWrVu6iCdkkF4at2jRImbNmoWFhQWJiYn06dNHK/8kPh/h4eE0adKEwoULM3z48BRBzm+//cbvv//OzJkzqVChAkZGRpw7d45BgwZhaGjI+vXrMTc3TzUC/nWvr5cBeO+HUkoLfBITEzEwMMDDw4Nr165x7NgxMmbMCMDIkSPZs2cPffv2pWnTpimuh76NLVu2sGXLFg4fPkzOnDk5cODAG8vqBQUF4enpia+vL5UrV6Zbt26UKlVKSvB95qKioli1ahWmpqbky5dPm7nO398fd3d3ihYtipGREY8ePUIpxdOnTwFo3Lgxffv2JUeOHFpb9+7dY/jw4dy8eZOtW7diY2OT4l4U6cebrvvVq1dp0qQJhQoVIlu2bHh7e2ufRefPn2f48OHkzp2bmTNnkilTJkaNGsWmTZuoUKECbm5uxMfH88cff+Dg4EDHjh2xtLRMUQUlKiqK2bNna2MpunfvjrOzc4ptvngfNaFDvHeBgYFq6tSpat26dSmWS+7p50OfK7Zx40bl6OioNm3apOUMxsbGqlatWqnvv/9e216/bsWKFap8+fJq0qRJKdr5J8cU/97r76FVq1apJk2aKKWU2r9/vypVqpTy8vLS1t+5c0e5urqqH374QQUHByul/rqWya/HgwcPVOvWrZWDg4M6duxYquP6+PgoBwcH5ebmpjZt2qQiIyPf+7mJ9+/w4cOqfPnyqkyZMqpIkSLKwcFBjR07Vj148EAppdSePXtU586dVbt27dT06dPVunXr1NOnT9Vvv/2mnJyc1MmTJ7W2kn9mVKlSRR06dOiTnJP4tBITE9+YX66/PyZMmKAcHBzUzz//rJRSKj4+XttmypQpytnZWRvr8Pz5c7Vs2TJVpkwZ5ejoqAoXLqx+/PFHbbxE8v2fPHmiWrVqpcqUKaOWLl2qnj9//oHO8PMmAfIXIPkbKPkbRHx6ya9HQkKCatGihfruu++Uv7+/UupVgNywYUPVrFkzFRISom2nlFKhoaGqefPmqnz58tr2bxuMkzwACwoK+iDnkl7FxMSobdu2qdq1a6s2bdqohw8fqvDwcDVixAhVunRp9fjxY23bOXPmqHLlyqlZs2Zpy+Li4rT7QP/n1atXlZOTk9q3b59S6q/rFxMToxYsWKA8PT3Vs2fPPtYpiv/g1KlT6uLFi6p3796qf//+6vz58+rChQvK09NTOTg4qH79+mnXPTo6Win16p7QO336tHJwcNAG5Sr114+zoKAg5eDgoA4ePKiUkh++6Unyz/qnT5+qQ4cOqTt37qgXL15oy0NDQ1XNmjVVrVq11P3795VSShucFxAQoJo1a6aaNm2qHj58qO0TEBCg/Pz81Llz57Rlr99XL168UHv37lV//vlnur7nJAf5C6AfxKWUkkewnwn1/5lL+uuxdetWIiIi6NevH9euXWPPnj1aLliFChW4d+8ed+/eBV5dz/j4eLJnz07FihUJDw9n7ty52ro3HUen0xEVFcWkSZP45ptvuHTp0sc61S9K8lzvpKQkJkyYwE8//cTOnTspU6YMkyZNwtbWlixZstC4cWNMTU2ZOXOmto+Hhwf58+dny5YtnDlzhmfPnrFu3ToWL14M/HU/FCxYkKxZs6a6TiYmJrRt25a+ffuSM2fOj3DG4r8ICQnh559/pkePHvj7+9O+fXtKlSpFyZIl6devH9999x2HDh1izZo1ANrjaSMjI2JjY7l27RqLFy/GyclJmw0VXo0pSEpK4vz585iYmPDy5UtAal+nJ4aGhiQkJDBu3Djq1KlDv379+P777xk5cqQ2UDd79ux4eHgQEhLCpk2bADA2NiYxMREbGxsaNWpEcHAwq1ev1tq1sbHBycmJ0qVLA6/Sxl6/r8zMzKhVqxZ2dnbp+p6TAPkLIbVRPx8qWb5YWFgYrVu3ZtCgQVy+fJkyZcrQqFEjVq5cye3btwFo0qQJSUlJ+Pr6EhwcDKDNspaQkICtrS07duxg+/btqY6lP866dev49ttv2bp1K7169aJgwYIf41S/OMlzuBMSErCysuLIkSMcP36c9u3bY21trQXRjo6OtGjRAl9fX202KjMzM9q1a8fLly/p0aMHHTp0YPTo0cTGxqY4zp07d4iPj9cCpuTvXSMjI3kvpxHZs2dn0KBBhISEEBAQoFUv0V/vzp07kz17drZv3054eDg6nY7IyEh8fHzw9vZm4MCBXLp0CQ8PD61OLbz6cXbnzh0mT56MpaUlxYoV+yTnJz6du3fv0rFjRw4ePEjHjh0ZPXo0rVu35sSJEwwdOlTbrk2bNnz99dfs3LmTc+fOAX/90P/+++/JlSsXW7du5eHDh288joxTeTsJkIV4z3Q6HQ8fPsTb25uNGzeSmJjIjBkzKFGiBAA9e/YkMjKSTZs2ERERgYODAy1btmTdunWsXLkSeDVl8aVLlzh+/DjVq1endu3aWkkdlWxc7ZkzZ2jcuDGjR4+mTp06rFq1irZt22JqavrxT/wLcOLECUaNGqWVzatXrx41atQgMjKS+/fvA38F0RkzZqR27doULlyYadOmaRUr6tSpw5gxY6hTpw4lSpRg9+7d9OzZUzvGw4cPWbFiBc+fP9fuCfF52LVrlzZ97rswMDDA2dkZV1dXlFLaviYmJiQlJZE/f35cXFy4c+cODx48ICkpiSNHjrBgwQI2b95MsWLF2LJlC/Xr10/VbnR0NN9++y3r1q3Dzs7uvZ6n+Pzt37+fsLAw+vXrR+vWrXFzc6Nv375Ur16dPXv2sH//fm3bbt268ezZMzZu3EhSUhJGRkbEx8djbGxM//79WbhwodxD/8YnTO8Q4ovwphytefPmKQcHB1W3bl01ZcqUVOvnzJmjnJyc1IEDB7RlHTp0UA4ODqp27dqqU6dOqnbt2qpx48ZabnJygYGBqnfv3srBwUF17txZnT9/XvLP34Nhw4ap0qVLqw0bNiilXuUMHzhwQBUtWlRNnTpVvXz5Uin1V45oQkKC2rRpkypSpIjavHlziraS5xAmJCRo+1y/fl317NlTyysVn4dHjx4pBwcHNXLkSBUREaGUevec32PHjqlixYqpiRMnajmi+lzQffv2KQcHB3Xp0iWl1Kv8zkuXLqXIC01ISEh1rPSc+5me6a/7mTNn1LZt27Tlt2/fVq1atVLFixdXZcuWVXXr1k0xgLdXr17KxcVF2+f18SpyP/1z0oMsxL+UmJiYqvyOvhexa9euFCpUCH9/f23Slvj4eG27Nm3aYGNjw4oVK3j06BEAkydPZvz48dja2vLs2TNKly7NokWLsLCwAFLmx/r4+PDgwQM8PT2ZPn26lAD7j/R1aDt16kT+/PlZv349wcHBZMiQgdKlS1OnTh02bNiAv78/8NcU34aGhlSqVAlnZ2eGDx+eYvpo/aPLpKQkDA0NtZ5nR0dHZs6ciYuLy0c+S/F3cufOTdeuXdmxY4dWr/xdU11KlSqFu7s769ev12phGxsbA3Ds2DEMDQ21tCkzMzOKFy+Ora2tNj20oaFhqmNJmk36pL/uZcuWpW7dugAsXrwYd3d3lFJ4eXkxZMgQ/P39WbdunbZfjx49ePr0KZs3byYmJiZV6oTcT//Cp43PhUibkpcAu3r1qtq2bZu6evVqiuk6jxw5ohwcHNTAgQO13qTkv+K3bdumHBwc1Nq1a7X1Sr0a4a7vwVIqZU+A/rgRERHvPDWoSEl/DdauXatWrVqlLdf/2y5evFg5OzurmTNnausuXbqkypUrp0aMGPHG3sWDBw+qiRMnSi9+GhceHq6qV6+uunfvrp4+ffqP9r1+/bpydnZW5cqVU0uXLlUHDx5Ua9euVVWqVFFdunSR6aDFv7oHrl+/rmrUqKHGjBmjnjx5opR61Zvs4OCgqlWrlqKKjq+vr1S/eY+kB1mIf8HAwIDQ0FB69uxJixYtGDVqFM2aNaN79+7cuXOHpKQkqlatSqVKlTh06BCnT59O1UbdunWpWrUqixYt0nom4dUgLXNz8xS9S8mPC2Bubk7WrFk//Il+AZKSklL0vut0OuLj45kxYwbr16/XBkvqe/+bNm1KwYIF2b17N9evXwegUKFC/Pjjj/j6+nLx4kWtHX271apVY9CgQdKLn8ZlyZKFXr16sX//fk6cOKHdE++iYMGCtG/fnoiICHx8fJg+fTpeXl4ULVqUsWPHymCodCgqKopDhw5puen/5h7Yv38/T548wcPDg1y5cgGvBhDny5ePwMBAxo0bp23bsGFDcubM+T9nXhXvRgJkId7B61+UkZGR/PLLL1y5coWBAwcydepUfvrpJ27dusXAgQM5deoUAMOHDycsLIwtW7YQFhaWIqgC6N69O3/++ac2xWdyOp3ub2fOE/+bfuY7AwMDAgICOHToEA8fPsTIyIiBAwfy+PFjtmzZArz6YZKQkICZmRnNmjXj+fPnWnmkjBkz0qRJE+zs7PD29tZmQNNfH/3jS/liSjvU/5fGfJ27uzslS5bU0pjelZGREa6urhQvXhwrKyvGjh3L8uXLmT9/Pjly5JB7Ix0KDg7Gy8uL6dOnA68C5hEjRrB37953bkM/66o+Re/evXvMmjWLYsWKMXLkSGrUqJFqH/neeD/kX1GId6D/5X/z5k0A7t+/z6FDh2jYsCEtW7bE2dmZrl27MnfuXK5du8bGjRsJDQ2lYMGC/PDDD2zbto1Dhw4BKT+8SpcuzenTp6lQocLHP6l0QF9LdOzYsbi7u+Pp6anVJG7YsCHFihVj3759nDhxAvgr0HVzc8PW1padO3dy5MgR4FX90ObNm3Px4sU3/qAB+WJKKxISErTSmMmDZPX/Ywr69evH1atX2b17NzExMe/cbu7cufHw8ODChQtcuHBBK/uWkJAg90Y6lDdvXooWLcrFixfp3r075cuX12pbv6tChQpRrFgxGjduTKdOnRgwYAB+fn58//33/Pjjj3z//fcf8AzSN3nHCvEGb+pZmjhxIn369CE0NJT79+8TGxtLuXLltO0TExMpV64c33//PXv37uX8+fMADBkyhMyZM/PHH39oPVLJ2zczM/tHj3LFuwsNDaVLly7s3LmT9u3bM2jQIPr166et79WrFyEhIWzZsoWXL19iaGio1bCtUqUKsbGxrFy5ktjYWIyMjPjuu+84ceIE9vb2n+qUxH+gXpvAZ8WKFYwZM4bZs2fz4MEDbX3ZsmVp1KgRq1at+kdl33Q6HdWqVcPFxYXly5dr6TuSepP+JCQkAJA/f34iIyM5fPgwrVq1YsGCBf+oQ8TJyYlJkyZRs2ZNXrx4gY2NDT4+PlSsWBF483eVeD8kQBbiNeq1yhR6dnZ2PHnyhNDQUK2mpH5Go6SkJG2fHj16EBcXx59//gm8qonaq1cvjh07po1wf719yU/8ME6cOMGpU6fo0qULrVq1omrVqpiZmZGUlIRSCicnJ2rXrs3x48fZtm0b8Op6BQUFceDAAXLnzs2pU6c4fPgw8OrHjKmpqfygSaP077sTJ05Qp04dZsyYwZEjR5g9ezadOnVKURWgV69eREVFsXHjRsLCwoB3C0ayZs1Kx44dCQkJYdWqVSkqm4gvW/LPBf2PIj8/P/Lnz4+xsTFGRkbkzp1bq3DyrgoWLIinpyfe3t7Mnj2b/Pnza59hUp3iw5EAWYjX6D9wFi1ahLe3t7bczc0NQ0NDTp8+jZ2dHXny5GHhwoVAygA3c+bMWFlZaQO8AFq1asXAgQNp3rz5RzqL9ONNwao+kDl27BjZsmWjdevWmJqaanmn+jJtAD///DOWlpbMmzePAwcOcOHCBVavXk3u3LmZNWsWc+fO5ZtvvknRvvygSbtOnTrFhAkTKFKkCDNmzGDVqlUcP36c3Llz4+3trQ2YzZ07Nx07dsTX15cLFy4Af18qK3nwXKhQIerUqcOlS5ck9zgd0F9jQ0ND4uLiePbsmbZs7NixeHt7Y29vz969e7ly5Qrw5s+tv2NoaIi5ubm2r4GBgQTHH5gEyCLd0/8ST+7MmTNMnjyZKVOmsGPHDuBVz2KZMmXw9fXFwsKCBg0acPXqVdasWQP8lX/68OFDgoODU9U/bt++PRkyZJBHYu9J8i8lgCdPnmjpEfovjj///BNzc3MCAwO15cm/VJKSkrC0tKRnz55YWVnRrVs32rZty8KFCylTpgwFCxakUqVKgDzKTGteD0D01y8sLIyIiAg6dOhA5cqVyZkzJxkyZCA6OppHjx4xe/ZsbR8PDw9sbGxYuXIlAQEBbzyOvl2dTseDBw8YMGAA+/fvp1+/fmzYsIEsWbJ8oDMUn5r+HtN/9i9dupQ6derQsmVLhg0bRmBgIMbGxtjY2NC6dWueP3+uzZZqaGj4r388yQ/0j0MCZJHu6X+JJx+MkyVLFipWrIi1tTVjxoxh+/btmJqa4uTkRFBQEP7+/jRu3JjKlSszbtw4li9fzr179zh//jxLliwhZ86c2kQQ+gkC4O3pG+Kf038pbd68GTc3N1q2bMm3337LsmXLePjwIQDly5cnMDCQZ8+eafvpAxp9dQuA6tWrM3fuXCZMmECvXr3Ys2cPbdu2TXE8uW5pg/4Hrz6I0Kc46K/fN998wx9//EHRokUBmDdvHlWqVOHly5c4Oztz6NAhbUCtqakpffr04dixY+zZsyfFZD96Op2O6Ohopk6dSqNGjbh48SJ2dnZkz579Y5yu+IT095hSiqVLl7J48WLKlStHgQIF2LNnD3379uXly5cA1KtXj/Lly3P06FGtisU/Hbipv5flqcTHoVPSLSIE3t7e7N27lxEjRlCsWDGio6Pp1asXxYoV4/79+wQEBNCqVStKliyJq6srnp6e1K9fnzt37jBhwgSOHTuGmZkZxsbGxMTEMHToUBld/IHFx8czZcoU1qxZg5ubG1ZWVjx69Ij9+/dTo0YNpkyZwvXr12nbti2urq4MHjxYe0QJrwbwzZ07l7p161K6dOlU7ctjzLTl9R+fR48eZfHixRgaGuLm5oaLiwuWlpbEx8djZGREaGgoY8aM4ciRI3Ts2JHWrVtz5coV2rdvT8mSJbWePqUUTZs2xcHBgVGjRqXKH920aRNTp04lKSmJjh074u7uLsFxOhETE8PAgQPJlSsXAQEBFCpUiHbt2mFmZsa2bdsYPHgw3bp1o3379mTMmJHz58/Tp08fChUqpKXnPXz4kLi4OAoWLEhSUpIWNCe/n8+dO8esWbMoW7YsP/300yc733TnQ89EIkRacPLkSVWiRAnVsWNHdf36daWUUrNmzVINGzZUz549UxMnTlTly5dX586dU02aNFGDBw/W9n3x4oXatWuX8vb2VosXL1aRkZHauuSzrYl/700zUN28eVO5uLioadOmqcDAQG15//79tRkKlVJq+vTpqnDhwmrWrFkqOjpaKaXUnTt31NSpU1XDhg3VsWPHUrUt1y1tW7dunSpZsqRyd3dX33zzjSpcuLAaNGhQiut64sQJVaFCBbVkyRIVHh6uLa9Zs6ZycnJSixYt0pYlX6935swZ9cMPP6iiRYuqkSNHqnv37sl98wV702dQSEiI6t27t3JwcFBubm4qKipKWxcVFaXGjh2rypUrp65evaotnzhxoipdurTq27evWrVqlWrUqJFq06aNiomJSdX+06dPVb9+/ZSjo6Nq27at9t0kPg6pPSMEUKFCBUaPHs28efMYOnQoGzZsoGnTpixatIg7d+4waNAgwsLC6NGjB1999RUHDhwgIiKCLFmyYGZmRu3atVO0l5CQgKGhofQ+/keJiYkYGhpqjzKvXr1Krly5sLCwwM/Pj+joaFq2bEnOnDkJDw/Hy8uLbdu2Ua1aNfLkyQO8yv2+d+8es2fPZs2aNeTOnZvIyEgePXpE586dtRzj5OS6pU1z5swhMTGRgIAAOnToQOPGjfnqq68YM2YMu3btYsmSJbRv357ExET8/f2JiIjA1dWVLFmykJSUxPr164mMjCRfvnz8/vvvtGjRgkyZMml5xPr78caNG3To0IESJUqwdOlSSpYsKaXcvlD6CkX6z6CEhATtWltYWNCiRQvu3LlDZGQkISEhZMqUCZ1OR+bMmWnRogW7d+9m6dKljBw5EjMzMzp06EBwcDCHDx/mwIEDVKxYkTFjxqSojRwfH4+3tzfe3t7kypWLSZMm4erqSubMmT/Jv0F6JSkWQiSzbds2fv31V5ydnWndujWrV6/G2NiYMWPGANC5c2cuXLhAZGQkEyZMwN3dXdtX/f8jMSV5xu/drl278PLyIioqiu+//57OnTsze/ZsNm3axLZt21i3bh2zZ88mc+bMdOrUiVq1amFpaal9kcXHx7N792727NlDTEwM2bJlo1u3btpASrlmacubrldQUBAtW7YkJiaGTJkysXjxYm2ijkePHjF27Fhu3rzJ77//jrW1NZs3b2bChAnUqVMHDw8P7t27h4+PDzVr1qREiRLky5ePbNmyvfH4kZGR3L17l6+//lqClnTi3LlzrFy5EhMTEwoVKkTVqlWxt7fXphafPXs2Xl5e2tgTeBVc+/j48NtvvzFv3jyqVq2KgYEB0dHR3Lt3DxMTEwoWLAj89ePL19eXuXPn8vz5c9q2bUuzZs3IkSPHpzrtdE0CZCGSSUpKYu/evfTu3ZtGjRoRHR1NxowZ6dGjB3Z2djx48IApU6awf/9+vLy8qFq16qd+yV+0iIgIxo8fz44dO3B1dcXe3h4XFxccHR05duwYnTt3xsrKirCwMJo3b84PP/yAra2tFhjfvHmTbNmyadO1wqu8wYwZMwKSZ5zWKKVISkp66yj+devWMWnSJHLlysWWLVtSDNjbtm0bkydPpnLlyowfP57o6GgmT57M2rVrMTU1JSoqCkdHR6ZNm6bVOdcHLSL9iomJwdPTk99//x17e3sSExP5888/MTMzY/ny5eTPnx9/f3/69u1L5syZ8fb2xszMTNs/ODiYnj17opRi1qxZ5MyZM0X7Klnpybt371K3bl3c3d3p1KkTBQoUkM+mT+mTJHYI8ZnS5xDq84+dnJzUt99+qx48eKBt8/jxY/XixYtP9RLTlT179qjKlSur1atXq6dPn6ZYd/v2bdWuXTtVvHhxdebMGfXy5UttXWJiotqwYYNyd3dXx48ff2Pbb8opFJ+HuLi4VMuS5/cGBwerpUuXqj/++EOdPn06xX7du3dXDg4O2nXXt/Xy5Us1bNgwVb58eXXmzBmllFLPnj1Thw8fVvPmzVO7du36kKck0qjjx4+rKlWqqAULFqj79+8rpZS6cOGCcnV1Vc2bN1chISEqISFBrVu3Tjk6OqrVq1enamPbtm3KwcFB+fn5vfU4iYmJSqlXn2uxsbEf5mTEPyJl3oR4g86dO9OwYUOyZ8/O/fv3tbJPAF999ZVMD/0RJCUlcfjwYaysrHB1ddV6gYODg4mLiyNnzpzUqlULAwMD9u3bp5Xpe/z4Mdu3b9cepefPn/+N7UvP4Odpy5YttG7dWqtdrafvSfP29qZmzZp4e3szcOBAWrduzYQJE7h//z5GRkY0a9YMa2trli5dCrwqs5iQkECmTJlo0KABNjY2zJ07F4CcOXPi7OxM165dtXEE8r5Of/6ubNqSJUuwsbGhbdu2WkrWuXPnCAgIIC4ujvDwcAwNDalSpQouLi7Mnz+fp0+fpmijVq1anDx5Eicnp7ceR1+94uuvv/7HM+2JD0NGFQiRjD6H2NjYmObNm2NkZMSkSZO0R67JSYD1YRkYGGBra8uuXbvYsWMH1tbW7N69m2fPnnHt2jVKlSpFlSpVGDBggDYIK1++fMTHx+Pn50e5cuUYNmwYNjY2n/pUxD+g0+m4ePEiN27cSJEaA7B//34WLFhA06ZNqV+/PgYGBvzxxx+sWLGC4OBgpkyZgouLCy4uLuzcuZMNGzbQpEkTbf8KFSpQqVIlli5dypkzZyhXrpy2Tv1/XrO8r9OX5Gk0yf8/ISEBAwMDEhMTKVSoEEZGRhw6dIiJEyfy9OlTOnXqRP369bU891y5ctGiRQsGDBjAokWLGDZsmHYMExMTTExMJGUnjZEcZCHEZ61Lly5cuHCBiIgISpUqRc6cOSlQoABbtmwhMTGRTZs24efnx7Zt23jx4gUA3333Hd9++y0gA/DSmpiYGO7fv4+jo2OqdQMGDOD48eNs2bIFCwsLbXn//v3Zv3+/Vn/8+vXrDBgwAGNjY5YuXUqWLFmIi4vD2NiYhw8fEhkZSeHChT/maYnPWEREBDNmzCA2NpaSJUumqGHfunVr4uLiyJQpEydPnqROnTp06NABe3t7rfJEZGQkZmZmBAcHM3HiRE6dOsUff/wh9bDTOAmQhXgHyUv7iI9LX5LNwsKCxMREcuXKBcDKlSuZOnUqXl5eWk+gfhIIPemxSbsiIyM5fPgwrq6uGBsbExkZiYeHB0ZGRtr07vqg9+rVq/Tp0wdra2u8vb3JmDEj8+bNw8fHh2bNmtG7d+9UP5RUsimiRfrx+mfC3bt36datGy9fviQhIYHnz5/Tvn17bZrxDRs2MGzYMOzs7Bg0aBCVK1fWSrkBbNy4kXXr1rF69WrgVYpXzpw5JU3iCyA5yEK8AwmOPx0zMzMcHR2xsrLSguOQkBCuXbuGpaVlihxjfXCszyOV4DhteD3vNzExkaVLl9K3b18uX76MUgozMzNy5MjBy5cvtfxkfRBStGhRypQpw71797h9+zYAP/zwA5aWlmzYsIHg4OBUgbBOp5PgOJ1JXgFFP2bh6NGjFChQgEWLFrFmzRq6dOnC0qVLtemg3d3dcXR0xMDAADMzMzJnzoxOpyMiIoLt27ezdu1a8uTJQ0REBAC5c+fG2NiYhISET3OS4r2Rb30hRJrx6NEjbt26xcGDB9mzZw9t27YlW7ZsqXoHJTBOG5KXYQN4+vQpNjY2GBoa0rBhQ3bu3MnixYv5+uuvyZo1K3Xq1GHYsGGcPXuWevXqAX/1IpcvXx5fX1+tdnH27NkZOnQotra2Ukc2ndNP4awvpTZz5kySkpJwcXHh5s2b1KpVC3t7ewD69OnDgQMH8PHxoUiRIpQuXZrhw4fTuXNnBg8eTIcOHUhISODx48ds3bqVQoUK0aFDB20yGT3pVEn75AoKIdKERYsWMWvWLC3VYujQoSkmahFpi/r/2q+AFrTcuHGDtm3bavWs27dvz7Bhwzh27Bh169alXLlylChRAi8vLwoWLIijoyPGxsbExcVx6NAhzM3NyZAhg/YYXT9Loj5AEumT/trfvn2bNm3aYG5uTkJCArt37wagRYsWAERHR5MpUyZGjhxJq1at2LFjB/b29pQtW5bffvuNDRs2MGHCBLJnz07GjBlp27YtXbt2/WTnJT4sCZCFEGlCgwYNiIiIwM7OLsUgGgl+0iadTkdSUhKzZ89m0aJFFClShEqVKmmTuBgYGFCzZk0qVqzI/PnzKV++PHZ2dnTv3p3evXszaNAgunTpgpWVFdeuXePMmTM0btxYS8NJTu6P9G3r1q1s3bqVihUr4uzsTJcuXbCzs8PLy4s5c+bwxx9/UKxYMTJlykRSUhJly5blm2++wdfXlwoVKuDq6qr9FxoaSlBQEHZ2dtosijLW4cskg/SEEGnG62WY5DFm2vGmHzInTpxgwIABWtm2vHnzprqmx48fp3PnzvTq1YtOnTqh0+nYvn0706ZN4+HDh5ibmxMdHU2DBg0YMWKETP2cjr1t4OXo0aNZt24dGTNmZNy4cVrN65iYGNq1a0doaCiTJ0/GyclJS9kJDAykVq1afPPNNwwZMgQrK6tUgbDMxPllk5/VQog0w9DQUJuaVYLjtEEppQUSQIrBS9u2bSNTpkx06dKFggULkiFDBl6+fAn8NXCvVKlSNGzYkGXLlnHnzh0A6tati4+PD97e3gwdOpRNmzYxYcIEMmfO/LeTPogvV0JCgjbw8vV+v2bNmlGyZEliYmLIkycPALGxsWTMmJE2bdoQFhbGypUrgVcDP+Pj47G2tqZNmzbs2LGDM2fOAKnHNhgaGkpw/AWTbxghRJoiX0hph77X2NDQkKdPn+Lj46PNgGhnZ0eBAgXYunUr9+7dw9jYmJ07dxIQEMDt27extramZ8+eFCpUiHbt2nHw4EFWrVrFwIEDyZQpEzY2NikmgdEHxpJOkb7oB+hmyJABpRTz5s3j+fPnFC5cmMaNGwPg6OhI7dq1OXPmDL6+vjg6OmrBbp06ddixYwfHjh1j165dWv10eFV3O2fOnNqAUJG+SIqFEEKID2rKlCn4+PhgbW1NoUKF+Omnn3B0dMTf35+hQ4dy48YNYmNjsbW1JUuWLOTIkYMLFy5QpEgRJk2ahI2NDV5eXkyfPp1ly5ZRoUKFFO3LZDDpU/Lr7u/vT/fu3QkLCwMgPDycFi1a0KVLF2xsbAgMDGTUqFGcOnWKLVu2YGtrq6VTXL58mZ9//pkCBQowffp0zMzMUqVwyT2W/kgPshBCiA8iKSmJ3377jV27dtGlSxecnZ3Jly8fZmZmABQsWJAFCxawb98+cuXKhbm5OV9//TUZM2Zk48aNDBs2jNjYWAAaNmxIVFTUG2fAk8Al/dCnWOlzfy9dusTjx4+5ceMGdnZ2TJ48GTMzM44cOcKECRPImzcvLVq0wNramoYNG+Ln58fkyZOZMWMGRkZGKKUoXrw4NWrUYOXKlRw6dIh69epJcCwkQBZCCPHfvG1w1PPnzzlw4ADOzs60a9dOq1CRfL+sWbNqj8L1wsPDuXbtGlmzZtXa/uqrr+jXr98HPAuRFujzjENDQzEyMmLOnDkcOnQIe3t7fv75Z5ycnAAoUKAAJ06cYMmSJTg4OFCpUiVq1qzJyZMn2bBhAydOnKBSpUpaL3K3bt2oUqUKNWvWfOMxRfojyVpCCCHeyesz3kHKwVGvD5ALCQkhPDwcR0dHMmbMSGRkJDt27GDTpk0sWrSIa9eupdg+Ojqaq1evsnbtWvbu3Yu7uzt58+ZNsY1kBaY/r99Xc+bMoUWLFhw7dowBAwZgbm7OrVu3tFk14+LiAPjll18IDAxk+/bthIaGkjFjRho0aED+/Pnx9PQEXg3KU0qRI0cOLTiWe0yA9CALIYR4BxMmTCAoKIipU6emWK5/FL1kyRLu3btH9uzZcXV1pUiRItjb21O0aFFmzJjB6tWrCQoKwtTUlNDQUKKjo8mRIwe//PILrq6u+Pr6snjxYpKSkrh//z4eHh7069fvjVNEi/RFP/Dyxo0bmJqasnfvXmrVqkW+fPn4+uuvadmyJV5eXly6dIkCBQpoUz3b2NjQoUMHVqxYQaVKlbTJZqpVq8bixYs5evQoVatWlXtMvJEM0hNCCPFWYWFh9O3blxs3btC8eXM6duyIqamptv7cuXMMHTqUsLAwvvrqK/z9/cmcOTNNmzalX79+BAcHs3XrVs6fP0+xYsWwsbHB2dmZgIAAevToQbly5fjtt9/Yv38/+/fvx9zcnJYtW2JrawvIRDDilTlz5jBv3jzy5s1L/vz5mThxopbLHhoaSqNGjShcuDAjR47E1tY2xSC7KlWqULx4cQYPHky+fPl4+PAhERERFC1a9FOekvjMSQ+yEEKIt7p9+zZXr16lW7dutGjRAhMTE21dYmIiixcvJnv27IwePZrChQtjYmLC+PHj8fb2pnz58jg7O9OmTRvatm2bol19L5++3JazszMVK1ZMMTuZgYGBBMfpzOuTceh/IJUoUYI8efIQGBhIt27dMDMz0+4RCwsLunfvzq+//sqRI0do1qwZGTJk0PKL+/bty7Bhw6hRowb58uXDzs5Oa18G4Im3kU8eIYQQbxUTE0N4eDh58uTBxMSE6Oho7t27B7zqPd63bx+dOnWiQoUKZMmShbCwMO7evQu8Cq4h5SPrpKQk/P39WbJkCRkyZKBWrVoAGBkZacFxUlKSTMKQzujzjPXB8cOHD4mLi9N+IFWtWhUXFxciIyO5deuWtp/+HmnatCnFixdnzZo12nojIyMAmjRpwoABA/juu+9SHVfuMfE2EiALIYR4o6SkJEqXLk2hQoVYu3Ytnp6eVK5cmdWrVwOv0i8sLCwoVaoUAJMmTaJWrVq8fPmSOXPm4O7uDrzqpUtISMDb25spU6YwePBgFixYQMOGDalevXqq40qvcfqjv+YHDhygUaNGeHh44OrqysyZM/H39wegefPmFCpUiK1bt/Ls2TMMDQ21gaMZMmSgd+/e3Lp1i7179xIVFYVOp9NmbuzQoYM2IE+IdyEpFkIIId7IwMAAU1NTypQpw5o1azh58iQ//vgjTZs21dZHRETg5eXFtm3b0Ol09O/fn7p162Jpaan1BupnOtuxYwdRUVE4ODgwfvx4ChUqBMhjbvHqHli0aBFz5szBxcWFfPnyERISwpIlS7h27Rrjx48nX758uLu7M3/+fJYuXcrAgQNTpGPoy7TNmTMHZ2dnSpYsKfWMxb8mg/SEEEJoXg8iNm3axMSJEwHIkiULM2bMoEiRIgDEx8fj7u6Ov78/TZo0oWvXrlhbW2NsbAy8Gjw1btw4atasSb169QgKCiImJkbLAZXpodOnNw28DAwMpF27dpQrV45OnTppgzR/++03Fi9eTPv27Rk4cCCRkZH07NmTe/fuMXv2bIoVK5Yib/n+/fscPnwYDw+Pj35e4ssin0pCCCG0R9Gv97CVL1+e2bNnM3LkSAICAjh48CDR0dHatj/++CMA9vb2WFlZYWxsrJVqmzt3Lrdu3cLS0hKAnDlzasGxDMJLv/TX/MiRI9qyGzduEBAQwA8//KBNAz1v3jxWrFhB2bJlKVu2LHFxcZiZmdG8eXMSEhJYsmQJ8CpvWd/Xly9fPgmOxXshKRZCCCG0R9G+vr7cunWLnDlzUq1aNfLkyUPu3LmJiYmhQYMG+Pj4ULlyZe3xdePGjTl48CBTp07lzp07ODs7ExgYyP79+7l27RodOnSgTJkyqY6X/NG4SF+UUixfvpwJEyawefNmHB0dCQoKwsDAgFy5crFlyxamTp1KdHQ0PXr0oH79+lhZWWmD7urUqcPBgwfZunUr1atXp0GDBql+2Ek6hfivJEAWQgjBnTt3GDx4MP7+/mTLlo2AgABmz57NDz/8QP/+/TExMaF169YcOHCANWvWkC9fPrJly0amTJmYPHkyM2fOxNfXF19fX8zNzbGzs2PhwoUUL178U5+a+IReL9sGr548ZMmShUyZMhESEgKAk5MTiYmJNG7cmKCgIBo2bIiHhwf58+fXUnYePHiAUop8+fLRpEkTHjx4QM6cOd94XAmOxX8lAbIQQqQzySdR0Fu0aBEhISFMnjwZBwcHjIyMGDZsGAsXLuTrr7/G3d2dr7/+mtatWzNv3jy++eYbatSogU6nI1u2bIwcOZLOnTsTExNDVFSUNgmD5Bmnb68Hx/r84ypVqqCU4vHjxwBkypSJGjVqsHPnTmbOnEmVKlW0sn8A+/btY+bMmXTp0oV8+fJRrlw5rZqKEB+CfGIJIUQ6oc/T1AfHW7ZsISIigkePHrFp0yYaNGiAq6srdnZ22NjYMHDgQMqXL4+npyeRkZEYGxvTqFEj8uXLx9KlS3n27FmK9m1sbMiXL58WHEuesTh58iSNGjXi2rVr2v0Ar+7F/Pnzc/DgQQDs7OyoWbMmZmZm7Nu3j/DwcODV4L19+/bh4+ODmZkZDg4OKdrXl3kT4n2TTy0hhEgHkudkBgQEUKdOHcaPH8+9e/cICwvDwMBAe1wdHx8PgIODA02bNuX58+esWrUKAGtrazp37sz58+fZvn27tu2bSJ5x+qGUemOwGhsbS3h4OD///DPr16/XlltbW2NmZsaLFy8IDg4GoEaNGvTu3ZvNmzfTrFkz2rRpw6BBgxgwYAAvX75k+PDhFCxYMEX7co+JD0UCZCGESAd0Oh1Xr15lyZIlrFu3jq+++orJkydTvHhxsmTJQoYMGbh9+zYxMTEYGRlpwU7p0qXJmzcvR44cIS4uDiMjI5ydnSlSpAjnz5+XHjxBYmIiOp0OQ0NDXr58SWhoqFbpxMXFhZUrV2JlZcWECRPw9fUlMjISeFW3+NatW2TKlAkAMzMzWrZsyfz586lRowYZM2bEyMiIoUOHsnbtWgoXLvzJzlGkP5KDLIQQXyD94KjkPcfLly/H19eXvHnz4uHhQdWqVQHIkycPVapU4fDhw9SvX5/y5ctr7eTOnRszMzMMDQ21wVLZs2dnzpw5bx0gJdIXfS/u7Nmz8fX1JTExkYwZM9KzZ08qVqxI7ty5GTFiBEuXLmXYsGHcvn2b/v37U7hwYaKiojh16hQ1a9bU7lkXFxdcXFy0lAz9/fumAX9CfCjSgyyEEF8Q/aA4fSCh760D+OWXX8iePTsPHjygQIECwKtH4AA9evTg2bNnrFmzhkePHmn7X7lyhbt376Z6tK0PjqUHWbx48YKffvqJZcuW4ezsTPXq1cmUKRMDBgxg5syZADg6OjJx4kTc3NxYuXIlkyZNwtjYGDMzMx49egSkTpcwNDREp9OluqeF+BikB1kIIb4A+uoA+kFQu3bt0iZScHBwoGHDhpQpU4b+/fszbNgwjh49SqVKlTAxMSEpKYlixYrRvXt3Zs+eTUBAAO3btycyMpKdO3dibGxMgwYN3nhcCVrSlzf14l65coUjR47QrVs3WrVqhZmZGdHR0QwcOJDVq1dTsWJFvv32WwCGDBmCg4MDnp6evHjxgsjISJ4+ffrWtkEqoIhPQ6aaFkKIL8zGjRsZP348RYsWJSEhgZs3b2JgYMDUqVOpWrUqDRs2JCYmBk9PT63+rD4wWbBgAcuXLyc4OJhMmTJhZWXFyJEjqVKlyic+K/EpJU/ViYuLQymFiYkJAN7e3kyZMoWzZ89iZmZGfHw8RkZGXLlyheHDhxMdHc2uXbtStLdkyRJ8fHx48uQJhQsXZtOmTR/9nIT4OxIgCyHEFyApKYlJkyYRFRVFxowZAejUqRPW1tZcvHiRESNGEBMTw/z584mIiOCHH36gXbt29O7dGxMTkxS1kZ89e0ZISAhhYWFUqlRJO4bMTpb+vN6re+fOHTw8PPj111/55ptvABg/fjy///47y5Yto1SpUtrTDICZM2eycOFCPD09qV27dop1W7du5cGDB7Rt2xZTU9OPf3JC/A15biGEEF+ApKQkgoODWb9+Pdu3b6d58+ZYW1sDULJkScaOHcvDhw9ZunQpJUuWxNXVlY0bN3Ly5EmAFBOHWFlZUbhwYS04TkhIAGR2svRIHxyfOHECLy8vbt68SWhoKAcOHODhw4cAVK9enbi4OG7evEl8fDwGBgbExcUBr6pYxMXFERERAbxKl9D3y9WrV48ePXpgamoquezisyMBshBCfAEyZMhAt27dKFasGAkJCSQmJmqDm5KSkihRogQNGzZk3759BAUFMWTIEF68eMGWLVu06X7f9kDx9Vn3RPoRGRnJgAED6NChAwcPHmT9+vVkzJiRHTt2cPbsWRISEihbtiwlS5Zk7dq13L17F0CreKIPsJP3Qut/aOn/VEpJLrv47EiALIQQX4iCBQtSu3ZtIiIiuHbtWoreOnjVYxcSEsKNGzfInTs3P/74I1u3buXMmTOA9BCL1E6dOsWuXbvo2bMnnp6eLFmyhAkTJpArVy58fHz4888/MTY2pmvXrly/fp158+ZpVSkePXrE8uXLsba2pnLlym89htx34nMk3QJCCPGF0Ol0NGnShH379jF37lxq166NqamplvOp71HWVw0YNmwYefPmpU6dOp/sNYvP29atWzE1NaVBgwbY2toC8O2335I5c2a6du3Kzp07adeuHdWrV+enn35i1qxZHD58mBIlShAdHc2VK1fo3r07OXPmlBx2kaZID7IQQnxBLC0t8fDwICAggClTphAQEACg5Y1mzZpVmwhEp9PRunVr4O3pFeLLERoa+s7b6n9MmZubkylTJmxtbUlKSkIphYGBAdWqVaNOnTqsXbuWq1evAq9qaS9cuJBatWphYGBAtmzZWLFiBd27d08x4YcQaYH0IAshxBemVq1afPPNN6xevRo/Pz+KFy9OREQEu3btokmTJql686Rn78s3btw4zpw5w+TJkylUqND/vOb6pw52dnY8efKE48ePU7ly5RQzNNaoUYMdO3awe/du8ubNS86cOalatSpVq1YlOjpam0JaH2xLPWORlsjdKoQQX5iMGTPSqlUr8ufPz+PHj8mfPz/R0dGMHz+eMWPGkDlz5hTBkQTHXy79k4HSpUtz48YNDh8+TEJCwv+85vr9SpYsiY2NDYsWLQJeDbbTl2rT10Hevn07V65cSbG/PjjWTxctwbFIa+SOFUKIL1CpUqWoVq0a4eHhVK1aFS8vL9zd3QGZHjo90QfCbm5uVKpUidWrV3Pp0qV33q9cuXLUrFmTU6dO4e3tDbzqCQ4JCWHv3r00a9aMzJkzs2HDBl6+fJmqHalOIdIqmShECCG+UA8fPqRDhw5YWFjw+++/v3UqX/Fl01/3e/fu4ebmRqtWrfj5558xNzf/21QLfU/xgwcP8PT0ZM+ePTRo0ID8+fPz9OlTjh49ytixY3nw4AHjx49n7969WFtbS8qO+CJID7IQQnyh7Ozs+PHHH7l48SJbt27VHo+L9EHf/2VoaEhiYiL58+enRYsWbNy4kdOnTwN/n16jT4vImzcvQ4cOpVWrVhw6dIiFCxeyd+9eOnbsSOXKlbG0tCQ+Pp5z5879zzaFSCukB1kIIb5g4eHhdO3alcuXL3PhwgWMjIw+9UsSH9ibnhTEx8djZGREXFwcFStWpFKlSowYMQIbG5t/1OP74sUL7ty5Q9GiRbXJQObPn8/MmTPx8fGhdOnS7/18hPgUpAdZCCG+YFmzZqVt27Z4eHjIQKl0ICkpSQuOz58/z86dO7lx4wbx8fHAqxnu+vbty759+zh8+DDw7j2+SUlJmJubU6pUKYyNjQkODmbfvn2sXbuWihUrUrBgwQ9zUkJ8AtKDLIQQXzjJCU1fHj58yLBhw7h06RKGhobExcVRpUoVhg0bhp2dHfBq0J6pqSnjx4/H3t7+H90jT58+1aaVPnnyJBYWFkycOBEnJ6cPeVpCfFTSnSCEEF84CY7Tj5CQEIYPH05QUBDDhw9n2rRpdOjQgWPHjjFixAitgsXw4cO5cuUKu3fvJi4uDp1O97eTxSRfFx8fz+PHj3n69Cldu3Zl+/btEhyLL45MFCKEEEKkMW/r8b1y5Qpnz55l6NChfP/99wBUq1YNe3t7+vbtyx9//IGDgwNVqlTBxcWF9evXU758ecqXL/+3P6R0Oh3R0dGcOHFCy182NjbW8pCF+NJID7IQQgiRRiQlJZGYmPjWYPbGjRvodDptsJxSCqUUdevWxdnZmZ07d3LhwgUARo0aRVBQENu2beP58+fa9m+yefNmXFxcmD9/Pk+fPsXMzEyCY/FFkwBZCCGESAP0dYkNDQ0JDAxk27ZtXLx4kYCAAG2bggULkpCQQExMjLaPvrRfz549CQ4OJiQkBICvvvqKli1bsmbNGi314vVUiwsXLtC8eXNGjBiBm5sbEydOJH/+/B/rlIX4ZCTFQgghhEgDDAwMUEoxZcoUVqxYgYmJCREREeTJk4cRI0ZQpUoVvvrqKywsLJg/fz5eXl4pKpdYWFhgamrKjRs3qFevHgBDhw4ld+7cuLi4aNvpdDoCAwPx9PRky5YtVKlShSVLllCyZEkyZJCwQaQP0oMshBBCpBGzZs1i3bp1tG/fnsmTJzNy5EiMjY3p378/O3bsoHDhwnz77bccPHiQQ4cOodPptHQMf39/oqKisLW1BSAhIQGANm3aAH+lVyxfvhwXFxdu3LjBb7/9xqxZsyhbtqwExyJdkTJvQgghxGfkbQPwIiIiaN68OV999RVz587FxMQEgKCgINzd3bG2tmbOnDm8fPmSgQMH8ujRIwYOHEi5cuUICAhg4cKFPHnyhHnz5pEnT563HnPjxo3cv38fDw8PcuTI8eFPWIjPkATIQgghxCcQExPD8OHDKVOmDC1atCAxMRGl1Ft7am/cuMH3339Pjx496NatG0opEhISMDIyYtOmTQwZMoTBgwfTtm1brl27xsiRI7ly5QpZs2YFXk05PXr0aFxdXf/na5Pa2SK9k+clQgghxCcQGBiIv78/x48fp1GjRmTOnFlbt379egDKly+v9fZaWVmRLVs2bZBdUlKSNnX4d999h5eXF+fPn6dt27YUKVKE+fPnc+bMGf78809MTU358ccf3znoleBYpHeSgyyEEEJ8Annz5qVTp05ERUUxc+ZMAE6ePEnNmjX59ddfGT58OB06dGDr1q3Aq17dUqVKsWHDBiIjIzE0NCQhIYHExEQA8ufPz+3bt7XcYktLS+rUqUPnzp1p2bIlOp1OWyeE+HsSIAshhBAfmb70WsWKFXFzc8PHx4fAwED2799PkSJFmDt3LtOmTcPS0pIRI0YQGBiIpaUltWrVIjExkXHjxgGQIUMGDA0NiYyM5O7du1pvs7795P4ufUMIkZK8U4QQQoiPKDExEUNDQ+BV6bUGDRpw9OhRPDw8sLCwYMiQIdrUzZaWlgwePJgxY8Ywe/ZsatSoQZMmTVi9ejXZsmWjYcOG6HQ6du3aRXh4OG5ubm8NgiVtQoh3JwGyEEII8RHoA2N9cBwYGIi1tTUlSpSgadOmLFq0iOzZs+Pg4KDtU7JkSTw8PJg4cSKHDh3CxcWFLl26YGhoyLJly1ixYgUWFhYEBwfTpk0b6tev/6lOT4gvilSxEEIIIT6iAwcOsHz5csLDw5k0aRKFChXi8uXLjB8/nsDAQLZs2YKpqalWSeLPP/9k8ODBREdHs27dOq2H+NChQzx8+JCIiAjq1KlDgQIFAKlAIcT7IDnIQgghxEcQERHB4MGD6dmzJ1mzZsXBwYHo6GgAihcvTv369Xny5IlWwUKfR2xnZ0erVq24ffs2a9eu1dpzcXGhVatWdO/enQIFCmjTSktwLMR/Jz3IQgghxEfg6+vLmDFj6N69O/Xq1cPa2hp4FQgbGBjw6NEjxowZw8WLF9m9ezdZs2bV1oWEhDBu3Di2b9/O8ePHsbCwAP7qLdZvJ4R4P+TdJIQQQrwn+sk+ktNP6LFo0SIKFChAy5YtUwXHALa2tnz33XfExcUxY8aMFG1YWlrStGlTPDw8MDU11Zbre4slOBbi/ZJ3lBBCCPEfKaW0QXg6nY5r166xd+9eoqKi0Ol0xMTEEBsbS+7cuTExMdHSJ/SBrf7vlSpVokGDBqxZs4abN29iYGCg1S6uWLEiQ4cO1aaYFkJ8OBIgCyGEEP+RTqfD0NCQgIAAevToQbt27Zg9e7Y2yYeRkRE6nY6goCCePXuGgYGB1tOcvBc5a9as1K9fn+zZszNo0CAAbVCevrf4TTWOhRDvlwTIQgghxHtw/PhxmjVrRlBQEL169WLYsGE0b94cABMTE+rWrYufnx9nz54FUqZHKKXYu3cvAE5OTrRs2ZJvvvnmjceRdAohPjwZpCeEEEL8R0opBg4cyP379xk1ahSFChVKlQrx/Plz6tevj52dHUOGDKFEiRLa8j/++AMvLy/WrVuHra1tislEhBAfn0wUIoQQQvxH0dHRPHjwgIIFC1KsWDHg1UQgL168ICEhATMzM2xtbRk+fDj9+/enT58+tG/fnoSEBO7cucO+fftwdXUlS5YsAFpwLDWNhfg0JEAWQggh/iOdToejoyP79+/Hy8uLO3fucPfuXYKCgggNDSVPnjxMnz4dNzc34uPjWbVqFWPHjiVLliyYmZnRp08fmjVr9sZ2hRAfn6RYCCGEEP+Bvpf30qVLrFmzhm3btlGwYEGKFi1KgQIFyJQpE7NmzaJYsWJ4eXkBEBcXR2hoKIGBgRQrVkzrMZbUCiE+DxIgCyGEELy/dIawsDAyZcpEYmIimTNnBmDgwIGcOHGCnTt3pqhjrCeBsRCfFxkKK4QQIt26desWS5cuJSIiAp1OR2JiIomJif+6PaUU2bJlw8TEhMyZM/Py5UtOnz6Nn58fbm5ubwyOAQmOhfjMSA6yEEKIdGv+/Pls27YNGxsb6tSpowWq0dHR3L59G2tra6ytrd+5d1m/TXh4OKdOneLu3bts2rQJY2Nj6tev/0HPRQjx/kiKhRBCiHRHPzlHZGQkLi4ulClThl9//ZVcuXKxePFi5s2bR2xsLJkzZ2bMmDFUqVKFzJkzv1OgHBoaysCBA/H39yc2NpZatWoxdOhQMmXK9JHOTgjxX0mALIQQIl3S5/0uXLgQT09PRo4cSdGiRRk+fDhVq1Yle/bsHD16lBs3bjB06FDc3d3fue2zZ8/y7NkznJycsLW1TXE8IcTnTwJkIYQQ6Y5+umb9rHS1atUiW7ZsmJubky1bNn799VeyZs2KUoo6deqQPXt2RowYQdGiRVNMDf26N/Uwv34sIcTnT96tQggh0h0DA4MUAevw4cO5evUq169fp2fPnmTNmpW4uDh0Oh0DBgzg3r17bN26FaXU3wa6+uBY3/ek316CYyHSFnnHCiGE+OIlJiaS/IHp0aNHadKkifb3GjVq4OrqSnh4OH5+fgBkyPBqHLurqytly5Zl//79HD58GID/9fD1fVTEEEJ8OhIgCyGE+GKEhoamWqaUwtDQEJ1Ox5UrVzhx4gTHjh3j6tWrrF27VttuwIABABw7doznz59jYGBAXFwcAD179uTFixds376dFy9eoNPptNSJ5MfR27x5M927d+fcuXMf4jSFEB+YBMhCCCG+CBMmTKBNmzbcvHkzxXKdTkdERAR9+vThxx9/ZMqUKezevRsAT09PYmNjAcibNy8tW7Zk165dHDlyBABjY2OSkpJwdHSkUaNGHDx4EF9fX+CvnGJ9YKzT6bh8+TKtWrViyJAh5MiRg6+//vqjnLsQ4v2SAFkIIUSapg9Q7e3tuX37Ni9evEi1zdq1a9mzZw/9+/dn/PjxbN68maZNmxIREcGECRO07QYNGoSJiQkbN27k4cOHAFqaROfOncmaNSvm5uYpjqvT6QgJCWHo0KE0a9YMQ0NDli5dyi+//IKlpeUHPXchxIchVSyEEEJ8MQIDA7G2tk6xLC4ujiZNmpAlSxZ8fHy0nt/Q0FAmT57Mpk2b2LZtGwULFgRg5cqVjBkzhkGDBtGuXTsA4uPjMTIyIjo6OkU948TERLy9vVm0aBEWFhZ06dKFb7/99q0z5gkh0gbpQRZCCPHFsLa25tKlSyxbtkzrSY6MjCQhIQELCwstOE5MTMTCwgJ3d3dy5MjBmDFjtDZatmyJvb09Pj4+Wg6xkZERAJkyZSIpKUnrPV6xYgXTp0/Hw8MDHx8fGjduLMGxEF8ACZCFEEKkCeHh4dy+fRv4q7bwm6pErFy5kgkTJnD27FkALCwsMDEx4enTp1rahF7p0qUpVaoUJ0+e1PKOAfr378/z58+1wDg5AwMDrZxbkyZN2L59Oz169MDKyur9nKgQ4pOTAFkIIcRnLyoqig4dOmgpD/oAVT8zXVBQkLbtL7/8grm5OZs2beLx48cAuLu7c/nyZU6ePKntl5CQgJGREUWKFAFg9OjRWhvVqlXj7NmzODk5/e3rMjMzo0CBAlLnWIgvjLyjhRBCfPZMTU1p3rw5YWFhrFixQguQd+3ahbu7O23atGH8+PFcv36dTJky0a1bN3bv3s3x48cBaNGiBXnz5mXFihWcOXMGeFXn+OnTp+zatQtXV1eCgoJYsmQJAAkJCVoQLYRIf2SQnhBCiDQhKiqKvn37cuHCBQ4ePMjBgwcZPXo0xYsXJyEhgcuXL2Ntbc22bdsA+Pbbb8mSJQvjxo3D3t6e48eP06lTJ7766iv69esHgJ+fH5cvX+ann37C29ubsLAwVq5cScaMGT/lqQohPjHpQRZCCJEmmJqa0rZtW2JiYvD09MTPz4+GDRsybtw4lixZwrBhw3jy5Anjxo0DYMSIEVy+fJm9e/cSGxtL5cqVGT16NObm5vTu3Zs+ffqwZs0a3NzcqFixIrlz5yYoKIjnz59/4jMVQnxqGT71CxBCCCHeVbly5WjcuDGrV68mV65cLF68WBscV7NmTe7evcuSJUv47rvvqFq1KlWrVmXdunWUKVOGChUq0KRJE2rXrs3t27cJCAigevXqWtWJ+Ph4IiIiUpRxE0KkT9KDLIQQIs3IkCEDLVu2xNHRkfj4eIyNjbVKFlmzZqVevXrY2toyZcoUAEaNGkVgYCA7d+4kLCwMAHNzc0qXLk29evUwNTUlLCyMnTt3sm/fPr777juyZcv2ic5OCPG5kABZCCFEmvL111/j5uZGUFAQfn5+KQbTFSxYkNatW3Ps2DF27tyJnZ0dLVq0YPXq1Vy+fDlFO0+ePGHatGksWrSIX375BRsbG5o3b/4pTkkI8ZmRQXpCCCHSnJCQELp160ZYWBi7du1Cp9OhlEKn0/H48WNGjhzJo0eP2LlzJwA+Pj54eHikaOPGjRu0adOGbNmy0bBhQ3r06PEpTkUI8RmSAFkIIUSatG3bNvr370+fPn3o3LkziYmJWl3k7du307dvX7y8vKhevbq2jz6ITkpKwsDAgJs3b5I7d27MzMw+0VkIIT5HEiALIYRIk2JiYhgwYABHjx5l7969WFpaakHyixcvCA8Px9bWVtteHxwLIcT/IjnIQggh0qSMGTPSrl07MmbMyKRJk4C/ZtgzNzfH1tYWpRT6fiAJjoUQ70oCZCGEEGlWiRIlqFGjBn/88QeBgYGppnzW6XQSGAsh/jFJsRBCCJGmBQcHA5AjR45P/EqEEF8KCZCFEEJ8EZIP0hNCiP9CAmQhhBBCCCGSkRxkIYQQQgghkpEAWQghhBBCiGQkQBZCCCGEECIZCZCFEEIIIYRIRgJkIYQQQgghkpEAWQghPpDWrVtTs2bNT/0y/pVTp07h4ODAxo0bP/VLEUKIjy7Dp34BQgjxrk6dOoWHh0eKZcbGxlhZWVG+fHk6duxIwYIFP9GrE0II8aWQAFkIkebUr1+fatWqARAbG8vNmzdZt24du3btYsuWLeTOnfsTv0IhhBBpmQTIQog0p0iRIjRq1CjFsrx58zJu3Dj27NlD27ZtP80L+wJERkZiZmb2qV+GEEJ8UpKDLIT4IlhZWQFgZGSUYvn27dtp0aIFpUqVokSJEjRt2pSdO3em2t/BwYHBgwdz4cIFWrVqRcmSJalQoQLDhg0jKioq1fZBQUGMHTuWWrVqUaxYMSpVqkS7du04duxYqm0DAwPp27cv5cqVo0SJEnTo0IF79+6l2Gbjxo04ODhw4sQJZs+eTY0aNXBycqJp06ZcvHgRgNOnT9OiRQtKlixJ1apVmTNnTqpjHT16lN69e1OrVi2cnJwoW7Ys7du35/Tp06m21edIP3z4kF69elG+fHnKlCnz9n9kYNOmTRQtWpRevXoRGxsLwPnz5+nYsSNVqlShePHiODs706lTJ+11CyFEWiM9yEKINCc6OprQ0FDgVYrFrVu3mDZtGtmzZ6d27dradtOmTcPLywtnZ2d+/vlnDAwM2LNnDz///DMjR46kZcuWKdq9fv06Xbt2pXHjxtSvX5/Tp0+zfv16DAwMGDNmjLbdo0ePaNGiBSEhITRq1IhixYoRHR2Nn58fx48fp0qVKtq2L1++pFWrVpQoUYI+ffrw6NEjli9fTvfu3dm6dSuGhoYpXoOnpydJSUl4eHgQHx/P4sWLad++Pb/99hvDhg2jWbNmNGjQgB07djBz5kxsbW1T9KZv2rSJ8PBw3N3dsbGxITAwkHXr1tG2bVuWL19O2bJlUxwvKiqKVq1aUbp0aXr37q39u76Jl5cX06ZNo2XLlgwfPhwDAwPu3r1L+/btyZEjBx4eHlhaWhISEsK5c+e4ceMGJUuWfPcLK4QQnwslhBBpxMmTJ5W9vf0b/6tbt666c+eOtu2VK1eUvb29mjJlSqp2unXrpkqVKqVevHihLbO3t1cODg7q4sWLKbbt1KmTKlKkiIqMjNSWdezYUdnb26vDhw+najsxMVH7/1atWil7e3u1YMGCFNt4e3un2n/Dhg3K3t5eubu7q9jYWG353r17lb29vSpSpIi6dOmStjw2NlZVqVJFNWvWLEXbUVFRqV5TUFCQKl++vOrYsWOK5frXN3Xq1FT76P+tN2zYoBITE9Uvv/yi7O3t1bx581Jst2zZMmVvb6/8/PxStSGEEGmVpFgIIdKc5s2bs2TJEpYsWYKXlxf9+/fn+fPndO7cmcePHwOwZcsWdDod7u7uhIaGpvivZs2aREVFpUoBKFmyJCVKlEixrGLFiiQkJGjthoWFceTIEZydnXF2dk712gwMDFL9/fXKGxUrVgTgwYMHqfZv0aIFxsbG2t/1Pb5OTk4UL15cW25sbEzx4sW5f/9+iv0zZ86s/X9UVBTPnz/HwMCAEiVKcOnSpVTHA+jQocMbl8OrHvpevXqxdu1aJk6cSNeuXVOsNzc3B2Dfvn1ayoUQQqR1kmIhhEhz8ubNS+XKlbW/16hRg/Lly9OsWTM8PT2ZNm0a/v7+KKVwc3N7azvBwcEp/m5nZ5dqm2zZsgGvAmOAP//8E6UURYoUeafXamVlhYmJyd+2+XevIWvWrADY2tqm2jZr1qyp2vjzzz+ZNm0aR48eJSIiIsU6nU6Xqg0LCwuyZMny1tc/efJkoqKi8PT0pEGDBqnW16tXjz/++AMvLy+WLl1KiRIlqFq1KvXq1ZNqIkKINEsCZCHEF6FEiRKYm5tz8uRJAJRS6HQ6vL29U+X56n399dcp/v627fTt/Rv/tM3Xe6DfpR29qKgoWrZsSXR0NG3atMHe3h5TU1MMDAyYP3++9m+TXKZMmf62TVdXV3bv3s2iRYuoWrUq2bNnT7He2NiYJUuWcOnSJY4cOcLZs2eZOXMms2fPZsqUKXzzzTf/83ULIcTnRgJkIcQXIzExkbi4OADy5cvHkSNH+Oqrr97r5CF58uRBp9Nx/fr199bm+3LixAmePXvG+PHjadKkSYp106dP/1dtVqxYkSZNmtC1a1c8PDxYunQplpaWqbZzcnLCyckJgICAANzd3Zk+fboEyEKINElykIUQX4Rjx47x8uVLihYtCkDDhg0BmDp1KomJiam2fz294l1ly5aNatWqcfjwYY4fP55q/b/taX4f9L3Mr7+Go0eP4ufn96/brVChAt7e3jx+/BgPDw+CgoK0dW+qemFjY4OFhQXh4eH/+phCCPEpSQ+yECLNuXbtGr6+vgDExcVx584d1q5di5GREb179wZe9Wj27NmTWbNm4e7uzrfffou1tTXPnj3j6tWrHD58mCtXrvyr448YMYJr167RqVMn3N3dKVq0KLGxsfj5+ZE7d24GDBjwvk71HylTpgw5c+Zk0qRJPH78GBsbG65fv46vry/29vbcunXrX7ddtmxZFi9eTMeOHWndujXLli3D2tqaefPmcezYMapXr46trS1KKQ4cOMDdu3fp2LHjezw7IYT4eCRAFkKkOVu3bmXr1q3Aq5zdbNmyUaVKFTp37qw95gf46aefKFasGD4+PixfvpyXL19iaWlJoUKFGDZs2L8+vp2dHRs2bGDOnDkcPnwYX19fsmTJgqOjI82bN//P5/dvZcmShYULFzJ58mRWrFhBQkICxYoVw9vbm/Xr1/+nABleVflYsmQJHTp00IJkV1dXgoKC2LlzJ8HBwWTMmJG8efMyduxYvv/++/d0ZkII8XHp1Kd8HiiEEEIIIcRnRnKQhRBCCCGESEYCZCGEEEIIIZKRAFkIIYQQQohkJEAWQgghhBAiGQmQhRBCCCGESEYCZCGEEEIIIZKRAFkIIYQQQohkJEAWQgghhBAiGQmQhRBCCCGESEYCZCGEEEIIIZL5P3xzrHqFTm93AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot top 5 benchmarks and use different colors for each benchmark\n",
    "plt.title('Top 5 benchmarks')\n",
    "plt.xticks(rotation=30)\n",
    "plt.xlabel('Benchmarks')\n",
    "plt.ylabel('Spearman correlation')\n",
    "plt.ylim(0.82, 0.88)\n",
    "for i, benchmark in enumerate(list(corr_dict.keys())[:5]):\n",
    "    plt.bar(benchmark.replace(\"hendrycksTest-\", \"\"), corr_dict[benchmark], color=plt.cm.Set1(i))\n",
    "plt.savefig('assets/top5_benchmarks.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot bottom 5 benchmarks and use different colors for each benchmark\n",
    "plt.title('Bottom 5 benchmarks')\n",
    "plt.xticks(rotation=30)\n",
    "plt.xlabel('Benchmarks')\n",
    "plt.ylabel('Spearman correlation')\n",
    "plt.ylim(0.0, 0.65)\n",
    "for i, benchmark in enumerate(list(corr_dict.keys())[-5:]):\n",
    "    plt.bar(benchmark.replace(\"hendrycksTest-\", \"\"), corr_dict[benchmark], color=plt.cm.Set1(i))\n",
    "plt.savefig('assets/bottom5_benchmarks.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LASSO to select benchmarks\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for model in filtered_hf_models:\n",
    "    x = []\n",
    "    for benchmark in common_benchmarks:\n",
    "        result = filtered_hf_models[model]['results'][benchmark]\n",
    "        if 'acc_norm' in result:\n",
    "            x.append(result['acc_norm'])\n",
    "        elif 'mc2' in result:\n",
    "            x.append(result['mc2'])\n",
    "\n",
    "    X.append(x)\n",
    "    y.append(elo.loc[model]['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7893568583658496"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Randomly split into train and test\n",
    "np.random.seed(0)\n",
    "idx = np.random.permutation(len(X))\n",
    "X_train = X[idx[:int(0.8 * len(X))]]\n",
    "y_train = y[idx[:int(0.8 * len(X))]]\n",
    "X_test = X[idx[int(0.8 * len(X)):]]\n",
    "y_test = y[idx[int(0.8 * len(X)):]]\n",
    "\n",
    "reg = Lasso(alpha=0.5, max_iter=10000)\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "        -0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,  98.18643732,   0.        ,   0.        ,\n",
       "         8.94064115,  -0.        ,   0.        ,   0.        ,\n",
       "        -0.        ,   0.        ,  -0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,  19.25616947,   0.        ,   0.        ,\n",
       "         0.        ,  -0.        ,   0.        ,   0.        ,\n",
       "       115.63763795,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        , 101.7670315 ,\n",
       "         0.        ,   0.        ,   0.        ,  -0.        ,\n",
       "        -0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,  43.41625442,   0.        ,   0.        ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hendrycksTest-high_school_us_history 98.18643731996913\n",
      "hendrycksTest-high_school_government_and_politics 8.940641147551098\n",
      "hendrycksTest-high_school_psychology 19.256169471555346\n",
      "hendrycksTest-us_foreign_policy 115.6376379509916\n",
      "hendrycksTest-sociology 101.76703149585038\n",
      "hendrycksTest-marketing 43.416254420620994\n"
     ]
    }
   ],
   "source": [
    "# Print selected benchmarks\n",
    "selected_benchmarks = {}\n",
    "for i in range(len(common_benchmarks)):\n",
    "    if np.abs(reg.coef_[i]) > 1e-3:\n",
    "        selected_benchmarks[common_benchmarks[i]] = reg.coef_[i]\n",
    "        print(common_benchmarks[i].split('|')[1], reg.coef_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by coefficient\n",
    "selected_benchmarks = {k: v for k, v in sorted(selected_benchmarks.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot selected benchmarks\n",
    "plt.title('Selected benchmarks')\n",
    "plt.xticks(rotation=30)\n",
    "plt.xlabel('Benchmarks')\n",
    "plt.ylabel('LASSO coefficient')\n",
    "for i, benchmark in enumerate(selected_benchmarks.keys()):\n",
    "    plt.bar(benchmark.split('|')[1].replace(\"hendrycksTest-\", \"\"), selected_benchmarks[benchmark], color=plt.cm.Set1(i))\n",
    "plt.savefig('assets/selected_benchmarks.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.942857142857143\n"
     ]
    }
   ],
   "source": [
    "# Check Spearman correlation between elo and each benchmark\n",
    "\n",
    "elo_scores = []\n",
    "benchmark_scores = []\n",
    "for i in idx[int(0.8 * len(X)):]:\n",
    "    model = list(filtered_hf_models.keys())[i]\n",
    "    elo_scores.append(elo.loc[model]['rating'])\n",
    "    benchmark_score = 0\n",
    "    for benchmark in selected_benchmarks.keys():\n",
    "        result = filtered_hf_models[model]['results'][benchmark]\n",
    "        if 'acc_norm' in result:\n",
    "            benchmark_score += result['acc_norm'] * selected_benchmarks[benchmark]\n",
    "        elif 'mc2' in result:\n",
    "            benchmark_score += result['mc2'] * selected_benchmarks[benchmark]\n",
    "    benchmark_scores.append(benchmark_score)\n",
    "corr = spearmanr(elo_scores, benchmark_scores).correlation\n",
    "\n",
    "print(corr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
