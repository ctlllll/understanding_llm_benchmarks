{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle\n",
    "import json\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Chatbot Arena\n",
    "chatbot_arena_files = glob('data/chatbot-arena-leaderboard/*.csv')\n",
    "chatbot_arena_files.sort(key=lambda x: int(x[-12:-4]))\n",
    "# Head: key,Model,MT-bench (score),MMLU,License,Organization,Link\n",
    "chatbot_arena = pd.read_csv(chatbot_arena_files[-1], sep=',', header=0)\n",
    "\n",
    "chatbot_arena_elo_files = glob('data/chatbot-arena-leaderboard/*.pkl')\n",
    "chatbot_arena_elo_files.sort(key=lambda x: int(x[-12:-4]))\n",
    "chatbot_arena_elo = pickle.load(open(chatbot_arena_elo_files[-1], 'rb'))\n",
    "\n",
    "# Use elo as ground truth\n",
    "elo = chatbot_arena_elo['leaderboard_table_df']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>Model</th>\n",
       "      <th>MT-bench (score)</th>\n",
       "      <th>MMLU</th>\n",
       "      <th>License</th>\n",
       "      <th>Organization</th>\n",
       "      <th>Link</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>wizardlm-30b</td>\n",
       "      <td>WizardLM-30B</td>\n",
       "      <td>7.01</td>\n",
       "      <td>0.587</td>\n",
       "      <td>Non-commercial</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>https://huggingface.co/WizardLM/WizardLM-30B-V1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>vicuna-13b-16k</td>\n",
       "      <td>Vicuna-13B-16k</td>\n",
       "      <td>6.92</td>\n",
       "      <td>0.545</td>\n",
       "      <td>Llama 2 Community</td>\n",
       "      <td>LMSYS</td>\n",
       "      <td>https://huggingface.co/lmsys/vicuna-13b-v1.5-16k</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>wizardlm-13b-v1.1</td>\n",
       "      <td>WizardLM-13B-v1.1</td>\n",
       "      <td>6.76</td>\n",
       "      <td>0.500</td>\n",
       "      <td>Non-commercial</td>\n",
       "      <td>Microsoft</td>\n",
       "      <td>https://huggingface.co/WizardLM/WizardLM-13B-V1.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>tulu-30b</td>\n",
       "      <td>Tulu-30B</td>\n",
       "      <td>6.43</td>\n",
       "      <td>0.581</td>\n",
       "      <td>Non-commercial</td>\n",
       "      <td>AllenAI/UW</td>\n",
       "      <td>https://huggingface.co/allenai/tulu-30b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>guanaco-65b</td>\n",
       "      <td>Guanaco-65B</td>\n",
       "      <td>6.41</td>\n",
       "      <td>0.621</td>\n",
       "      <td>Non-commercial</td>\n",
       "      <td>UW</td>\n",
       "      <td>https://huggingface.co/timdettmers/guanaco-65b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>63</th>\n",
       "      <td>stablelm-tuned-alpha-7b</td>\n",
       "      <td>StableLM-Tuned-Alpha-7B</td>\n",
       "      <td>2.75</td>\n",
       "      <td>0.244</td>\n",
       "      <td>CC-BY-NC-SA-4.0</td>\n",
       "      <td>Stability AI</td>\n",
       "      <td>https://huggingface.co/stabilityai/stablelm-tu...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>dolly-v2-12b</td>\n",
       "      <td>Dolly-V2-12B</td>\n",
       "      <td>3.28</td>\n",
       "      <td>0.257</td>\n",
       "      <td>MIT</td>\n",
       "      <td>Databricks</td>\n",
       "      <td>https://huggingface.co/databricks/dolly-v2-12b</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>llama-13b</td>\n",
       "      <td>LLaMA-13B</td>\n",
       "      <td>2.61</td>\n",
       "      <td>0.470</td>\n",
       "      <td>Non-commercial</td>\n",
       "      <td>Meta</td>\n",
       "      <td>https://arxiv.org/abs/2302.13971</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>66</th>\n",
       "      <td>mistral-medium</td>\n",
       "      <td>Mistral Medium</td>\n",
       "      <td>8.61</td>\n",
       "      <td>0.753</td>\n",
       "      <td>Proprietary</td>\n",
       "      <td>Mistral</td>\n",
       "      <td>https://mistral.ai/news/la-plateforme/</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>67</th>\n",
       "      <td>llama2-70b-steerlm-chat</td>\n",
       "      <td>Llama2-70B-SteerLM-Chat</td>\n",
       "      <td>7.54</td>\n",
       "      <td>-</td>\n",
       "      <td>Llama 2 Community</td>\n",
       "      <td>Nvidia</td>\n",
       "      <td>https://huggingface.co/nvidia/Llama2-70B-Steer...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>68 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                        key                    Model MT-bench (score)   MMLU  \\\n",
       "0              wizardlm-30b             WizardLM-30B             7.01  0.587   \n",
       "1            vicuna-13b-16k           Vicuna-13B-16k             6.92  0.545   \n",
       "2         wizardlm-13b-v1.1        WizardLM-13B-v1.1             6.76  0.500   \n",
       "3                  tulu-30b                 Tulu-30B             6.43  0.581   \n",
       "4               guanaco-65b              Guanaco-65B             6.41  0.621   \n",
       "..                      ...                      ...              ...    ...   \n",
       "63  stablelm-tuned-alpha-7b  StableLM-Tuned-Alpha-7B             2.75  0.244   \n",
       "64             dolly-v2-12b             Dolly-V2-12B             3.28  0.257   \n",
       "65                llama-13b                LLaMA-13B             2.61  0.470   \n",
       "66           mistral-medium           Mistral Medium             8.61  0.753   \n",
       "67  llama2-70b-steerlm-chat  Llama2-70B-SteerLM-Chat             7.54      -   \n",
       "\n",
       "              License  Organization  \\\n",
       "0      Non-commercial     Microsoft   \n",
       "1   Llama 2 Community         LMSYS   \n",
       "2      Non-commercial     Microsoft   \n",
       "3      Non-commercial    AllenAI/UW   \n",
       "4      Non-commercial            UW   \n",
       "..                ...           ...   \n",
       "63    CC-BY-NC-SA-4.0  Stability AI   \n",
       "64                MIT    Databricks   \n",
       "65     Non-commercial          Meta   \n",
       "66        Proprietary       Mistral   \n",
       "67  Llama 2 Community        Nvidia   \n",
       "\n",
       "                                                 Link  \n",
       "0   https://huggingface.co/WizardLM/WizardLM-30B-V1.0  \n",
       "1    https://huggingface.co/lmsys/vicuna-13b-v1.5-16k  \n",
       "2   https://huggingface.co/WizardLM/WizardLM-13B-V1.1  \n",
       "3             https://huggingface.co/allenai/tulu-30b  \n",
       "4   https://huggingface.co/timdettmers/guanaco-65b...  \n",
       "..                                                ...  \n",
       "63  https://huggingface.co/stabilityai/stablelm-tu...  \n",
       "64     https://huggingface.co/databricks/dolly-v2-12b  \n",
       "65                   https://arxiv.org/abs/2302.13971  \n",
       "66             https://mistral.ai/news/la-plateforme/  \n",
       "67  https://huggingface.co/nvidia/Llama2-70B-Steer...  \n",
       "\n",
       "[68 rows x 7 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chatbot_arena"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Alpaca Eval\n",
    "# Head: name,win_rate,avg_length,link,samples,filter\n",
    "alpaca_eval = pd.read_csv('data/alpaca_eval_gpt4_leaderboard.csv', sep=',', header=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>win_rate</th>\n",
       "      <th>avg_length</th>\n",
       "      <th>link</th>\n",
       "      <th>samples</th>\n",
       "      <th>filter</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>GPT-4 Turbo</td>\n",
       "      <td>97.699005</td>\n",
       "      <td>2049</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>XwinLM 70b V0.1</td>\n",
       "      <td>95.568040</td>\n",
       "      <td>1775</td>\n",
       "      <td>https://github.com/Xwin-LM/Xwin-LM</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>PairRM+Tulu 2+DPO 70B (best-of-16)</td>\n",
       "      <td>95.398010</td>\n",
       "      <td>1607</td>\n",
       "      <td>https://huggingface.co/llm-blender/PairRM</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>GPT-4</td>\n",
       "      <td>95.279503</td>\n",
       "      <td>1365</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Tulu 2+DPO 70B</td>\n",
       "      <td>95.031056</td>\n",
       "      <td>1418</td>\n",
       "      <td>https://huggingface.co/allenai/tulu-2-dpo-70b</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>Alpaca 7B</td>\n",
       "      <td>26.459627</td>\n",
       "      <td>396</td>\n",
       "      <td>https://huggingface.co/tatsu-lab/alpaca-7b-wdiff</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>minimal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>Pythia 12B OASST SFT</td>\n",
       "      <td>25.962733</td>\n",
       "      <td>726</td>\n",
       "      <td>https://huggingface.co/OpenAssistant/oasst-sft...</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>Falcon 7B Instruct</td>\n",
       "      <td>23.602484</td>\n",
       "      <td>478</td>\n",
       "      <td>https://huggingface.co/tiiuae/falcon-7b-instruct</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>verified</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>Baichuan-13B-Chat</td>\n",
       "      <td>21.801242</td>\n",
       "      <td>1727</td>\n",
       "      <td>https://huggingface.co/baichuan-inc/Baichuan-1...</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>community</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>Davinci001</td>\n",
       "      <td>15.174129</td>\n",
       "      <td>296</td>\n",
       "      <td>NaN</td>\n",
       "      <td>https://github.com/tatsu-lab/alpaca_eval/blob/...</td>\n",
       "      <td>minimal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>97 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                  name   win_rate  avg_length  \\\n",
       "0                          GPT-4 Turbo  97.699005        2049   \n",
       "1                      XwinLM 70b V0.1  95.568040        1775   \n",
       "2   PairRM+Tulu 2+DPO 70B (best-of-16)  95.398010        1607   \n",
       "3                                GPT-4  95.279503        1365   \n",
       "4                       Tulu 2+DPO 70B  95.031056        1418   \n",
       "..                                 ...        ...         ...   \n",
       "92                           Alpaca 7B  26.459627         396   \n",
       "93                Pythia 12B OASST SFT  25.962733         726   \n",
       "94                  Falcon 7B Instruct  23.602484         478   \n",
       "95                   Baichuan-13B-Chat  21.801242        1727   \n",
       "96                          Davinci001  15.174129         296   \n",
       "\n",
       "                                                 link  \\\n",
       "0                                                 NaN   \n",
       "1                  https://github.com/Xwin-LM/Xwin-LM   \n",
       "2           https://huggingface.co/llm-blender/PairRM   \n",
       "3                                                 NaN   \n",
       "4       https://huggingface.co/allenai/tulu-2-dpo-70b   \n",
       "..                                                ...   \n",
       "92   https://huggingface.co/tatsu-lab/alpaca-7b-wdiff   \n",
       "93  https://huggingface.co/OpenAssistant/oasst-sft...   \n",
       "94   https://huggingface.co/tiiuae/falcon-7b-instruct   \n",
       "95  https://huggingface.co/baichuan-inc/Baichuan-1...   \n",
       "96                                                NaN   \n",
       "\n",
       "                                              samples     filter  \n",
       "0   https://github.com/tatsu-lab/alpaca_eval/blob/...    minimal  \n",
       "1   https://github.com/tatsu-lab/alpaca_eval/blob/...  community  \n",
       "2   https://github.com/tatsu-lab/alpaca_eval/blob/...  community  \n",
       "3   https://github.com/tatsu-lab/alpaca_eval/blob/...    minimal  \n",
       "4   https://github.com/tatsu-lab/alpaca_eval/blob/...  community  \n",
       "..                                                ...        ...  \n",
       "92  https://github.com/tatsu-lab/alpaca_eval/blob/...    minimal  \n",
       "93  https://github.com/tatsu-lab/alpaca_eval/blob/...   verified  \n",
       "94  https://github.com/tatsu-lab/alpaca_eval/blob/...   verified  \n",
       "95  https://github.com/tatsu-lab/alpaca_eval/blob/...  community  \n",
       "96  https://github.com/tatsu-lab/alpaca_eval/blob/...    minimal  \n",
       "\n",
       "[97 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "alpaca_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Number of lines in elo\n",
    "len(elo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rating</th>\n",
       "      <th>variance</th>\n",
       "      <th>rating_q975</th>\n",
       "      <th>rating_q025</th>\n",
       "      <th>num_battles</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>RWKV-4-Raven-14B</th>\n",
       "      <td>924.231727</td>\n",
       "      <td>56.102110</td>\n",
       "      <td>939.938277</td>\n",
       "      <td>910.571472</td>\n",
       "      <td>5231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>alpaca-13b</th>\n",
       "      <td>903.741314</td>\n",
       "      <td>48.294819</td>\n",
       "      <td>918.853719</td>\n",
       "      <td>890.549086</td>\n",
       "      <td>6206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatglm-6b</th>\n",
       "      <td>882.464210</td>\n",
       "      <td>52.359596</td>\n",
       "      <td>896.432612</td>\n",
       "      <td>868.335295</td>\n",
       "      <td>5266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatglm2-6b</th>\n",
       "      <td>928.856627</td>\n",
       "      <td>73.475978</td>\n",
       "      <td>946.275697</td>\n",
       "      <td>913.007148</td>\n",
       "      <td>2924</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>chatglm3-6b</th>\n",
       "      <td>959.975825</td>\n",
       "      <td>66.490085</td>\n",
       "      <td>977.777208</td>\n",
       "      <td>945.182976</td>\n",
       "      <td>3696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-1</th>\n",
       "      <td>1149.379341</td>\n",
       "      <td>47.590691</td>\n",
       "      <td>1164.315198</td>\n",
       "      <td>1136.601676</td>\n",
       "      <td>16956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-2.0</th>\n",
       "      <td>1130.616704</td>\n",
       "      <td>52.432423</td>\n",
       "      <td>1145.070539</td>\n",
       "      <td>1117.537633</td>\n",
       "      <td>11204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-2.1</th>\n",
       "      <td>1118.772077</td>\n",
       "      <td>46.599912</td>\n",
       "      <td>1133.145271</td>\n",
       "      <td>1106.276687</td>\n",
       "      <td>20883</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>claude-instant-1</th>\n",
       "      <td>1109.380031</td>\n",
       "      <td>47.385636</td>\n",
       "      <td>1124.636944</td>\n",
       "      <td>1096.643598</td>\n",
       "      <td>16182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>codellama-34b-instruct</th>\n",
       "      <td>1041.413340</td>\n",
       "      <td>51.029353</td>\n",
       "      <td>1057.412081</td>\n",
       "      <td>1029.251071</td>\n",
       "      <td>6464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dolly-v2-12b</th>\n",
       "      <td>822.353912</td>\n",
       "      <td>62.582951</td>\n",
       "      <td>838.585948</td>\n",
       "      <td>808.132744</td>\n",
       "      <td>3716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>dolphin-2.2.1-mistral-7b</th>\n",
       "      <td>1064.768876</td>\n",
       "      <td>78.750698</td>\n",
       "      <td>1083.265298</td>\n",
       "      <td>1047.314159</td>\n",
       "      <td>1817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>falcon-180b-chat</th>\n",
       "      <td>1031.113032</td>\n",
       "      <td>106.252106</td>\n",
       "      <td>1052.685022</td>\n",
       "      <td>1012.965179</td>\n",
       "      <td>1420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>fastchat-t5-3b</th>\n",
       "      <td>873.504920</td>\n",
       "      <td>54.712636</td>\n",
       "      <td>888.513353</td>\n",
       "      <td>860.286757</td>\n",
       "      <td>4592</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-pro</th>\n",
       "      <td>1113.770208</td>\n",
       "      <td>55.986817</td>\n",
       "      <td>1129.495825</td>\n",
       "      <td>1100.714562</td>\n",
       "      <td>6981</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gemini-pro-dev-api</th>\n",
       "      <td>1120.446710</td>\n",
       "      <td>87.216253</td>\n",
       "      <td>1138.509585</td>\n",
       "      <td>1102.130002</td>\n",
       "      <td>1898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0314</th>\n",
       "      <td>1104.473206</td>\n",
       "      <td>51.538926</td>\n",
       "      <td>1118.994330</td>\n",
       "      <td>1091.161583</td>\n",
       "      <td>5961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-0613</th>\n",
       "      <td>1116.167858</td>\n",
       "      <td>46.265192</td>\n",
       "      <td>1129.292757</td>\n",
       "      <td>1103.156929</td>\n",
       "      <td>26583</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-3.5-turbo-1106</th>\n",
       "      <td>1072.312769</td>\n",
       "      <td>51.981770</td>\n",
       "      <td>1086.650442</td>\n",
       "      <td>1058.491161</td>\n",
       "      <td>11892</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0314</th>\n",
       "      <td>1190.498739</td>\n",
       "      <td>46.843298</td>\n",
       "      <td>1204.909427</td>\n",
       "      <td>1176.584615</td>\n",
       "      <td>16237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-0613</th>\n",
       "      <td>1159.625234</td>\n",
       "      <td>45.537522</td>\n",
       "      <td>1173.198148</td>\n",
       "      <td>1147.474296</td>\n",
       "      <td>20884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt-4-turbo</th>\n",
       "      <td>1249.334661</td>\n",
       "      <td>47.673083</td>\n",
       "      <td>1263.406855</td>\n",
       "      <td>1236.312400</td>\n",
       "      <td>23069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>gpt4all-13b-snoozy</th>\n",
       "      <td>936.706295</td>\n",
       "      <td>90.889155</td>\n",
       "      <td>954.252226</td>\n",
       "      <td>917.672765</td>\n",
       "      <td>1938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>guanaco-33b</th>\n",
       "      <td>1030.921900</td>\n",
       "      <td>70.252167</td>\n",
       "      <td>1048.594417</td>\n",
       "      <td>1016.927777</td>\n",
       "      <td>3243</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>koala-13b</th>\n",
       "      <td>965.468002</td>\n",
       "      <td>47.989863</td>\n",
       "      <td>980.399131</td>\n",
       "      <td>952.830071</td>\n",
       "      <td>7420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-13b</th>\n",
       "      <td>800.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>800.000000</td>\n",
       "      <td>2600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-2-13b-chat</th>\n",
       "      <td>1036.726952</td>\n",
       "      <td>52.534565</td>\n",
       "      <td>1051.544813</td>\n",
       "      <td>1024.016891</td>\n",
       "      <td>10926</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-2-70b-chat</th>\n",
       "      <td>1079.026713</td>\n",
       "      <td>49.730905</td>\n",
       "      <td>1094.483283</td>\n",
       "      <td>1065.938859</td>\n",
       "      <td>13514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama-2-7b-chat</th>\n",
       "      <td>1017.263710</td>\n",
       "      <td>54.535924</td>\n",
       "      <td>1031.644764</td>\n",
       "      <td>1003.435207</td>\n",
       "      <td>6579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>llama2-70b-steerlm-chat</th>\n",
       "      <td>1076.677334</td>\n",
       "      <td>136.489899</td>\n",
       "      <td>1098.699928</td>\n",
       "      <td>1052.097510</td>\n",
       "      <td>902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-7b-instruct</th>\n",
       "      <td>1016.125896</td>\n",
       "      <td>59.753307</td>\n",
       "      <td>1031.987495</td>\n",
       "      <td>1002.254832</td>\n",
       "      <td>6499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mistral-medium</th>\n",
       "      <td>1149.866992</td>\n",
       "      <td>54.689082</td>\n",
       "      <td>1164.881520</td>\n",
       "      <td>1137.110738</td>\n",
       "      <td>6586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mixtral-8x7b-instruct-v0.1</th>\n",
       "      <td>1123.483354</td>\n",
       "      <td>50.099729</td>\n",
       "      <td>1138.422271</td>\n",
       "      <td>1110.213375</td>\n",
       "      <td>12469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mpt-30b-chat</th>\n",
       "      <td>1042.337339</td>\n",
       "      <td>66.770031</td>\n",
       "      <td>1058.679541</td>\n",
       "      <td>1027.374729</td>\n",
       "      <td>2872</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mpt-7b-chat</th>\n",
       "      <td>930.081278</td>\n",
       "      <td>59.190757</td>\n",
       "      <td>946.299573</td>\n",
       "      <td>915.892727</td>\n",
       "      <td>4274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>oasst-pythia-12b</th>\n",
       "      <td>896.371111</td>\n",
       "      <td>47.305148</td>\n",
       "      <td>910.567532</td>\n",
       "      <td>882.906863</td>\n",
       "      <td>6711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openchat-3.5</th>\n",
       "      <td>1076.346554</td>\n",
       "      <td>57.231535</td>\n",
       "      <td>1090.433089</td>\n",
       "      <td>1062.961093</td>\n",
       "      <td>6701</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>openhermes-2.5-mistral-7b</th>\n",
       "      <td>1078.870526</td>\n",
       "      <td>63.656642</td>\n",
       "      <td>1095.151793</td>\n",
       "      <td>1064.536235</td>\n",
       "      <td>3077</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>palm-2</th>\n",
       "      <td>1004.453952</td>\n",
       "      <td>50.836705</td>\n",
       "      <td>1019.682167</td>\n",
       "      <td>991.498447</td>\n",
       "      <td>9420</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pplx-70b-online</th>\n",
       "      <td>1072.953906</td>\n",
       "      <td>58.767088</td>\n",
       "      <td>1089.549251</td>\n",
       "      <td>1059.026585</td>\n",
       "      <td>4996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>pplx-7b-online</th>\n",
       "      <td>1034.725132</td>\n",
       "      <td>70.789004</td>\n",
       "      <td>1052.130330</td>\n",
       "      <td>1019.844920</td>\n",
       "      <td>4421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>qwen-14b-chat</th>\n",
       "      <td>1036.205943</td>\n",
       "      <td>57.965385</td>\n",
       "      <td>1052.433991</td>\n",
       "      <td>1021.953429</td>\n",
       "      <td>4315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>solar-10.7b-instruct-v1.0</th>\n",
       "      <td>1064.646974</td>\n",
       "      <td>62.711149</td>\n",
       "      <td>1080.911189</td>\n",
       "      <td>1049.839653</td>\n",
       "      <td>4598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>stablelm-tuned-alpha-7b</th>\n",
       "      <td>844.426061</td>\n",
       "      <td>56.977440</td>\n",
       "      <td>860.575227</td>\n",
       "      <td>830.496042</td>\n",
       "      <td>3518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>starling-lm-7b-alpha</th>\n",
       "      <td>1092.284114</td>\n",
       "      <td>70.860354</td>\n",
       "      <td>1109.471498</td>\n",
       "      <td>1076.054019</td>\n",
       "      <td>3947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tulu-2-dpo-70b</th>\n",
       "      <td>1106.076435</td>\n",
       "      <td>61.743800</td>\n",
       "      <td>1122.131282</td>\n",
       "      <td>1090.985676</td>\n",
       "      <td>4494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-13b</th>\n",
       "      <td>1040.659617</td>\n",
       "      <td>43.058316</td>\n",
       "      <td>1053.391708</td>\n",
       "      <td>1028.132395</td>\n",
       "      <td>15049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-33b</th>\n",
       "      <td>1093.778618</td>\n",
       "      <td>49.367996</td>\n",
       "      <td>1108.368994</td>\n",
       "      <td>1081.385716</td>\n",
       "      <td>15632</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vicuna-7b</th>\n",
       "      <td>1004.065525</td>\n",
       "      <td>49.960610</td>\n",
       "      <td>1018.228388</td>\n",
       "      <td>991.467511</td>\n",
       "      <td>7562</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizardlm-13b</th>\n",
       "      <td>1057.001411</td>\n",
       "      <td>54.522107</td>\n",
       "      <td>1072.514809</td>\n",
       "      <td>1042.792519</td>\n",
       "      <td>7767</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>wizardlm-70b</th>\n",
       "      <td>1104.218972</td>\n",
       "      <td>54.607665</td>\n",
       "      <td>1120.951499</td>\n",
       "      <td>1090.371642</td>\n",
       "      <td>7531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>yi-34b-chat</th>\n",
       "      <td>1110.896308</td>\n",
       "      <td>61.806389</td>\n",
       "      <td>1127.163691</td>\n",
       "      <td>1096.357334</td>\n",
       "      <td>5055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyr-7b-alpha</th>\n",
       "      <td>1037.230490</td>\n",
       "      <td>95.319603</td>\n",
       "      <td>1056.172817</td>\n",
       "      <td>1016.866037</td>\n",
       "      <td>1943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>zephyr-7b-beta</th>\n",
       "      <td>1049.230704</td>\n",
       "      <td>53.183853</td>\n",
       "      <td>1064.177045</td>\n",
       "      <td>1035.788742</td>\n",
       "      <td>10531</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 rating    variance  rating_q975  rating_q025  \\\n",
       "RWKV-4-Raven-14B             924.231727   56.102110   939.938277   910.571472   \n",
       "alpaca-13b                   903.741314   48.294819   918.853719   890.549086   \n",
       "chatglm-6b                   882.464210   52.359596   896.432612   868.335295   \n",
       "chatglm2-6b                  928.856627   73.475978   946.275697   913.007148   \n",
       "chatglm3-6b                  959.975825   66.490085   977.777208   945.182976   \n",
       "claude-1                    1149.379341   47.590691  1164.315198  1136.601676   \n",
       "claude-2.0                  1130.616704   52.432423  1145.070539  1117.537633   \n",
       "claude-2.1                  1118.772077   46.599912  1133.145271  1106.276687   \n",
       "claude-instant-1            1109.380031   47.385636  1124.636944  1096.643598   \n",
       "codellama-34b-instruct      1041.413340   51.029353  1057.412081  1029.251071   \n",
       "dolly-v2-12b                 822.353912   62.582951   838.585948   808.132744   \n",
       "dolphin-2.2.1-mistral-7b    1064.768876   78.750698  1083.265298  1047.314159   \n",
       "falcon-180b-chat            1031.113032  106.252106  1052.685022  1012.965179   \n",
       "fastchat-t5-3b               873.504920   54.712636   888.513353   860.286757   \n",
       "gemini-pro                  1113.770208   55.986817  1129.495825  1100.714562   \n",
       "gemini-pro-dev-api          1120.446710   87.216253  1138.509585  1102.130002   \n",
       "gpt-3.5-turbo-0314          1104.473206   51.538926  1118.994330  1091.161583   \n",
       "gpt-3.5-turbo-0613          1116.167858   46.265192  1129.292757  1103.156929   \n",
       "gpt-3.5-turbo-1106          1072.312769   51.981770  1086.650442  1058.491161   \n",
       "gpt-4-0314                  1190.498739   46.843298  1204.909427  1176.584615   \n",
       "gpt-4-0613                  1159.625234   45.537522  1173.198148  1147.474296   \n",
       "gpt-4-turbo                 1249.334661   47.673083  1263.406855  1236.312400   \n",
       "gpt4all-13b-snoozy           936.706295   90.889155   954.252226   917.672765   \n",
       "guanaco-33b                 1030.921900   70.252167  1048.594417  1016.927777   \n",
       "koala-13b                    965.468002   47.989863   980.399131   952.830071   \n",
       "llama-13b                    800.000000    0.000000   800.000000   800.000000   \n",
       "llama-2-13b-chat            1036.726952   52.534565  1051.544813  1024.016891   \n",
       "llama-2-70b-chat            1079.026713   49.730905  1094.483283  1065.938859   \n",
       "llama-2-7b-chat             1017.263710   54.535924  1031.644764  1003.435207   \n",
       "llama2-70b-steerlm-chat     1076.677334  136.489899  1098.699928  1052.097510   \n",
       "mistral-7b-instruct         1016.125896   59.753307  1031.987495  1002.254832   \n",
       "mistral-medium              1149.866992   54.689082  1164.881520  1137.110738   \n",
       "mixtral-8x7b-instruct-v0.1  1123.483354   50.099729  1138.422271  1110.213375   \n",
       "mpt-30b-chat                1042.337339   66.770031  1058.679541  1027.374729   \n",
       "mpt-7b-chat                  930.081278   59.190757   946.299573   915.892727   \n",
       "oasst-pythia-12b             896.371111   47.305148   910.567532   882.906863   \n",
       "openchat-3.5                1076.346554   57.231535  1090.433089  1062.961093   \n",
       "openhermes-2.5-mistral-7b   1078.870526   63.656642  1095.151793  1064.536235   \n",
       "palm-2                      1004.453952   50.836705  1019.682167   991.498447   \n",
       "pplx-70b-online             1072.953906   58.767088  1089.549251  1059.026585   \n",
       "pplx-7b-online              1034.725132   70.789004  1052.130330  1019.844920   \n",
       "qwen-14b-chat               1036.205943   57.965385  1052.433991  1021.953429   \n",
       "solar-10.7b-instruct-v1.0   1064.646974   62.711149  1080.911189  1049.839653   \n",
       "stablelm-tuned-alpha-7b      844.426061   56.977440   860.575227   830.496042   \n",
       "starling-lm-7b-alpha        1092.284114   70.860354  1109.471498  1076.054019   \n",
       "tulu-2-dpo-70b              1106.076435   61.743800  1122.131282  1090.985676   \n",
       "vicuna-13b                  1040.659617   43.058316  1053.391708  1028.132395   \n",
       "vicuna-33b                  1093.778618   49.367996  1108.368994  1081.385716   \n",
       "vicuna-7b                   1004.065525   49.960610  1018.228388   991.467511   \n",
       "wizardlm-13b                1057.001411   54.522107  1072.514809  1042.792519   \n",
       "wizardlm-70b                1104.218972   54.607665  1120.951499  1090.371642   \n",
       "yi-34b-chat                 1110.896308   61.806389  1127.163691  1096.357334   \n",
       "zephyr-7b-alpha             1037.230490   95.319603  1056.172817  1016.866037   \n",
       "zephyr-7b-beta              1049.230704   53.183853  1064.177045  1035.788742   \n",
       "\n",
       "                            num_battles  \n",
       "RWKV-4-Raven-14B                   5231  \n",
       "alpaca-13b                         6206  \n",
       "chatglm-6b                         5266  \n",
       "chatglm2-6b                        2924  \n",
       "chatglm3-6b                        3696  \n",
       "claude-1                          16956  \n",
       "claude-2.0                        11204  \n",
       "claude-2.1                        20883  \n",
       "claude-instant-1                  16182  \n",
       "codellama-34b-instruct             6464  \n",
       "dolly-v2-12b                       3716  \n",
       "dolphin-2.2.1-mistral-7b           1817  \n",
       "falcon-180b-chat                   1420  \n",
       "fastchat-t5-3b                     4592  \n",
       "gemini-pro                         6981  \n",
       "gemini-pro-dev-api                 1898  \n",
       "gpt-3.5-turbo-0314                 5961  \n",
       "gpt-3.5-turbo-0613                26583  \n",
       "gpt-3.5-turbo-1106                11892  \n",
       "gpt-4-0314                        16237  \n",
       "gpt-4-0613                        20884  \n",
       "gpt-4-turbo                       23069  \n",
       "gpt4all-13b-snoozy                 1938  \n",
       "guanaco-33b                        3243  \n",
       "koala-13b                          7420  \n",
       "llama-13b                          2600  \n",
       "llama-2-13b-chat                  10926  \n",
       "llama-2-70b-chat                  13514  \n",
       "llama-2-7b-chat                    6579  \n",
       "llama2-70b-steerlm-chat             902  \n",
       "mistral-7b-instruct                6499  \n",
       "mistral-medium                     6586  \n",
       "mixtral-8x7b-instruct-v0.1        12469  \n",
       "mpt-30b-chat                       2872  \n",
       "mpt-7b-chat                        4274  \n",
       "oasst-pythia-12b                   6711  \n",
       "openchat-3.5                       6701  \n",
       "openhermes-2.5-mistral-7b          3077  \n",
       "palm-2                             9420  \n",
       "pplx-70b-online                    4996  \n",
       "pplx-7b-online                     4421  \n",
       "qwen-14b-chat                      4315  \n",
       "solar-10.7b-instruct-v1.0          4598  \n",
       "stablelm-tuned-alpha-7b            3518  \n",
       "starling-lm-7b-alpha               3947  \n",
       "tulu-2-dpo-70b                     4494  \n",
       "vicuna-13b                        15049  \n",
       "vicuna-33b                        15632  \n",
       "vicuna-7b                          7562  \n",
       "wizardlm-13b                       7767  \n",
       "wizardlm-70b                       7531  \n",
       "yi-34b-chat                        5055  \n",
       "zephyr-7b-alpha                    1943  \n",
       "zephyr-7b-beta                    10531  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "elo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out models with benchmark results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = {}\n",
    "\n",
    "for key in elo.index:\n",
    "    if chatbot_arena.loc[chatbot_arena['key'] == key].empty:\n",
    "        print(key)\n",
    "    else:\n",
    "        model = chatbot_arena.loc[chatbot_arena['key'] == key].iloc[0].to_dict()\n",
    "        models[key] = model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_models = {}\n",
    "for key in models.keys():\n",
    "    if 'huggingface' in models[key]['Link']:\n",
    "        hf_models[key] = models[key]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_hf_models = {}\n",
    "\n",
    "# Retrieve hf model results from open-llm-leaderboard\n",
    "for key in hf_models.keys():\n",
    "    link = hf_models[key]['Link']\n",
    "    path = 'data/open-llm-leaderboard/' + '/'.join(link.split('/')[-2:])\n",
    "    if os.path.exists(path):\n",
    "        result_files = glob(path + '/*.json')\n",
    "        results = {}\n",
    "        for file in result_files:\n",
    "            with open(file) as f:\n",
    "                result = json.load(f)\n",
    "                results |= result.get('results', {})\n",
    "\n",
    "        filtered_hf_models[key] = hf_models[key]\n",
    "        filtered_hf_models[key]['results'] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "26"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_hf_models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chatglm2-6b': {'key': 'chatglm2-6b',\n",
       "  'Model': 'ChatGLM2-6B',\n",
       "  'MT-bench (score)': '4.96',\n",
       "  'MMLU': '0.455',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'Tsinghua',\n",
       "  'Link': 'https://huggingface.co/THUDM/chatglm2-6b',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.37457337883959047,\n",
       "    'acc_stderr': 0.014144193471893444,\n",
       "    'acc_norm': 0.38822525597269625,\n",
       "    'acc_norm_stderr': 0.014241614207414044},\n",
       "   'harness|hellaswag|10': {'acc': 0.4596693885680143,\n",
       "    'acc_stderr': 0.004973522582431221,\n",
       "    'acc_norm': 0.5902210714997013,\n",
       "    'acc_norm_stderr': 0.004907877144720029},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4148148148148148,\n",
       "    'acc_stderr': 0.042561937679014075,\n",
       "    'acc_norm': 0.4148148148148148,\n",
       "    'acc_norm_stderr': 0.042561937679014075},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5328947368421053,\n",
       "    'acc_stderr': 0.04060127035236395,\n",
       "    'acc_norm': 0.5328947368421053,\n",
       "    'acc_norm_stderr': 0.04060127035236395},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.61,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.61,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.4867924528301887,\n",
       "    'acc_stderr': 0.030762134874500476,\n",
       "    'acc_norm': 0.4867924528301887,\n",
       "    'acc_norm_stderr': 0.030762134874500476},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.04076663253918567,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.04076663253918567},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.42196531791907516,\n",
       "    'acc_stderr': 0.0376574669386515,\n",
       "    'acc_norm': 0.42196531791907516,\n",
       "    'acc_norm_stderr': 0.0376574669386515},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.21568627450980393,\n",
       "    'acc_stderr': 0.040925639582376556,\n",
       "    'acc_norm': 0.21568627450980393,\n",
       "    'acc_norm_stderr': 0.040925639582376556},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.03202563076101736,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.03202563076101736},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.30701754385964913,\n",
       "    'acc_stderr': 0.043391383225798615,\n",
       "    'acc_norm': 0.30701754385964913,\n",
       "    'acc_norm_stderr': 0.043391383225798615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4689655172413793,\n",
       "    'acc_stderr': 0.04158632762097828,\n",
       "    'acc_norm': 0.4689655172413793,\n",
       "    'acc_norm_stderr': 0.04158632762097828},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.31746031746031744,\n",
       "    'acc_stderr': 0.023973861998992072,\n",
       "    'acc_norm': 0.31746031746031744,\n",
       "    'acc_norm_stderr': 0.023973861998992072},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3492063492063492,\n",
       "    'acc_stderr': 0.04263906892795133,\n",
       "    'acc_norm': 0.3492063492063492,\n",
       "    'acc_norm_stderr': 0.04263906892795133},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.5580645161290323,\n",
       "    'acc_stderr': 0.02825155790684973,\n",
       "    'acc_norm': 0.5580645161290323,\n",
       "    'acc_norm_stderr': 0.02825155790684973},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.45320197044334976,\n",
       "    'acc_stderr': 0.03502544650845872,\n",
       "    'acc_norm': 0.45320197044334976,\n",
       "    'acc_norm_stderr': 0.03502544650845872},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.42,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.42,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.038254602783800246,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.038254602783800246},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.035402943770953675,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.035402943770953675},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.5699481865284974,\n",
       "    'acc_stderr': 0.03572954333144809,\n",
       "    'acc_norm': 0.5699481865284974,\n",
       "    'acc_norm_stderr': 0.03572954333144809},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.44871794871794873,\n",
       "    'acc_stderr': 0.025217315184846482,\n",
       "    'acc_norm': 0.44871794871794873,\n",
       "    'acc_norm_stderr': 0.025217315184846482},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.26296296296296295,\n",
       "    'acc_stderr': 0.026842057873833706,\n",
       "    'acc_norm': 0.26296296296296295,\n",
       "    'acc_norm_stderr': 0.026842057873833706},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.4327731092436975,\n",
       "    'acc_stderr': 0.03218358107742613,\n",
       "    'acc_norm': 0.4327731092436975,\n",
       "    'acc_norm_stderr': 0.03218358107742613},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2913907284768212,\n",
       "    'acc_stderr': 0.03710185726119994,\n",
       "    'acc_norm': 0.2913907284768212,\n",
       "    'acc_norm_stderr': 0.03710185726119994},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.5926605504587156,\n",
       "    'acc_stderr': 0.021065986244412895,\n",
       "    'acc_norm': 0.5926605504587156,\n",
       "    'acc_norm_stderr': 0.021065986244412895},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.33796296296296297,\n",
       "    'acc_stderr': 0.032259413526312945,\n",
       "    'acc_norm': 0.33796296296296297,\n",
       "    'acc_norm_stderr': 0.032259413526312945},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.5882352941176471,\n",
       "    'acc_stderr': 0.03454236585380609,\n",
       "    'acc_norm': 0.5882352941176471,\n",
       "    'acc_norm_stderr': 0.03454236585380609},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6624472573839663,\n",
       "    'acc_stderr': 0.03078154910202621,\n",
       "    'acc_norm': 0.6624472573839663,\n",
       "    'acc_norm_stderr': 0.03078154910202621},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.4663677130044843,\n",
       "    'acc_stderr': 0.033481800170603065,\n",
       "    'acc_norm': 0.4663677130044843,\n",
       "    'acc_norm_stderr': 0.033481800170603065},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.48091603053435117,\n",
       "    'acc_stderr': 0.04382094705550988,\n",
       "    'acc_norm': 0.48091603053435117,\n",
       "    'acc_norm_stderr': 0.04382094705550988},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.6033057851239669,\n",
       "    'acc_stderr': 0.04465869780531009,\n",
       "    'acc_norm': 0.6033057851239669,\n",
       "    'acc_norm_stderr': 0.04465869780531009},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04766075165356461,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04766075165356461},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.49079754601226994,\n",
       "    'acc_stderr': 0.03927705600787443,\n",
       "    'acc_norm': 0.49079754601226994,\n",
       "    'acc_norm_stderr': 0.03927705600787443},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.4375,\n",
       "    'acc_stderr': 0.04708567521880525,\n",
       "    'acc_norm': 0.4375,\n",
       "    'acc_norm_stderr': 0.04708567521880525},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6213592233009708,\n",
       "    'acc_stderr': 0.048026946982589726,\n",
       "    'acc_norm': 0.6213592233009708,\n",
       "    'acc_norm_stderr': 0.048026946982589726},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.6923076923076923,\n",
       "    'acc_stderr': 0.030236389942173102,\n",
       "    'acc_norm': 0.6923076923076923,\n",
       "    'acc_norm_stderr': 0.030236389942173102},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.5964240102171137,\n",
       "    'acc_stderr': 0.017544332237926417,\n",
       "    'acc_norm': 0.5964240102171137,\n",
       "    'acc_norm_stderr': 0.017544332237926417},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5289017341040463,\n",
       "    'acc_stderr': 0.026874085883518348,\n",
       "    'acc_norm': 0.5289017341040463,\n",
       "    'acc_norm_stderr': 0.026874085883518348},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.23575418994413408,\n",
       "    'acc_stderr': 0.014196375686290804,\n",
       "    'acc_norm': 0.23575418994413408,\n",
       "    'acc_norm_stderr': 0.014196375686290804},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5326797385620915,\n",
       "    'acc_stderr': 0.028568699752225868,\n",
       "    'acc_norm': 0.5326797385620915,\n",
       "    'acc_norm_stderr': 0.028568699752225868},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.4983922829581994,\n",
       "    'acc_stderr': 0.02839794490780661,\n",
       "    'acc_norm': 0.4983922829581994,\n",
       "    'acc_norm_stderr': 0.02839794490780661},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.49382716049382713,\n",
       "    'acc_stderr': 0.02781862396258329,\n",
       "    'acc_norm': 0.49382716049382713,\n",
       "    'acc_norm_stderr': 0.02781862396258329},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.02812163604063988,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.02812163604063988},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3559322033898305,\n",
       "    'acc_stderr': 0.012228645537277566,\n",
       "    'acc_norm': 0.3559322033898305,\n",
       "    'acc_norm_stderr': 0.012228645537277566},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.35294117647058826,\n",
       "    'acc_stderr': 0.029029422815681393,\n",
       "    'acc_norm': 0.35294117647058826,\n",
       "    'acc_norm_stderr': 0.029029422815681393},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.020102583895887188,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.020102583895887188},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5818181818181818,\n",
       "    'acc_stderr': 0.04724577405731572,\n",
       "    'acc_norm': 0.5818181818181818,\n",
       "    'acc_norm_stderr': 0.04724577405731572},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5673469387755102,\n",
       "    'acc_stderr': 0.031717528240626645,\n",
       "    'acc_norm': 0.5673469387755102,\n",
       "    'acc_norm_stderr': 0.031717528240626645},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6218905472636815,\n",
       "    'acc_stderr': 0.034288678487786564,\n",
       "    'acc_norm': 0.6218905472636815,\n",
       "    'acc_norm_stderr': 0.034288678487786564},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.41566265060240964,\n",
       "    'acc_stderr': 0.03836722176598052,\n",
       "    'acc_norm': 0.41566265060240964,\n",
       "    'acc_norm_stderr': 0.03836722176598052},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.038295098689947286,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.038295098689947286},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.2998776009791922,\n",
       "    'mc1_stderr': 0.016040352966713616,\n",
       "    'mc2': 0.48081249614578586,\n",
       "    'mc2_stderr': 0.01612488256095027},\n",
       "   'all': {'acc': 0.46492840933020524,\n",
       "    'acc_stderr': 0.03535258865761692,\n",
       "    'acc_norm': 0.46737253746689666,\n",
       "    'acc_norm_stderr': 0.035353127221986566,\n",
       "    'mc1': 0.2998776009791922,\n",
       "    'mc1_stderr': 0.016040352966713616,\n",
       "    'mc2': 0.48081249614578586,\n",
       "    'mc2_stderr': 0.01612488256095027}}},\n",
       " 'codellama-34b-instruct': {'key': 'codellama-34b-instruct',\n",
       "  'Model': 'CodeLlama-34B-instruct',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.537',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf',\n",
       "  'results': {'harness|drop|3': {'em': 0.0014681208053691276,\n",
       "    'em_stderr': 0.00039210421902985756,\n",
       "    'f1': 0.05437604865771826,\n",
       "    'f1_stderr': 0.0011898517432786567},\n",
       "   'harness|gsm8k|5': {'acc': 0.3100833965125095,\n",
       "    'acc_stderr': 0.012740305717376268},\n",
       "   'harness|winogrande|5': {'acc': 0.745067087608524,\n",
       "    'acc_stderr': 0.012248806969376422},\n",
       "   'all': {'em': 0.0014681208053691276,\n",
       "    'em_stderr': 0.00039210421902985756,\n",
       "    'f1': 0.05437604865771826,\n",
       "    'f1_stderr': 0.0011898517432786567,\n",
       "    'acc': 0.5275752420605168,\n",
       "    'acc_stderr': 0.012494556343376345},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5093856655290102,\n",
       "    'acc_stderr': 0.014608816322065,\n",
       "    'acc_norm': 0.5426621160409556,\n",
       "    'acc_norm_stderr': 0.01455810654392406},\n",
       "   'harness|hellaswag|10': {'acc': 0.5637323242381995,\n",
       "    'acc_stderr': 0.004949080334816024,\n",
       "    'acc_norm': 0.7691694881497709,\n",
       "    'acc_norm_stderr': 0.004205030476886528},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.45925925925925926,\n",
       "    'acc_stderr': 0.04304979692464244,\n",
       "    'acc_norm': 0.45925925925925926,\n",
       "    'acc_norm_stderr': 0.04304979692464244},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5921052631578947,\n",
       "    'acc_stderr': 0.03999309712777472,\n",
       "    'acc_norm': 0.5921052631578947,\n",
       "    'acc_norm_stderr': 0.03999309712777472},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.61,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.61,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.49056603773584906,\n",
       "    'acc_stderr': 0.0307673947078081,\n",
       "    'acc_norm': 0.49056603773584906,\n",
       "    'acc_norm_stderr': 0.0307673947078081},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.04179596617581,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.04179596617581},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4508670520231214,\n",
       "    'acc_stderr': 0.03794012674697028,\n",
       "    'acc_norm': 0.4508670520231214,\n",
       "    'acc_norm_stderr': 0.03794012674697028},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3431372549019608,\n",
       "    'acc_stderr': 0.047240073523838876,\n",
       "    'acc_norm': 0.3431372549019608,\n",
       "    'acc_norm_stderr': 0.047240073523838876},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.69,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.69,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.49361702127659574,\n",
       "    'acc_stderr': 0.03268335899936336,\n",
       "    'acc_norm': 0.49361702127659574,\n",
       "    'acc_norm_stderr': 0.03268335899936336},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.42105263157894735,\n",
       "    'acc_stderr': 0.046446020912223177,\n",
       "    'acc_norm': 0.42105263157894735,\n",
       "    'acc_norm_stderr': 0.046446020912223177},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5241379310344828,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.5241379310344828,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3915343915343915,\n",
       "    'acc_stderr': 0.025138091388851102,\n",
       "    'acc_norm': 0.3915343915343915,\n",
       "    'acc_norm_stderr': 0.025138091388851102},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4603174603174603,\n",
       "    'acc_stderr': 0.04458029125470973,\n",
       "    'acc_norm': 0.4603174603174603,\n",
       "    'acc_norm_stderr': 0.04458029125470973},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6096774193548387,\n",
       "    'acc_stderr': 0.027751256636969576,\n",
       "    'acc_norm': 0.6096774193548387,\n",
       "    'acc_norm_stderr': 0.027751256636969576},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3891625615763547,\n",
       "    'acc_stderr': 0.034304624161038716,\n",
       "    'acc_norm': 0.3891625615763547,\n",
       "    'acc_norm_stderr': 0.034304624161038716},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.036639749943912434,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.036639749943912434},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.03191178226713549,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.03191178226713549},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7616580310880829,\n",
       "    'acc_stderr': 0.03074890536390989,\n",
       "    'acc_norm': 0.7616580310880829,\n",
       "    'acc_norm_stderr': 0.03074890536390989},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5153846153846153,\n",
       "    'acc_stderr': 0.02533900301010651,\n",
       "    'acc_norm': 0.5153846153846153,\n",
       "    'acc_norm_stderr': 0.02533900301010651},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.34074074074074073,\n",
       "    'acc_stderr': 0.02889774874113114,\n",
       "    'acc_norm': 0.34074074074074073,\n",
       "    'acc_norm_stderr': 0.02889774874113114},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5168067226890757,\n",
       "    'acc_stderr': 0.03246013680375308,\n",
       "    'acc_norm': 0.5168067226890757,\n",
       "    'acc_norm_stderr': 0.03246013680375308},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.36423841059602646,\n",
       "    'acc_stderr': 0.03929111781242742,\n",
       "    'acc_norm': 0.36423841059602646,\n",
       "    'acc_norm_stderr': 0.03929111781242742},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7064220183486238,\n",
       "    'acc_stderr': 0.019525151122639667,\n",
       "    'acc_norm': 0.7064220183486238,\n",
       "    'acc_norm_stderr': 0.019525151122639667},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.033247089118091176,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.033247089118091176},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.029102254389674082,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.029102254389674082},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7637130801687764,\n",
       "    'acc_stderr': 0.027652153144159256,\n",
       "    'acc_norm': 0.7637130801687764,\n",
       "    'acc_norm_stderr': 0.027652153144159256},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6098654708520179,\n",
       "    'acc_stderr': 0.03273766725459156,\n",
       "    'acc_norm': 0.6098654708520179,\n",
       "    'acc_norm_stderr': 0.03273766725459156},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6259541984732825,\n",
       "    'acc_stderr': 0.042438692422305246,\n",
       "    'acc_norm': 0.6259541984732825,\n",
       "    'acc_norm_stderr': 0.042438692422305246},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7355371900826446,\n",
       "    'acc_stderr': 0.040261875275912046,\n",
       "    'acc_norm': 0.7355371900826446,\n",
       "    'acc_norm_stderr': 0.040261875275912046},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.04557239513497751,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.04557239513497751},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6993865030674846,\n",
       "    'acc_stderr': 0.03602511318806771,\n",
       "    'acc_norm': 0.6993865030674846,\n",
       "    'acc_norm_stderr': 0.03602511318806771},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.38392857142857145,\n",
       "    'acc_stderr': 0.04616143075028547,\n",
       "    'acc_norm': 0.38392857142857145,\n",
       "    'acc_norm_stderr': 0.04616143075028547},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.046561471100123514,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.046561471100123514},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.811965811965812,\n",
       "    'acc_stderr': 0.025598193686652265,\n",
       "    'acc_norm': 0.811965811965812,\n",
       "    'acc_norm_stderr': 0.025598193686652265},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.0498887651569859,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.0498887651569859},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7075351213282248,\n",
       "    'acc_stderr': 0.016267000684598645,\n",
       "    'acc_norm': 0.7075351213282248,\n",
       "    'acc_norm_stderr': 0.016267000684598645},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5780346820809249,\n",
       "    'acc_stderr': 0.02658923114217426,\n",
       "    'acc_norm': 0.5780346820809249,\n",
       "    'acc_norm_stderr': 0.02658923114217426},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.31620111731843575,\n",
       "    'acc_stderr': 0.015551673652172554,\n",
       "    'acc_norm': 0.31620111731843575,\n",
       "    'acc_norm_stderr': 0.015551673652172554},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5915032679738562,\n",
       "    'acc_stderr': 0.028146405993096358,\n",
       "    'acc_norm': 0.5915032679738562,\n",
       "    'acc_norm_stderr': 0.028146405993096358},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.594855305466238,\n",
       "    'acc_stderr': 0.027882383791325963,\n",
       "    'acc_norm': 0.594855305466238,\n",
       "    'acc_norm_stderr': 0.027882383791325963},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6141975308641975,\n",
       "    'acc_stderr': 0.027085401226132143,\n",
       "    'acc_norm': 0.6141975308641975,\n",
       "    'acc_norm_stderr': 0.027085401226132143},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.40425531914893614,\n",
       "    'acc_stderr': 0.029275532159704725,\n",
       "    'acc_norm': 0.40425531914893614,\n",
       "    'acc_norm_stderr': 0.029275532159704725},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3891786179921773,\n",
       "    'acc_stderr': 0.012452613934287012,\n",
       "    'acc_norm': 0.3891786179921773,\n",
       "    'acc_norm_stderr': 0.012452613934287012},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.47794117647058826,\n",
       "    'acc_stderr': 0.030343264224213535,\n",
       "    'acc_norm': 0.47794117647058826,\n",
       "    'acc_norm_stderr': 0.030343264224213535},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.020227834851568375,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.020227834851568375},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6489795918367347,\n",
       "    'acc_stderr': 0.03055531675557364,\n",
       "    'acc_norm': 0.6489795918367347,\n",
       "    'acc_norm_stderr': 0.03055531675557364},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7611940298507462,\n",
       "    'acc_stderr': 0.03014777593540922,\n",
       "    'acc_norm': 0.7611940298507462,\n",
       "    'acc_norm_stderr': 0.03014777593540922},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.041633319989322626,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.041633319989322626},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.42168674698795183,\n",
       "    'acc_stderr': 0.03844453181770917,\n",
       "    'acc_norm': 0.42168674698795183,\n",
       "    'acc_norm_stderr': 0.03844453181770917},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7485380116959064,\n",
       "    'acc_stderr': 0.033275044238468436,\n",
       "    'acc_norm': 0.7485380116959064,\n",
       "    'acc_norm_stderr': 0.033275044238468436},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.44437538633055657,\n",
       "    'mc2_stderr': 0.014550940721814704}}},\n",
       " 'dolly-v2-12b': {'key': 'dolly-v2-12b',\n",
       "  'Model': 'Dolly-V2-12B',\n",
       "  'MT-bench (score)': '3.28',\n",
       "  'MMLU': '0.257',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'Databricks',\n",
       "  'Link': 'https://huggingface.co/databricks/dolly-v2-12b',\n",
       "  'results': {'harness|drop|3': {'em': 0.0016778523489932886,\n",
       "    'em_stderr': 0.0004191330178826844,\n",
       "    'f1': 0.06285968959731549,\n",
       "    'f1_stderr': 0.0014820300080071475},\n",
       "   'harness|gsm8k|5': {'acc': 0.012130401819560273,\n",
       "    'acc_stderr': 0.0030152942428909495},\n",
       "   'harness|winogrande|5': {'acc': 0.6085240726124704,\n",
       "    'acc_stderr': 0.013717487071290854},\n",
       "   'all': {'acc': 0.2661068958576839,\n",
       "    'acc_stderr': 0.03186337801557282,\n",
       "    'acc_norm': 0.26986300658126056,\n",
       "    'acc_norm_stderr': 0.0318588678937482,\n",
       "    'mc1': 0.200734394124847,\n",
       "    'mc1_stderr': 0.014022045717482156,\n",
       "    'mc2': 0.33827081326692277,\n",
       "    'mc2_stderr': 0.014862542606576355},\n",
       "   'harness|arc:challenge|25': {'acc': 0.38139931740614336,\n",
       "    'acc_stderr': 0.01419438908668526,\n",
       "    'acc_norm': 0.42406143344709896,\n",
       "    'acc_norm_stderr': 0.0144418896274644},\n",
       "   'harness|hellaswag|10': {'acc': 0.5463055168293168,\n",
       "    'acc_stderr': 0.0049683371441363675,\n",
       "    'acc_norm': 0.7252539334793866,\n",
       "    'acc_norm_stderr': 0.00445473941570504},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.04072314811876837,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.04072314811876837},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.2631578947368421,\n",
       "    'acc_stderr': 0.03583496176361064,\n",
       "    'acc_norm': 0.2631578947368421,\n",
       "    'acc_norm_stderr': 0.03583496176361064},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816507,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816507},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.22264150943396227,\n",
       "    'acc_stderr': 0.0256042334708991,\n",
       "    'acc_norm': 0.22264150943396227,\n",
       "    'acc_norm_stderr': 0.0256042334708991},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2638888888888889,\n",
       "    'acc_stderr': 0.03685651095897532,\n",
       "    'acc_norm': 0.2638888888888889,\n",
       "    'acc_norm_stderr': 0.03685651095897532},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.18,\n",
       "    'acc_stderr': 0.038612291966536955,\n",
       "    'acc_norm': 0.18,\n",
       "    'acc_norm_stderr': 0.038612291966536955},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.2,\n",
       "    'acc_stderr': 0.04020151261036845,\n",
       "    'acc_norm': 0.2,\n",
       "    'acc_norm_stderr': 0.04020151261036845},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.23699421965317918,\n",
       "    'acc_stderr': 0.03242414757483099,\n",
       "    'acc_norm': 0.23699421965317918,\n",
       "    'acc_norm_stderr': 0.03242414757483099},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.23529411764705882,\n",
       "    'acc_stderr': 0.04220773659171452,\n",
       "    'acc_norm': 0.23529411764705882,\n",
       "    'acc_norm_stderr': 0.04220773659171452},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.23829787234042554,\n",
       "    'acc_stderr': 0.027851252973889788,\n",
       "    'acc_norm': 0.23829787234042554,\n",
       "    'acc_norm_stderr': 0.027851252973889788},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2543859649122807,\n",
       "    'acc_stderr': 0.040969851398436695,\n",
       "    'acc_norm': 0.2543859649122807,\n",
       "    'acc_norm_stderr': 0.040969851398436695},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.2827586206896552,\n",
       "    'acc_stderr': 0.03752833958003337,\n",
       "    'acc_norm': 0.2827586206896552,\n",
       "    'acc_norm_stderr': 0.03752833958003337},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.2698412698412698,\n",
       "    'acc_stderr': 0.02286083830923207,\n",
       "    'acc_norm': 0.2698412698412698,\n",
       "    'acc_norm_stderr': 0.02286083830923207},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.1984126984126984,\n",
       "    'acc_stderr': 0.035670166752768635,\n",
       "    'acc_norm': 0.1984126984126984,\n",
       "    'acc_norm_stderr': 0.035670166752768635},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.26129032258064516,\n",
       "    'acc_stderr': 0.024993053397764822,\n",
       "    'acc_norm': 0.26129032258064516,\n",
       "    'acc_norm_stderr': 0.024993053397764822},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.2512315270935961,\n",
       "    'acc_stderr': 0.030516530732694436,\n",
       "    'acc_norm': 0.2512315270935961,\n",
       "    'acc_norm_stderr': 0.030516530732694436},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.20606060606060606,\n",
       "    'acc_stderr': 0.03158415324047708,\n",
       "    'acc_norm': 0.20606060606060606,\n",
       "    'acc_norm_stderr': 0.03158415324047708},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.20707070707070707,\n",
       "    'acc_stderr': 0.02886977846026705,\n",
       "    'acc_norm': 0.20707070707070707,\n",
       "    'acc_norm_stderr': 0.02886977846026705},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.23316062176165803,\n",
       "    'acc_stderr': 0.03051611137147601,\n",
       "    'acc_norm': 0.23316062176165803,\n",
       "    'acc_norm_stderr': 0.03051611137147601},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.23076923076923078,\n",
       "    'acc_stderr': 0.02136202772522273,\n",
       "    'acc_norm': 0.23076923076923078,\n",
       "    'acc_norm_stderr': 0.02136202772522273},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.26666666666666666,\n",
       "    'acc_stderr': 0.026962424325073835,\n",
       "    'acc_norm': 0.26666666666666666,\n",
       "    'acc_norm_stderr': 0.026962424325073835},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.226890756302521,\n",
       "    'acc_stderr': 0.02720537153827949,\n",
       "    'acc_norm': 0.226890756302521,\n",
       "    'acc_norm_stderr': 0.02720537153827949},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.23178807947019867,\n",
       "    'acc_stderr': 0.03445406271987054,\n",
       "    'acc_norm': 0.23178807947019867,\n",
       "    'acc_norm_stderr': 0.03445406271987054},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.26605504587155965,\n",
       "    'acc_stderr': 0.018946022322225593,\n",
       "    'acc_norm': 0.26605504587155965,\n",
       "    'acc_norm_stderr': 0.018946022322225593},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.20833333333333334,\n",
       "    'acc_stderr': 0.02769691071309394,\n",
       "    'acc_norm': 0.20833333333333334,\n",
       "    'acc_norm_stderr': 0.02769691071309394},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.2696078431372549,\n",
       "    'acc_stderr': 0.031145570659486782,\n",
       "    'acc_norm': 0.2696078431372549,\n",
       "    'acc_norm_stderr': 0.031145570659486782},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.32489451476793246,\n",
       "    'acc_stderr': 0.030486039389105307,\n",
       "    'acc_norm': 0.32489451476793246,\n",
       "    'acc_norm_stderr': 0.030486039389105307},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.29596412556053814,\n",
       "    'acc_stderr': 0.0306365913486998,\n",
       "    'acc_norm': 0.29596412556053814,\n",
       "    'acc_norm_stderr': 0.0306365913486998},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.22137404580152673,\n",
       "    'acc_stderr': 0.03641297081313729,\n",
       "    'acc_norm': 0.22137404580152673,\n",
       "    'acc_norm_stderr': 0.03641297081313729},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.38016528925619836,\n",
       "    'acc_stderr': 0.04431324501968431,\n",
       "    'acc_norm': 0.38016528925619836,\n",
       "    'acc_norm_stderr': 0.04431324501968431},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.2222222222222222,\n",
       "    'acc_stderr': 0.040191074725573483,\n",
       "    'acc_norm': 0.2222222222222222,\n",
       "    'acc_norm_stderr': 0.040191074725573483},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2392638036809816,\n",
       "    'acc_stderr': 0.03351953879521272,\n",
       "    'acc_norm': 0.2392638036809816,\n",
       "    'acc_norm_stderr': 0.03351953879521272},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.3482142857142857,\n",
       "    'acc_stderr': 0.045218299028335865,\n",
       "    'acc_norm': 0.3482142857142857,\n",
       "    'acc_norm_stderr': 0.045218299028335865},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.24271844660194175,\n",
       "    'acc_stderr': 0.04245022486384493,\n",
       "    'acc_norm': 0.24271844660194175,\n",
       "    'acc_norm_stderr': 0.04245022486384493},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.27350427350427353,\n",
       "    'acc_stderr': 0.029202540153431187,\n",
       "    'acc_norm': 0.27350427350427353,\n",
       "    'acc_norm_stderr': 0.029202540153431187},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.041633319989322695,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.041633319989322695},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.29246487867177523,\n",
       "    'acc_stderr': 0.016267000684598645,\n",
       "    'acc_norm': 0.29246487867177523,\n",
       "    'acc_norm_stderr': 0.016267000684598645},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.2630057803468208,\n",
       "    'acc_stderr': 0.023703099525258176,\n",
       "    'acc_norm': 0.2630057803468208,\n",
       "    'acc_norm_stderr': 0.023703099525258176},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961443,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961443},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.24509803921568626,\n",
       "    'acc_stderr': 0.02463004897982478,\n",
       "    'acc_norm': 0.24509803921568626,\n",
       "    'acc_norm_stderr': 0.02463004897982478},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.3054662379421222,\n",
       "    'acc_stderr': 0.026160584450140495,\n",
       "    'acc_norm': 0.3054662379421222,\n",
       "    'acc_norm_stderr': 0.026160584450140495},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.02409347123262133,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.02409347123262133},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2695035460992908,\n",
       "    'acc_stderr': 0.02646903681859062,\n",
       "    'acc_norm': 0.2695035460992908,\n",
       "    'acc_norm_stderr': 0.02646903681859062},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.2692307692307692,\n",
       "    'acc_stderr': 0.011328734403140308,\n",
       "    'acc_norm': 0.2692307692307692,\n",
       "    'acc_norm_stderr': 0.011328734403140308},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.18382352941176472,\n",
       "    'acc_stderr': 0.02352924218519311,\n",
       "    'acc_norm': 0.18382352941176472,\n",
       "    'acc_norm_stderr': 0.02352924218519311},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.01784808957491322,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.01784808957491322},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.2818181818181818,\n",
       "    'acc_stderr': 0.0430911870994646,\n",
       "    'acc_norm': 0.2818181818181818,\n",
       "    'acc_norm_stderr': 0.0430911870994646},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.17551020408163265,\n",
       "    'acc_stderr': 0.024352800722970015,\n",
       "    'acc_norm': 0.17551020408163265,\n",
       "    'acc_norm_stderr': 0.024352800722970015},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.2935323383084577,\n",
       "    'acc_stderr': 0.03220024104534205,\n",
       "    'acc_norm': 0.2935323383084577,\n",
       "    'acc_norm_stderr': 0.03220024104534205},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.044084400227680794,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.044084400227680794},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.26506024096385544,\n",
       "    'acc_stderr': 0.03436024037944967,\n",
       "    'acc_norm': 0.26506024096385544,\n",
       "    'acc_norm_stderr': 0.03436024037944967},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.03565079670708311,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.03565079670708311},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.200734394124847,\n",
       "    'mc1_stderr': 0.014022045717482156,\n",
       "    'mc2': 0.33827081326692277,\n",
       "    'mc2_stderr': 0.014862542606576355}}},\n",
       " 'dolphin-2.2.1-mistral-7b': {'key': 'dolphin-2.2.1-mistral-7b',\n",
       "  'Model': 'Dolphin-2.2.1-Mistral-7B',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '-',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'Cognitive Computations',\n",
       "  'Link': 'https://huggingface.co/ehartford/dolphin-2.2.1-mistral-7b',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6049488054607508,\n",
       "    'acc_stderr': 0.014285898292938167,\n",
       "    'acc_norm': 0.6331058020477816,\n",
       "    'acc_norm_stderr': 0.014084133118104301},\n",
       "   'harness|hellaswag|10': {'acc': 0.6431985660227046,\n",
       "    'acc_stderr': 0.004780764443411322,\n",
       "    'acc_norm': 0.8375821549492133,\n",
       "    'acc_norm_stderr': 0.0036807989505319113},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.04218506215368879,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.04218506215368879},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6776315789473685,\n",
       "    'acc_stderr': 0.03803510248351585,\n",
       "    'acc_norm': 0.6776315789473685,\n",
       "    'acc_norm_stderr': 0.03803510248351585},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.028727502957880267,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.028727502957880267},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03621034121889507,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03621034121889507},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.04878317312145633,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.04878317312145633},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6358381502890174,\n",
       "    'acc_stderr': 0.03669072477416907,\n",
       "    'acc_norm': 0.6358381502890174,\n",
       "    'acc_norm_stderr': 0.03669072477416907},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.35294117647058826,\n",
       "    'acc_stderr': 0.04755129616062947,\n",
       "    'acc_norm': 0.35294117647058826,\n",
       "    'acc_norm_stderr': 0.04755129616062947},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.04093601807403326,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.04093601807403326},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5446808510638298,\n",
       "    'acc_stderr': 0.03255525359340354,\n",
       "    'acc_norm': 0.5446808510638298,\n",
       "    'acc_norm_stderr': 0.03255525359340354},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.45614035087719296,\n",
       "    'acc_stderr': 0.04685473041907789,\n",
       "    'acc_norm': 0.45614035087719296,\n",
       "    'acc_norm_stderr': 0.04685473041907789},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.593103448275862,\n",
       "    'acc_stderr': 0.04093793981266236,\n",
       "    'acc_norm': 0.593103448275862,\n",
       "    'acc_norm_stderr': 0.04093793981266236},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3862433862433862,\n",
       "    'acc_stderr': 0.025075981767601684,\n",
       "    'acc_norm': 0.3862433862433862,\n",
       "    'acc_norm_stderr': 0.025075981767601684},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3968253968253968,\n",
       "    'acc_stderr': 0.04375888492727062,\n",
       "    'acc_norm': 0.3968253968253968,\n",
       "    'acc_norm_stderr': 0.04375888492727062},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7709677419354839,\n",
       "    'acc_stderr': 0.02390491431178265,\n",
       "    'acc_norm': 0.7709677419354839,\n",
       "    'acc_norm_stderr': 0.02390491431178265},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4876847290640394,\n",
       "    'acc_stderr': 0.035169204442208966,\n",
       "    'acc_norm': 0.4876847290640394,\n",
       "    'acc_norm_stderr': 0.035169204442208966},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.69,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.69,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7696969696969697,\n",
       "    'acc_stderr': 0.0328766675860349,\n",
       "    'acc_norm': 0.7696969696969697,\n",
       "    'acc_norm_stderr': 0.0328766675860349},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7929292929292929,\n",
       "    'acc_stderr': 0.028869778460267025,\n",
       "    'acc_norm': 0.7929292929292929,\n",
       "    'acc_norm_stderr': 0.028869778460267025},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8704663212435233,\n",
       "    'acc_stderr': 0.02423353229775873,\n",
       "    'acc_norm': 0.8704663212435233,\n",
       "    'acc_norm_stderr': 0.02423353229775873},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6512820512820513,\n",
       "    'acc_stderr': 0.02416278028401772,\n",
       "    'acc_norm': 0.6512820512820513,\n",
       "    'acc_norm_stderr': 0.02416278028401772},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028593,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.030388353551886783,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.030388353551886783},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.304635761589404,\n",
       "    'acc_stderr': 0.03757949922943343,\n",
       "    'acc_norm': 0.304635761589404,\n",
       "    'acc_norm_stderr': 0.03757949922943343},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8348623853211009,\n",
       "    'acc_stderr': 0.015919557829976044,\n",
       "    'acc_norm': 0.8348623853211009,\n",
       "    'acc_norm_stderr': 0.015919557829976044},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.034076320938540516,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.034076320938540516},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7696078431372549,\n",
       "    'acc_stderr': 0.02955429260569508,\n",
       "    'acc_norm': 0.7696078431372549,\n",
       "    'acc_norm_stderr': 0.02955429260569508},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7763713080168776,\n",
       "    'acc_stderr': 0.027123298205229966,\n",
       "    'acc_norm': 0.7763713080168776,\n",
       "    'acc_norm_stderr': 0.027123298205229966},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6771300448430493,\n",
       "    'acc_stderr': 0.03138147637575499,\n",
       "    'acc_norm': 0.6771300448430493,\n",
       "    'acc_norm_stderr': 0.03138147637575499},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7786259541984732,\n",
       "    'acc_stderr': 0.0364129708131373,\n",
       "    'acc_norm': 0.7786259541984732,\n",
       "    'acc_norm_stderr': 0.0364129708131373},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7933884297520661,\n",
       "    'acc_stderr': 0.03695980128098823,\n",
       "    'acc_norm': 0.7933884297520661,\n",
       "    'acc_norm_stderr': 0.03695980128098823},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8055555555555556,\n",
       "    'acc_stderr': 0.03826076324884866,\n",
       "    'acc_norm': 0.8055555555555556,\n",
       "    'acc_norm_stderr': 0.03826076324884866},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7607361963190185,\n",
       "    'acc_stderr': 0.033519538795212696,\n",
       "    'acc_norm': 0.7607361963190185,\n",
       "    'acc_norm_stderr': 0.033519538795212696},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7669902912621359,\n",
       "    'acc_stderr': 0.04185832598928315,\n",
       "    'acc_norm': 0.7669902912621359,\n",
       "    'acc_norm_stderr': 0.04185832598928315},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8632478632478633,\n",
       "    'acc_stderr': 0.022509033937077805,\n",
       "    'acc_norm': 0.8632478632478633,\n",
       "    'acc_norm_stderr': 0.022509033937077805},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768078,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768078},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8135376756066411,\n",
       "    'acc_stderr': 0.013927751372001506,\n",
       "    'acc_norm': 0.8135376756066411,\n",
       "    'acc_norm_stderr': 0.013927751372001506},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7138728323699421,\n",
       "    'acc_stderr': 0.02433214677913413,\n",
       "    'acc_norm': 0.7138728323699421,\n",
       "    'acc_norm_stderr': 0.02433214677913413},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.37988826815642457,\n",
       "    'acc_stderr': 0.016232826818678502,\n",
       "    'acc_norm': 0.37988826815642457,\n",
       "    'acc_norm_stderr': 0.016232826818678502},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7058823529411765,\n",
       "    'acc_stderr': 0.026090162504279053,\n",
       "    'acc_norm': 0.7058823529411765,\n",
       "    'acc_norm_stderr': 0.026090162504279053},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7202572347266881,\n",
       "    'acc_stderr': 0.025494259350694912,\n",
       "    'acc_norm': 0.7202572347266881,\n",
       "    'acc_norm_stderr': 0.025494259350694912},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7098765432098766,\n",
       "    'acc_stderr': 0.025251173936495026,\n",
       "    'acc_norm': 0.7098765432098766,\n",
       "    'acc_norm_stderr': 0.025251173936495026},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4787234042553192,\n",
       "    'acc_stderr': 0.029800481645628693,\n",
       "    'acc_norm': 0.4787234042553192,\n",
       "    'acc_norm_stderr': 0.029800481645628693},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4471968709256845,\n",
       "    'acc_stderr': 0.012698825252435111,\n",
       "    'acc_norm': 0.4471968709256845,\n",
       "    'acc_norm_stderr': 0.012698825252435111},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6470588235294118,\n",
       "    'acc_stderr': 0.029029422815681397,\n",
       "    'acc_norm': 0.6470588235294118,\n",
       "    'acc_norm_stderr': 0.029029422815681397},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6503267973856209,\n",
       "    'acc_stderr': 0.01929196189506638,\n",
       "    'acc_norm': 0.6503267973856209,\n",
       "    'acc_norm_stderr': 0.01929196189506638},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.726530612244898,\n",
       "    'acc_stderr': 0.02853556033712844,\n",
       "    'acc_norm': 0.726530612244898,\n",
       "    'acc_norm_stderr': 0.02853556033712844},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.835820895522388,\n",
       "    'acc_stderr': 0.026193923544454125,\n",
       "    'acc_norm': 0.835820895522388,\n",
       "    'acc_norm_stderr': 0.026193923544454125},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.03379976689896309,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.03379976689896309},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5481927710843374,\n",
       "    'acc_stderr': 0.03874371556587953,\n",
       "    'acc_norm': 0.5481927710843374,\n",
       "    'acc_norm_stderr': 0.03874371556587953},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8304093567251462,\n",
       "    'acc_stderr': 0.02878210810540171,\n",
       "    'acc_norm': 0.8304093567251462,\n",
       "    'acc_norm_stderr': 0.02878210810540171},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3659730722154223,\n",
       "    'mc1_stderr': 0.016862941684088365,\n",
       "    'mc2': 0.5311447373702662,\n",
       "    'mc2_stderr': 0.015062742496541512},\n",
       "   'harness|winogrande|5': {'acc': 0.7813733228097869,\n",
       "    'acc_stderr': 0.01161619821577323},\n",
       "   'harness|drop|3': {'em': 0.004299496644295302,\n",
       "    'em_stderr': 0.0006700586558630193,\n",
       "    'f1': 0.081867659395973,\n",
       "    'f1_stderr': 0.0016972961971096978},\n",
       "   'harness|gsm8k|5': {'acc': 0.4806671721000758,\n",
       "    'acc_stderr': 0.013762185709851354},\n",
       "   'all': {'acc': 0.6314567324183159,\n",
       "    'acc_stderr': 0.032318316802746,\n",
       "    'acc_norm': 0.6352434028495076,\n",
       "    'acc_norm_stderr': 0.032961647633460475,\n",
       "    'mc1': 0.3659730722154223,\n",
       "    'mc1_stderr': 0.016862941684088365,\n",
       "    'mc2': 0.5311447373702662,\n",
       "    'mc2_stderr': 0.015062742496541512}}},\n",
       " 'falcon-180b-chat': {'key': 'falcon-180b-chat',\n",
       "  'Model': 'falcon-180b-chat',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.680',\n",
       "  'License': 'Falcon-180B TII License',\n",
       "  'Organization': 'TII',\n",
       "  'Link': 'https://huggingface.co/tiiuae/falcon-180B-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6143344709897611,\n",
       "    'acc_stderr': 0.014224250973257182,\n",
       "    'acc_norm': 0.64419795221843,\n",
       "    'acc_norm_stderr': 0.01399057113791876},\n",
       "   'harness|hellaswag|10': {'acc': 0.6904999004182434,\n",
       "    'acc_stderr': 0.004613427745209517,\n",
       "    'acc_norm': 0.8804023102967536,\n",
       "    'acc_norm_stderr': 0.0032382732952847414},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.04725815626252606,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.04725815626252606},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6518518518518519,\n",
       "    'acc_stderr': 0.041153246103369526,\n",
       "    'acc_norm': 0.6518518518518519,\n",
       "    'acc_norm_stderr': 0.041153246103369526},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.743421052631579,\n",
       "    'acc_stderr': 0.0355418036802569,\n",
       "    'acc_norm': 0.743421052631579,\n",
       "    'acc_norm_stderr': 0.0355418036802569},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.7018867924528301,\n",
       "    'acc_stderr': 0.028152837942493857,\n",
       "    'acc_norm': 0.7018867924528301,\n",
       "    'acc_norm_stderr': 0.028152837942493857},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.8263888888888888,\n",
       "    'acc_stderr': 0.03167473383795718,\n",
       "    'acc_norm': 0.8263888888888888,\n",
       "    'acc_norm_stderr': 0.03167473383795718},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6473988439306358,\n",
       "    'acc_stderr': 0.036430371689585475,\n",
       "    'acc_norm': 0.6473988439306358,\n",
       "    'acc_norm_stderr': 0.036430371689585475},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3137254901960784,\n",
       "    'acc_stderr': 0.04617034827006717,\n",
       "    'acc_norm': 0.3137254901960784,\n",
       "    'acc_norm_stderr': 0.04617034827006717},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932263,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932263},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.6510638297872341,\n",
       "    'acc_stderr': 0.03115852213135778,\n",
       "    'acc_norm': 0.6510638297872341,\n",
       "    'acc_norm_stderr': 0.03115852213135778},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.4824561403508772,\n",
       "    'acc_stderr': 0.04700708033551038,\n",
       "    'acc_norm': 0.4824561403508772,\n",
       "    'acc_norm_stderr': 0.04700708033551038},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.6137931034482759,\n",
       "    'acc_stderr': 0.04057324734419036,\n",
       "    'acc_norm': 0.6137931034482759,\n",
       "    'acc_norm_stderr': 0.04057324734419036},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.455026455026455,\n",
       "    'acc_stderr': 0.02564692836104939,\n",
       "    'acc_norm': 0.455026455026455,\n",
       "    'acc_norm_stderr': 0.02564692836104939},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4365079365079365,\n",
       "    'acc_stderr': 0.04435932892851466,\n",
       "    'acc_norm': 0.4365079365079365,\n",
       "    'acc_norm_stderr': 0.04435932892851466},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7967741935483871,\n",
       "    'acc_stderr': 0.02289168798455495,\n",
       "    'acc_norm': 0.7967741935483871,\n",
       "    'acc_norm_stderr': 0.02289168798455495},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.541871921182266,\n",
       "    'acc_stderr': 0.03505630140785741,\n",
       "    'acc_norm': 0.541871921182266,\n",
       "    'acc_norm_stderr': 0.03505630140785741},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.03123475237772117,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.03123475237772117},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8484848484848485,\n",
       "    'acc_stderr': 0.025545650426603617,\n",
       "    'acc_norm': 0.8484848484848485,\n",
       "    'acc_norm_stderr': 0.025545650426603617},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9481865284974094,\n",
       "    'acc_stderr': 0.01599622932024412,\n",
       "    'acc_norm': 0.9481865284974094,\n",
       "    'acc_norm_stderr': 0.01599622932024412},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6820512820512821,\n",
       "    'acc_stderr': 0.02361088430892786,\n",
       "    'acc_norm': 0.6820512820512821,\n",
       "    'acc_norm_stderr': 0.02361088430892786},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028597,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028597},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7689075630252101,\n",
       "    'acc_stderr': 0.027381406927868886,\n",
       "    'acc_norm': 0.7689075630252101,\n",
       "    'acc_norm_stderr': 0.027381406927868886},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3973509933774834,\n",
       "    'acc_stderr': 0.039955240076816806,\n",
       "    'acc_norm': 0.3973509933774834,\n",
       "    'acc_norm_stderr': 0.039955240076816806},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8715596330275229,\n",
       "    'acc_stderr': 0.014344977542914318,\n",
       "    'acc_norm': 0.8715596330275229,\n",
       "    'acc_norm_stderr': 0.014344977542914318},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5416666666666666,\n",
       "    'acc_stderr': 0.033981108902946366,\n",
       "    'acc_norm': 0.5416666666666666,\n",
       "    'acc_norm_stderr': 0.033981108902946366},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8578431372549019,\n",
       "    'acc_stderr': 0.02450980392156862,\n",
       "    'acc_norm': 0.8578431372549019,\n",
       "    'acc_norm_stderr': 0.02450980392156862},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8607594936708861,\n",
       "    'acc_stderr': 0.0225355263526927,\n",
       "    'acc_norm': 0.8607594936708861,\n",
       "    'acc_norm_stderr': 0.0225355263526927},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7668161434977578,\n",
       "    'acc_stderr': 0.028380391147094702,\n",
       "    'acc_norm': 0.7668161434977578,\n",
       "    'acc_norm_stderr': 0.028380391147094702},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8396946564885496,\n",
       "    'acc_stderr': 0.03217829420744631,\n",
       "    'acc_norm': 0.8396946564885496,\n",
       "    'acc_norm_stderr': 0.03217829420744631},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03520893951097653,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03520893951097653},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8611111111111112,\n",
       "    'acc_stderr': 0.0334327006286962,\n",
       "    'acc_norm': 0.8611111111111112,\n",
       "    'acc_norm_stderr': 0.0334327006286962},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.8343558282208589,\n",
       "    'acc_stderr': 0.029208296231259104,\n",
       "    'acc_norm': 0.8343558282208589,\n",
       "    'acc_norm_stderr': 0.029208296231259104},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.5446428571428571,\n",
       "    'acc_stderr': 0.04726835553719098,\n",
       "    'acc_norm': 0.5446428571428571,\n",
       "    'acc_norm_stderr': 0.04726835553719098},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8446601941747572,\n",
       "    'acc_stderr': 0.03586594738573975,\n",
       "    'acc_norm': 0.8446601941747572,\n",
       "    'acc_norm_stderr': 0.03586594738573975},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8931623931623932,\n",
       "    'acc_stderr': 0.02023714900899093,\n",
       "    'acc_norm': 0.8931623931623932,\n",
       "    'acc_norm_stderr': 0.02023714900899093},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932261,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932261},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8671775223499362,\n",
       "    'acc_stderr': 0.012136303209884562,\n",
       "    'acc_norm': 0.8671775223499362,\n",
       "    'acc_norm_stderr': 0.012136303209884562},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7745664739884393,\n",
       "    'acc_stderr': 0.022497230190967558,\n",
       "    'acc_norm': 0.7745664739884393,\n",
       "    'acc_norm_stderr': 0.022497230190967558},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.4659217877094972,\n",
       "    'acc_stderr': 0.016683615837486863,\n",
       "    'acc_norm': 0.4659217877094972,\n",
       "    'acc_norm_stderr': 0.016683615837486863},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7287581699346405,\n",
       "    'acc_stderr': 0.02545775669666788,\n",
       "    'acc_norm': 0.7287581699346405,\n",
       "    'acc_norm_stderr': 0.02545775669666788},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7877813504823151,\n",
       "    'acc_stderr': 0.02322275679743512,\n",
       "    'acc_norm': 0.7877813504823151,\n",
       "    'acc_norm_stderr': 0.02322275679743512},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7870370370370371,\n",
       "    'acc_stderr': 0.022779719088733403,\n",
       "    'acc_norm': 0.7870370370370371,\n",
       "    'acc_norm_stderr': 0.022779719088733403},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5460992907801419,\n",
       "    'acc_stderr': 0.02970045324729146,\n",
       "    'acc_norm': 0.5460992907801419,\n",
       "    'acc_norm_stderr': 0.02970045324729146},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5345501955671447,\n",
       "    'acc_stderr': 0.012739711554045713,\n",
       "    'acc_norm': 0.5345501955671447,\n",
       "    'acc_norm_stderr': 0.012739711554045713},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6875,\n",
       "    'acc_stderr': 0.02815637344037142,\n",
       "    'acc_norm': 0.6875,\n",
       "    'acc_norm_stderr': 0.02815637344037142},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.7271241830065359,\n",
       "    'acc_stderr': 0.018020474148393577,\n",
       "    'acc_norm': 0.7271241830065359,\n",
       "    'acc_norm_stderr': 0.018020474148393577},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.763265306122449,\n",
       "    'acc_stderr': 0.027212835884073156,\n",
       "    'acc_norm': 0.763265306122449,\n",
       "    'acc_norm_stderr': 0.027212835884073156},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8258706467661692,\n",
       "    'acc_stderr': 0.026814951200421603,\n",
       "    'acc_norm': 0.8258706467661692,\n",
       "    'acc_norm_stderr': 0.026814951200421603},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.89,\n",
       "    'acc_stderr': 0.03144660377352203,\n",
       "    'acc_norm': 0.89,\n",
       "    'acc_norm_stderr': 0.03144660377352203},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5240963855421686,\n",
       "    'acc_stderr': 0.03887971849597264,\n",
       "    'acc_norm': 0.5240963855421686,\n",
       "    'acc_norm_stderr': 0.03887971849597264},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8421052631578947,\n",
       "    'acc_stderr': 0.02796678585916089,\n",
       "    'acc_norm': 0.8421052631578947,\n",
       "    'acc_norm_stderr': 0.02796678585916089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.37576499388004897,\n",
       "    'mc1_stderr': 0.016954584060214297,\n",
       "    'mc2': 0.5369457047083463,\n",
       "    'mc2_stderr': 0.014662579093423517},\n",
       "   'all': {'acc': 0.6793461177088693,\n",
       "    'acc_stderr': 0.031533098229447415,\n",
       "    'acc_norm': 0.6830709633208555,\n",
       "    'acc_norm_stderr': 0.03150582985173109,\n",
       "    'mc1': 0.37576499388004897,\n",
       "    'mc1_stderr': 0.016954584060214297,\n",
       "    'mc2': 0.5369457047083463,\n",
       "    'mc2_stderr': 0.014662579093423517}}},\n",
       " 'guanaco-33b': {'key': 'guanaco-33b',\n",
       "  'Model': 'Guanaco-33B',\n",
       "  'MT-bench (score)': '6.53',\n",
       "  'MMLU': '0.576',\n",
       "  'License': 'Non-commercial',\n",
       "  'Organization': 'UW',\n",
       "  'Link': 'https://huggingface.co/timdettmers/guanaco-33b-merged',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5870307167235495,\n",
       "    'acc_stderr': 0.014388344935398326,\n",
       "    'acc_norm': 0.6245733788395904,\n",
       "    'acc_norm_stderr': 0.014150631435111726},\n",
       "   'harness|hellaswag|10': {'acc': 0.6446922923720374,\n",
       "    'acc_stderr': 0.004776283203468098,\n",
       "    'acc_norm': 0.8447520414260108,\n",
       "    'acc_norm_stderr': 0.003614007841341989},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.047609522856952365,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.047609522856952365},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5657894736842105,\n",
       "    'acc_stderr': 0.04033565667848319,\n",
       "    'acc_norm': 0.5657894736842105,\n",
       "    'acc_norm_stderr': 0.04033565667848319},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620332,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620332},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5622641509433962,\n",
       "    'acc_stderr': 0.030533338430467516,\n",
       "    'acc_norm': 0.5622641509433962,\n",
       "    'acc_norm_stderr': 0.030533338430467516},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5625,\n",
       "    'acc_stderr': 0.04148415739394154,\n",
       "    'acc_norm': 0.5625,\n",
       "    'acc_norm_stderr': 0.04148415739394154},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3431372549019608,\n",
       "    'acc_stderr': 0.04724007352383889,\n",
       "    'acc_norm': 0.3431372549019608,\n",
       "    'acc_norm_stderr': 0.04724007352383889},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4595744680851064,\n",
       "    'acc_stderr': 0.03257901482099835,\n",
       "    'acc_norm': 0.4595744680851064,\n",
       "    'acc_norm_stderr': 0.03257901482099835},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3684210526315789,\n",
       "    'acc_stderr': 0.04537815354939391,\n",
       "    'acc_norm': 0.3684210526315789,\n",
       "    'acc_norm_stderr': 0.04537815354939391},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4413793103448276,\n",
       "    'acc_stderr': 0.04137931034482758,\n",
       "    'acc_norm': 0.4413793103448276,\n",
       "    'acc_norm_stderr': 0.04137931034482758},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.31216931216931215,\n",
       "    'acc_stderr': 0.0238652068369726,\n",
       "    'acc_norm': 0.31216931216931215,\n",
       "    'acc_norm_stderr': 0.0238652068369726},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.29365079365079366,\n",
       "    'acc_stderr': 0.04073524322147126,\n",
       "    'acc_norm': 0.29365079365079366,\n",
       "    'acc_norm_stderr': 0.04073524322147126},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6290322580645161,\n",
       "    'acc_stderr': 0.027480541887953593,\n",
       "    'acc_norm': 0.6290322580645161,\n",
       "    'acc_norm_stderr': 0.027480541887953593},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3793103448275862,\n",
       "    'acc_stderr': 0.03413963805906235,\n",
       "    'acc_norm': 0.3793103448275862,\n",
       "    'acc_norm_stderr': 0.03413963805906235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7212121212121212,\n",
       "    'acc_stderr': 0.035014387062967806,\n",
       "    'acc_norm': 0.7212121212121212,\n",
       "    'acc_norm_stderr': 0.035014387062967806},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.03191178226713547,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.03191178226713547},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7150259067357513,\n",
       "    'acc_stderr': 0.032577140777096614,\n",
       "    'acc_norm': 0.7150259067357513,\n",
       "    'acc_norm_stderr': 0.032577140777096614},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.49230769230769234,\n",
       "    'acc_stderr': 0.025348006031534778,\n",
       "    'acc_norm': 0.49230769230769234,\n",
       "    'acc_norm_stderr': 0.025348006031534778},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2777777777777778,\n",
       "    'acc_stderr': 0.02730914058823019,\n",
       "    'acc_norm': 0.2777777777777778,\n",
       "    'acc_norm_stderr': 0.02730914058823019},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03242225027115006,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03242225027115006},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.03822746937658752,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.03822746937658752},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7302752293577982,\n",
       "    'acc_stderr': 0.019028486711115438,\n",
       "    'acc_norm': 0.7302752293577982,\n",
       "    'acc_norm_stderr': 0.019028486711115438},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.375,\n",
       "    'acc_stderr': 0.033016908987210894,\n",
       "    'acc_norm': 0.375,\n",
       "    'acc_norm_stderr': 0.033016908987210894},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.02910225438967409,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.02910225438967409},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7679324894514767,\n",
       "    'acc_stderr': 0.02747974455080851,\n",
       "    'acc_norm': 0.7679324894514767,\n",
       "    'acc_norm_stderr': 0.02747974455080851},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5874439461883408,\n",
       "    'acc_stderr': 0.03304062175449297,\n",
       "    'acc_norm': 0.5874439461883408,\n",
       "    'acc_norm_stderr': 0.03304062175449297},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6412213740458015,\n",
       "    'acc_stderr': 0.04206739313864908,\n",
       "    'acc_norm': 0.6412213740458015,\n",
       "    'acc_norm_stderr': 0.04206739313864908},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7272727272727273,\n",
       "    'acc_stderr': 0.04065578140908705,\n",
       "    'acc_norm': 0.7272727272727273,\n",
       "    'acc_norm_stderr': 0.04065578140908705},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6111111111111112,\n",
       "    'acc_stderr': 0.04712821257426769,\n",
       "    'acc_norm': 0.6111111111111112,\n",
       "    'acc_norm_stderr': 0.04712821257426769},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6380368098159509,\n",
       "    'acc_stderr': 0.037757007291414416,\n",
       "    'acc_norm': 0.6380368098159509,\n",
       "    'acc_norm_stderr': 0.037757007291414416},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.38392857142857145,\n",
       "    'acc_stderr': 0.04616143075028546,\n",
       "    'acc_norm': 0.38392857142857145,\n",
       "    'acc_norm_stderr': 0.04616143075028546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6310679611650486,\n",
       "    'acc_stderr': 0.0477761518115674,\n",
       "    'acc_norm': 0.6310679611650486,\n",
       "    'acc_norm_stderr': 0.0477761518115674},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7863247863247863,\n",
       "    'acc_stderr': 0.026853450377009154,\n",
       "    'acc_norm': 0.7863247863247863,\n",
       "    'acc_norm_stderr': 0.026853450377009154},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6756066411238825,\n",
       "    'acc_stderr': 0.0167409290471627,\n",
       "    'acc_norm': 0.6756066411238825,\n",
       "    'acc_norm_stderr': 0.0167409290471627},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5491329479768786,\n",
       "    'acc_stderr': 0.026788811931562757,\n",
       "    'acc_norm': 0.5491329479768786,\n",
       "    'acc_norm_stderr': 0.026788811931562757},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2569832402234637,\n",
       "    'acc_stderr': 0.01461446582196632,\n",
       "    'acc_norm': 0.2569832402234637,\n",
       "    'acc_norm_stderr': 0.01461446582196632},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5718954248366013,\n",
       "    'acc_stderr': 0.028332397483664278,\n",
       "    'acc_norm': 0.5718954248366013,\n",
       "    'acc_norm_stderr': 0.028332397483664278},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6012861736334405,\n",
       "    'acc_stderr': 0.027809322585774496,\n",
       "    'acc_norm': 0.6012861736334405,\n",
       "    'acc_norm_stderr': 0.027809322585774496},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6172839506172839,\n",
       "    'acc_stderr': 0.027044538138402595,\n",
       "    'acc_norm': 0.6172839506172839,\n",
       "    'acc_norm_stderr': 0.027044538138402595},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.42907801418439717,\n",
       "    'acc_stderr': 0.029525914302558555,\n",
       "    'acc_norm': 0.42907801418439717,\n",
       "    'acc_norm_stderr': 0.029525914302558555},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.424380704041721,\n",
       "    'acc_stderr': 0.01262334375743002,\n",
       "    'acc_norm': 0.424380704041721,\n",
       "    'acc_norm_stderr': 0.01262334375743002},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5808823529411765,\n",
       "    'acc_stderr': 0.02997280717046462,\n",
       "    'acc_norm': 0.5808823529411765,\n",
       "    'acc_norm_stderr': 0.02997280717046462},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5212418300653595,\n",
       "    'acc_stderr': 0.020209572388600248,\n",
       "    'acc_norm': 0.5212418300653595,\n",
       "    'acc_norm_stderr': 0.020209572388600248},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.04607582090719976,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.04607582090719976},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5918367346938775,\n",
       "    'acc_stderr': 0.03146465712827424,\n",
       "    'acc_norm': 0.5918367346938775,\n",
       "    'acc_norm_stderr': 0.03146465712827424},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6915422885572139,\n",
       "    'acc_stderr': 0.032658195885126966,\n",
       "    'acc_norm': 0.6915422885572139,\n",
       "    'acc_norm_stderr': 0.032658195885126966},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.041633319989322626,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.041633319989322626},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4578313253012048,\n",
       "    'acc_stderr': 0.038786267710023595,\n",
       "    'acc_norm': 0.4578313253012048,\n",
       "    'acc_norm_stderr': 0.038786267710023595},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.34761321909424725,\n",
       "    'mc1_stderr': 0.016670769188897306,\n",
       "    'mc2': 0.5121992740888713,\n",
       "    'mc2_stderr': 0.014650490351006002},\n",
       "   'all': {'acc': 0.5404553018205109,\n",
       "    'acc_stderr': 0.03488622237927161,\n",
       "    'acc_norm': 0.5444824613318672,\n",
       "    'acc_norm_stderr': 0.03486249375448495,\n",
       "    'mc1': 0.34761321909424725,\n",
       "    'mc1_stderr': 0.016670769188897306,\n",
       "    'mc2': 0.5121992740888713,\n",
       "    'mc2_stderr': 0.014650490351006002}}},\n",
       " 'llama-2-13b-chat': {'key': 'llama-2-13b-chat',\n",
       "  'Model': 'Llama-2-13b-chat',\n",
       "  'MT-bench (score)': '6.65',\n",
       "  'MMLU': '0.536',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-13b-chat-hf',\n",
       "  'results': {'harness|drop|3': {'em': 0.1782718120805369,\n",
       "    'em_stderr': 0.003919630092588375,\n",
       "    'f1': 0.2387195889261742,\n",
       "    'f1_stderr': 0.003944947017182046},\n",
       "   'harness|gsm8k|5': {'acc': 0.15238817285822592,\n",
       "    'acc_stderr': 0.009899572254794204},\n",
       "   'harness|winogrande|5': {'acc': 0.745067087608524,\n",
       "    'acc_stderr': 0.012248806969376422},\n",
       "   'all': {'acc': 0.5479380524707899,\n",
       "    'acc_stderr': 0.03451142729909022,\n",
       "    'acc_norm': 0.5517368945804153,\n",
       "    'acc_norm_stderr': 0.03449229816957583,\n",
       "    'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.4411794590119937,\n",
       "    'mc2_stderr': 0.015755921757439843},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5563139931740614,\n",
       "    'acc_stderr': 0.014518421825670449,\n",
       "    'acc_norm': 0.590443686006826,\n",
       "    'acc_norm_stderr': 0.01437035863247244},\n",
       "   'harness|hellaswag|10': {'acc': 0.6293567018522207,\n",
       "    'acc_stderr': 0.004819899945342489,\n",
       "    'acc_norm': 0.8193586934873531,\n",
       "    'acc_norm_stderr': 0.0038393444971919545},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.046482319871173156,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.046482319871173156},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4740740740740741,\n",
       "    'acc_stderr': 0.04313531696750574,\n",
       "    'acc_norm': 0.4740740740740741,\n",
       "    'acc_norm_stderr': 0.04313531696750574},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5460526315789473,\n",
       "    'acc_stderr': 0.04051646342874142,\n",
       "    'acc_norm': 0.5460526315789473,\n",
       "    'acc_norm_stderr': 0.04051646342874142},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5849056603773585,\n",
       "    'acc_stderr': 0.03032594578928611,\n",
       "    'acc_norm': 0.5849056603773585,\n",
       "    'acc_norm_stderr': 0.03032594578928611},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04122728707651282,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04122728707651282},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4624277456647399,\n",
       "    'acc_stderr': 0.0380168510452446,\n",
       "    'acc_norm': 0.4624277456647399,\n",
       "    'acc_norm_stderr': 0.0380168510452446},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3137254901960784,\n",
       "    'acc_stderr': 0.04617034827006717,\n",
       "    'acc_norm': 0.3137254901960784,\n",
       "    'acc_norm_stderr': 0.04617034827006717},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.03202563076101735,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.03202563076101735},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.043727482902780064,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.043727482902780064},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.503448275862069,\n",
       "    'acc_stderr': 0.0416656757710158,\n",
       "    'acc_norm': 0.503448275862069,\n",
       "    'acc_norm_stderr': 0.0416656757710158},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.024419234966819064,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.024419234966819064},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.30952380952380953,\n",
       "    'acc_stderr': 0.04134913018303316,\n",
       "    'acc_norm': 0.30952380952380953,\n",
       "    'acc_norm_stderr': 0.04134913018303316},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6419354838709678,\n",
       "    'acc_stderr': 0.02727389059430064,\n",
       "    'acc_norm': 0.6419354838709678,\n",
       "    'acc_norm_stderr': 0.02727389059430064},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4630541871921182,\n",
       "    'acc_stderr': 0.035083705204426656,\n",
       "    'acc_norm': 0.4630541871921182,\n",
       "    'acc_norm_stderr': 0.035083705204426656},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.03663974994391244,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.03663974994391244},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.702020202020202,\n",
       "    'acc_stderr': 0.03258630383836556,\n",
       "    'acc_norm': 0.702020202020202,\n",
       "    'acc_norm_stderr': 0.03258630383836556},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7875647668393783,\n",
       "    'acc_stderr': 0.029519282616817234,\n",
       "    'acc_norm': 0.7875647668393783,\n",
       "    'acc_norm_stderr': 0.029519282616817234},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.49230769230769234,\n",
       "    'acc_stderr': 0.025348006031534788,\n",
       "    'acc_norm': 0.49230769230769234,\n",
       "    'acc_norm_stderr': 0.025348006031534788},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3111111111111111,\n",
       "    'acc_stderr': 0.028226446749683522,\n",
       "    'acc_norm': 0.3111111111111111,\n",
       "    'acc_norm_stderr': 0.028226446749683522},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03242225027115007,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03242225027115007},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.33774834437086093,\n",
       "    'acc_stderr': 0.038615575462551684,\n",
       "    'acc_norm': 0.33774834437086093,\n",
       "    'acc_norm_stderr': 0.038615575462551684},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7321100917431193,\n",
       "    'acc_stderr': 0.018987462257978652,\n",
       "    'acc_norm': 0.7321100917431193,\n",
       "    'acc_norm_stderr': 0.018987462257978652},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.03324708911809117,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.03324708911809117},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03039153369274154,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03039153369274154},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7172995780590717,\n",
       "    'acc_stderr': 0.02931281415395592,\n",
       "    'acc_norm': 0.7172995780590717,\n",
       "    'acc_norm_stderr': 0.02931281415395592},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6457399103139013,\n",
       "    'acc_stderr': 0.032100621541349864,\n",
       "    'acc_norm': 0.6457399103139013,\n",
       "    'acc_norm_stderr': 0.032100621541349864},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6335877862595419,\n",
       "    'acc_stderr': 0.04225875451969637,\n",
       "    'acc_norm': 0.6335877862595419,\n",
       "    'acc_norm_stderr': 0.04225875451969637},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.768595041322314,\n",
       "    'acc_stderr': 0.03849856098794089,\n",
       "    'acc_norm': 0.768595041322314,\n",
       "    'acc_norm_stderr': 0.03849856098794089},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6944444444444444,\n",
       "    'acc_stderr': 0.044531975073749834,\n",
       "    'acc_norm': 0.6944444444444444,\n",
       "    'acc_norm_stderr': 0.044531975073749834},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6503067484662577,\n",
       "    'acc_stderr': 0.037466683254700206,\n",
       "    'acc_norm': 0.6503067484662577,\n",
       "    'acc_norm_stderr': 0.037466683254700206},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.35714285714285715,\n",
       "    'acc_stderr': 0.04547960999764376,\n",
       "    'acc_norm': 0.35714285714285715,\n",
       "    'acc_norm_stderr': 0.04547960999764376},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.04354631077260595,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.04354631077260595},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7863247863247863,\n",
       "    'acc_stderr': 0.026853450377009175,\n",
       "    'acc_norm': 0.7863247863247863,\n",
       "    'acc_norm_stderr': 0.026853450377009175},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7471264367816092,\n",
       "    'acc_stderr': 0.015543377313719681,\n",
       "    'acc_norm': 0.7471264367816092,\n",
       "    'acc_norm_stderr': 0.015543377313719681},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6127167630057804,\n",
       "    'acc_stderr': 0.026226158605124655,\n",
       "    'acc_norm': 0.6127167630057804,\n",
       "    'acc_norm_stderr': 0.026226158605124655},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30502793296089387,\n",
       "    'acc_stderr': 0.015398723510916716,\n",
       "    'acc_norm': 0.30502793296089387,\n",
       "    'acc_norm_stderr': 0.015398723510916716},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5947712418300654,\n",
       "    'acc_stderr': 0.028110928492809068,\n",
       "    'acc_norm': 0.5947712418300654,\n",
       "    'acc_norm_stderr': 0.028110928492809068},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5884244372990354,\n",
       "    'acc_stderr': 0.02795048149440127,\n",
       "    'acc_norm': 0.5884244372990354,\n",
       "    'acc_norm_stderr': 0.02795048149440127},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6111111111111112,\n",
       "    'acc_stderr': 0.02712511551316687,\n",
       "    'acc_norm': 0.6111111111111112,\n",
       "    'acc_norm_stderr': 0.02712511551316687},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.38652482269503546,\n",
       "    'acc_stderr': 0.029049190342543454,\n",
       "    'acc_norm': 0.38652482269503546,\n",
       "    'acc_norm_stderr': 0.029049190342543454},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.39113428943937417,\n",
       "    'acc_stderr': 0.012463861839982058,\n",
       "    'acc_norm': 0.39113428943937417,\n",
       "    'acc_norm_stderr': 0.012463861839982058},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.030372836961539352,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.030372836961539352},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5424836601307189,\n",
       "    'acc_stderr': 0.020154685712590888,\n",
       "    'acc_norm': 0.5424836601307189,\n",
       "    'acc_norm_stderr': 0.020154685712590888},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302505,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302505},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6408163265306123,\n",
       "    'acc_stderr': 0.030713560455108493,\n",
       "    'acc_norm': 0.6408163265306123,\n",
       "    'acc_norm_stderr': 0.030713560455108493},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7512437810945274,\n",
       "    'acc_stderr': 0.030567675938916714,\n",
       "    'acc_norm': 0.7512437810945274,\n",
       "    'acc_norm_stderr': 0.030567675938916714},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4819277108433735,\n",
       "    'acc_stderr': 0.038899512528272166,\n",
       "    'acc_norm': 0.4819277108433735,\n",
       "    'acc_norm_stderr': 0.038899512528272166},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7309941520467836,\n",
       "    'acc_stderr': 0.03401052620104089,\n",
       "    'acc_norm': 0.7309941520467836,\n",
       "    'acc_norm_stderr': 0.03401052620104089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.4411794590119937,\n",
       "    'mc2_stderr': 0.015755921757439843}}},\n",
       " 'llama-2-70b-chat': {'key': 'llama-2-70b-chat',\n",
       "  'Model': 'Llama-2-70b-chat',\n",
       "  'MT-bench (score)': '6.86',\n",
       "  'MMLU': '0.630',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-70b-chat-hf',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6049488054607508,\n",
       "    'acc_stderr': 0.01428589829293817,\n",
       "    'acc_norm': 0.6459044368600683,\n",
       "    'acc_norm_stderr': 0.013975454122756564},\n",
       "   'harness|hellaswag|10': {'acc': 0.6693885680143398,\n",
       "    'acc_stderr': 0.004694718918225751,\n",
       "    'acc_norm': 0.8587930691097391,\n",
       "    'acc_norm_stderr': 0.003475231889452833},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3561811505507956,\n",
       "    'mc1_stderr': 0.016763790728446335,\n",
       "    'mc2': 0.5280473232260097,\n",
       "    'mc2_stderr': 0.01553022126123046},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.04793724854411021,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.04793724854411021},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5185185185185185,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.5185185185185185,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.7302631578947368,\n",
       "    'acc_stderr': 0.03611780560284898,\n",
       "    'acc_norm': 0.7302631578947368,\n",
       "    'acc_norm_stderr': 0.03611780560284898},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6377358490566037,\n",
       "    'acc_stderr': 0.029582245128384303,\n",
       "    'acc_norm': 0.6377358490566037,\n",
       "    'acc_norm_stderr': 0.029582245128384303},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03621034121889507,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03621034121889507},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237101,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237101},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6011560693641619,\n",
       "    'acc_stderr': 0.0373362665538351,\n",
       "    'acc_norm': 0.6011560693641619,\n",
       "    'acc_norm_stderr': 0.0373362665538351},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.04690650298201943,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.04690650298201943},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5829787234042553,\n",
       "    'acc_stderr': 0.032232762667117124,\n",
       "    'acc_norm': 0.5829787234042553,\n",
       "    'acc_norm_stderr': 0.032232762667117124},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.41228070175438597,\n",
       "    'acc_stderr': 0.04630653203366595,\n",
       "    'acc_norm': 0.41228070175438597,\n",
       "    'acc_norm_stderr': 0.04630653203366595},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5793103448275863,\n",
       "    'acc_stderr': 0.0411391498118926,\n",
       "    'acc_norm': 0.5793103448275863,\n",
       "    'acc_norm_stderr': 0.0411391498118926},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.41005291005291006,\n",
       "    'acc_stderr': 0.02533120243894442,\n",
       "    'acc_norm': 0.41005291005291006,\n",
       "    'acc_norm_stderr': 0.02533120243894442},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4126984126984127,\n",
       "    'acc_stderr': 0.04403438954768176,\n",
       "    'acc_norm': 0.4126984126984127,\n",
       "    'acc_norm_stderr': 0.04403438954768176},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7645161290322581,\n",
       "    'acc_stderr': 0.02413763242933771,\n",
       "    'acc_norm': 0.7645161290322581,\n",
       "    'acc_norm_stderr': 0.02413763242933771},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4630541871921182,\n",
       "    'acc_stderr': 0.035083705204426656,\n",
       "    'acc_norm': 0.4630541871921182,\n",
       "    'acc_norm_stderr': 0.035083705204426656},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03011768892950359,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03011768892950359},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.02805779167298902,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.02805779167298902},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8911917098445595,\n",
       "    'acc_stderr': 0.022473253332768783,\n",
       "    'acc_norm': 0.8911917098445595,\n",
       "    'acc_norm_stderr': 0.022473253332768783},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6410256410256411,\n",
       "    'acc_stderr': 0.02432173848460235,\n",
       "    'acc_norm': 0.6410256410256411,\n",
       "    'acc_norm_stderr': 0.02432173848460235},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.027940457136228416,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.027940457136228416},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6596638655462185,\n",
       "    'acc_stderr': 0.030778057422931673,\n",
       "    'acc_norm': 0.6596638655462185,\n",
       "    'acc_norm_stderr': 0.030778057422931673},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.423841059602649,\n",
       "    'acc_stderr': 0.04034846678603397,\n",
       "    'acc_norm': 0.423841059602649,\n",
       "    'acc_norm_stderr': 0.04034846678603397},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8385321100917431,\n",
       "    'acc_stderr': 0.015776239256163255,\n",
       "    'acc_norm': 0.8385321100917431,\n",
       "    'acc_norm_stderr': 0.015776239256163255},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.03407632093854052,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.03407632093854052},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8578431372549019,\n",
       "    'acc_stderr': 0.024509803921568606,\n",
       "    'acc_norm': 0.8578431372549019,\n",
       "    'acc_norm_stderr': 0.024509803921568606},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8438818565400844,\n",
       "    'acc_stderr': 0.02362715946031867,\n",
       "    'acc_norm': 0.8438818565400844,\n",
       "    'acc_norm_stderr': 0.02362715946031867},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.726457399103139,\n",
       "    'acc_stderr': 0.02991858670779883,\n",
       "    'acc_norm': 0.726457399103139,\n",
       "    'acc_norm_stderr': 0.02991858670779883},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7099236641221374,\n",
       "    'acc_stderr': 0.039800662464677665,\n",
       "    'acc_norm': 0.7099236641221374,\n",
       "    'acc_norm_stderr': 0.039800662464677665},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8016528925619835,\n",
       "    'acc_stderr': 0.03640118271990946,\n",
       "    'acc_norm': 0.8016528925619835,\n",
       "    'acc_norm_stderr': 0.03640118271990946},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8240740740740741,\n",
       "    'acc_stderr': 0.036809181416738807,\n",
       "    'acc_norm': 0.8240740740740741,\n",
       "    'acc_norm_stderr': 0.036809181416738807},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7607361963190185,\n",
       "    'acc_stderr': 0.033519538795212696,\n",
       "    'acc_norm': 0.7607361963190185,\n",
       "    'acc_norm_stderr': 0.033519538795212696},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8058252427184466,\n",
       "    'acc_stderr': 0.03916667762822584,\n",
       "    'acc_norm': 0.8058252427184466,\n",
       "    'acc_norm_stderr': 0.03916667762822584},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8717948717948718,\n",
       "    'acc_stderr': 0.02190190511507332,\n",
       "    'acc_norm': 0.8717948717948718,\n",
       "    'acc_norm_stderr': 0.02190190511507332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8275862068965517,\n",
       "    'acc_stderr': 0.013507943909371798,\n",
       "    'acc_norm': 0.8275862068965517,\n",
       "    'acc_norm_stderr': 0.013507943909371798},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7167630057803468,\n",
       "    'acc_stderr': 0.02425790170532338,\n",
       "    'acc_norm': 0.7167630057803468,\n",
       "    'acc_norm_stderr': 0.02425790170532338},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.39553072625698327,\n",
       "    'acc_stderr': 0.01635341541007577,\n",
       "    'acc_norm': 0.39553072625698327,\n",
       "    'acc_norm_stderr': 0.01635341541007577},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6993464052287581,\n",
       "    'acc_stderr': 0.026256053835718968,\n",
       "    'acc_norm': 0.6993464052287581,\n",
       "    'acc_norm_stderr': 0.026256053835718968},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7041800643086816,\n",
       "    'acc_stderr': 0.02592237178881877,\n",
       "    'acc_norm': 0.7041800643086816,\n",
       "    'acc_norm_stderr': 0.02592237178881877},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7098765432098766,\n",
       "    'acc_stderr': 0.025251173936495036,\n",
       "    'acc_norm': 0.7098765432098766,\n",
       "    'acc_norm_stderr': 0.025251173936495036},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5070921985815603,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.5070921985815603,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4771838331160365,\n",
       "    'acc_stderr': 0.012756933382823694,\n",
       "    'acc_norm': 0.4771838331160365,\n",
       "    'acc_norm_stderr': 0.012756933382823694},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5772058823529411,\n",
       "    'acc_stderr': 0.030008562845003476,\n",
       "    'acc_norm': 0.5772058823529411,\n",
       "    'acc_norm_stderr': 0.030008562845003476},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6699346405228758,\n",
       "    'acc_stderr': 0.019023726160724556,\n",
       "    'acc_norm': 0.6699346405228758,\n",
       "    'acc_norm_stderr': 0.019023726160724556},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7877551020408163,\n",
       "    'acc_stderr': 0.026176967197866767,\n",
       "    'acc_norm': 0.7877551020408163,\n",
       "    'acc_norm_stderr': 0.026176967197866767},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8706467661691543,\n",
       "    'acc_stderr': 0.023729830881018526,\n",
       "    'acc_norm': 0.8706467661691543,\n",
       "    'acc_norm_stderr': 0.023729830881018526},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.03379976689896309,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.03379976689896309},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5120481927710844,\n",
       "    'acc_stderr': 0.03891364495835817,\n",
       "    'acc_norm': 0.5120481927710844,\n",
       "    'acc_norm_stderr': 0.03891364495835817},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8187134502923976,\n",
       "    'acc_stderr': 0.029547741687640038,\n",
       "    'acc_norm': 0.8187134502923976,\n",
       "    'acc_norm_stderr': 0.029547741687640038},\n",
       "   'all': {'em': 0.040373322147651006,\n",
       "    'em_stderr': 0.0020157564185176837,\n",
       "    'f1': 0.1050272651006715,\n",
       "    'f1_stderr': 0.0023756238577676155,\n",
       "    'acc': 0.5359600711595986,\n",
       "    'acc_stderr': 0.011658939983913113},\n",
       "   'harness|drop|3': {'em': 0.040373322147651006,\n",
       "    'em_stderr': 0.0020157564185176837,\n",
       "    'f1': 0.1050272651006715,\n",
       "    'f1_stderr': 0.0023756238577676155},\n",
       "   'harness|gsm8k|5': {'acc': 0.266868840030326,\n",
       "    'acc_stderr': 0.012183780551887957},\n",
       "   'harness|winogrande|5': {'acc': 0.8050513022888713,\n",
       "    'acc_stderr': 0.011134099415938268}}},\n",
       " 'llama-2-7b-chat': {'key': 'llama-2-7b-chat',\n",
       "  'Model': 'Llama-2-7b-chat',\n",
       "  'MT-bench (score)': '6.27',\n",
       "  'MMLU': '0.458',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-7b-chat-hf',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.4948805460750853,\n",
       "    'acc_stderr': 0.01461062489030916,\n",
       "    'acc_norm': 0.5290102389078498,\n",
       "    'acc_norm_stderr': 0.014586776355294323},\n",
       "   'harness|hellaswag|10': {'acc': 0.5978888667596096,\n",
       "    'acc_stderr': 0.004893220635011792,\n",
       "    'acc_norm': 0.7855008962358097,\n",
       "    'acc_norm_stderr': 0.004096355125117511},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542129,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542129},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.42962962962962964,\n",
       "    'acc_stderr': 0.04276349494376599,\n",
       "    'acc_norm': 0.42962962962962964,\n",
       "    'acc_norm_stderr': 0.04276349494376599},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.4868421052631579,\n",
       "    'acc_stderr': 0.04067533136309173,\n",
       "    'acc_norm': 0.4868421052631579,\n",
       "    'acc_norm_stderr': 0.04067533136309173},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5358490566037736,\n",
       "    'acc_stderr': 0.030693675018458003,\n",
       "    'acc_norm': 0.5358490566037736,\n",
       "    'acc_norm_stderr': 0.030693675018458003},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5208333333333334,\n",
       "    'acc_stderr': 0.041775789507399935,\n",
       "    'acc_norm': 0.5208333333333334,\n",
       "    'acc_norm_stderr': 0.041775789507399935},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.04560480215720684,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.04560480215720684},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.3988439306358382,\n",
       "    'acc_stderr': 0.037336266553835096,\n",
       "    'acc_norm': 0.3988439306358382,\n",
       "    'acc_norm_stderr': 0.037336266553835096},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.22549019607843138,\n",
       "    'acc_stderr': 0.041583075330832865,\n",
       "    'acc_norm': 0.22549019607843138,\n",
       "    'acc_norm_stderr': 0.041583075330832865},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.58,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.58,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4085106382978723,\n",
       "    'acc_stderr': 0.03213418026701576,\n",
       "    'acc_norm': 0.4085106382978723,\n",
       "    'acc_norm_stderr': 0.03213418026701576},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.37719298245614036,\n",
       "    'acc_stderr': 0.045595221419582166,\n",
       "    'acc_norm': 0.37719298245614036,\n",
       "    'acc_norm_stderr': 0.045595221419582166},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4896551724137931,\n",
       "    'acc_stderr': 0.04165774775728762,\n",
       "    'acc_norm': 0.4896551724137931,\n",
       "    'acc_norm_stderr': 0.04165774775728762},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.29894179894179895,\n",
       "    'acc_stderr': 0.023577604791655805,\n",
       "    'acc_norm': 0.29894179894179895,\n",
       "    'acc_norm_stderr': 0.023577604791655805},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.25396825396825395,\n",
       "    'acc_stderr': 0.03893259610604675,\n",
       "    'acc_norm': 0.25396825396825395,\n",
       "    'acc_norm_stderr': 0.03893259610604675},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.5225806451612903,\n",
       "    'acc_stderr': 0.02841498501970786,\n",
       "    'acc_norm': 0.5225806451612903,\n",
       "    'acc_norm_stderr': 0.02841498501970786},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3645320197044335,\n",
       "    'acc_stderr': 0.033864057460620905,\n",
       "    'acc_norm': 0.3645320197044335,\n",
       "    'acc_norm_stderr': 0.033864057460620905},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.5878787878787879,\n",
       "    'acc_stderr': 0.03843566993588718,\n",
       "    'acc_norm': 0.5878787878787879,\n",
       "    'acc_norm_stderr': 0.03843566993588718},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6060606060606061,\n",
       "    'acc_stderr': 0.034812853382329624,\n",
       "    'acc_norm': 0.6060606060606061,\n",
       "    'acc_norm_stderr': 0.034812853382329624},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7150259067357513,\n",
       "    'acc_stderr': 0.032577140777096614,\n",
       "    'acc_norm': 0.7150259067357513,\n",
       "    'acc_norm_stderr': 0.032577140777096614},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.4256410256410256,\n",
       "    'acc_stderr': 0.02506909438729654,\n",
       "    'acc_norm': 0.4256410256410256,\n",
       "    'acc_norm_stderr': 0.02506909438729654},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.25555555555555554,\n",
       "    'acc_stderr': 0.02659393910184408,\n",
       "    'acc_norm': 0.25555555555555554,\n",
       "    'acc_norm_stderr': 0.02659393910184408},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.42436974789915966,\n",
       "    'acc_stderr': 0.03210479051015776,\n",
       "    'acc_norm': 0.42436974789915966,\n",
       "    'acc_norm_stderr': 0.03210479051015776},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2913907284768212,\n",
       "    'acc_stderr': 0.03710185726119995,\n",
       "    'acc_norm': 0.2913907284768212,\n",
       "    'acc_norm_stderr': 0.03710185726119995},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.6752293577981652,\n",
       "    'acc_stderr': 0.020077729109310327,\n",
       "    'acc_norm': 0.6752293577981652,\n",
       "    'acc_norm_stderr': 0.020077729109310327},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.0321495214780275,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.0321495214780275},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.0328347205610856,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.0328347205610856},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.03068582059661079,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.03068582059661079},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5605381165919282,\n",
       "    'acc_stderr': 0.03331092511038179,\n",
       "    'acc_norm': 0.5605381165919282,\n",
       "    'acc_norm_stderr': 0.03331092511038179},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5725190839694656,\n",
       "    'acc_stderr': 0.04338920305792401,\n",
       "    'acc_norm': 0.5725190839694656,\n",
       "    'acc_norm_stderr': 0.04338920305792401},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.628099173553719,\n",
       "    'acc_stderr': 0.04412015806624504,\n",
       "    'acc_norm': 0.628099173553719,\n",
       "    'acc_norm_stderr': 0.04412015806624504},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04766075165356461,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04766075165356461},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.5521472392638037,\n",
       "    'acc_stderr': 0.03906947479456606,\n",
       "    'acc_norm': 0.5521472392638037,\n",
       "    'acc_norm_stderr': 0.03906947479456606},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.30357142857142855,\n",
       "    'acc_stderr': 0.04364226155841044,\n",
       "    'acc_norm': 0.30357142857142855,\n",
       "    'acc_norm_stderr': 0.04364226155841044},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.04656147110012351,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.04656147110012351},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7094017094017094,\n",
       "    'acc_stderr': 0.029745048572674074,\n",
       "    'acc_norm': 0.7094017094017094,\n",
       "    'acc_norm_stderr': 0.029745048572674074},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6756066411238825,\n",
       "    'acc_stderr': 0.0167409290471627,\n",
       "    'acc_norm': 0.6756066411238825,\n",
       "    'acc_norm_stderr': 0.0167409290471627},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.026907849856282542,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.026907849856282542},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2201117318435754,\n",
       "    'acc_stderr': 0.013856994024227175,\n",
       "    'acc_norm': 0.2201117318435754,\n",
       "    'acc_norm_stderr': 0.013856994024227175},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5196078431372549,\n",
       "    'acc_stderr': 0.028607893699576066,\n",
       "    'acc_norm': 0.5196078431372549,\n",
       "    'acc_norm_stderr': 0.028607893699576066},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5659163987138264,\n",
       "    'acc_stderr': 0.02815023224453559,\n",
       "    'acc_norm': 0.5659163987138264,\n",
       "    'acc_norm_stderr': 0.02815023224453559},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5679012345679012,\n",
       "    'acc_stderr': 0.027563010971606676,\n",
       "    'acc_norm': 0.5679012345679012,\n",
       "    'acc_norm_stderr': 0.027563010971606676},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3723404255319149,\n",
       "    'acc_stderr': 0.028838921471251458,\n",
       "    'acc_norm': 0.3723404255319149,\n",
       "    'acc_norm_stderr': 0.028838921471251458},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3500651890482399,\n",
       "    'acc_stderr': 0.012182552313215175,\n",
       "    'acc_norm': 0.3500651890482399,\n",
       "    'acc_norm_stderr': 0.012182552313215175},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.45588235294117646,\n",
       "    'acc_stderr': 0.030254372573976684,\n",
       "    'acc_norm': 0.45588235294117646,\n",
       "    'acc_norm_stderr': 0.030254372573976684},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4803921568627451,\n",
       "    'acc_stderr': 0.020212274976302957,\n",
       "    'acc_norm': 0.4803921568627451,\n",
       "    'acc_norm_stderr': 0.020212274976302957},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5272727272727272,\n",
       "    'acc_stderr': 0.04782001791380061,\n",
       "    'acc_norm': 0.5272727272727272,\n",
       "    'acc_norm_stderr': 0.04782001791380061},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5265306122448979,\n",
       "    'acc_stderr': 0.03196412734523272,\n",
       "    'acc_norm': 0.5265306122448979,\n",
       "    'acc_norm_stderr': 0.03196412734523272},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6467661691542289,\n",
       "    'acc_stderr': 0.03379790611796777,\n",
       "    'acc_norm': 0.6467661691542289,\n",
       "    'acc_norm_stderr': 0.03379790611796777},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.43373493975903615,\n",
       "    'acc_stderr': 0.03858158940685517,\n",
       "    'acc_norm': 0.43373493975903615,\n",
       "    'acc_norm_stderr': 0.03858158940685517},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.034240429246915824,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.034240429246915824},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3011015911872705,\n",
       "    'mc1_stderr': 0.016058999026100616,\n",
       "    'mc2': 0.45570370195101134,\n",
       "    'mc2_stderr': 0.015691038880908878},\n",
       "   'all': {'em': 0.06763842281879194,\n",
       "    'em_stderr': 0.0025717489509556085,\n",
       "    'f1': 0.13085570469798627,\n",
       "    'f1_stderr': 0.0028825856446422905,\n",
       "    'acc': 0.39549166962367155,\n",
       "    'acc_stderr': 0.009921949302668327},\n",
       "   'harness|drop|3': {'em': 0.06763842281879194,\n",
       "    'em_stderr': 0.0025717489509556085,\n",
       "    'f1': 0.13085570469798627,\n",
       "    'f1_stderr': 0.0028825856446422905},\n",
       "   'harness|gsm8k|5': {'acc': 0.07354056103108415,\n",
       "    'acc_stderr': 0.0071898357543652685},\n",
       "   'harness|winogrande|5': {'acc': 0.7174427782162589,\n",
       "    'acc_stderr': 0.012654062850971384}}},\n",
       " 'mistral-7b-instruct': {'key': 'mistral-7b-instruct',\n",
       "  'Model': 'Mistral-7B-Instruct-v0.1',\n",
       "  'MT-bench (score)': '6.84',\n",
       "  'MMLU': '0.554',\n",
       "  'License': 'Apache 2.0',\n",
       "  'Organization': 'Mistral',\n",
       "  'Link': 'https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.523037542662116,\n",
       "    'acc_stderr': 0.014595873205358269,\n",
       "    'acc_norm': 0.5452218430034129,\n",
       "    'acc_norm_stderr': 0.014551507060836357},\n",
       "   'harness|hellaswag|10': {'acc': 0.5694084843656642,\n",
       "    'acc_stderr': 0.004941470620074867,\n",
       "    'acc_norm': 0.7563234415455089,\n",
       "    'acc_norm_stderr': 0.0042842240337755385},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.047609522856952365,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.047609522856952365},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4222222222222222,\n",
       "    'acc_stderr': 0.04266763404099582,\n",
       "    'acc_norm': 0.4222222222222222,\n",
       "    'acc_norm_stderr': 0.04266763404099582},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5657894736842105,\n",
       "    'acc_stderr': 0.040335656678483205,\n",
       "    'acc_norm': 0.5657894736842105,\n",
       "    'acc_norm_stderr': 0.040335656678483205},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5849056603773585,\n",
       "    'acc_stderr': 0.030325945789286105,\n",
       "    'acc_norm': 0.5849056603773585,\n",
       "    'acc_norm_stderr': 0.030325945789286105},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5902777777777778,\n",
       "    'acc_stderr': 0.04112490974670787,\n",
       "    'acc_norm': 0.5902777777777778,\n",
       "    'acc_norm_stderr': 0.04112490974670787},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5202312138728323,\n",
       "    'acc_stderr': 0.03809342081273956,\n",
       "    'acc_norm': 0.5202312138728323,\n",
       "    'acc_norm_stderr': 0.03809342081273956},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.30392156862745096,\n",
       "    'acc_stderr': 0.045766654032077636,\n",
       "    'acc_norm': 0.30392156862745096,\n",
       "    'acc_norm_stderr': 0.045766654032077636},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.047258156262526094,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.047258156262526094},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4808510638297872,\n",
       "    'acc_stderr': 0.032662042990646775,\n",
       "    'acc_norm': 0.4808510638297872,\n",
       "    'acc_norm_stderr': 0.032662042990646775},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.37719298245614036,\n",
       "    'acc_stderr': 0.04559522141958216,\n",
       "    'acc_norm': 0.37719298245614036,\n",
       "    'acc_norm_stderr': 0.04559522141958216},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5517241379310345,\n",
       "    'acc_stderr': 0.04144311810878151,\n",
       "    'acc_norm': 0.5517241379310345,\n",
       "    'acc_norm_stderr': 0.04144311810878151},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36507936507936506,\n",
       "    'acc_stderr': 0.024796060602699947,\n",
       "    'acc_norm': 0.36507936507936506,\n",
       "    'acc_norm_stderr': 0.024796060602699947},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.38095238095238093,\n",
       "    'acc_stderr': 0.043435254289490965,\n",
       "    'acc_norm': 0.38095238095238093,\n",
       "    'acc_norm_stderr': 0.043435254289490965},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6419354838709678,\n",
       "    'acc_stderr': 0.027273890594300642,\n",
       "    'acc_norm': 0.6419354838709678,\n",
       "    'acc_norm_stderr': 0.027273890594300642},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.41379310344827586,\n",
       "    'acc_stderr': 0.03465304488406795,\n",
       "    'acc_norm': 0.41379310344827586,\n",
       "    'acc_norm_stderr': 0.03465304488406795},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.036810508691615486,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.036810508691615486},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.031911782267135466,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.031911782267135466},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7253886010362695,\n",
       "    'acc_stderr': 0.03221024508041154,\n",
       "    'acc_norm': 0.7253886010362695,\n",
       "    'acc_norm_stderr': 0.03221024508041154},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5205128205128206,\n",
       "    'acc_stderr': 0.02532966316348994,\n",
       "    'acc_norm': 0.5205128205128206,\n",
       "    'acc_norm_stderr': 0.02532966316348994},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.027940457136228416,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.027940457136228416},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5462184873949579,\n",
       "    'acc_stderr': 0.032339434681820885,\n",
       "    'acc_norm': 0.5462184873949579,\n",
       "    'acc_norm_stderr': 0.032339434681820885},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.03822746937658753,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.03822746937658753},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.710091743119266,\n",
       "    'acc_stderr': 0.019453066609201597,\n",
       "    'acc_norm': 0.710091743119266,\n",
       "    'acc_norm_stderr': 0.019453066609201597},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4537037037037037,\n",
       "    'acc_stderr': 0.03395322726375797,\n",
       "    'acc_norm': 0.4537037037037037,\n",
       "    'acc_norm_stderr': 0.03395322726375797},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7058823529411765,\n",
       "    'acc_stderr': 0.03198001660115072,\n",
       "    'acc_norm': 0.7058823529411765,\n",
       "    'acc_norm_stderr': 0.03198001660115072},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6962025316455697,\n",
       "    'acc_stderr': 0.0299366963871386,\n",
       "    'acc_norm': 0.6962025316455697,\n",
       "    'acc_norm_stderr': 0.0299366963871386},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6547085201793722,\n",
       "    'acc_stderr': 0.03191100192835794,\n",
       "    'acc_norm': 0.6547085201793722,\n",
       "    'acc_norm_stderr': 0.03191100192835794},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6717557251908397,\n",
       "    'acc_stderr': 0.04118438565806299,\n",
       "    'acc_norm': 0.6717557251908397,\n",
       "    'acc_norm_stderr': 0.04118438565806299},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.6776859504132231,\n",
       "    'acc_stderr': 0.042664163633521685,\n",
       "    'acc_norm': 0.6776859504132231,\n",
       "    'acc_norm_stderr': 0.042664163633521685},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6944444444444444,\n",
       "    'acc_stderr': 0.044531975073749834,\n",
       "    'acc_norm': 0.6944444444444444,\n",
       "    'acc_norm_stderr': 0.044531975073749834},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6503067484662577,\n",
       "    'acc_stderr': 0.03746668325470021,\n",
       "    'acc_norm': 0.6503067484662577,\n",
       "    'acc_norm_stderr': 0.03746668325470021},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.4732142857142857,\n",
       "    'acc_stderr': 0.047389751192741546,\n",
       "    'acc_norm': 0.4732142857142857,\n",
       "    'acc_norm_stderr': 0.047389751192741546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6893203883495146,\n",
       "    'acc_stderr': 0.04582124160161551,\n",
       "    'acc_norm': 0.6893203883495146,\n",
       "    'acc_norm_stderr': 0.04582124160161551},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8418803418803419,\n",
       "    'acc_stderr': 0.023902325549560392,\n",
       "    'acc_norm': 0.8418803418803419,\n",
       "    'acc_norm_stderr': 0.023902325549560392},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.047258156262526094,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.047258156262526094},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7432950191570882,\n",
       "    'acc_stderr': 0.015620480263064533,\n",
       "    'acc_norm': 0.7432950191570882,\n",
       "    'acc_norm_stderr': 0.015620480263064533},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5895953757225434,\n",
       "    'acc_stderr': 0.026483392042098174,\n",
       "    'acc_norm': 0.5895953757225434,\n",
       "    'acc_norm_stderr': 0.026483392042098174},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2446927374301676,\n",
       "    'acc_stderr': 0.014378169884098417,\n",
       "    'acc_norm': 0.2446927374301676,\n",
       "    'acc_norm_stderr': 0.014378169884098417},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6143790849673203,\n",
       "    'acc_stderr': 0.02787074527829027,\n",
       "    'acc_norm': 0.6143790849673203,\n",
       "    'acc_norm_stderr': 0.02787074527829027},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6077170418006431,\n",
       "    'acc_stderr': 0.027731258647012,\n",
       "    'acc_norm': 0.6077170418006431,\n",
       "    'acc_norm_stderr': 0.027731258647012},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5771604938271605,\n",
       "    'acc_stderr': 0.027487472980871595,\n",
       "    'acc_norm': 0.5771604938271605,\n",
       "    'acc_norm_stderr': 0.027487472980871595},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3829787234042553,\n",
       "    'acc_stderr': 0.028999080904806185,\n",
       "    'acc_norm': 0.3829787234042553,\n",
       "    'acc_norm_stderr': 0.028999080904806185},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.40091264667535853,\n",
       "    'acc_stderr': 0.012516960350640824,\n",
       "    'acc_norm': 0.40091264667535853,\n",
       "    'acc_norm_stderr': 0.012516960350640824},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5808823529411765,\n",
       "    'acc_stderr': 0.02997280717046462,\n",
       "    'acc_norm': 0.5808823529411765,\n",
       "    'acc_norm_stderr': 0.02997280717046462},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5163398692810458,\n",
       "    'acc_stderr': 0.02021703065318646,\n",
       "    'acc_norm': 0.5163398692810458,\n",
       "    'acc_norm_stderr': 0.02021703065318646},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302505,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302505},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6775510204081633,\n",
       "    'acc_stderr': 0.02992310056368391,\n",
       "    'acc_norm': 0.6775510204081633,\n",
       "    'acc_norm_stderr': 0.02992310056368391},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.746268656716418,\n",
       "    'acc_stderr': 0.03076944496729602,\n",
       "    'acc_norm': 0.746268656716418,\n",
       "    'acc_norm_stderr': 0.03076944496729602},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.46987951807228917,\n",
       "    'acc_stderr': 0.03885425420866766,\n",
       "    'acc_norm': 0.46987951807228917,\n",
       "    'acc_norm_stderr': 0.03885425420866766},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3953488372093023,\n",
       "    'mc1_stderr': 0.017115815632418194,\n",
       "    'mc2': 0.5628382292113293,\n",
       "    'mc2_stderr': 0.015351892312006444},\n",
       "   'all': {'em': 0.37038590604026844,\n",
       "    'em_stderr': 0.00494543044549648,\n",
       "    'f1': 0.43100566275167973,\n",
       "    'f1_stderr': 0.00478990485809286,\n",
       "    'acc': 0.4398533245809979,\n",
       "    'acc_stderr': 0.01100025548646791},\n",
       "   'harness|drop|3': {'em': 0.37038590604026844,\n",
       "    'em_stderr': 0.00494543044549648,\n",
       "    'f1': 0.43100566275167973,\n",
       "    'f1_stderr': 0.00478990485809286},\n",
       "   'harness|gsm8k|5': {'acc': 0.1425322213798332,\n",
       "    'acc_stderr': 0.009629588445673814},\n",
       "   'harness|winogrande|5': {'acc': 0.7371744277821626,\n",
       "    'acc_stderr': 0.012370922527262006}}},\n",
       " 'mpt-30b-chat': {'key': 'mpt-30b-chat',\n",
       "  'Model': 'MPT-30B-chat',\n",
       "  'MT-bench (score)': '6.39',\n",
       "  'MMLU': '0.504',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'MosaicML',\n",
       "  'Link': 'https://huggingface.co/mosaicml/mpt-30b-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5477815699658704,\n",
       "    'acc_stderr': 0.014544519880633829,\n",
       "    'acc_norm': 0.5836177474402731,\n",
       "    'acc_norm_stderr': 0.014405618279436176},\n",
       "   'harness|hellaswag|10': {'acc': 0.6325433180641307,\n",
       "    'acc_stderr': 0.0048112699754506005,\n",
       "    'acc_norm': 0.8241386178052181,\n",
       "    'acc_norm_stderr': 0.0037992414085029525},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.45925925925925926,\n",
       "    'acc_stderr': 0.04304979692464243,\n",
       "    'acc_norm': 0.45925925925925926,\n",
       "    'acc_norm_stderr': 0.04304979692464243},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.04068942293855797,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.04068942293855797},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5509433962264151,\n",
       "    'acc_stderr': 0.030612730713641095,\n",
       "    'acc_norm': 0.5509433962264151,\n",
       "    'acc_norm_stderr': 0.030612730713641095},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5902777777777778,\n",
       "    'acc_stderr': 0.04112490974670788,\n",
       "    'acc_norm': 0.5902777777777778,\n",
       "    'acc_norm_stderr': 0.04112490974670788},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.043898699568087785,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.043898699568087785},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4297872340425532,\n",
       "    'acc_stderr': 0.03236214467715564,\n",
       "    'acc_norm': 0.4297872340425532,\n",
       "    'acc_norm_stderr': 0.03236214467715564},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2807017543859649,\n",
       "    'acc_stderr': 0.042270544512322,\n",
       "    'acc_norm': 0.2807017543859649,\n",
       "    'acc_norm_stderr': 0.042270544512322},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.47586206896551725,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.47586206896551725,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.0242785680243077,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.0242785680243077},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.31746031746031744,\n",
       "    'acc_stderr': 0.04163453031302859,\n",
       "    'acc_norm': 0.31746031746031744,\n",
       "    'acc_norm_stderr': 0.04163453031302859},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6193548387096774,\n",
       "    'acc_stderr': 0.027621717832907046,\n",
       "    'acc_norm': 0.6193548387096774,\n",
       "    'acc_norm_stderr': 0.027621717832907046},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3842364532019704,\n",
       "    'acc_stderr': 0.03422398565657551,\n",
       "    'acc_norm': 0.3842364532019704,\n",
       "    'acc_norm_stderr': 0.03422398565657551},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6303030303030303,\n",
       "    'acc_stderr': 0.03769430314512568,\n",
       "    'acc_norm': 0.6303030303030303,\n",
       "    'acc_norm_stderr': 0.03769430314512568},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6919191919191919,\n",
       "    'acc_stderr': 0.03289477330098616,\n",
       "    'acc_norm': 0.6919191919191919,\n",
       "    'acc_norm_stderr': 0.03289477330098616},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.6476683937823834,\n",
       "    'acc_stderr': 0.03447478286414357,\n",
       "    'acc_norm': 0.6476683937823834,\n",
       "    'acc_norm_stderr': 0.03447478286414357},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.45384615384615384,\n",
       "    'acc_stderr': 0.02524277098712618,\n",
       "    'acc_norm': 0.45384615384615384,\n",
       "    'acc_norm_stderr': 0.02524277098712618},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.28888888888888886,\n",
       "    'acc_stderr': 0.027634907264178544,\n",
       "    'acc_norm': 0.28888888888888886,\n",
       "    'acc_norm_stderr': 0.027634907264178544},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.47058823529411764,\n",
       "    'acc_stderr': 0.03242225027115006,\n",
       "    'acc_norm': 0.47058823529411764,\n",
       "    'acc_norm_stderr': 0.03242225027115006},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3443708609271523,\n",
       "    'acc_stderr': 0.038796870240733264,\n",
       "    'acc_norm': 0.3443708609271523,\n",
       "    'acc_norm_stderr': 0.038796870240733264},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.728440366972477,\n",
       "    'acc_stderr': 0.019069098363191442,\n",
       "    'acc_norm': 0.728440366972477,\n",
       "    'acc_norm_stderr': 0.019069098363191442},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4166666666666667,\n",
       "    'acc_stderr': 0.03362277436608044,\n",
       "    'acc_norm': 0.4166666666666667,\n",
       "    'acc_norm_stderr': 0.03362277436608044},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7598039215686274,\n",
       "    'acc_stderr': 0.02998373305591362,\n",
       "    'acc_norm': 0.7598039215686274,\n",
       "    'acc_norm_stderr': 0.02998373305591362},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7088607594936709,\n",
       "    'acc_stderr': 0.029571601065753374,\n",
       "    'acc_norm': 0.7088607594936709,\n",
       "    'acc_norm_stderr': 0.029571601065753374},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5291479820627802,\n",
       "    'acc_stderr': 0.03350073248773403,\n",
       "    'acc_norm': 0.5291479820627802,\n",
       "    'acc_norm_stderr': 0.03350073248773403},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5725190839694656,\n",
       "    'acc_stderr': 0.04338920305792401,\n",
       "    'acc_norm': 0.5725190839694656,\n",
       "    'acc_norm_stderr': 0.04338920305792401},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.4793388429752066,\n",
       "    'acc_stderr': 0.04560456086387235,\n",
       "    'acc_norm': 0.4793388429752066,\n",
       "    'acc_norm_stderr': 0.04560456086387235},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.04812917324536823,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.04812917324536823},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6012269938650306,\n",
       "    'acc_stderr': 0.03847021420456023,\n",
       "    'acc_norm': 0.6012269938650306,\n",
       "    'acc_norm_stderr': 0.03847021420456023},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.36607142857142855,\n",
       "    'acc_stderr': 0.045723723587374296,\n",
       "    'acc_norm': 0.36607142857142855,\n",
       "    'acc_norm_stderr': 0.045723723587374296},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6990291262135923,\n",
       "    'acc_stderr': 0.045416094465039476,\n",
       "    'acc_norm': 0.6990291262135923,\n",
       "    'acc_norm_stderr': 0.045416094465039476},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7948717948717948,\n",
       "    'acc_stderr': 0.026453508054040332,\n",
       "    'acc_norm': 0.7948717948717948,\n",
       "    'acc_norm_stderr': 0.026453508054040332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.04999999999999999,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.04999999999999999},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.698595146871009,\n",
       "    'acc_stderr': 0.01640909109726878,\n",
       "    'acc_norm': 0.698595146871009,\n",
       "    'acc_norm_stderr': 0.01640909109726878},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5491329479768786,\n",
       "    'acc_stderr': 0.026788811931562757,\n",
       "    'acc_norm': 0.5491329479768786,\n",
       "    'acc_norm_stderr': 0.026788811931562757},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2860335195530726,\n",
       "    'acc_stderr': 0.015113972129062141,\n",
       "    'acc_norm': 0.2860335195530726,\n",
       "    'acc_norm_stderr': 0.015113972129062141},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5032679738562091,\n",
       "    'acc_stderr': 0.02862930519400354,\n",
       "    'acc_norm': 0.5032679738562091,\n",
       "    'acc_norm_stderr': 0.02862930519400354},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5594855305466238,\n",
       "    'acc_stderr': 0.028196400574197422,\n",
       "    'acc_norm': 0.5594855305466238,\n",
       "    'acc_norm_stderr': 0.028196400574197422},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.027431623722415012,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.027431623722415012},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3829787234042553,\n",
       "    'acc_stderr': 0.028999080904806178,\n",
       "    'acc_norm': 0.3829787234042553,\n",
       "    'acc_norm_stderr': 0.028999080904806178},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.37157757496740546,\n",
       "    'acc_stderr': 0.012341828514528298,\n",
       "    'acc_norm': 0.37157757496740546,\n",
       "    'acc_norm_stderr': 0.012341828514528298},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.48161764705882354,\n",
       "    'acc_stderr': 0.03035230339535196,\n",
       "    'acc_norm': 0.48161764705882354,\n",
       "    'acc_norm_stderr': 0.03035230339535196},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.49019607843137253,\n",
       "    'acc_stderr': 0.0202239460050743,\n",
       "    'acc_norm': 0.49019607843137253,\n",
       "    'acc_norm_stderr': 0.0202239460050743},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5727272727272728,\n",
       "    'acc_stderr': 0.04738198703545483,\n",
       "    'acc_norm': 0.5727272727272728,\n",
       "    'acc_norm_stderr': 0.04738198703545483},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5755102040816327,\n",
       "    'acc_stderr': 0.031642094879429414,\n",
       "    'acc_norm': 0.5755102040816327,\n",
       "    'acc_norm_stderr': 0.031642094879429414},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6169154228855721,\n",
       "    'acc_stderr': 0.034375193373382504,\n",
       "    'acc_norm': 0.6169154228855721,\n",
       "    'acc_norm_stderr': 0.034375193373382504},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.77,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.77,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.45180722891566266,\n",
       "    'acc_stderr': 0.038743715565879536,\n",
       "    'acc_norm': 0.45180722891566266,\n",
       "    'acc_norm_stderr': 0.038743715565879536},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3390452876376989,\n",
       "    'mc1_stderr': 0.01657179791062661,\n",
       "    'mc2': 0.5199824927914821,\n",
       "    'mc2_stderr': 0.01582403747940678},\n",
       "   'all': {'em': 0.0017827181208053692,\n",
       "    'em_stderr': 0.00043200973460389824,\n",
       "    'f1': 0.06156250000000011,\n",
       "    'f1_stderr': 0.001404482472524456,\n",
       "    'acc': 0.4371318828152442,\n",
       "    'acc_stderr': 0.010557145720065584},\n",
       "   'harness|drop|3': {'em': 0.0017827181208053692,\n",
       "    'em_stderr': 0.00043200973460389824,\n",
       "    'f1': 0.06156250000000011,\n",
       "    'f1_stderr': 0.001404482472524456},\n",
       "   'harness|gsm8k|5': {'acc': 0.12130401819560273,\n",
       "    'acc_stderr': 0.008992888497275597},\n",
       "   'harness|winogrande|5': {'acc': 0.7529597474348856,\n",
       "    'acc_stderr': 0.01212140294285557}}},\n",
       " 'mpt-7b-chat': {'key': 'mpt-7b-chat',\n",
       "  'Model': 'MPT-7B-Chat',\n",
       "  'MT-bench (score)': '5.42',\n",
       "  'MMLU': '0.320',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'MosaicML',\n",
       "  'Link': 'https://huggingface.co/mosaicml/mpt-7b-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.431740614334471,\n",
       "    'acc_stderr': 0.014474591427196204,\n",
       "    'acc_norm': 0.46501706484641636,\n",
       "    'acc_norm_stderr': 0.014575583922019669},\n",
       "   'harness|hellaswag|10': {'acc': 0.5710017924716192,\n",
       "    'acc_stderr': 0.004939215682191771,\n",
       "    'acc_norm': 0.7551284604660427,\n",
       "    'acc_norm_stderr': 0.00429132188812274},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.3925925925925926,\n",
       "    'acc_stderr': 0.04218506215368879,\n",
       "    'acc_norm': 0.3925925925925926,\n",
       "    'acc_norm_stderr': 0.04218506215368879},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.3684210526315789,\n",
       "    'acc_stderr': 0.03925523381052932,\n",
       "    'acc_norm': 0.3684210526315789,\n",
       "    'acc_norm_stderr': 0.03925523381052932},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.3849056603773585,\n",
       "    'acc_stderr': 0.029946498567699948,\n",
       "    'acc_norm': 0.3849056603773585,\n",
       "    'acc_norm_stderr': 0.029946498567699948},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.04076663253918567,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.04076663253918567},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621504,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621504},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.32947976878612717,\n",
       "    'acc_stderr': 0.03583901754736411,\n",
       "    'acc_norm': 0.32947976878612717,\n",
       "    'acc_norm_stderr': 0.03583901754736411},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.17647058823529413,\n",
       "    'acc_stderr': 0.0379328118530781,\n",
       "    'acc_norm': 0.17647058823529413,\n",
       "    'acc_norm_stderr': 0.0379328118530781},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.049888765156985884,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.049888765156985884},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.3702127659574468,\n",
       "    'acc_stderr': 0.031565646822367836,\n",
       "    'acc_norm': 0.3702127659574468,\n",
       "    'acc_norm_stderr': 0.031565646822367836},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.21929824561403508,\n",
       "    'acc_stderr': 0.03892431106518754,\n",
       "    'acc_norm': 0.21929824561403508,\n",
       "    'acc_norm_stderr': 0.03892431106518754},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.45517241379310347,\n",
       "    'acc_stderr': 0.04149886942192117,\n",
       "    'acc_norm': 0.45517241379310347,\n",
       "    'acc_norm_stderr': 0.04149886942192117},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.24338624338624337,\n",
       "    'acc_stderr': 0.02210112878741543,\n",
       "    'acc_norm': 0.24338624338624337,\n",
       "    'acc_norm_stderr': 0.02210112878741543},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.25396825396825395,\n",
       "    'acc_stderr': 0.038932596106046755,\n",
       "    'acc_norm': 0.25396825396825395,\n",
       "    'acc_norm_stderr': 0.038932596106046755},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768077,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768077},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.4032258064516129,\n",
       "    'acc_stderr': 0.027906150826041143,\n",
       "    'acc_norm': 0.4032258064516129,\n",
       "    'acc_norm_stderr': 0.027906150826041143},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.2413793103448276,\n",
       "    'acc_stderr': 0.030108330718011625,\n",
       "    'acc_norm': 0.2413793103448276,\n",
       "    'acc_norm_stderr': 0.030108330718011625},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.3212121212121212,\n",
       "    'acc_stderr': 0.036462049632538115,\n",
       "    'acc_norm': 0.3212121212121212,\n",
       "    'acc_norm_stderr': 0.036462049632538115},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.36363636363636365,\n",
       "    'acc_stderr': 0.034273086529999365,\n",
       "    'acc_norm': 0.36363636363636365,\n",
       "    'acc_norm_stderr': 0.034273086529999365},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.49222797927461137,\n",
       "    'acc_stderr': 0.03608003225569653,\n",
       "    'acc_norm': 0.49222797927461137,\n",
       "    'acc_norm_stderr': 0.03608003225569653},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.3871794871794872,\n",
       "    'acc_stderr': 0.02469721693087894,\n",
       "    'acc_norm': 0.3871794871794872,\n",
       "    'acc_norm_stderr': 0.02469721693087894},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.21851851851851853,\n",
       "    'acc_stderr': 0.025195752251823786,\n",
       "    'acc_norm': 0.21851851851851853,\n",
       "    'acc_norm_stderr': 0.025195752251823786},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.3907563025210084,\n",
       "    'acc_stderr': 0.031693802357129965,\n",
       "    'acc_norm': 0.3907563025210084,\n",
       "    'acc_norm_stderr': 0.031693802357129965},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2847682119205298,\n",
       "    'acc_stderr': 0.03684881521389024,\n",
       "    'acc_norm': 0.2847682119205298,\n",
       "    'acc_norm_stderr': 0.03684881521389024},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.5100917431192661,\n",
       "    'acc_stderr': 0.02143295620345332,\n",
       "    'acc_norm': 0.5100917431192661,\n",
       "    'acc_norm_stderr': 0.02143295620345332},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.35185185185185186,\n",
       "    'acc_stderr': 0.03256850570293648,\n",
       "    'acc_norm': 0.35185185185185186,\n",
       "    'acc_norm_stderr': 0.03256850570293648},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.3284313725490196,\n",
       "    'acc_stderr': 0.032962451101722294,\n",
       "    'acc_norm': 0.3284313725490196,\n",
       "    'acc_norm_stderr': 0.032962451101722294},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.4092827004219409,\n",
       "    'acc_stderr': 0.032007041833595914,\n",
       "    'acc_norm': 0.4092827004219409,\n",
       "    'acc_norm_stderr': 0.032007041833595914},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.484304932735426,\n",
       "    'acc_stderr': 0.0335412657542081,\n",
       "    'acc_norm': 0.484304932735426,\n",
       "    'acc_norm_stderr': 0.0335412657542081},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.46564885496183206,\n",
       "    'acc_stderr': 0.04374928560599738,\n",
       "    'acc_norm': 0.46564885496183206,\n",
       "    'acc_norm_stderr': 0.04374928560599738},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.4049586776859504,\n",
       "    'acc_stderr': 0.044811377559424694,\n",
       "    'acc_norm': 0.4049586776859504,\n",
       "    'acc_norm_stderr': 0.044811377559424694},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.4074074074074074,\n",
       "    'acc_stderr': 0.047500773411999854,\n",
       "    'acc_norm': 0.4074074074074074,\n",
       "    'acc_norm_stderr': 0.047500773411999854},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2883435582822086,\n",
       "    'acc_stderr': 0.03559039531617342,\n",
       "    'acc_norm': 0.2883435582822086,\n",
       "    'acc_norm_stderr': 0.03559039531617342},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.33035714285714285,\n",
       "    'acc_stderr': 0.04464285714285713,\n",
       "    'acc_norm': 0.33035714285714285,\n",
       "    'acc_norm_stderr': 0.04464285714285713},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.36893203883495146,\n",
       "    'acc_stderr': 0.04777615181156739,\n",
       "    'acc_norm': 0.36893203883495146,\n",
       "    'acc_norm_stderr': 0.04777615181156739},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.5512820512820513,\n",
       "    'acc_stderr': 0.032583346493868806,\n",
       "    'acc_norm': 0.5512820512820513,\n",
       "    'acc_norm_stderr': 0.032583346493868806},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.5325670498084292,\n",
       "    'acc_stderr': 0.01784199575052087,\n",
       "    'acc_norm': 0.5325670498084292,\n",
       "    'acc_norm_stderr': 0.01784199575052087},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.41040462427745666,\n",
       "    'acc_stderr': 0.026483392042098187,\n",
       "    'acc_norm': 0.41040462427745666,\n",
       "    'acc_norm_stderr': 0.026483392042098187},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961443,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961443},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.4019607843137255,\n",
       "    'acc_stderr': 0.02807415894760065,\n",
       "    'acc_norm': 0.4019607843137255,\n",
       "    'acc_norm_stderr': 0.02807415894760065},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.40836012861736337,\n",
       "    'acc_stderr': 0.027917050748484624,\n",
       "    'acc_norm': 0.40836012861736337,\n",
       "    'acc_norm_stderr': 0.027917050748484624},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.36728395061728397,\n",
       "    'acc_stderr': 0.026822801759507894,\n",
       "    'acc_norm': 0.36728395061728397,\n",
       "    'acc_norm_stderr': 0.026822801759507894},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2695035460992908,\n",
       "    'acc_stderr': 0.026469036818590627,\n",
       "    'acc_norm': 0.2695035460992908,\n",
       "    'acc_norm_stderr': 0.026469036818590627},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.2907431551499348,\n",
       "    'acc_stderr': 0.011598062372851988,\n",
       "    'acc_norm': 0.2907431551499348,\n",
       "    'acc_norm_stderr': 0.011598062372851988},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.3897058823529412,\n",
       "    'acc_stderr': 0.029624663581159696,\n",
       "    'acc_norm': 0.3897058823529412,\n",
       "    'acc_norm_stderr': 0.029624663581159696},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.3202614379084967,\n",
       "    'acc_stderr': 0.018875682938069446,\n",
       "    'acc_norm': 0.3202614379084967,\n",
       "    'acc_norm_stderr': 0.018875682938069446},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.4636363636363636,\n",
       "    'acc_stderr': 0.047764491623961985,\n",
       "    'acc_norm': 0.4636363636363636,\n",
       "    'acc_norm_stderr': 0.047764491623961985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.4857142857142857,\n",
       "    'acc_stderr': 0.03199615232806286,\n",
       "    'acc_norm': 0.4857142857142857,\n",
       "    'acc_norm_stderr': 0.03199615232806286},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.48258706467661694,\n",
       "    'acc_stderr': 0.03533389234739245,\n",
       "    'acc_norm': 0.48258706467661694,\n",
       "    'acc_norm_stderr': 0.03533389234739245},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.04975698519562426,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.04975698519562426},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.42771084337349397,\n",
       "    'acc_stderr': 0.038515976837185335,\n",
       "    'acc_norm': 0.42771084337349397,\n",
       "    'acc_norm_stderr': 0.038515976837185335},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.49707602339181284,\n",
       "    'acc_stderr': 0.03834759370936839,\n",
       "    'acc_norm': 0.49707602339181284,\n",
       "    'acc_norm_stderr': 0.03834759370936839},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.27050183598531213,\n",
       "    'mc1_stderr': 0.015550778332842895,\n",
       "    'mc2': 0.40163647231251104,\n",
       "    'mc2_stderr': 0.014753108409806075},\n",
       "   'all': {'em': 0.06952600671140939,\n",
       "    'em_stderr': 0.002604746204517829,\n",
       "    'f1': 0.12196937919463072,\n",
       "    'f1_stderr': 0.002840521979064293,\n",
       "    'acc': 0.3626168565432783,\n",
       "    'acc_stderr': 0.009260585769647573},\n",
       "   'harness|drop|3': {'em': 0.06952600671140939,\n",
       "    'em_stderr': 0.002604746204517829,\n",
       "    'f1': 0.12196937919463072,\n",
       "    'f1_stderr': 0.002840521979064293},\n",
       "   'harness|gsm8k|5': {'acc': 0.04094010614101592,\n",
       "    'acc_stderr': 0.005458076796294338},\n",
       "   'harness|winogrande|5': {'acc': 0.6842936069455406,\n",
       "    'acc_stderr': 0.01306309474300081}}},\n",
       " 'oasst-pythia-12b': {'key': 'oasst-pythia-12b',\n",
       "  'Model': 'OpenAssistant-Pythia-12B',\n",
       "  'MT-bench (score)': '4.32',\n",
       "  'MMLU': '0.270',\n",
       "  'License': 'Apache 2.0',\n",
       "  'Organization': 'OpenAssistant',\n",
       "  'Link': 'https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',\n",
       "  'results': {'harness|drop|3': {'em': 0.001363255033557047,\n",
       "    'em_stderr': 0.00037786091964606887,\n",
       "    'f1': 0.059077181208053976,\n",
       "    'f1_stderr': 0.001394848925611238},\n",
       "   'harness|gsm8k|5': {'acc': 0.030326004548900682,\n",
       "    'acc_stderr': 0.004723487465514761},\n",
       "   'harness|winogrande|5': {'acc': 0.659037095501184,\n",
       "    'acc_stderr': 0.013322681435934807},\n",
       "   'all': {'acc': 0.2748440247593001,\n",
       "    'acc_stderr': 0.03228989271589345,\n",
       "    'acc_norm': 0.2784679867417539,\n",
       "    'acc_norm_stderr': 0.03228683184562667,\n",
       "    'mc1': 0.2386780905752754,\n",
       "    'mc1_stderr': 0.014922629695456418,\n",
       "    'mc2': 0.37807719406726975,\n",
       "    'mc2_stderr': 0.014701655079555453},\n",
       "   'harness|arc:challenge|25': {'acc': 0.4121160409556314,\n",
       "    'acc_stderr': 0.0143839153022254,\n",
       "    'acc_norm': 0.45733788395904434,\n",
       "    'acc_norm_stderr': 0.014558106543924067},\n",
       "   'harness|hellaswag|10': {'acc': 0.5173272256522605,\n",
       "    'acc_stderr': 0.004986784319771785,\n",
       "    'acc_norm': 0.6859191396136228,\n",
       "    'acc_norm_stderr': 0.004632001732332984},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768081,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768081},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.31851851851851853,\n",
       "    'acc_stderr': 0.04024778401977109,\n",
       "    'acc_norm': 0.31851851851851853,\n",
       "    'acc_norm_stderr': 0.04024778401977109},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.23026315789473684,\n",
       "    'acc_stderr': 0.03426059424403165,\n",
       "    'acc_norm': 0.23026315789473684,\n",
       "    'acc_norm_stderr': 0.03426059424403165},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.33584905660377357,\n",
       "    'acc_stderr': 0.029067220146644833,\n",
       "    'acc_norm': 0.33584905660377357,\n",
       "    'acc_norm_stderr': 0.029067220146644833},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2847222222222222,\n",
       "    'acc_stderr': 0.03773809990686936,\n",
       "    'acc_norm': 0.2847222222222222,\n",
       "    'acc_norm_stderr': 0.03773809990686936},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.2,\n",
       "    'acc_stderr': 0.040201512610368445,\n",
       "    'acc_norm': 0.2,\n",
       "    'acc_norm_stderr': 0.040201512610368445},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.24855491329479767,\n",
       "    'acc_stderr': 0.03295304696818317,\n",
       "    'acc_norm': 0.24855491329479767,\n",
       "    'acc_norm_stderr': 0.03295304696818317},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.28431372549019607,\n",
       "    'acc_stderr': 0.04488482852329017,\n",
       "    'acc_norm': 0.28431372549019607,\n",
       "    'acc_norm_stderr': 0.04488482852329017},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768079,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768079},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.30638297872340425,\n",
       "    'acc_stderr': 0.030135906478517563,\n",
       "    'acc_norm': 0.30638297872340425,\n",
       "    'acc_norm_stderr': 0.030135906478517563},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.22807017543859648,\n",
       "    'acc_stderr': 0.03947152782669416,\n",
       "    'acc_norm': 0.22807017543859648,\n",
       "    'acc_norm_stderr': 0.03947152782669416},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.25517241379310346,\n",
       "    'acc_stderr': 0.03632984052707841,\n",
       "    'acc_norm': 0.25517241379310346,\n",
       "    'acc_norm_stderr': 0.03632984052707841},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.25132275132275134,\n",
       "    'acc_stderr': 0.022340482339643895,\n",
       "    'acc_norm': 0.25132275132275134,\n",
       "    'acc_norm_stderr': 0.022340482339643895},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.1746031746031746,\n",
       "    'acc_stderr': 0.033954900208561116,\n",
       "    'acc_norm': 0.1746031746031746,\n",
       "    'acc_norm_stderr': 0.033954900208561116},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.04560480215720684,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.04560480215720684},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.267741935483871,\n",
       "    'acc_stderr': 0.025189006660212385,\n",
       "    'acc_norm': 0.267741935483871,\n",
       "    'acc_norm_stderr': 0.025189006660212385},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.21674876847290642,\n",
       "    'acc_stderr': 0.028990331252516235,\n",
       "    'acc_norm': 0.21674876847290642,\n",
       "    'acc_norm_stderr': 0.028990331252516235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.28484848484848485,\n",
       "    'acc_stderr': 0.03524390844511784,\n",
       "    'acc_norm': 0.28484848484848485,\n",
       "    'acc_norm_stderr': 0.03524390844511784},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.25757575757575757,\n",
       "    'acc_stderr': 0.031156269519646836,\n",
       "    'acc_norm': 0.25757575757575757,\n",
       "    'acc_norm_stderr': 0.031156269519646836},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.23834196891191708,\n",
       "    'acc_stderr': 0.03074890536390989,\n",
       "    'acc_norm': 0.23834196891191708,\n",
       "    'acc_norm_stderr': 0.03074890536390989},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.2641025641025641,\n",
       "    'acc_stderr': 0.022352193737453282,\n",
       "    'acc_norm': 0.2641025641025641,\n",
       "    'acc_norm_stderr': 0.022352193737453282},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2518518518518518,\n",
       "    'acc_stderr': 0.02646611753895991,\n",
       "    'acc_norm': 0.2518518518518518,\n",
       "    'acc_norm_stderr': 0.02646611753895991},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.24369747899159663,\n",
       "    'acc_stderr': 0.02788682807838058,\n",
       "    'acc_norm': 0.24369747899159663,\n",
       "    'acc_norm_stderr': 0.02788682807838058},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.26490066225165565,\n",
       "    'acc_stderr': 0.03603038545360384,\n",
       "    'acc_norm': 0.26490066225165565,\n",
       "    'acc_norm_stderr': 0.03603038545360384},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.24587155963302754,\n",
       "    'acc_stderr': 0.018461940968708446,\n",
       "    'acc_norm': 0.24587155963302754,\n",
       "    'acc_norm_stderr': 0.018461940968708446},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.24074074074074073,\n",
       "    'acc_stderr': 0.02915752218460561,\n",
       "    'acc_norm': 0.24074074074074073,\n",
       "    'acc_norm_stderr': 0.02915752218460561},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.29411764705882354,\n",
       "    'acc_stderr': 0.03198001660115071,\n",
       "    'acc_norm': 0.29411764705882354,\n",
       "    'acc_norm_stderr': 0.03198001660115071},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.23628691983122363,\n",
       "    'acc_stderr': 0.027652153144159263,\n",
       "    'acc_norm': 0.23628691983122363,\n",
       "    'acc_norm_stderr': 0.027652153144159263},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.3004484304932735,\n",
       "    'acc_stderr': 0.030769352008229136,\n",
       "    'acc_norm': 0.3004484304932735,\n",
       "    'acc_norm_stderr': 0.030769352008229136},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.22137404580152673,\n",
       "    'acc_stderr': 0.036412970813137276,\n",
       "    'acc_norm': 0.22137404580152673,\n",
       "    'acc_norm_stderr': 0.036412970813137276},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.3305785123966942,\n",
       "    'acc_stderr': 0.04294340845212094,\n",
       "    'acc_norm': 0.3305785123966942,\n",
       "    'acc_norm_stderr': 0.04294340845212094},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.28703703703703703,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.28703703703703703,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2822085889570552,\n",
       "    'acc_stderr': 0.03536117886664743,\n",
       "    'acc_norm': 0.2822085889570552,\n",
       "    'acc_norm_stderr': 0.03536117886664743},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.26785714285714285,\n",
       "    'acc_stderr': 0.04203277291467764,\n",
       "    'acc_norm': 0.26785714285714285,\n",
       "    'acc_norm_stderr': 0.04203277291467764},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.18446601941747573,\n",
       "    'acc_stderr': 0.03840423627288276,\n",
       "    'acc_norm': 0.18446601941747573,\n",
       "    'acc_norm_stderr': 0.03840423627288276},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.29914529914529914,\n",
       "    'acc_stderr': 0.029996951858349497,\n",
       "    'acc_norm': 0.29914529914529914,\n",
       "    'acc_norm_stderr': 0.029996951858349497},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.28607918263090676,\n",
       "    'acc_stderr': 0.01616087140512754,\n",
       "    'acc_norm': 0.28607918263090676,\n",
       "    'acc_norm_stderr': 0.01616087140512754},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.25722543352601157,\n",
       "    'acc_stderr': 0.02353292543104429,\n",
       "    'acc_norm': 0.25722543352601157,\n",
       "    'acc_norm_stderr': 0.02353292543104429},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2424581005586592,\n",
       "    'acc_stderr': 0.014333522059217889,\n",
       "    'acc_norm': 0.2424581005586592,\n",
       "    'acc_norm_stderr': 0.014333522059217889},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.27124183006535946,\n",
       "    'acc_stderr': 0.025457756696667864,\n",
       "    'acc_norm': 0.27124183006535946,\n",
       "    'acc_norm_stderr': 0.025457756696667864},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.2861736334405145,\n",
       "    'acc_stderr': 0.02567025924218895,\n",
       "    'acc_norm': 0.2861736334405145,\n",
       "    'acc_norm_stderr': 0.02567025924218895},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.2716049382716049,\n",
       "    'acc_stderr': 0.02474862449053737,\n",
       "    'acc_norm': 0.2716049382716049,\n",
       "    'acc_norm_stderr': 0.02474862449053737},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2730496453900709,\n",
       "    'acc_stderr': 0.026577860943307857,\n",
       "    'acc_norm': 0.2730496453900709,\n",
       "    'acc_norm_stderr': 0.026577860943307857},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.27835723598435463,\n",
       "    'acc_stderr': 0.011446990197380985,\n",
       "    'acc_norm': 0.27835723598435463,\n",
       "    'acc_norm_stderr': 0.011446990197380985},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.23529411764705882,\n",
       "    'acc_stderr': 0.025767252010855973,\n",
       "    'acc_norm': 0.23529411764705882,\n",
       "    'acc_norm_stderr': 0.025767252010855973},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.2777777777777778,\n",
       "    'acc_stderr': 0.018120224251484584,\n",
       "    'acc_norm': 0.2777777777777778,\n",
       "    'acc_norm_stderr': 0.018120224251484584},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.32727272727272727,\n",
       "    'acc_stderr': 0.04494290866252089,\n",
       "    'acc_norm': 0.32727272727272727,\n",
       "    'acc_norm_stderr': 0.04494290866252089},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.2571428571428571,\n",
       "    'acc_stderr': 0.027979823538744543,\n",
       "    'acc_norm': 0.2571428571428571,\n",
       "    'acc_norm_stderr': 0.027979823538744543},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.27860696517412936,\n",
       "    'acc_stderr': 0.031700561834973086,\n",
       "    'acc_norm': 0.27860696517412936,\n",
       "    'acc_norm_stderr': 0.031700561834973086},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.3373493975903614,\n",
       "    'acc_stderr': 0.0368078369072758,\n",
       "    'acc_norm': 0.3373493975903614,\n",
       "    'acc_norm_stderr': 0.0368078369072758},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.26900584795321636,\n",
       "    'acc_stderr': 0.03401052620104089,\n",
       "    'acc_norm': 0.26900584795321636,\n",
       "    'acc_norm_stderr': 0.03401052620104089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.2386780905752754,\n",
       "    'mc1_stderr': 0.014922629695456418,\n",
       "    'mc2': 0.37807719406726975,\n",
       "    'mc2_stderr': 0.014701655079555453}}},\n",
       " 'openchat-3.5': {'key': 'openchat-3.5',\n",
       "  'Model': 'OpenChat-3.5',\n",
       "  'MT-bench (score)': '7.81',\n",
       "  'MMLU': '0.643',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'OpenChat',\n",
       "  'Link': 'https://huggingface.co/openchat/openchat_3.5',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5972696245733788,\n",
       "    'acc_stderr': 0.014332236306790147,\n",
       "    'acc_norm': 0.6382252559726962,\n",
       "    'acc_norm_stderr': 0.014041957945038078},\n",
       "   'harness|hellaswag|10': {'acc': 0.6579366660027883,\n",
       "    'acc_stderr': 0.004734311435009196,\n",
       "    'acc_norm': 0.8480382393945429,\n",
       "    'acc_norm_stderr': 0.003582501596564539},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5703703703703704,\n",
       "    'acc_stderr': 0.042763494943765995,\n",
       "    'acc_norm': 0.5703703703703704,\n",
       "    'acc_norm_stderr': 0.042763494943765995},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6973684210526315,\n",
       "    'acc_stderr': 0.037385206761196686,\n",
       "    'acc_norm': 0.6973684210526315,\n",
       "    'acc_norm_stderr': 0.037385206761196686},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.028544793319055326,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.028544793319055326},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7708333333333334,\n",
       "    'acc_stderr': 0.035146974678623884,\n",
       "    'acc_norm': 0.7708333333333334,\n",
       "    'acc_norm_stderr': 0.035146974678623884},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.653179190751445,\n",
       "    'acc_stderr': 0.036291466701596636,\n",
       "    'acc_norm': 0.653179190751445,\n",
       "    'acc_norm_stderr': 0.036291466701596636},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4117647058823529,\n",
       "    'acc_stderr': 0.04897104952726366,\n",
       "    'acc_norm': 0.4117647058823529,\n",
       "    'acc_norm_stderr': 0.04897104952726366},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.574468085106383,\n",
       "    'acc_stderr': 0.032321469162244675,\n",
       "    'acc_norm': 0.574468085106383,\n",
       "    'acc_norm_stderr': 0.032321469162244675},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5175438596491229,\n",
       "    'acc_stderr': 0.04700708033551038,\n",
       "    'acc_norm': 0.5175438596491229,\n",
       "    'acc_norm_stderr': 0.04700708033551038},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370332,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370332},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42328042328042326,\n",
       "    'acc_stderr': 0.02544636563440679,\n",
       "    'acc_norm': 0.42328042328042326,\n",
       "    'acc_norm_stderr': 0.02544636563440679},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5158730158730159,\n",
       "    'acc_stderr': 0.044698818540726076,\n",
       "    'acc_norm': 0.5158730158730159,\n",
       "    'acc_norm_stderr': 0.044698818540726076},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7967741935483871,\n",
       "    'acc_stderr': 0.02289168798455496,\n",
       "    'acc_norm': 0.7967741935483871,\n",
       "    'acc_norm_stderr': 0.02289168798455496},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5073891625615764,\n",
       "    'acc_stderr': 0.035176035403610105,\n",
       "    'acc_norm': 0.5073891625615764,\n",
       "    'acc_norm_stderr': 0.035176035403610105},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7757575757575758,\n",
       "    'acc_stderr': 0.032568666616811015,\n",
       "    'acc_norm': 0.7757575757575758,\n",
       "    'acc_norm_stderr': 0.032568666616811015},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.803030303030303,\n",
       "    'acc_stderr': 0.028335609732463362,\n",
       "    'acc_norm': 0.803030303030303,\n",
       "    'acc_norm_stderr': 0.028335609732463362},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9067357512953368,\n",
       "    'acc_stderr': 0.02098685459328974,\n",
       "    'acc_norm': 0.9067357512953368,\n",
       "    'acc_norm_stderr': 0.02098685459328974},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.023901157979402538,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.023901157979402538},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.36666666666666664,\n",
       "    'acc_stderr': 0.02938162072646507,\n",
       "    'acc_norm': 0.36666666666666664,\n",
       "    'acc_norm_stderr': 0.02938162072646507},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6680672268907563,\n",
       "    'acc_stderr': 0.03058869701378364,\n",
       "    'acc_norm': 0.6680672268907563,\n",
       "    'acc_norm_stderr': 0.03058869701378364},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.038227469376587525,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.038227469376587525},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8550458715596331,\n",
       "    'acc_stderr': 0.015094215699700469,\n",
       "    'acc_norm': 0.8550458715596331,\n",
       "    'acc_norm_stderr': 0.015094215699700469},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.03395322726375798,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.03395322726375798},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8333333333333334,\n",
       "    'acc_stderr': 0.026156867523931045,\n",
       "    'acc_norm': 0.8333333333333334,\n",
       "    'acc_norm_stderr': 0.026156867523931045},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8227848101265823,\n",
       "    'acc_stderr': 0.024856364184503234,\n",
       "    'acc_norm': 0.8227848101265823,\n",
       "    'acc_norm_stderr': 0.024856364184503234},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.726457399103139,\n",
       "    'acc_stderr': 0.029918586707798827,\n",
       "    'acc_norm': 0.726457399103139,\n",
       "    'acc_norm_stderr': 0.029918586707798827},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7786259541984732,\n",
       "    'acc_stderr': 0.03641297081313729,\n",
       "    'acc_norm': 0.7786259541984732,\n",
       "    'acc_norm_stderr': 0.03641297081313729},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03520893951097653,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03520893951097653},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.0401910747255735,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.0401910747255735},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7791411042944786,\n",
       "    'acc_stderr': 0.03259177392742178,\n",
       "    'acc_norm': 0.7791411042944786,\n",
       "    'acc_norm_stderr': 0.03259177392742178},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.44642857142857145,\n",
       "    'acc_stderr': 0.04718471485219588,\n",
       "    'acc_norm': 0.44642857142857145,\n",
       "    'acc_norm_stderr': 0.04718471485219588},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8252427184466019,\n",
       "    'acc_stderr': 0.03760178006026621,\n",
       "    'acc_norm': 0.8252427184466019,\n",
       "    'acc_norm_stderr': 0.03760178006026621},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8760683760683761,\n",
       "    'acc_stderr': 0.02158649400128138,\n",
       "    'acc_norm': 0.8760683760683761,\n",
       "    'acc_norm_stderr': 0.02158649400128138},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8212005108556832,\n",
       "    'acc_stderr': 0.013702643715368982,\n",
       "    'acc_norm': 0.8212005108556832,\n",
       "    'acc_norm_stderr': 0.013702643715368982},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7456647398843931,\n",
       "    'acc_stderr': 0.023445826276545546,\n",
       "    'acc_norm': 0.7456647398843931,\n",
       "    'acc_norm_stderr': 0.023445826276545546},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.42681564245810055,\n",
       "    'acc_stderr': 0.016542401954631906,\n",
       "    'acc_norm': 0.42681564245810055,\n",
       "    'acc_norm_stderr': 0.016542401954631906},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7189542483660131,\n",
       "    'acc_stderr': 0.025738854797818733,\n",
       "    'acc_norm': 0.7189542483660131,\n",
       "    'acc_norm_stderr': 0.025738854797818733},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7202572347266881,\n",
       "    'acc_stderr': 0.025494259350694912,\n",
       "    'acc_norm': 0.7202572347266881,\n",
       "    'acc_norm_stderr': 0.025494259350694912},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7314814814814815,\n",
       "    'acc_stderr': 0.02465968518596728,\n",
       "    'acc_norm': 0.7314814814814815,\n",
       "    'acc_norm_stderr': 0.02465968518596728},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.48226950354609927,\n",
       "    'acc_stderr': 0.02980873964223777,\n",
       "    'acc_norm': 0.48226950354609927,\n",
       "    'acc_norm_stderr': 0.02980873964223777},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.47392438070404175,\n",
       "    'acc_stderr': 0.012752858346533134,\n",
       "    'acc_norm': 0.47392438070404175,\n",
       "    'acc_norm_stderr': 0.012752858346533134},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6801470588235294,\n",
       "    'acc_stderr': 0.02833295951403121,\n",
       "    'acc_norm': 0.6801470588235294,\n",
       "    'acc_norm_stderr': 0.02833295951403121},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6748366013071896,\n",
       "    'acc_stderr': 0.01895088677080631,\n",
       "    'acc_norm': 0.6748366013071896,\n",
       "    'acc_norm_stderr': 0.01895088677080631},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.0449429086625209,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.0449429086625209},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.028263889943784603,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.028263889943784603},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.835820895522388,\n",
       "    'acc_stderr': 0.026193923544454125,\n",
       "    'acc_norm': 0.835820895522388,\n",
       "    'acc_norm_stderr': 0.026193923544454125},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.88,\n",
       "    'acc_stderr': 0.03265986323710906,\n",
       "    'acc_norm': 0.88,\n",
       "    'acc_norm_stderr': 0.03265986323710906},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.536144578313253,\n",
       "    'acc_stderr': 0.038823108508905954,\n",
       "    'acc_norm': 0.536144578313253,\n",
       "    'acc_norm_stderr': 0.038823108508905954},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.30966952264381886,\n",
       "    'mc1_stderr': 0.0161857443551449,\n",
       "    'mc2': 0.46392847173087287,\n",
       "    'mc2_stderr': 0.015046687408684912},\n",
       "   'harness|winogrande|5': {'acc': 0.8074191002367798,\n",
       "    'acc_stderr': 0.011082538847491902},\n",
       "   'harness|drop|3': {'em': 0.0019924496644295304,\n",
       "    'em_stderr': 0.0004566676462666931,\n",
       "    'f1': 0.07226510067114073,\n",
       "    'f1_stderr': 0.0014840662284336832},\n",
       "   'harness|gsm8k|5': {'acc': 0.26611068991660347,\n",
       "    'acc_stderr': 0.012172750939040322},\n",
       "   'all': {'acc': 0.6453244910928128,\n",
       "    'acc_stderr': 0.031930893551459144,\n",
       "    'acc_norm': 0.6529207012084622,\n",
       "    'acc_norm_stderr': 0.03259469709580134,\n",
       "    'mc1': 0.30966952264381886,\n",
       "    'mc1_stderr': 0.0161857443551449,\n",
       "    'mc2': 0.46392847173087287,\n",
       "    'mc2_stderr': 0.015046687408684912,\n",
       "    'em': 0.0019924496644295304,\n",
       "    'em_stderr': 0.0004566676462666931,\n",
       "    'f1': 0.07226510067114073,\n",
       "    'f1_stderr': 0.0014840662284336832}}},\n",
       " 'openhermes-2.5-mistral-7b': {'key': 'openhermes-2.5-mistral-7b',\n",
       "  'Model': 'OpenHermes-2.5-Mistral-7b',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '-',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'NousResearch',\n",
       "  'Link': 'https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6126279863481229,\n",
       "    'acc_stderr': 0.014235872487909869,\n",
       "    'acc_norm': 0.6493174061433447,\n",
       "    'acc_norm_stderr': 0.013944635930726099},\n",
       "   'harness|hellaswag|10': {'acc': 0.6519617606054571,\n",
       "    'acc_stderr': 0.004753746951620152,\n",
       "    'acc_norm': 0.8429595698068114,\n",
       "    'acc_norm_stderr': 0.003630952999843739},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695236,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695236},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.0421850621536888,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.0421850621536888},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6973684210526315,\n",
       "    'acc_stderr': 0.037385206761196686,\n",
       "    'acc_norm': 0.6973684210526315,\n",
       "    'acc_norm_stderr': 0.037385206761196686},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.02854479331905533,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.02854479331905533},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7569444444444444,\n",
       "    'acc_stderr': 0.03586879280080341,\n",
       "    'acc_norm': 0.7569444444444444,\n",
       "    'acc_norm_stderr': 0.03586879280080341},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.46,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.46,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6242774566473989,\n",
       "    'acc_stderr': 0.036928207672648664,\n",
       "    'acc_norm': 0.6242774566473989,\n",
       "    'acc_norm_stderr': 0.036928207672648664},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107223,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107223},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5659574468085107,\n",
       "    'acc_stderr': 0.03240038086792747,\n",
       "    'acc_norm': 0.5659574468085107,\n",
       "    'acc_norm_stderr': 0.03240038086792747},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.49122807017543857,\n",
       "    'acc_stderr': 0.04702880432049615,\n",
       "    'acc_norm': 0.49122807017543857,\n",
       "    'acc_norm_stderr': 0.04702880432049615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5241379310344828,\n",
       "    'acc_stderr': 0.0416180850350153,\n",
       "    'acc_norm': 0.5241379310344828,\n",
       "    'acc_norm_stderr': 0.0416180850350153},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42592592592592593,\n",
       "    'acc_stderr': 0.02546714904546955,\n",
       "    'acc_norm': 0.42592592592592593,\n",
       "    'acc_norm_stderr': 0.02546714904546955},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.46825396825396826,\n",
       "    'acc_stderr': 0.04463112720677172,\n",
       "    'acc_norm': 0.46825396825396826,\n",
       "    'acc_norm_stderr': 0.04463112720677172},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7935483870967742,\n",
       "    'acc_stderr': 0.02302589961718871,\n",
       "    'acc_norm': 0.7935483870967742,\n",
       "    'acc_norm_stderr': 0.02302589961718871},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.035158955511656986,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.035158955511656986},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7818181818181819,\n",
       "    'acc_stderr': 0.032250781083062896,\n",
       "    'acc_norm': 0.7818181818181819,\n",
       "    'acc_norm_stderr': 0.032250781083062896},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.028057791672989017,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.028057791672989017},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8911917098445595,\n",
       "    'acc_stderr': 0.022473253332768776,\n",
       "    'acc_norm': 0.8911917098445595,\n",
       "    'acc_norm_stderr': 0.022473253332768776},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6128205128205129,\n",
       "    'acc_stderr': 0.024697216930878937,\n",
       "    'acc_norm': 0.6128205128205129,\n",
       "    'acc_norm_stderr': 0.024697216930878937},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3037037037037037,\n",
       "    'acc_stderr': 0.02803792996911499,\n",
       "    'acc_norm': 0.3037037037037037,\n",
       "    'acc_norm_stderr': 0.02803792996911499},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.680672268907563,\n",
       "    'acc_stderr': 0.030283995525884396,\n",
       "    'acc_norm': 0.680672268907563,\n",
       "    'acc_norm_stderr': 0.030283995525884396},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.31788079470198677,\n",
       "    'acc_stderr': 0.038020397601079024,\n",
       "    'acc_norm': 0.31788079470198677,\n",
       "    'acc_norm_stderr': 0.038020397601079024},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8330275229357799,\n",
       "    'acc_stderr': 0.01599015488507338,\n",
       "    'acc_norm': 0.8330275229357799,\n",
       "    'acc_norm_stderr': 0.01599015488507338},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5092592592592593,\n",
       "    'acc_stderr': 0.034093869469927006,\n",
       "    'acc_norm': 0.5092592592592593,\n",
       "    'acc_norm_stderr': 0.034093869469927006},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7990196078431373,\n",
       "    'acc_stderr': 0.02812597226565437,\n",
       "    'acc_norm': 0.7990196078431373,\n",
       "    'acc_norm_stderr': 0.02812597226565437},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8143459915611815,\n",
       "    'acc_stderr': 0.025310495376944856,\n",
       "    'acc_norm': 0.8143459915611815,\n",
       "    'acc_norm_stderr': 0.025310495376944856},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7040358744394619,\n",
       "    'acc_stderr': 0.030636591348699803,\n",
       "    'acc_norm': 0.7040358744394619,\n",
       "    'acc_norm_stderr': 0.030636591348699803},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7938931297709924,\n",
       "    'acc_stderr': 0.035477710041594654,\n",
       "    'acc_norm': 0.7938931297709924,\n",
       "    'acc_norm_stderr': 0.035477710041594654},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7603305785123967,\n",
       "    'acc_stderr': 0.03896878985070416,\n",
       "    'acc_norm': 0.7603305785123967,\n",
       "    'acc_norm_stderr': 0.03896878985070416},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7870370370370371,\n",
       "    'acc_stderr': 0.039578354719809805,\n",
       "    'acc_norm': 0.7870370370370371,\n",
       "    'acc_norm_stderr': 0.039578354719809805},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7852760736196319,\n",
       "    'acc_stderr': 0.032262193772867744,\n",
       "    'acc_norm': 0.7852760736196319,\n",
       "    'acc_norm_stderr': 0.032262193772867744},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.5089285714285714,\n",
       "    'acc_stderr': 0.04745033255489123,\n",
       "    'acc_norm': 0.5089285714285714,\n",
       "    'acc_norm_stderr': 0.04745033255489123},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7766990291262136,\n",
       "    'acc_stderr': 0.04123553189891431,\n",
       "    'acc_norm': 0.7766990291262136,\n",
       "    'acc_norm_stderr': 0.04123553189891431},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.022801382534597528,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.022801382534597528},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8301404853128991,\n",
       "    'acc_stderr': 0.013428186370608306,\n",
       "    'acc_norm': 0.8301404853128991,\n",
       "    'acc_norm_stderr': 0.013428186370608306},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7167630057803468,\n",
       "    'acc_stderr': 0.02425790170532338,\n",
       "    'acc_norm': 0.7167630057803468,\n",
       "    'acc_norm_stderr': 0.02425790170532338},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30837988826815643,\n",
       "    'acc_stderr': 0.01544571691099888,\n",
       "    'acc_norm': 0.30837988826815643,\n",
       "    'acc_norm_stderr': 0.01544571691099888},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7549019607843137,\n",
       "    'acc_stderr': 0.024630048979824782,\n",
       "    'acc_norm': 0.7549019607843137,\n",
       "    'acc_norm_stderr': 0.024630048979824782},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.684887459807074,\n",
       "    'acc_stderr': 0.026385273703464485,\n",
       "    'acc_norm': 0.684887459807074,\n",
       "    'acc_norm_stderr': 0.026385273703464485},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7530864197530864,\n",
       "    'acc_stderr': 0.02399350170904211,\n",
       "    'acc_norm': 0.7530864197530864,\n",
       "    'acc_norm_stderr': 0.02399350170904211},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5070921985815603,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.5070921985815603,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.46936114732724904,\n",
       "    'acc_stderr': 0.012746237711716634,\n",
       "    'acc_norm': 0.46936114732724904,\n",
       "    'acc_norm_stderr': 0.012746237711716634},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.028418208619406762,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.028418208619406762},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.673202614379085,\n",
       "    'acc_stderr': 0.018975427920507215,\n",
       "    'acc_norm': 0.673202614379085,\n",
       "    'acc_norm_stderr': 0.018975427920507215},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.028263889943784596,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.028263889943784596},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8159203980099502,\n",
       "    'acc_stderr': 0.027403859410786845,\n",
       "    'acc_norm': 0.8159203980099502,\n",
       "    'acc_norm_stderr': 0.027403859410786845},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.033799766898963086,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.033799766898963086},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5542168674698795,\n",
       "    'acc_stderr': 0.03869543323472101,\n",
       "    'acc_norm': 0.5542168674698795,\n",
       "    'acc_norm_stderr': 0.03869543323472101},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3598531211750306,\n",
       "    'mc1_stderr': 0.01680186046667716,\n",
       "    'mc2': 0.5230564118100686,\n",
       "    'mc2_stderr': 0.015250230546286025},\n",
       "   'harness|winogrande|5': {'acc': 0.7790055248618785,\n",
       "    'acc_stderr': 0.011661223637643412},\n",
       "   'harness|drop|3': {'em': 0.30830536912751677,\n",
       "    'em_stderr': 0.004729196914949925,\n",
       "    'f1': 0.35989723154362524,\n",
       "    'f1_stderr': 0.004629324720589026},\n",
       "   'harness|gsm8k|5': {'acc': 0.25473843821076575,\n",
       "    'acc_stderr': 0.012001731232879136},\n",
       "   'all': {'acc': 0.6340440103659418,\n",
       "    'acc_stderr': 0.03220763540877311,\n",
       "    'acc_norm': 0.6418750491228201,\n",
       "    'acc_norm_stderr': 0.032874386009418256,\n",
       "    'mc1': 0.3598531211750306,\n",
       "    'mc1_stderr': 0.01680186046667716,\n",
       "    'mc2': 0.5230564118100686,\n",
       "    'mc2_stderr': 0.015250230546286025,\n",
       "    'em': 0.30830536912751677,\n",
       "    'em_stderr': 0.004729196914949925,\n",
       "    'f1': 0.35989723154362524,\n",
       "    'f1_stderr': 0.004629324720589026}}},\n",
       " 'solar-10.7b-instruct-v1.0': {'key': 'solar-10.7b-instruct-v1.0',\n",
       "  'Model': 'SOLAR-10.7B-Instruct-v1.0',\n",
       "  'MT-bench (score)': '7.58',\n",
       "  'MMLU': '0.662',\n",
       "  'License': 'CC-BY-NC-4.0',\n",
       "  'Organization': 'Upstage AI',\n",
       "  'Link': 'https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6808873720136519,\n",
       "    'acc_stderr': 0.013621696119173307,\n",
       "    'acc_norm': 0.7107508532423208,\n",
       "    'acc_norm_stderr': 0.01325001257939344},\n",
       "   'harness|hellaswag|10': {'acc': 0.7070304720175263,\n",
       "    'acc_stderr': 0.004541944342035901,\n",
       "    'acc_norm': 0.8815972913762199,\n",
       "    'acc_norm_stderr': 0.003224240722351317},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.7368421052631579,\n",
       "    'acc_stderr': 0.03583496176361072,\n",
       "    'acc_norm': 0.7368421052631579,\n",
       "    'acc_norm_stderr': 0.03583496176361072},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.02872750295788027,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.02872750295788027},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7638888888888888,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.7638888888888888,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6647398843930635,\n",
       "    'acc_stderr': 0.03599586301247077,\n",
       "    'acc_norm': 0.6647398843930635,\n",
       "    'acc_norm_stderr': 0.03599586301247077},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107223,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107223},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.6297872340425532,\n",
       "    'acc_stderr': 0.03156564682236785,\n",
       "    'acc_norm': 0.6297872340425532,\n",
       "    'acc_norm_stderr': 0.03156564682236785},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.047036043419179864,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.047036043419179864},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.6413793103448275,\n",
       "    'acc_stderr': 0.039966295748767186,\n",
       "    'acc_norm': 0.6413793103448275,\n",
       "    'acc_norm_stderr': 0.039966295748767186},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.47883597883597884,\n",
       "    'acc_stderr': 0.025728230952130726,\n",
       "    'acc_norm': 0.47883597883597884,\n",
       "    'acc_norm_stderr': 0.025728230952130726},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.044444444444444495,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.044444444444444495},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.8032258064516129,\n",
       "    'acc_stderr': 0.022616409420742025,\n",
       "    'acc_norm': 0.8032258064516129,\n",
       "    'acc_norm_stderr': 0.022616409420742025},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.03515895551165698,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.03515895551165698},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.031234752377721175,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.031234752377721175},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8737373737373737,\n",
       "    'acc_stderr': 0.02366435940288023,\n",
       "    'acc_norm': 0.8737373737373737,\n",
       "    'acc_norm_stderr': 0.02366435940288023},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9067357512953368,\n",
       "    'acc_stderr': 0.02098685459328973,\n",
       "    'acc_norm': 0.9067357512953368,\n",
       "    'acc_norm_stderr': 0.02098685459328973},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6615384615384615,\n",
       "    'acc_stderr': 0.023991500500313036,\n",
       "    'acc_norm': 0.6615384615384615,\n",
       "    'acc_norm_stderr': 0.023991500500313036},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3814814814814815,\n",
       "    'acc_stderr': 0.029616718927497593,\n",
       "    'acc_norm': 0.3814814814814815,\n",
       "    'acc_norm_stderr': 0.029616718927497593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7184873949579832,\n",
       "    'acc_stderr': 0.02921354941437217,\n",
       "    'acc_norm': 0.7184873949579832,\n",
       "    'acc_norm_stderr': 0.02921354941437217},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3708609271523179,\n",
       "    'acc_stderr': 0.03943966699183629,\n",
       "    'acc_norm': 0.3708609271523179,\n",
       "    'acc_norm_stderr': 0.03943966699183629},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.03388857118502325,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.03388857118502325},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8480392156862745,\n",
       "    'acc_stderr': 0.0251956584289318,\n",
       "    'acc_norm': 0.8480392156862745,\n",
       "    'acc_norm_stderr': 0.0251956584289318},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8565400843881856,\n",
       "    'acc_stderr': 0.022818291821017012,\n",
       "    'acc_norm': 0.8565400843881856,\n",
       "    'acc_norm_stderr': 0.022818291821017012},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6816143497757847,\n",
       "    'acc_stderr': 0.03126580522513713,\n",
       "    'acc_norm': 0.6816143497757847,\n",
       "    'acc_norm_stderr': 0.03126580522513713},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7480916030534351,\n",
       "    'acc_stderr': 0.03807387116306086,\n",
       "    'acc_norm': 0.7480916030534351,\n",
       "    'acc_norm_stderr': 0.03807387116306086},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7768595041322314,\n",
       "    'acc_stderr': 0.03800754475228733,\n",
       "    'acc_norm': 0.7768595041322314,\n",
       "    'acc_norm_stderr': 0.03800754475228733},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8055555555555556,\n",
       "    'acc_stderr': 0.038260763248848646,\n",
       "    'acc_norm': 0.8055555555555556,\n",
       "    'acc_norm_stderr': 0.038260763248848646},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.754601226993865,\n",
       "    'acc_stderr': 0.03380939813943354,\n",
       "    'acc_norm': 0.754601226993865,\n",
       "    'acc_norm_stderr': 0.03380939813943354},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.44642857142857145,\n",
       "    'acc_stderr': 0.047184714852195886,\n",
       "    'acc_norm': 0.44642857142857145,\n",
       "    'acc_norm_stderr': 0.047184714852195886},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8252427184466019,\n",
       "    'acc_stderr': 0.03760178006026621,\n",
       "    'acc_norm': 0.8252427184466019,\n",
       "    'acc_norm_stderr': 0.03760178006026621},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.02280138253459753,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.02280138253459753},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8033205619412516,\n",
       "    'acc_stderr': 0.014214138556913917,\n",
       "    'acc_norm': 0.8033205619412516,\n",
       "    'acc_norm_stderr': 0.014214138556913917},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7601156069364162,\n",
       "    'acc_stderr': 0.022989592543123567,\n",
       "    'acc_norm': 0.7601156069364162,\n",
       "    'acc_norm_stderr': 0.022989592543123567},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.39329608938547483,\n",
       "    'acc_stderr': 0.016337268694270112,\n",
       "    'acc_norm': 0.39329608938547483,\n",
       "    'acc_norm_stderr': 0.016337268694270112},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7581699346405228,\n",
       "    'acc_stderr': 0.024518195641879334,\n",
       "    'acc_norm': 0.7581699346405228,\n",
       "    'acc_norm_stderr': 0.024518195641879334},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.729903536977492,\n",
       "    'acc_stderr': 0.02521804037341062,\n",
       "    'acc_norm': 0.729903536977492,\n",
       "    'acc_norm_stderr': 0.02521804037341062},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7901234567901234,\n",
       "    'acc_stderr': 0.02265834408598137,\n",
       "    'acc_norm': 0.7901234567901234,\n",
       "    'acc_norm_stderr': 0.02265834408598137},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.49645390070921985,\n",
       "    'acc_stderr': 0.02982674915328092,\n",
       "    'acc_norm': 0.49645390070921985,\n",
       "    'acc_norm_stderr': 0.02982674915328092},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4934810951760104,\n",
       "    'acc_stderr': 0.012769150688867503,\n",
       "    'acc_norm': 0.4934810951760104,\n",
       "    'acc_norm_stderr': 0.012769150688867503},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.7389705882352942,\n",
       "    'acc_stderr': 0.026679252270103135,\n",
       "    'acc_norm': 0.7389705882352942,\n",
       "    'acc_norm_stderr': 0.026679252270103135},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.018690850273595294,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.018690850273595294},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.0282638899437846,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.0282638899437846},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8407960199004975,\n",
       "    'acc_stderr': 0.02587064676616913,\n",
       "    'acc_norm': 0.8407960199004975,\n",
       "    'acc_norm_stderr': 0.02587064676616913},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.9,\n",
       "    'acc_stderr': 0.030151134457776334,\n",
       "    'acc_norm': 0.9,\n",
       "    'acc_norm_stderr': 0.030151134457776334},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5843373493975904,\n",
       "    'acc_stderr': 0.03836722176598052,\n",
       "    'acc_norm': 0.5843373493975904,\n",
       "    'acc_norm_stderr': 0.03836722176598052},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7894736842105263,\n",
       "    'acc_stderr': 0.03126781714663179,\n",
       "    'acc_norm': 0.7894736842105263,\n",
       "    'acc_norm_stderr': 0.03126781714663179},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.5667074663402693,\n",
       "    'mc1_stderr': 0.017347024450107485,\n",
       "    'mc2': 0.7142943510205136,\n",
       "    'mc2_stderr': 0.015024530295000761},\n",
       "   'harness|winogrande|5': {'acc': 0.8358326756116812,\n",
       "    'acc_stderr': 0.01041084977522279},\n",
       "   'harness|gsm8k|5': {'acc': 0.6474601971190296,\n",
       "    'acc_stderr': 0.013159909755930337},\n",
       "   'all': {'acc': 0.6657586984797939,\n",
       "    'acc_stderr': 0.03165995758526614,\n",
       "    'acc_norm': 0.6666511531376961,\n",
       "    'acc_norm_stderr': 0.0323050384069596,\n",
       "    'mc1': 0.5667074663402693,\n",
       "    'mc1_stderr': 0.017347024450107485,\n",
       "    'mc2': 0.7142943510205136,\n",
       "    'mc2_stderr': 0.015024530295000761}}},\n",
       " 'stablelm-tuned-alpha-7b': {'key': 'stablelm-tuned-alpha-7b',\n",
       "  'Model': 'StableLM-Tuned-Alpha-7B',\n",
       "  'MT-bench (score)': '2.75',\n",
       "  'MMLU': '0.244',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'Stability AI',\n",
       "  'Link': 'https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b',\n",
       "  'results': {'harness|drop|3': {'em': 0.0041946308724832215,\n",
       "    'em_stderr': 0.0006618716168266466,\n",
       "    'f1': 0.05621224832214779,\n",
       "    'f1_stderr': 0.0014117433231649174},\n",
       "   'harness|gsm8k|5': {'acc': 0.008339651250947688,\n",
       "    'acc_stderr': 0.002504942226860537},\n",
       "   'harness|winogrande|5': {'acc': 0.5311760063141279,\n",
       "    'acc_stderr': 0.014025142640639513},\n",
       "   'all': {'acc': 0.2479374638777667,\n",
       "    'acc_stderr': 0.03127884661282089,\n",
       "    'acc_norm': 0.2503417754412081,\n",
       "    'acc_norm_stderr': 0.031283748741553784,\n",
       "    'mc1': 0.23990208078335373,\n",
       "    'mc1_stderr': 0.014948812679062137,\n",
       "    'mc2': 0.4037161827493261,\n",
       "    'mc2_stderr': 0.014483731026265276},\n",
       "   'harness|arc:challenge|25': {'acc': 0.3003412969283277,\n",
       "    'acc_stderr': 0.013395909309956997,\n",
       "    'acc_norm': 0.3191126279863481,\n",
       "    'acc_norm_stderr': 0.0136216961191733},\n",
       "   'harness|hellaswag|10': {'acc': 0.41286596295558653,\n",
       "    'acc_stderr': 0.004913429010559072,\n",
       "    'acc_norm': 0.5359490141406095,\n",
       "    'acc_norm_stderr': 0.004976867796583554},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.26666666666666666,\n",
       "    'acc_stderr': 0.03820169914517904,\n",
       "    'acc_norm': 0.26666666666666666,\n",
       "    'acc_norm_stderr': 0.03820169914517904},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.17105263157894737,\n",
       "    'acc_stderr': 0.030643607071677088,\n",
       "    'acc_norm': 0.17105263157894737,\n",
       "    'acc_norm_stderr': 0.030643607071677088},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.24,\n",
       "    'acc_stderr': 0.04292346959909281,\n",
       "    'acc_norm': 0.24,\n",
       "    'acc_norm_stderr': 0.04292346959909281},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.27547169811320754,\n",
       "    'acc_stderr': 0.027495663683724064,\n",
       "    'acc_norm': 0.27547169811320754,\n",
       "    'acc_norm_stderr': 0.027495663683724064},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2361111111111111,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.2361111111111111,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.041633319989322695,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.041633319989322695},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.23121387283236994,\n",
       "    'acc_stderr': 0.0321473730202947,\n",
       "    'acc_norm': 0.23121387283236994,\n",
       "    'acc_norm_stderr': 0.0321473730202947},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2549019607843137,\n",
       "    'acc_stderr': 0.04336432707993176,\n",
       "    'acc_norm': 0.2549019607843137,\n",
       "    'acc_norm_stderr': 0.04336432707993176},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.3148936170212766,\n",
       "    'acc_stderr': 0.03036358219723816,\n",
       "    'acc_norm': 0.3148936170212766,\n",
       "    'acc_norm_stderr': 0.03036358219723816},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.21052631578947367,\n",
       "    'acc_stderr': 0.038351539543994194,\n",
       "    'acc_norm': 0.21052631578947367,\n",
       "    'acc_norm_stderr': 0.038351539543994194},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.27586206896551724,\n",
       "    'acc_stderr': 0.037245636197746325,\n",
       "    'acc_norm': 0.27586206896551724,\n",
       "    'acc_norm_stderr': 0.037245636197746325},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.24338624338624337,\n",
       "    'acc_stderr': 0.022101128787415426,\n",
       "    'acc_norm': 0.24338624338624337,\n",
       "    'acc_norm_stderr': 0.022101128787415426},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.24603174603174602,\n",
       "    'acc_stderr': 0.03852273364924316,\n",
       "    'acc_norm': 0.24603174603174602,\n",
       "    'acc_norm_stderr': 0.03852273364924316},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.19,\n",
       "    'acc_stderr': 0.03942772444036624,\n",
       "    'acc_norm': 0.19,\n",
       "    'acc_norm_stderr': 0.03942772444036624},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.2129032258064516,\n",
       "    'acc_stderr': 0.023287665127268525,\n",
       "    'acc_norm': 0.2129032258064516,\n",
       "    'acc_norm_stderr': 0.023287665127268525},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.18226600985221675,\n",
       "    'acc_stderr': 0.02716334085964515,\n",
       "    'acc_norm': 0.18226600985221675,\n",
       "    'acc_norm_stderr': 0.02716334085964515},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.04163331998932269,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.04163331998932269},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.22424242424242424,\n",
       "    'acc_stderr': 0.03256866661681102,\n",
       "    'acc_norm': 0.22424242424242424,\n",
       "    'acc_norm_stderr': 0.03256866661681102},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.18686868686868688,\n",
       "    'acc_stderr': 0.027772533334218977,\n",
       "    'acc_norm': 0.18686868686868688,\n",
       "    'acc_norm_stderr': 0.027772533334218977},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.21243523316062177,\n",
       "    'acc_stderr': 0.029519282616817254,\n",
       "    'acc_norm': 0.21243523316062177,\n",
       "    'acc_norm_stderr': 0.029519282616817254},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.2743589743589744,\n",
       "    'acc_stderr': 0.022622765767493218,\n",
       "    'acc_norm': 0.2743589743589744,\n",
       "    'acc_norm_stderr': 0.022622765767493218},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.25925925925925924,\n",
       "    'acc_stderr': 0.026719240783712163,\n",
       "    'acc_norm': 0.25925925925925924,\n",
       "    'acc_norm_stderr': 0.026719240783712163},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.19747899159663865,\n",
       "    'acc_stderr': 0.02585916412205145,\n",
       "    'acc_norm': 0.19747899159663865,\n",
       "    'acc_norm_stderr': 0.02585916412205145},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2185430463576159,\n",
       "    'acc_stderr': 0.03374235550425694,\n",
       "    'acc_norm': 0.2185430463576159,\n",
       "    'acc_norm_stderr': 0.03374235550425694},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.22935779816513763,\n",
       "    'acc_stderr': 0.018025349724618684,\n",
       "    'acc_norm': 0.22935779816513763,\n",
       "    'acc_norm_stderr': 0.018025349724618684},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.35185185185185186,\n",
       "    'acc_stderr': 0.03256850570293647,\n",
       "    'acc_norm': 0.35185185185185186,\n",
       "    'acc_norm_stderr': 0.03256850570293647},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.24019607843137256,\n",
       "    'acc_stderr': 0.02998373305591361,\n",
       "    'acc_norm': 0.24019607843137256,\n",
       "    'acc_norm_stderr': 0.02998373305591361},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.2869198312236287,\n",
       "    'acc_stderr': 0.02944377302259469,\n",
       "    'acc_norm': 0.2869198312236287,\n",
       "    'acc_norm_stderr': 0.02944377302259469},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.3542600896860987,\n",
       "    'acc_stderr': 0.03210062154134987,\n",
       "    'acc_norm': 0.3542600896860987,\n",
       "    'acc_norm_stderr': 0.03210062154134987},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.2900763358778626,\n",
       "    'acc_stderr': 0.03980066246467765,\n",
       "    'acc_norm': 0.2900763358778626,\n",
       "    'acc_norm_stderr': 0.03980066246467765},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.2396694214876033,\n",
       "    'acc_stderr': 0.03896878985070417,\n",
       "    'acc_norm': 0.2396694214876033,\n",
       "    'acc_norm_stderr': 0.03896878985070417},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.28703703703703703,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.28703703703703703,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.27607361963190186,\n",
       "    'acc_stderr': 0.0351238528370505,\n",
       "    'acc_norm': 0.27607361963190186,\n",
       "    'acc_norm_stderr': 0.0351238528370505},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.24107142857142858,\n",
       "    'acc_stderr': 0.04059867246952687,\n",
       "    'acc_norm': 0.24107142857142858,\n",
       "    'acc_norm_stderr': 0.04059867246952687},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.1650485436893204,\n",
       "    'acc_stderr': 0.036756688322331886,\n",
       "    'acc_norm': 0.1650485436893204,\n",
       "    'acc_norm_stderr': 0.036756688322331886},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.24786324786324787,\n",
       "    'acc_stderr': 0.028286324075564397,\n",
       "    'acc_norm': 0.24786324786324787,\n",
       "    'acc_norm_stderr': 0.028286324075564397},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.23499361430395913,\n",
       "    'acc_stderr': 0.015162024152278434,\n",
       "    'acc_norm': 0.23499361430395913,\n",
       "    'acc_norm_stderr': 0.015162024152278434},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.2254335260115607,\n",
       "    'acc_stderr': 0.02249723019096756,\n",
       "    'acc_norm': 0.2254335260115607,\n",
       "    'acc_norm_stderr': 0.02249723019096756},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961459,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961459},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.25163398692810457,\n",
       "    'acc_stderr': 0.024848018263875195,\n",
       "    'acc_norm': 0.25163398692810457,\n",
       "    'acc_norm_stderr': 0.024848018263875195},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.21864951768488747,\n",
       "    'acc_stderr': 0.023475581417861106,\n",
       "    'acc_norm': 0.21864951768488747,\n",
       "    'acc_norm_stderr': 0.023475581417861106},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.2222222222222222,\n",
       "    'acc_stderr': 0.023132376234543325,\n",
       "    'acc_norm': 0.2222222222222222,\n",
       "    'acc_norm_stderr': 0.023132376234543325},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.24822695035460993,\n",
       "    'acc_stderr': 0.025770015644290403,\n",
       "    'acc_norm': 0.24822695035460993,\n",
       "    'acc_norm_stderr': 0.025770015644290403},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.22946544980443284,\n",
       "    'acc_stderr': 0.010739489382279503,\n",
       "    'acc_norm': 0.22946544980443284,\n",
       "    'acc_norm_stderr': 0.010739489382279503},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.16544117647058823,\n",
       "    'acc_stderr': 0.022571771025494767,\n",
       "    'acc_norm': 0.16544117647058823,\n",
       "    'acc_norm_stderr': 0.022571771025494767},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.25163398692810457,\n",
       "    'acc_stderr': 0.017555818091322263,\n",
       "    'acc_norm': 0.25163398692810457,\n",
       "    'acc_norm_stderr': 0.017555818091322263},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.23636363636363636,\n",
       "    'acc_stderr': 0.04069306319721378,\n",
       "    'acc_norm': 0.23636363636363636,\n",
       "    'acc_norm_stderr': 0.04069306319721378},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.1673469387755102,\n",
       "    'acc_stderr': 0.023897144768914524,\n",
       "    'acc_norm': 0.1673469387755102,\n",
       "    'acc_norm_stderr': 0.023897144768914524},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.263681592039801,\n",
       "    'acc_stderr': 0.03115715086935558,\n",
       "    'acc_norm': 0.263681592039801,\n",
       "    'acc_norm_stderr': 0.03115715086935558},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.24,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.24,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.2891566265060241,\n",
       "    'acc_stderr': 0.03529486801511115,\n",
       "    'acc_norm': 0.2891566265060241,\n",
       "    'acc_norm_stderr': 0.03529486801511115},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.24561403508771928,\n",
       "    'acc_stderr': 0.03301405946987249,\n",
       "    'acc_norm': 0.24561403508771928,\n",
       "    'acc_norm_stderr': 0.03301405946987249},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.23990208078335373,\n",
       "    'mc1_stderr': 0.014948812679062137,\n",
       "    'mc2': 0.4037161827493261,\n",
       "    'mc2_stderr': 0.014483731026265276}}},\n",
       " 'starling-lm-7b-alpha': {'key': 'starling-lm-7b-alpha',\n",
       "  'Model': 'Starling-LM-7B-alpha',\n",
       "  'MT-bench (score)': '8.09',\n",
       "  'MMLU': '0.639',\n",
       "  'License': 'CC-BY-NC-4.0',\n",
       "  'Organization': 'UC Berkeley',\n",
       "  'Link': 'https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5998293515358362,\n",
       "    'acc_stderr': 0.014317197787809172,\n",
       "    'acc_norm': 0.6382252559726962,\n",
       "    'acc_norm_stderr': 0.01404195794503808},\n",
       "   'harness|hellaswag|10': {'acc': 0.665803624775941,\n",
       "    'acc_stderr': 0.004707447244200621,\n",
       "    'acc_norm': 0.8490340569607648,\n",
       "    'acc_norm_stderr': 0.0035728399695219874},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6907894736842105,\n",
       "    'acc_stderr': 0.037610708698674805,\n",
       "    'acc_norm': 0.6907894736842105,\n",
       "    'acc_norm_stderr': 0.037610708698674805},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.63,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.63,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.028544793319055326,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.028544793319055326},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7708333333333334,\n",
       "    'acc_stderr': 0.035146974678623884,\n",
       "    'acc_norm': 0.7708333333333334,\n",
       "    'acc_norm_stderr': 0.035146974678623884},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6589595375722543,\n",
       "    'acc_stderr': 0.03614665424180826,\n",
       "    'acc_norm': 0.6589595375722543,\n",
       "    'acc_norm_stderr': 0.03614665424180826},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4411764705882353,\n",
       "    'acc_stderr': 0.049406356306056595,\n",
       "    'acc_norm': 0.4411764705882353,\n",
       "    'acc_norm_stderr': 0.049406356306056595},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768079,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768079},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5531914893617021,\n",
       "    'acc_stderr': 0.032500536843658404,\n",
       "    'acc_norm': 0.5531914893617021,\n",
       "    'acc_norm_stderr': 0.032500536843658404},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.49122807017543857,\n",
       "    'acc_stderr': 0.04702880432049615,\n",
       "    'acc_norm': 0.49122807017543857,\n",
       "    'acc_norm_stderr': 0.04702880432049615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370332,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370332},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.41005291005291006,\n",
       "    'acc_stderr': 0.02533120243894444,\n",
       "    'acc_norm': 0.41005291005291006,\n",
       "    'acc_norm_stderr': 0.02533120243894444},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5238095238095238,\n",
       "    'acc_stderr': 0.04467062628403273,\n",
       "    'acc_norm': 0.5238095238095238,\n",
       "    'acc_norm_stderr': 0.04467062628403273},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.04163331998932269,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.04163331998932269},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7903225806451613,\n",
       "    'acc_stderr': 0.023157879349083525,\n",
       "    'acc_norm': 0.7903225806451613,\n",
       "    'acc_norm_stderr': 0.023157879349083525},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.46798029556650245,\n",
       "    'acc_stderr': 0.035107665979592154,\n",
       "    'acc_norm': 0.46798029556650245,\n",
       "    'acc_norm_stderr': 0.035107665979592154},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7636363636363637,\n",
       "    'acc_stderr': 0.03317505930009181,\n",
       "    'acc_norm': 0.7636363636363637,\n",
       "    'acc_norm_stderr': 0.03317505930009181},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.797979797979798,\n",
       "    'acc_stderr': 0.028606204289229865,\n",
       "    'acc_norm': 0.797979797979798,\n",
       "    'acc_norm_stderr': 0.028606204289229865},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9119170984455959,\n",
       "    'acc_stderr': 0.02045374660160103,\n",
       "    'acc_norm': 0.9119170984455959,\n",
       "    'acc_norm_stderr': 0.02045374660160103},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.676923076923077,\n",
       "    'acc_stderr': 0.02371088850197057,\n",
       "    'acc_norm': 0.676923076923077,\n",
       "    'acc_norm_stderr': 0.02371088850197057},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028593,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.680672268907563,\n",
       "    'acc_stderr': 0.0302839955258844,\n",
       "    'acc_norm': 0.680672268907563,\n",
       "    'acc_norm_stderr': 0.0302839955258844},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.37748344370860926,\n",
       "    'acc_stderr': 0.03958027231121569,\n",
       "    'acc_norm': 0.37748344370860926,\n",
       "    'acc_norm_stderr': 0.03958027231121569},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.034099716973523674,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.034099716973523674},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8235294117647058,\n",
       "    'acc_stderr': 0.026756401538078966,\n",
       "    'acc_norm': 0.8235294117647058,\n",
       "    'acc_norm_stderr': 0.026756401538078966},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8270042194092827,\n",
       "    'acc_stderr': 0.024621562866768434,\n",
       "    'acc_norm': 0.8270042194092827,\n",
       "    'acc_norm_stderr': 0.024621562866768434},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7130044843049327,\n",
       "    'acc_stderr': 0.030360379710291947,\n",
       "    'acc_norm': 0.7130044843049327,\n",
       "    'acc_norm_stderr': 0.030360379710291947},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7862595419847328,\n",
       "    'acc_stderr': 0.0359546161177469,\n",
       "    'acc_norm': 0.7862595419847328,\n",
       "    'acc_norm_stderr': 0.0359546161177469},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8264462809917356,\n",
       "    'acc_stderr': 0.0345727283691767,\n",
       "    'acc_norm': 0.8264462809917356,\n",
       "    'acc_norm_stderr': 0.0345727283691767},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7407407407407407,\n",
       "    'acc_stderr': 0.042365112580946336,\n",
       "    'acc_norm': 0.7407407407407407,\n",
       "    'acc_norm_stderr': 0.042365112580946336},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7730061349693251,\n",
       "    'acc_stderr': 0.03291099578615769,\n",
       "    'acc_norm': 0.7730061349693251,\n",
       "    'acc_norm_stderr': 0.03291099578615769},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.41964285714285715,\n",
       "    'acc_stderr': 0.04684099321077106,\n",
       "    'acc_norm': 0.41964285714285715,\n",
       "    'acc_norm_stderr': 0.04684099321077106},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8446601941747572,\n",
       "    'acc_stderr': 0.03586594738573973,\n",
       "    'acc_norm': 0.8446601941747572,\n",
       "    'acc_norm_stderr': 0.03586594738573973},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8760683760683761,\n",
       "    'acc_stderr': 0.02158649400128138,\n",
       "    'acc_norm': 0.8760683760683761,\n",
       "    'acc_norm_stderr': 0.02158649400128138},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.0446196043338474,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.0446196043338474},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8263090676883781,\n",
       "    'acc_stderr': 0.01354741565866226,\n",
       "    'acc_norm': 0.8263090676883781,\n",
       "    'acc_norm_stderr': 0.01354741565866226},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7369942196531792,\n",
       "    'acc_stderr': 0.023703099525258176,\n",
       "    'acc_norm': 0.7369942196531792,\n",
       "    'acc_norm_stderr': 0.023703099525258176},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.47039106145251397,\n",
       "    'acc_stderr': 0.016693154927383557,\n",
       "    'acc_norm': 0.47039106145251397,\n",
       "    'acc_norm_stderr': 0.016693154927383557},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7352941176470589,\n",
       "    'acc_stderr': 0.025261691219729484,\n",
       "    'acc_norm': 0.7352941176470589,\n",
       "    'acc_norm_stderr': 0.025261691219729484},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6977491961414791,\n",
       "    'acc_stderr': 0.02608270069539966,\n",
       "    'acc_norm': 0.6977491961414791,\n",
       "    'acc_norm_stderr': 0.02608270069539966},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7345679012345679,\n",
       "    'acc_stderr': 0.024569223600460845,\n",
       "    'acc_norm': 0.7345679012345679,\n",
       "    'acc_norm_stderr': 0.024569223600460845},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4645390070921986,\n",
       "    'acc_stderr': 0.029752389657427047,\n",
       "    'acc_norm': 0.4645390070921986,\n",
       "    'acc_norm_stderr': 0.029752389657427047},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4641460234680574,\n",
       "    'acc_stderr': 0.012737361318730581,\n",
       "    'acc_norm': 0.4641460234680574,\n",
       "    'acc_norm_stderr': 0.012737361318730581},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.028064998167040094,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.028064998167040094},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6633986928104575,\n",
       "    'acc_stderr': 0.019117213911495144,\n",
       "    'acc_norm': 0.6633986928104575,\n",
       "    'acc_norm_stderr': 0.019117213911495144},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.726530612244898,\n",
       "    'acc_stderr': 0.028535560337128448,\n",
       "    'acc_norm': 0.726530612244898,\n",
       "    'acc_norm_stderr': 0.028535560337128448},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.845771144278607,\n",
       "    'acc_stderr': 0.025538433368578334,\n",
       "    'acc_norm': 0.845771144278607,\n",
       "    'acc_norm_stderr': 0.025538433368578334},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.88,\n",
       "    'acc_stderr': 0.032659863237109066,\n",
       "    'acc_norm': 0.88,\n",
       "    'acc_norm_stderr': 0.032659863237109066},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5301204819277109,\n",
       "    'acc_stderr': 0.03885425420866767,\n",
       "    'acc_norm': 0.5301204819277109,\n",
       "    'acc_norm_stderr': 0.03885425420866767},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.847953216374269,\n",
       "    'acc_stderr': 0.027539122889061456,\n",
       "    'acc_norm': 0.847953216374269,\n",
       "    'acc_norm_stderr': 0.027539122889061456},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3047735618115055,\n",
       "    'mc1_stderr': 0.01611412415688245,\n",
       "    'mc2': 0.463936332301049,\n",
       "    'mc2_stderr': 0.015153266555511496},\n",
       "   'harness|winogrande|5': {'acc': 0.8058405682715075,\n",
       "    'acc_stderr': 0.01111698339239267},\n",
       "   'harness|drop|3': {'em': 0.0012583892617449664,\n",
       "    'em_stderr': 0.0003630560893118993,\n",
       "    'f1': 0.07203124999999957,\n",
       "    'f1_stderr': 0.0014672437300568956},\n",
       "   'harness|gsm8k|5': {'acc': 0.623199393479909,\n",
       "    'acc_stderr': 0.013347858757829158},\n",
       "   'all': {'acc': 0.623199393479909, 'acc_stderr': 0.013347858757829158}}},\n",
       " 'vicuna-13b': {'key': 'vicuna-13b',\n",
       "  'Model': 'Vicuna-13B',\n",
       "  'MT-bench (score)': '6.57',\n",
       "  'MMLU': '0.558',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-13b-v1.5',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5349829351535836,\n",
       "    'acc_stderr': 0.014575583922019672,\n",
       "    'acc_norm': 0.5656996587030717,\n",
       "    'acc_norm_stderr': 0.014484703048857357},\n",
       "   'harness|hellaswag|10': {'acc': 0.6115315674168492,\n",
       "    'acc_stderr': 0.004864058877626272,\n",
       "    'acc_norm': 0.8108942441744672,\n",
       "    'acc_norm_stderr': 0.0039079230108406094},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542129,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542129},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4888888888888889,\n",
       "    'acc_stderr': 0.04318275491977976,\n",
       "    'acc_norm': 0.4888888888888889,\n",
       "    'acc_norm_stderr': 0.04318275491977976},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5855263157894737,\n",
       "    'acc_stderr': 0.04008973785779205,\n",
       "    'acc_norm': 0.5855263157894737,\n",
       "    'acc_norm_stderr': 0.04008973785779205},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.630188679245283,\n",
       "    'acc_stderr': 0.02971142188010793,\n",
       "    'acc_norm': 0.630188679245283,\n",
       "    'acc_norm_stderr': 0.02971142188010793},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5972222222222222,\n",
       "    'acc_stderr': 0.04101405519842426,\n",
       "    'acc_norm': 0.5972222222222222,\n",
       "    'acc_norm_stderr': 0.04101405519842426},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5433526011560693,\n",
       "    'acc_stderr': 0.03798106566014498,\n",
       "    'acc_norm': 0.5433526011560693,\n",
       "    'acc_norm_stderr': 0.03798106566014498},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3235294117647059,\n",
       "    'acc_stderr': 0.046550104113196156,\n",
       "    'acc_norm': 0.3235294117647059,\n",
       "    'acc_norm_stderr': 0.046550104113196156},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.451063829787234,\n",
       "    'acc_stderr': 0.032529096196131965,\n",
       "    'acc_norm': 0.451063829787234,\n",
       "    'acc_norm_stderr': 0.032529096196131965},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2894736842105263,\n",
       "    'acc_stderr': 0.04266339443159394,\n",
       "    'acc_norm': 0.2894736842105263,\n",
       "    'acc_norm_stderr': 0.04266339443159394},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.47586206896551725,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.47586206896551725,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3306878306878307,\n",
       "    'acc_stderr': 0.024229965298425075,\n",
       "    'acc_norm': 0.3306878306878307,\n",
       "    'acc_norm_stderr': 0.024229965298425075},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.04240799327574924,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.04240799327574924},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001974,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001974},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6774193548387096,\n",
       "    'acc_stderr': 0.02659308451657227,\n",
       "    'acc_norm': 0.6774193548387096,\n",
       "    'acc_norm_stderr': 0.02659308451657227},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4482758620689655,\n",
       "    'acc_stderr': 0.034991131376767445,\n",
       "    'acc_norm': 0.4482758620689655,\n",
       "    'acc_norm_stderr': 0.034991131376767445},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.696969696969697,\n",
       "    'acc_stderr': 0.03588624800091707,\n",
       "    'acc_norm': 0.696969696969697,\n",
       "    'acc_norm_stderr': 0.03588624800091707},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7070707070707071,\n",
       "    'acc_stderr': 0.03242497958178815,\n",
       "    'acc_norm': 0.7070707070707071,\n",
       "    'acc_norm_stderr': 0.03242497958178815},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8134715025906736,\n",
       "    'acc_stderr': 0.028112091210117467,\n",
       "    'acc_norm': 0.8134715025906736,\n",
       "    'acc_norm_stderr': 0.028112091210117467},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5487179487179488,\n",
       "    'acc_stderr': 0.025230381238934833,\n",
       "    'acc_norm': 0.5487179487179488,\n",
       "    'acc_norm_stderr': 0.025230381238934833},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.0284934650910286,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.0284934650910286},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5798319327731093,\n",
       "    'acc_stderr': 0.03206183783236153,\n",
       "    'acc_norm': 0.5798319327731093,\n",
       "    'acc_norm_stderr': 0.03206183783236153},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.33112582781456956,\n",
       "    'acc_stderr': 0.038425817186598696,\n",
       "    'acc_norm': 0.33112582781456956,\n",
       "    'acc_norm_stderr': 0.038425817186598696},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7541284403669725,\n",
       "    'acc_stderr': 0.01846194096870842,\n",
       "    'acc_norm': 0.7541284403669725,\n",
       "    'acc_norm_stderr': 0.01846194096870842},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4583333333333333,\n",
       "    'acc_stderr': 0.03398110890294636,\n",
       "    'acc_norm': 0.4583333333333333,\n",
       "    'acc_norm_stderr': 0.03398110890294636},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7450980392156863,\n",
       "    'acc_stderr': 0.03058759135160426,\n",
       "    'acc_norm': 0.7450980392156863,\n",
       "    'acc_norm_stderr': 0.03058759135160426},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7637130801687764,\n",
       "    'acc_stderr': 0.027652153144159256,\n",
       "    'acc_norm': 0.7637130801687764,\n",
       "    'acc_norm_stderr': 0.027652153144159256},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6502242152466368,\n",
       "    'acc_stderr': 0.03200736719484503,\n",
       "    'acc_norm': 0.6502242152466368,\n",
       "    'acc_norm_stderr': 0.03200736719484503},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6412213740458015,\n",
       "    'acc_stderr': 0.04206739313864908,\n",
       "    'acc_norm': 0.6412213740458015,\n",
       "    'acc_norm_stderr': 0.04206739313864908},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7768595041322314,\n",
       "    'acc_stderr': 0.03800754475228732,\n",
       "    'acc_norm': 0.7768595041322314,\n",
       "    'acc_norm_stderr': 0.03800754475228732},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.043300437496507416,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.043300437496507416},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6932515337423313,\n",
       "    'acc_stderr': 0.03623089915724147,\n",
       "    'acc_norm': 0.6932515337423313,\n",
       "    'acc_norm_stderr': 0.03623089915724147},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.42857142857142855,\n",
       "    'acc_stderr': 0.04697113923010212,\n",
       "    'acc_norm': 0.42857142857142855,\n",
       "    'acc_norm_stderr': 0.04697113923010212},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.043546310772605956,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.043546310772605956},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8461538461538461,\n",
       "    'acc_stderr': 0.023636873317489288,\n",
       "    'acc_norm': 0.8461538461538461,\n",
       "    'acc_norm_stderr': 0.023636873317489288},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.58,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.58,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.756066411238825,\n",
       "    'acc_stderr': 0.015357212665829461,\n",
       "    'acc_norm': 0.756066411238825,\n",
       "    'acc_norm_stderr': 0.015357212665829461},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6069364161849711,\n",
       "    'acc_stderr': 0.026296227915613663,\n",
       "    'acc_norm': 0.6069364161849711,\n",
       "    'acc_norm_stderr': 0.026296227915613663},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2782122905027933,\n",
       "    'acc_stderr': 0.01498732543996355,\n",
       "    'acc_norm': 0.2782122905027933,\n",
       "    'acc_norm_stderr': 0.01498732543996355},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6372549019607843,\n",
       "    'acc_stderr': 0.027530078447110307,\n",
       "    'acc_norm': 0.6372549019607843,\n",
       "    'acc_norm_stderr': 0.027530078447110307},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6366559485530546,\n",
       "    'acc_stderr': 0.027316847674192717,\n",
       "    'acc_norm': 0.6366559485530546,\n",
       "    'acc_norm_stderr': 0.027316847674192717},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6388888888888888,\n",
       "    'acc_stderr': 0.026725868809100793,\n",
       "    'acc_norm': 0.6388888888888888,\n",
       "    'acc_norm_stderr': 0.026725868809100793},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.41134751773049644,\n",
       "    'acc_stderr': 0.02935491115994098,\n",
       "    'acc_norm': 0.41134751773049644,\n",
       "    'acc_norm_stderr': 0.02935491115994098},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.43285528031290743,\n",
       "    'acc_stderr': 0.012654565234622862,\n",
       "    'acc_norm': 0.43285528031290743,\n",
       "    'acc_norm_stderr': 0.012654565234622862},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5367647058823529,\n",
       "    'acc_stderr': 0.03029061918048569,\n",
       "    'acc_norm': 0.5367647058823529,\n",
       "    'acc_norm_stderr': 0.03029061918048569},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.019944914136873583,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.019944914136873583},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6122448979591837,\n",
       "    'acc_stderr': 0.031192230726795656,\n",
       "    'acc_norm': 0.6122448979591837,\n",
       "    'acc_norm_stderr': 0.031192230726795656},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7860696517412935,\n",
       "    'acc_stderr': 0.02899690969332891,\n",
       "    'acc_norm': 0.7860696517412935,\n",
       "    'acc_norm_stderr': 0.02899690969332891},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.82,\n",
       "    'acc_stderr': 0.038612291966536934,\n",
       "    'acc_norm': 0.82,\n",
       "    'acc_norm_stderr': 0.038612291966536934},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.45180722891566266,\n",
       "    'acc_stderr': 0.03874371556587953,\n",
       "    'acc_norm': 0.45180722891566266,\n",
       "    'acc_norm_stderr': 0.03874371556587953},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.783625730994152,\n",
       "    'acc_stderr': 0.03158149539338734,\n",
       "    'acc_norm': 0.783625730994152,\n",
       "    'acc_norm_stderr': 0.03158149539338734},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.34516523867809057,\n",
       "    'mc1_stderr': 0.01664310331927494,\n",
       "    'mc2': 0.5107449529759277,\n",
       "    'mc2_stderr': 0.015464470932268133},\n",
       "   'all': {'em': 0.21403104026845637,\n",
       "    'em_stderr': 0.004200304057589016,\n",
       "    'f1': 0.2773447986577177,\n",
       "    'f1_stderr': 0.004194161726605588,\n",
       "    'acc': 0.4298049932592257,\n",
       "    'acc_stderr': 0.010471546731533343},\n",
       "   'harness|drop|3': {'em': 0.21403104026845637,\n",
       "    'em_stderr': 0.004200304057589016,\n",
       "    'f1': 0.2773447986577177,\n",
       "    'f1_stderr': 0.004194161726605588},\n",
       "   'harness|gsm8k|5': {'acc': 0.11296436694465505,\n",
       "    'acc_stderr': 0.008719339028833057},\n",
       "   'harness|winogrande|5': {'acc': 0.7466456195737964,\n",
       "    'acc_stderr': 0.01222375443423363}}},\n",
       " 'vicuna-33b': {'key': 'vicuna-33b',\n",
       "  'Model': 'Vicuna-33B',\n",
       "  'MT-bench (score)': '7.12',\n",
       "  'MMLU': '0.592',\n",
       "  'License': 'Non-commercial',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-33b-v1.3',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5921501706484642,\n",
       "    'acc_stderr': 0.014361097288449707,\n",
       "    'acc_norm': 0.6194539249146758,\n",
       "    'acc_norm_stderr': 0.014188277712349807},\n",
       "   'harness|hellaswag|10': {'acc': 0.6249751045608445,\n",
       "    'acc_stderr': 0.00483139921850023,\n",
       "    'acc_norm': 0.82194781915953,\n",
       "    'acc_norm_stderr': 0.003817748269107777},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.27,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.27,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5333333333333333,\n",
       "    'acc_stderr': 0.043097329010363554,\n",
       "    'acc_norm': 0.5333333333333333,\n",
       "    'acc_norm_stderr': 0.043097329010363554},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.625,\n",
       "    'acc_stderr': 0.039397364351956274,\n",
       "    'acc_norm': 0.625,\n",
       "    'acc_norm_stderr': 0.039397364351956274},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6037735849056604,\n",
       "    'acc_stderr': 0.030102793781791197,\n",
       "    'acc_norm': 0.6037735849056604,\n",
       "    'acc_norm_stderr': 0.030102793781791197},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.6319444444444444,\n",
       "    'acc_stderr': 0.040329990539607195,\n",
       "    'acc_norm': 0.6319444444444444,\n",
       "    'acc_norm_stderr': 0.040329990539607195},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.49,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.49,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.04725815626252604,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.04725815626252604},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.27450980392156865,\n",
       "    'acc_stderr': 0.04440521906179326,\n",
       "    'acc_norm': 0.27450980392156865,\n",
       "    'acc_norm_stderr': 0.04440521906179326},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.46808510638297873,\n",
       "    'acc_stderr': 0.03261936918467382,\n",
       "    'acc_norm': 0.46808510638297873,\n",
       "    'acc_norm_stderr': 0.03261936918467382},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.34210526315789475,\n",
       "    'acc_stderr': 0.04462917535336936,\n",
       "    'acc_norm': 0.34210526315789475,\n",
       "    'acc_norm_stderr': 0.04462917535336936},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.04164188720169375,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.04164188720169375},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36507936507936506,\n",
       "    'acc_stderr': 0.024796060602699958,\n",
       "    'acc_norm': 0.36507936507936506,\n",
       "    'acc_norm_stderr': 0.024796060602699958},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.04240799327574924,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.04240799327574924},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6870967741935484,\n",
       "    'acc_stderr': 0.02637756702864586,\n",
       "    'acc_norm': 0.6870967741935484,\n",
       "    'acc_norm_stderr': 0.02637756702864586},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.39901477832512317,\n",
       "    'acc_stderr': 0.03445487686264716,\n",
       "    'acc_norm': 0.39901477832512317,\n",
       "    'acc_norm_stderr': 0.03445487686264716},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7151515151515152,\n",
       "    'acc_stderr': 0.03524390844511781,\n",
       "    'acc_norm': 0.7151515151515152,\n",
       "    'acc_norm_stderr': 0.03524390844511781},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7727272727272727,\n",
       "    'acc_stderr': 0.02985751567338642,\n",
       "    'acc_norm': 0.7727272727272727,\n",
       "    'acc_norm_stderr': 0.02985751567338642},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8393782383419689,\n",
       "    'acc_stderr': 0.02649905770139744,\n",
       "    'acc_norm': 0.8393782383419689,\n",
       "    'acc_norm_stderr': 0.02649905770139744},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5871794871794872,\n",
       "    'acc_stderr': 0.024962683564331806,\n",
       "    'acc_norm': 0.5871794871794872,\n",
       "    'acc_norm_stderr': 0.024962683564331806},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2740740740740741,\n",
       "    'acc_stderr': 0.027195934804085626,\n",
       "    'acc_norm': 0.2740740740740741,\n",
       "    'acc_norm_stderr': 0.027195934804085626},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5840336134453782,\n",
       "    'acc_stderr': 0.03201650100739611,\n",
       "    'acc_norm': 0.5840336134453782,\n",
       "    'acc_norm_stderr': 0.03201650100739611},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.40397350993377484,\n",
       "    'acc_stderr': 0.04006485685365342,\n",
       "    'acc_norm': 0.40397350993377484,\n",
       "    'acc_norm_stderr': 0.04006485685365342},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7761467889908257,\n",
       "    'acc_stderr': 0.017871217767790232,\n",
       "    'acc_norm': 0.7761467889908257,\n",
       "    'acc_norm_stderr': 0.017871217767790232},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4027777777777778,\n",
       "    'acc_stderr': 0.033448873829978666,\n",
       "    'acc_norm': 0.4027777777777778,\n",
       "    'acc_norm_stderr': 0.033448873829978666},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7745098039215687,\n",
       "    'acc_stderr': 0.029331162294251735,\n",
       "    'acc_norm': 0.7745098039215687,\n",
       "    'acc_norm_stderr': 0.029331162294251735},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8143459915611815,\n",
       "    'acc_stderr': 0.025310495376944863,\n",
       "    'acc_norm': 0.8143459915611815,\n",
       "    'acc_norm_stderr': 0.025310495376944863},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6771300448430493,\n",
       "    'acc_stderr': 0.03138147637575498,\n",
       "    'acc_norm': 0.6771300448430493,\n",
       "    'acc_norm_stderr': 0.03138147637575498},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6946564885496184,\n",
       "    'acc_stderr': 0.040393149787245626,\n",
       "    'acc_norm': 0.6946564885496184,\n",
       "    'acc_norm_stderr': 0.040393149787245626},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7520661157024794,\n",
       "    'acc_stderr': 0.03941897526516303,\n",
       "    'acc_norm': 0.7520661157024794,\n",
       "    'acc_norm_stderr': 0.03941897526516303},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7129629629629629,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.7129629629629629,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6993865030674846,\n",
       "    'acc_stderr': 0.03602511318806771,\n",
       "    'acc_norm': 0.6993865030674846,\n",
       "    'acc_norm_stderr': 0.03602511318806771},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.45535714285714285,\n",
       "    'acc_stderr': 0.04726835553719099,\n",
       "    'acc_norm': 0.45535714285714285,\n",
       "    'acc_norm_stderr': 0.04726835553719099},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7184466019417476,\n",
       "    'acc_stderr': 0.044532548363264673,\n",
       "    'acc_norm': 0.7184466019417476,\n",
       "    'acc_norm_stderr': 0.044532548363264673},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8803418803418803,\n",
       "    'acc_stderr': 0.021262719400406978,\n",
       "    'acc_norm': 0.8803418803418803,\n",
       "    'acc_norm_stderr': 0.021262719400406978},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7662835249042146,\n",
       "    'acc_stderr': 0.015133383278988829,\n",
       "    'acc_norm': 0.7662835249042146,\n",
       "    'acc_norm_stderr': 0.015133383278988829},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6705202312138728,\n",
       "    'acc_stderr': 0.0253052581318797,\n",
       "    'acc_norm': 0.6705202312138728,\n",
       "    'acc_norm_stderr': 0.0253052581318797},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.43798882681564244,\n",
       "    'acc_stderr': 0.016593394227564843,\n",
       "    'acc_norm': 0.43798882681564244,\n",
       "    'acc_norm_stderr': 0.016593394227564843},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6601307189542484,\n",
       "    'acc_stderr': 0.027121956071388856,\n",
       "    'acc_norm': 0.6601307189542484,\n",
       "    'acc_norm_stderr': 0.027121956071388856},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6688102893890675,\n",
       "    'acc_stderr': 0.026730620728004906,\n",
       "    'acc_norm': 0.6688102893890675,\n",
       "    'acc_norm_stderr': 0.026730620728004906},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6851851851851852,\n",
       "    'acc_stderr': 0.02584224870090217,\n",
       "    'acc_norm': 0.6851851851851852,\n",
       "    'acc_norm_stderr': 0.02584224870090217},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.425531914893617,\n",
       "    'acc_stderr': 0.02949482760014438,\n",
       "    'acc_norm': 0.425531914893617,\n",
       "    'acc_norm_stderr': 0.02949482760014438},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.46740547588005216,\n",
       "    'acc_stderr': 0.012743072942653352,\n",
       "    'acc_norm': 0.46740547588005216,\n",
       "    'acc_norm_stderr': 0.012743072942653352},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5772058823529411,\n",
       "    'acc_stderr': 0.03000856284500348,\n",
       "    'acc_norm': 0.5772058823529411,\n",
       "    'acc_norm_stderr': 0.03000856284500348},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6372549019607843,\n",
       "    'acc_stderr': 0.01945076843250551,\n",
       "    'acc_norm': 0.6372549019607843,\n",
       "    'acc_norm_stderr': 0.01945076843250551},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.046075820907199756,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.046075820907199756},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.689795918367347,\n",
       "    'acc_stderr': 0.029613459872484375,\n",
       "    'acc_norm': 0.689795918367347,\n",
       "    'acc_norm_stderr': 0.029613459872484375},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8109452736318408,\n",
       "    'acc_stderr': 0.027686913588013024,\n",
       "    'acc_norm': 0.8109452736318408,\n",
       "    'acc_norm_stderr': 0.027686913588013024},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.84,\n",
       "    'acc_stderr': 0.03684529491774708,\n",
       "    'acc_norm': 0.84,\n",
       "    'acc_norm_stderr': 0.03684529491774708},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4939759036144578,\n",
       "    'acc_stderr': 0.03892212195333045,\n",
       "    'acc_norm': 0.4939759036144578,\n",
       "    'acc_norm_stderr': 0.03892212195333045},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7719298245614035,\n",
       "    'acc_stderr': 0.03218093795602357,\n",
       "    'acc_norm': 0.7719298245614035,\n",
       "    'acc_norm_stderr': 0.03218093795602357},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3525091799265606,\n",
       "    'mc1_stderr': 0.016724646380756544,\n",
       "    'mc2': 0.5480260004479156,\n",
       "    'mc2_stderr': 0.01562805066884067},\n",
       "   'all': {'acc': 0.5856610701363549,\n",
       "    'acc_stderr': 0.0338972114882748,\n",
       "    'acc_norm': 0.5894623662188109,\n",
       "    'acc_norm_stderr': 0.0338771018183512,\n",
       "    'mc1': 0.3525091799265606,\n",
       "    'mc1_stderr': 0.016724646380756544,\n",
       "    'mc2': 0.5480260004479156,\n",
       "    'mc2_stderr': 0.01562805066884067},\n",
       "   'harness|drop|3': {'em': 0.24611996644295303,\n",
       "    'em_stderr': 0.004411275638567265,\n",
       "    'f1': 0.3191652684563765,\n",
       "    'f1_stderr': 0.004369271114420946},\n",
       "   'harness|gsm8k|5': {'acc': 0.1372251705837756,\n",
       "    'acc_stderr': 0.00947780824460041},\n",
       "   'harness|winogrande|5': {'acc': 0.7703235990528808,\n",
       "    'acc_stderr': 0.011821645601838243}}},\n",
       " 'vicuna-7b': {'key': 'vicuna-7b',\n",
       "  'Model': 'Vicuna-7B',\n",
       "  'MT-bench (score)': '6.17',\n",
       "  'MMLU': '0.498',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-7b-v1.5',\n",
       "  'results': {'harness|drop|3': {'em': 0.017932046979865772,\n",
       "    'em_stderr': 0.0013590184569504276,\n",
       "    'f1': 0.08961094798657747,\n",
       "    'f1_stderr': 0.002014243406072028},\n",
       "   'harness|gsm8k|5': {'acc': 0.08188021228203184,\n",
       "    'acc_stderr': 0.007552338527716956},\n",
       "   'harness|winogrande|5': {'acc': 0.7213891081294396,\n",
       "    'acc_stderr': 0.012599896649493878},\n",
       "   'all': {'acc': 0.509411874714016,\n",
       "    'acc_stderr': 0.03492615918680034,\n",
       "    'acc_norm': 0.5131451594579193,\n",
       "    'acc_norm_stderr': 0.03491300641703413,\n",
       "    'mc1': 0.33047735618115054,\n",
       "    'mc1_stderr': 0.016466769613698303,\n",
       "    'mc2': 0.5032970216179807,\n",
       "    'mc2_stderr': 0.015653721119290503},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5025597269624573,\n",
       "    'acc_stderr': 0.014611199329843784,\n",
       "    'acc_norm': 0.5324232081911263,\n",
       "    'acc_norm_stderr': 0.01458063756999542},\n",
       "   'harness|hellaswag|10': {'acc': 0.5835490938060147,\n",
       "    'acc_stderr': 0.0049196263806455115,\n",
       "    'acc_norm': 0.7739494124676359,\n",
       "    'acc_norm_stderr': 0.004174174724288079},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5037037037037037,\n",
       "    'acc_stderr': 0.04319223625811331,\n",
       "    'acc_norm': 0.5037037037037037,\n",
       "    'acc_norm_stderr': 0.04319223625811331},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.04063302731486671,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.04063302731486671},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5433962264150943,\n",
       "    'acc_stderr': 0.030656748696739428,\n",
       "    'acc_norm': 0.5433962264150943,\n",
       "    'acc_norm_stderr': 0.030656748696739428},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.04179596617581,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.04179596617581},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4682080924855491,\n",
       "    'acc_stderr': 0.03804749744364763,\n",
       "    'acc_norm': 0.4682080924855491,\n",
       "    'acc_norm_stderr': 0.03804749744364763},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.17647058823529413,\n",
       "    'acc_stderr': 0.0379328118530781,\n",
       "    'acc_norm': 0.17647058823529413,\n",
       "    'acc_norm_stderr': 0.0379328118530781},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4553191489361702,\n",
       "    'acc_stderr': 0.03255525359340355,\n",
       "    'acc_norm': 0.4553191489361702,\n",
       "    'acc_norm_stderr': 0.03255525359340355},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.04372748290278006,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.04372748290278006},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.42758620689655175,\n",
       "    'acc_stderr': 0.04122737111370331,\n",
       "    'acc_norm': 0.42758620689655175,\n",
       "    'acc_norm_stderr': 0.04122737111370331},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.30687830687830686,\n",
       "    'acc_stderr': 0.023752928712112143,\n",
       "    'acc_norm': 0.30687830687830686,\n",
       "    'acc_norm_stderr': 0.023752928712112143},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.373015873015873,\n",
       "    'acc_stderr': 0.04325506042017086,\n",
       "    'acc_norm': 0.373015873015873,\n",
       "    'acc_norm_stderr': 0.04325506042017086},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.535483870967742,\n",
       "    'acc_stderr': 0.028372287797962935,\n",
       "    'acc_norm': 0.535483870967742,\n",
       "    'acc_norm_stderr': 0.028372287797962935},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4039408866995074,\n",
       "    'acc_stderr': 0.0345245390382204,\n",
       "    'acc_norm': 0.4039408866995074,\n",
       "    'acc_norm_stderr': 0.0345245390382204},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6303030303030303,\n",
       "    'acc_stderr': 0.037694303145125674,\n",
       "    'acc_norm': 0.6303030303030303,\n",
       "    'acc_norm_stderr': 0.037694303145125674},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6161616161616161,\n",
       "    'acc_stderr': 0.034648816750163396,\n",
       "    'acc_norm': 0.6161616161616161,\n",
       "    'acc_norm_stderr': 0.034648816750163396},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7357512953367875,\n",
       "    'acc_stderr': 0.03182155050916645,\n",
       "    'acc_norm': 0.7357512953367875,\n",
       "    'acc_norm_stderr': 0.03182155050916645},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.4794871794871795,\n",
       "    'acc_stderr': 0.025329663163489943,\n",
       "    'acc_norm': 0.4794871794871795,\n",
       "    'acc_norm_stderr': 0.025329663163489943},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.24444444444444444,\n",
       "    'acc_stderr': 0.02620276653465215,\n",
       "    'acc_norm': 0.24444444444444444,\n",
       "    'acc_norm_stderr': 0.02620276653465215},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.453781512605042,\n",
       "    'acc_stderr': 0.03233943468182088,\n",
       "    'acc_norm': 0.453781512605042,\n",
       "    'acc_norm_stderr': 0.03233943468182088},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.271523178807947,\n",
       "    'acc_stderr': 0.03631329803969653,\n",
       "    'acc_norm': 0.271523178807947,\n",
       "    'acc_norm_stderr': 0.03631329803969653},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.6972477064220184,\n",
       "    'acc_stderr': 0.019698711434756336,\n",
       "    'acc_norm': 0.6972477064220184,\n",
       "    'acc_norm_stderr': 0.019698711434756336},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.03324708911809117,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.03324708911809117},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7156862745098039,\n",
       "    'acc_stderr': 0.031660096793998116,\n",
       "    'acc_norm': 0.7156862745098039,\n",
       "    'acc_norm_stderr': 0.031660096793998116},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7215189873417721,\n",
       "    'acc_stderr': 0.02917868230484253,\n",
       "    'acc_norm': 0.7215189873417721,\n",
       "    'acc_norm_stderr': 0.02917868230484253},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6188340807174888,\n",
       "    'acc_stderr': 0.03259625118416827,\n",
       "    'acc_norm': 0.6188340807174888,\n",
       "    'acc_norm_stderr': 0.03259625118416827},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6335877862595419,\n",
       "    'acc_stderr': 0.04225875451969637,\n",
       "    'acc_norm': 0.6335877862595419,\n",
       "    'acc_norm_stderr': 0.04225875451969637},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.5950413223140496,\n",
       "    'acc_stderr': 0.04481137755942469,\n",
       "    'acc_norm': 0.5950413223140496,\n",
       "    'acc_norm_stderr': 0.04481137755942469},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5648148148148148,\n",
       "    'acc_stderr': 0.04792898170907061,\n",
       "    'acc_norm': 0.5648148148148148,\n",
       "    'acc_norm_stderr': 0.04792898170907061},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.5337423312883436,\n",
       "    'acc_stderr': 0.039194155450484096,\n",
       "    'acc_norm': 0.5337423312883436,\n",
       "    'acc_norm_stderr': 0.039194155450484096},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.42857142857142855,\n",
       "    'acc_stderr': 0.04697113923010212,\n",
       "    'acc_norm': 0.42857142857142855,\n",
       "    'acc_norm_stderr': 0.04697113923010212},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.04656147110012349,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.04656147110012349},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7692307692307693,\n",
       "    'acc_stderr': 0.027601921381417593,\n",
       "    'acc_norm': 0.7692307692307693,\n",
       "    'acc_norm_stderr': 0.027601921381417593},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6883780332056194,\n",
       "    'acc_stderr': 0.016562433867284176,\n",
       "    'acc_norm': 0.6883780332056194,\n",
       "    'acc_norm_stderr': 0.016562433867284176},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5578034682080925,\n",
       "    'acc_stderr': 0.026738603643807403,\n",
       "    'acc_norm': 0.5578034682080925,\n",
       "    'acc_norm_stderr': 0.026738603643807403},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24022346368715083,\n",
       "    'acc_stderr': 0.014288343803925296,\n",
       "    'acc_norm': 0.24022346368715083,\n",
       "    'acc_norm_stderr': 0.014288343803925296},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5751633986928104,\n",
       "    'acc_stderr': 0.02830457667314112,\n",
       "    'acc_norm': 0.5751633986928104,\n",
       "    'acc_norm_stderr': 0.02830457667314112},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5852090032154341,\n",
       "    'acc_stderr': 0.027982680459759567,\n",
       "    'acc_norm': 0.5852090032154341,\n",
       "    'acc_norm_stderr': 0.027982680459759567},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.027648477877413324,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.027648477877413324},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.36524822695035464,\n",
       "    'acc_stderr': 0.028723863853281285,\n",
       "    'acc_norm': 0.36524822695035464,\n",
       "    'acc_norm_stderr': 0.028723863853281285},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.37353324641460234,\n",
       "    'acc_stderr': 0.012354994823515271,\n",
       "    'acc_norm': 0.37353324641460234,\n",
       "    'acc_norm_stderr': 0.012354994823515271},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03032024326500413,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03032024326500413},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4852941176470588,\n",
       "    'acc_stderr': 0.020219083895133924,\n",
       "    'acc_norm': 0.4852941176470588,\n",
       "    'acc_norm_stderr': 0.020219083895133924},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6272727272727273,\n",
       "    'acc_stderr': 0.04631381319425465,\n",
       "    'acc_norm': 0.6272727272727273,\n",
       "    'acc_norm_stderr': 0.04631381319425465},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6244897959183674,\n",
       "    'acc_stderr': 0.03100120903989484,\n",
       "    'acc_norm': 0.6244897959183674,\n",
       "    'acc_norm_stderr': 0.03100120903989484},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.03333333333333335,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.03333333333333335},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.04292346959909282,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.04292346959909282},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4397590361445783,\n",
       "    'acc_stderr': 0.03864139923699121,\n",
       "    'acc_norm': 0.4397590361445783,\n",
       "    'acc_norm_stderr': 0.03864139923699121},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7192982456140351,\n",
       "    'acc_stderr': 0.034462962170884265,\n",
       "    'acc_norm': 0.7192982456140351,\n",
       "    'acc_norm_stderr': 0.034462962170884265},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.33047735618115054,\n",
       "    'mc1_stderr': 0.016466769613698303,\n",
       "    'mc2': 0.5032970216179807,\n",
       "    'mc2_stderr': 0.015653721119290503}}},\n",
       " 'wizardlm-13b': {'key': 'wizardlm-13b',\n",
       "  'Model': 'WizardLM-13b-v1.2',\n",
       "  'MT-bench (score)': '7.20',\n",
       "  'MMLU': '0.527',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Microsoft',\n",
       "  'Link': 'https://huggingface.co/WizardLM/WizardLM-13B-V1.2',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5443686006825939,\n",
       "    'acc_stderr': 0.014553749939306861,\n",
       "    'acc_norm': 0.590443686006826,\n",
       "    'acc_norm_stderr': 0.014370358632472439},\n",
       "   'harness|hellaswag|10': {'acc': 0.6294562836088429,\n",
       "    'acc_stderr': 0.00481963366883254,\n",
       "    'acc_norm': 0.8221469826727743,\n",
       "    'acc_norm_stderr': 0.0038160747120605347},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5111111111111111,\n",
       "    'acc_stderr': 0.04318275491977976,\n",
       "    'acc_norm': 0.5111111111111111,\n",
       "    'acc_norm_stderr': 0.04318275491977976},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5394736842105263,\n",
       "    'acc_stderr': 0.04056242252249034,\n",
       "    'acc_norm': 0.5394736842105263,\n",
       "    'acc_norm_stderr': 0.04056242252249034},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.04999999999999999,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.04999999999999999},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6113207547169811,\n",
       "    'acc_stderr': 0.030000485448675986,\n",
       "    'acc_norm': 0.6113207547169811,\n",
       "    'acc_norm_stderr': 0.030000485448675986},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5694444444444444,\n",
       "    'acc_stderr': 0.04140685639111503,\n",
       "    'acc_norm': 0.5694444444444444,\n",
       "    'acc_norm_stderr': 0.04140685639111503},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4913294797687861,\n",
       "    'acc_stderr': 0.03811890988940412,\n",
       "    'acc_norm': 0.4913294797687861,\n",
       "    'acc_norm_stderr': 0.03811890988940412},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.04389869956808777,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.04389869956808777},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.42127659574468085,\n",
       "    'acc_stderr': 0.03227834510146268,\n",
       "    'acc_norm': 0.42127659574468085,\n",
       "    'acc_norm_stderr': 0.03227834510146268},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2894736842105263,\n",
       "    'acc_stderr': 0.04266339443159394,\n",
       "    'acc_norm': 0.2894736842105263,\n",
       "    'acc_norm_stderr': 0.04266339443159394},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4482758620689655,\n",
       "    'acc_stderr': 0.04144311810878151,\n",
       "    'acc_norm': 0.4482758620689655,\n",
       "    'acc_norm_stderr': 0.04144311810878151},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.02441923496681907,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.02441923496681907},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3492063492063492,\n",
       "    'acc_stderr': 0.04263906892795132,\n",
       "    'acc_norm': 0.3492063492063492,\n",
       "    'acc_norm_stderr': 0.04263906892795132},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.632258064516129,\n",
       "    'acc_stderr': 0.02743086657997347,\n",
       "    'acc_norm': 0.632258064516129,\n",
       "    'acc_norm_stderr': 0.02743086657997347},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.43842364532019706,\n",
       "    'acc_stderr': 0.03491207857486518,\n",
       "    'acc_norm': 0.43842364532019706,\n",
       "    'acc_norm_stderr': 0.03491207857486518},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.049236596391733084,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.049236596391733084},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6787878787878788,\n",
       "    'acc_stderr': 0.036462049632538115,\n",
       "    'acc_norm': 0.6787878787878788,\n",
       "    'acc_norm_stderr': 0.036462049632538115},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6919191919191919,\n",
       "    'acc_stderr': 0.03289477330098616,\n",
       "    'acc_norm': 0.6919191919191919,\n",
       "    'acc_norm_stderr': 0.03289477330098616},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8031088082901554,\n",
       "    'acc_stderr': 0.028697873971860695,\n",
       "    'acc_norm': 0.8031088082901554,\n",
       "    'acc_norm_stderr': 0.028697873971860695},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5230769230769231,\n",
       "    'acc_stderr': 0.025323990861736236,\n",
       "    'acc_norm': 0.5230769230769231,\n",
       "    'acc_norm_stderr': 0.025323990861736236},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028604,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028604},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5798319327731093,\n",
       "    'acc_stderr': 0.03206183783236152,\n",
       "    'acc_norm': 0.5798319327731093,\n",
       "    'acc_norm_stderr': 0.03206183783236152},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.304635761589404,\n",
       "    'acc_stderr': 0.03757949922943343,\n",
       "    'acc_norm': 0.304635761589404,\n",
       "    'acc_norm_stderr': 0.03757949922943343},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7321100917431193,\n",
       "    'acc_stderr': 0.018987462257978652,\n",
       "    'acc_norm': 0.7321100917431193,\n",
       "    'acc_norm_stderr': 0.018987462257978652},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4027777777777778,\n",
       "    'acc_stderr': 0.033448873829978666,\n",
       "    'acc_norm': 0.4027777777777778,\n",
       "    'acc_norm_stderr': 0.033448873829978666},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03039153369274154,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03039153369274154},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7257383966244726,\n",
       "    'acc_stderr': 0.029041333510598028,\n",
       "    'acc_norm': 0.7257383966244726,\n",
       "    'acc_norm_stderr': 0.029041333510598028},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6322869955156951,\n",
       "    'acc_stderr': 0.03236198350928276,\n",
       "    'acc_norm': 0.6322869955156951,\n",
       "    'acc_norm_stderr': 0.03236198350928276},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5954198473282443,\n",
       "    'acc_stderr': 0.043046937953806645,\n",
       "    'acc_norm': 0.5954198473282443,\n",
       "    'acc_norm_stderr': 0.043046937953806645},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7520661157024794,\n",
       "    'acc_stderr': 0.03941897526516303,\n",
       "    'acc_norm': 0.7520661157024794,\n",
       "    'acc_norm_stderr': 0.03941897526516303},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7314814814814815,\n",
       "    'acc_stderr': 0.042844679680521934,\n",
       "    'acc_norm': 0.7314814814814815,\n",
       "    'acc_norm_stderr': 0.042844679680521934},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6441717791411042,\n",
       "    'acc_stderr': 0.03761521380046734,\n",
       "    'acc_norm': 0.6441717791411042,\n",
       "    'acc_norm_stderr': 0.03761521380046734},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.33035714285714285,\n",
       "    'acc_stderr': 0.04464285714285712,\n",
       "    'acc_norm': 0.33035714285714285,\n",
       "    'acc_norm_stderr': 0.04464285714285712},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6601941747572816,\n",
       "    'acc_stderr': 0.046897659372781335,\n",
       "    'acc_norm': 0.6601941747572816,\n",
       "    'acc_norm_stderr': 0.046897659372781335},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8162393162393162,\n",
       "    'acc_stderr': 0.02537213967172293,\n",
       "    'acc_norm': 0.8162393162393162,\n",
       "    'acc_norm_stderr': 0.02537213967172293},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7279693486590039,\n",
       "    'acc_stderr': 0.015913367447500503,\n",
       "    'acc_norm': 0.7279693486590039,\n",
       "    'acc_norm_stderr': 0.015913367447500503},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5982658959537572,\n",
       "    'acc_stderr': 0.026394104177643634,\n",
       "    'acc_norm': 0.5982658959537572,\n",
       "    'acc_norm_stderr': 0.026394104177643634},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30502793296089387,\n",
       "    'acc_stderr': 0.015398723510916713,\n",
       "    'acc_norm': 0.30502793296089387,\n",
       "    'acc_norm_stderr': 0.015398723510916713},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6013071895424836,\n",
       "    'acc_stderr': 0.028036092273891776,\n",
       "    'acc_norm': 0.6013071895424836,\n",
       "    'acc_norm_stderr': 0.028036092273891776},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5980707395498392,\n",
       "    'acc_stderr': 0.02784647600593047,\n",
       "    'acc_norm': 0.5980707395498392,\n",
       "    'acc_norm_stderr': 0.02784647600593047},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5895061728395061,\n",
       "    'acc_stderr': 0.027371350925124764,\n",
       "    'acc_norm': 0.5895061728395061,\n",
       "    'acc_norm_stderr': 0.027371350925124764},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4219858156028369,\n",
       "    'acc_stderr': 0.029462189233370597,\n",
       "    'acc_norm': 0.4219858156028369,\n",
       "    'acc_norm_stderr': 0.029462189233370597},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4165580182529335,\n",
       "    'acc_stderr': 0.012591153245057388,\n",
       "    'acc_norm': 0.4165580182529335,\n",
       "    'acc_norm_stderr': 0.012591153245057388},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5625,\n",
       "    'acc_stderr': 0.030134614954403924,\n",
       "    'acc_norm': 0.5625,\n",
       "    'acc_norm_stderr': 0.030134614954403924},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5310457516339869,\n",
       "    'acc_stderr': 0.02018880445636189,\n",
       "    'acc_norm': 0.5310457516339869,\n",
       "    'acc_norm_stderr': 0.02018880445636189},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.0449429086625209,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.0449429086625209},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6693877551020408,\n",
       "    'acc_stderr': 0.0301164262965406,\n",
       "    'acc_norm': 0.6693877551020408,\n",
       "    'acc_norm_stderr': 0.0301164262965406},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6865671641791045,\n",
       "    'acc_stderr': 0.032801882053486435,\n",
       "    'acc_norm': 0.6865671641791045,\n",
       "    'acc_norm_stderr': 0.032801882053486435},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.463855421686747,\n",
       "    'acc_stderr': 0.03882310850890593,\n",
       "    'acc_norm': 0.463855421686747,\n",
       "    'acc_norm_stderr': 0.03882310850890593},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7543859649122807,\n",
       "    'acc_stderr': 0.0330140594698725,\n",
       "    'acc_norm': 0.7543859649122807,\n",
       "    'acc_norm_stderr': 0.0330140594698725},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.32558139534883723,\n",
       "    'mc1_stderr': 0.01640398946990783,\n",
       "    'mc2': 0.47267763319871686,\n",
       "    'mc2_stderr': 0.01512716043041388},\n",
       "   'all': {'em': 0.09133808724832215,\n",
       "    'em_stderr': 0.002950304012601038,\n",
       "    'f1': 0.1617292365771806,\n",
       "    'f1_stderr': 0.0032231699829319426,\n",
       "    'acc': 0.4269860152120696,\n",
       "    'acc_stderr': 0.011021928189223498},\n",
       "   'harness|drop|3': {'em': 0.09133808724832215,\n",
       "    'em_stderr': 0.002950304012601038,\n",
       "    'f1': 0.1617292365771806,\n",
       "    'f1_stderr': 0.0032231699829319426},\n",
       "   'harness|gsm8k|5': {'acc': 0.13495072024260804,\n",
       "    'acc_stderr': 0.009411315282571171},\n",
       "   'harness|winogrande|5': {'acc': 0.7190213101815311,\n",
       "    'acc_stderr': 0.012632541095875825}}},\n",
       " 'wizardlm-70b': {'key': 'wizardlm-70b',\n",
       "  'Model': 'WizardLM-70B-v1.0',\n",
       "  'MT-bench (score)': '7.71',\n",
       "  'MMLU': '0.637',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Microsoft',\n",
       "  'Link': 'https://huggingface.co/WizardLM/WizardLM-70B-V1.0',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.606655290102389,\n",
       "    'acc_stderr': 0.014275101465693024,\n",
       "    'acc_norm': 0.6407849829351536,\n",
       "    'acc_norm_stderr': 0.014020224155839157},\n",
       "   'harness|hellaswag|10': {'acc': 0.6654052977494523,\n",
       "    'acc_stderr': 0.0047088426001774385,\n",
       "    'acc_norm': 0.854511053574985,\n",
       "    'acc_norm_stderr': 0.003518725257365601},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5185185185185185,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.5185185185185185,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03523807393012047,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03523807393012047},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621503,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621503},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.02872750295788027,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.02872750295788027},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7638888888888888,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.7638888888888888,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6473988439306358,\n",
       "    'acc_stderr': 0.036430371689585475,\n",
       "    'acc_norm': 0.6473988439306358,\n",
       "    'acc_norm_stderr': 0.036430371689585475},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107224,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107224},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.04461960433384739,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.04461960433384739},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5957446808510638,\n",
       "    'acc_stderr': 0.03208115750788684,\n",
       "    'acc_norm': 0.5957446808510638,\n",
       "    'acc_norm_stderr': 0.03208115750788684},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.046970851366478626,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.046970851366478626},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5448275862068965,\n",
       "    'acc_stderr': 0.04149886942192117,\n",
       "    'acc_norm': 0.5448275862068965,\n",
       "    'acc_norm_stderr': 0.04149886942192117},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42328042328042326,\n",
       "    'acc_stderr': 0.02544636563440676,\n",
       "    'acc_norm': 0.42328042328042326,\n",
       "    'acc_norm_stderr': 0.02544636563440676},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.04444444444444449,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.04444444444444449},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7645161290322581,\n",
       "    'acc_stderr': 0.02413763242933771,\n",
       "    'acc_norm': 0.7645161290322581,\n",
       "    'acc_norm_stderr': 0.02413763242933771},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.49261083743842365,\n",
       "    'acc_stderr': 0.03517603540361008,\n",
       "    'acc_norm': 0.49261083743842365,\n",
       "    'acc_norm_stderr': 0.03517603540361008},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8121212121212121,\n",
       "    'acc_stderr': 0.03050193405942914,\n",
       "    'acc_norm': 0.8121212121212121,\n",
       "    'acc_norm_stderr': 0.03050193405942914},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.02805779167298902,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.02805779167298902},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9119170984455959,\n",
       "    'acc_stderr': 0.02045374660160103,\n",
       "    'acc_norm': 0.9119170984455959,\n",
       "    'acc_norm_stderr': 0.02045374660160103},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6461538461538462,\n",
       "    'acc_stderr': 0.024243783994062153,\n",
       "    'acc_norm': 0.6461538461538462,\n",
       "    'acc_norm_stderr': 0.024243783994062153},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3111111111111111,\n",
       "    'acc_stderr': 0.028226446749683515,\n",
       "    'acc_norm': 0.3111111111111111,\n",
       "    'acc_norm_stderr': 0.028226446749683515},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7142857142857143,\n",
       "    'acc_stderr': 0.029344572500634342,\n",
       "    'acc_norm': 0.7142857142857143,\n",
       "    'acc_norm_stderr': 0.029344572500634342},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.4105960264900662,\n",
       "    'acc_stderr': 0.04016689594849929,\n",
       "    'acc_norm': 0.4105960264900662,\n",
       "    'acc_norm_stderr': 0.04016689594849929},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.03408655867977749,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.03408655867977749},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8480392156862745,\n",
       "    'acc_stderr': 0.025195658428931792,\n",
       "    'acc_norm': 0.8480392156862745,\n",
       "    'acc_norm_stderr': 0.025195658428931792},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8396624472573839,\n",
       "    'acc_stderr': 0.02388438092596567,\n",
       "    'acc_norm': 0.8396624472573839,\n",
       "    'acc_norm_stderr': 0.02388438092596567},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7040358744394619,\n",
       "    'acc_stderr': 0.03063659134869981,\n",
       "    'acc_norm': 0.7040358744394619,\n",
       "    'acc_norm_stderr': 0.03063659134869981},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8015267175572519,\n",
       "    'acc_stderr': 0.034981493854624714,\n",
       "    'acc_norm': 0.8015267175572519,\n",
       "    'acc_norm_stderr': 0.034981493854624714},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8264462809917356,\n",
       "    'acc_stderr': 0.0345727283691767,\n",
       "    'acc_norm': 0.8264462809917356,\n",
       "    'acc_norm_stderr': 0.0345727283691767},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8240740740740741,\n",
       "    'acc_stderr': 0.036809181416738807,\n",
       "    'acc_norm': 0.8240740740740741,\n",
       "    'acc_norm_stderr': 0.036809181416738807},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7791411042944786,\n",
       "    'acc_stderr': 0.03259177392742178,\n",
       "    'acc_norm': 0.7791411042944786,\n",
       "    'acc_norm_stderr': 0.03259177392742178},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7961165048543689,\n",
       "    'acc_stderr': 0.0398913985953177,\n",
       "    'acc_norm': 0.7961165048543689,\n",
       "    'acc_norm_stderr': 0.0398913985953177},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8717948717948718,\n",
       "    'acc_stderr': 0.02190190511507332,\n",
       "    'acc_norm': 0.8717948717948718,\n",
       "    'acc_norm_stderr': 0.02190190511507332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8288633461047255,\n",
       "    'acc_stderr': 0.013468201614066307,\n",
       "    'acc_norm': 0.8288633461047255,\n",
       "    'acc_norm_stderr': 0.013468201614066307},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7283236994219653,\n",
       "    'acc_stderr': 0.023948512905468365,\n",
       "    'acc_norm': 0.7283236994219653,\n",
       "    'acc_norm_stderr': 0.023948512905468365},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.29832402234636873,\n",
       "    'acc_stderr': 0.01530184004512928,\n",
       "    'acc_norm': 0.29832402234636873,\n",
       "    'acc_norm_stderr': 0.01530184004512928},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6797385620915033,\n",
       "    'acc_stderr': 0.026716118380156847,\n",
       "    'acc_norm': 0.6797385620915033,\n",
       "    'acc_norm_stderr': 0.026716118380156847},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6881028938906752,\n",
       "    'acc_stderr': 0.02631185807185416,\n",
       "    'acc_norm': 0.6881028938906752,\n",
       "    'acc_norm_stderr': 0.02631185807185416},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7067901234567902,\n",
       "    'acc_stderr': 0.02532988817190092,\n",
       "    'acc_norm': 0.7067901234567902,\n",
       "    'acc_norm_stderr': 0.02532988817190092},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5425531914893617,\n",
       "    'acc_stderr': 0.029719281272236837,\n",
       "    'acc_norm': 0.5425531914893617,\n",
       "    'acc_norm_stderr': 0.029719281272236837},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5189048239895697,\n",
       "    'acc_stderr': 0.012761104871472652,\n",
       "    'acc_norm': 0.5189048239895697,\n",
       "    'acc_norm_stderr': 0.012761104871472652},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6544117647058824,\n",
       "    'acc_stderr': 0.028888193103988626,\n",
       "    'acc_norm': 0.6544117647058824,\n",
       "    'acc_norm_stderr': 0.028888193103988626},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.01869085027359529,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.01869085027359529},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.7090909090909091,\n",
       "    'acc_stderr': 0.04350271442923243,\n",
       "    'acc_norm': 0.7090909090909091,\n",
       "    'acc_norm_stderr': 0.04350271442923243},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7714285714285715,\n",
       "    'acc_stderr': 0.026882144922307744,\n",
       "    'acc_norm': 0.7714285714285715,\n",
       "    'acc_norm_stderr': 0.026882144922307744},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8557213930348259,\n",
       "    'acc_stderr': 0.024845753212306042,\n",
       "    'acc_norm': 0.8557213930348259,\n",
       "    'acc_norm_stderr': 0.024845753212306042},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.83,\n",
       "    'acc_stderr': 0.03775251680686371,\n",
       "    'acc_norm': 0.83,\n",
       "    'acc_norm_stderr': 0.03775251680686371},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.536144578313253,\n",
       "    'acc_stderr': 0.03882310850890594,\n",
       "    'acc_norm': 0.536144578313253,\n",
       "    'acc_norm_stderr': 0.03882310850890594},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8538011695906432,\n",
       "    'acc_stderr': 0.027097290118070792,\n",
       "    'acc_norm': 0.8538011695906432,\n",
       "    'acc_norm_stderr': 0.027097290118070792},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.386780905752754,\n",
       "    'mc1_stderr': 0.01704885701051511,\n",
       "    'mc2': 0.5476558772806095,\n",
       "    'mc2_stderr': 0.015451111828036913},\n",
       "   'all': {'acc': 0.6477083045829946,\n",
       "    'acc_stderr': 0.032583732941869364,\n",
       "    'acc_norm': 0.6514919562551693,\n",
       "    'acc_norm_stderr': 0.03255924150707842,\n",
       "    'mc1': 0.386780905752754,\n",
       "    'mc1_stderr': 0.01704885701051511,\n",
       "    'mc2': 0.5476558772806095,\n",
       "    'mc2_stderr': 0.015451111828036913},\n",
       "   'harness|drop|3': {'em': 0.26541526845637586,\n",
       "    'em_stderr': 0.004521927044730418,\n",
       "    'f1': 0.3270648070469802,\n",
       "    'f1_stderr': 0.004444377320494032},\n",
       "   'harness|gsm8k|5': {'acc': 0.17968157695223655,\n",
       "    'acc_stderr': 0.010575119964242244},\n",
       "   'harness|winogrande|5': {'acc': 0.8082083662194159,\n",
       "    'acc_stderr': 0.011065209664659527}}},\n",
       " 'yi-34b-chat': {'key': 'yi-34b-chat',\n",
       "  'Model': 'Yi-34B-Chat',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.735',\n",
       "  'License': 'Yi License',\n",
       "  'Organization': '01 AI',\n",
       "  'Link': 'https://huggingface.co/01-ai/Yi-34B-Chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6373720136518771,\n",
       "    'acc_stderr': 0.014049106564955012,\n",
       "    'acc_norm': 0.6544368600682594,\n",
       "    'acc_norm_stderr': 0.013896938461145678},\n",
       "   'harness|hellaswag|10': {'acc': 0.6536546504680343,\n",
       "    'acc_stderr': 0.004748324319714274,\n",
       "    'acc_norm': 0.8415654252141008,\n",
       "    'acc_norm_stderr': 0.003644017383711605},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.7111111111111111,\n",
       "    'acc_stderr': 0.03915450630414251,\n",
       "    'acc_norm': 0.7111111111111111,\n",
       "    'acc_norm_stderr': 0.03915450630414251},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.8552631578947368,\n",
       "    'acc_stderr': 0.028631951845930387,\n",
       "    'acc_norm': 0.8552631578947368,\n",
       "    'acc_norm_stderr': 0.028631951845930387},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.7924528301886793,\n",
       "    'acc_stderr': 0.02495991802891127,\n",
       "    'acc_norm': 0.7924528301886793,\n",
       "    'acc_norm_stderr': 0.02495991802891127},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.8472222222222222,\n",
       "    'acc_stderr': 0.030085743248565666,\n",
       "    'acc_norm': 0.8472222222222222,\n",
       "    'acc_norm_stderr': 0.030085743248565666},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237101,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237101},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6994219653179191,\n",
       "    'acc_stderr': 0.034961014811911786,\n",
       "    'acc_norm': 0.6994219653179191,\n",
       "    'acc_norm_stderr': 0.034961014811911786},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.46078431372549017,\n",
       "    'acc_stderr': 0.049598599663841815,\n",
       "    'acc_norm': 0.46078431372549017,\n",
       "    'acc_norm_stderr': 0.049598599663841815},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.82,\n",
       "    'acc_stderr': 0.03861229196653695,\n",
       "    'acc_norm': 0.82,\n",
       "    'acc_norm_stderr': 0.03861229196653695},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.7659574468085106,\n",
       "    'acc_stderr': 0.02767845257821239,\n",
       "    'acc_norm': 0.7659574468085106,\n",
       "    'acc_norm_stderr': 0.02767845257821239},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5526315789473685,\n",
       "    'acc_stderr': 0.046774730044911984,\n",
       "    'acc_norm': 0.5526315789473685,\n",
       "    'acc_norm_stderr': 0.046774730044911984},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.0333333333333333,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.0333333333333333},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.6349206349206349,\n",
       "    'acc_stderr': 0.024796060602699965,\n",
       "    'acc_norm': 0.6349206349206349,\n",
       "    'acc_norm_stderr': 0.024796060602699965},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5396825396825397,\n",
       "    'acc_stderr': 0.04458029125470973,\n",
       "    'acc_norm': 0.5396825396825397,\n",
       "    'acc_norm_stderr': 0.04458029125470973},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.049888765156985884,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.049888765156985884},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.8709677419354839,\n",
       "    'acc_stderr': 0.019070889254792767,\n",
       "    'acc_norm': 0.8709677419354839,\n",
       "    'acc_norm_stderr': 0.019070889254792767},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.6206896551724138,\n",
       "    'acc_stderr': 0.03413963805906235,\n",
       "    'acc_norm': 0.6206896551724138,\n",
       "    'acc_norm_stderr': 0.03413963805906235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8545454545454545,\n",
       "    'acc_stderr': 0.027530196355066573,\n",
       "    'acc_norm': 0.8545454545454545,\n",
       "    'acc_norm_stderr': 0.027530196355066573},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.898989898989899,\n",
       "    'acc_stderr': 0.021469735576055343,\n",
       "    'acc_norm': 0.898989898989899,\n",
       "    'acc_norm_stderr': 0.021469735576055343},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9533678756476683,\n",
       "    'acc_stderr': 0.015216761819262585,\n",
       "    'acc_norm': 0.9533678756476683,\n",
       "    'acc_norm_stderr': 0.015216761819262585},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.7846153846153846,\n",
       "    'acc_stderr': 0.020843034557462878,\n",
       "    'acc_norm': 0.7846153846153846,\n",
       "    'acc_norm_stderr': 0.020843034557462878},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.35555555555555557,\n",
       "    'acc_stderr': 0.029185714949857403,\n",
       "    'acc_norm': 0.35555555555555557,\n",
       "    'acc_norm_stderr': 0.029185714949857403},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.8361344537815126,\n",
       "    'acc_stderr': 0.024044054940440488,\n",
       "    'acc_norm': 0.8361344537815126,\n",
       "    'acc_norm_stderr': 0.024044054940440488},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.5033112582781457,\n",
       "    'acc_stderr': 0.04082393379449654,\n",
       "    'acc_norm': 0.5033112582781457,\n",
       "    'acc_norm_stderr': 0.04082393379449654},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.908256880733945,\n",
       "    'acc_stderr': 0.012376323409137123,\n",
       "    'acc_norm': 0.908256880733945,\n",
       "    'acc_norm_stderr': 0.012376323409137123},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.6342592592592593,\n",
       "    'acc_stderr': 0.03284738857647206,\n",
       "    'acc_norm': 0.6342592592592593,\n",
       "    'acc_norm_stderr': 0.03284738857647206},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.9068627450980392,\n",
       "    'acc_stderr': 0.020397853969426998,\n",
       "    'acc_norm': 0.9068627450980392,\n",
       "    'acc_norm_stderr': 0.020397853969426998},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.9029535864978903,\n",
       "    'acc_stderr': 0.019269323025640255,\n",
       "    'acc_norm': 0.9029535864978903,\n",
       "    'acc_norm_stderr': 0.019269323025640255},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.8116591928251121,\n",
       "    'acc_stderr': 0.026241132996407256,\n",
       "    'acc_norm': 0.8116591928251121,\n",
       "    'acc_norm_stderr': 0.026241132996407256},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8931297709923665,\n",
       "    'acc_stderr': 0.027096548624883733,\n",
       "    'acc_norm': 0.8931297709923665,\n",
       "    'acc_norm_stderr': 0.027096548624883733},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8925619834710744,\n",
       "    'acc_stderr': 0.028268812192540627,\n",
       "    'acc_norm': 0.8925619834710744,\n",
       "    'acc_norm_stderr': 0.028268812192540627},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8888888888888888,\n",
       "    'acc_stderr': 0.03038159675665167,\n",
       "    'acc_norm': 0.8888888888888888,\n",
       "    'acc_norm_stderr': 0.03038159675665167},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.852760736196319,\n",
       "    'acc_stderr': 0.027839915278339657,\n",
       "    'acc_norm': 0.852760736196319,\n",
       "    'acc_norm_stderr': 0.027839915278339657},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.6160714285714286,\n",
       "    'acc_stderr': 0.04616143075028546,\n",
       "    'acc_norm': 0.6160714285714286,\n",
       "    'acc_norm_stderr': 0.04616143075028546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8737864077669902,\n",
       "    'acc_stderr': 0.0328818027880863,\n",
       "    'acc_norm': 0.8737864077669902,\n",
       "    'acc_norm_stderr': 0.0328818027880863},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.9145299145299145,\n",
       "    'acc_stderr': 0.01831589168562586,\n",
       "    'acc_norm': 0.9145299145299145,\n",
       "    'acc_norm_stderr': 0.01831589168562586},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.033799766898963086,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.033799766898963086},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8991060025542784,\n",
       "    'acc_stderr': 0.01077047201488672,\n",
       "    'acc_norm': 0.8991060025542784,\n",
       "    'acc_norm_stderr': 0.01077047201488672},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.8092485549132948,\n",
       "    'acc_stderr': 0.021152676966575277,\n",
       "    'acc_norm': 0.8092485549132948,\n",
       "    'acc_norm_stderr': 0.021152676966575277},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.7016759776536313,\n",
       "    'acc_stderr': 0.01530184004512928,\n",
       "    'acc_norm': 0.7016759776536313,\n",
       "    'acc_norm_stderr': 0.01530184004512928},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.8300653594771242,\n",
       "    'acc_stderr': 0.02150538312123137,\n",
       "    'acc_norm': 0.8300653594771242,\n",
       "    'acc_norm_stderr': 0.02150538312123137},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.8102893890675241,\n",
       "    'acc_stderr': 0.02226819625878322,\n",
       "    'acc_norm': 0.8102893890675241,\n",
       "    'acc_norm_stderr': 0.02226819625878322},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.8703703703703703,\n",
       "    'acc_stderr': 0.018689725721062072,\n",
       "    'acc_norm': 0.8703703703703703,\n",
       "    'acc_norm_stderr': 0.018689725721062072},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.6170212765957447,\n",
       "    'acc_stderr': 0.02899908090480618,\n",
       "    'acc_norm': 0.6170212765957447,\n",
       "    'acc_norm_stderr': 0.02899908090480618},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5495436766623207,\n",
       "    'acc_stderr': 0.012707390438502348,\n",
       "    'acc_norm': 0.5495436766623207,\n",
       "    'acc_norm_stderr': 0.012707390438502348},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.7830882352941176,\n",
       "    'acc_stderr': 0.025035845227711274,\n",
       "    'acc_norm': 0.7830882352941176,\n",
       "    'acc_norm_stderr': 0.025035845227711274},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.826797385620915,\n",
       "    'acc_stderr': 0.015309329266969138,\n",
       "    'acc_norm': 0.826797385620915,\n",
       "    'acc_norm_stderr': 0.015309329266969138},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.7272727272727273,\n",
       "    'acc_stderr': 0.04265792110940589,\n",
       "    'acc_norm': 0.7272727272727273,\n",
       "    'acc_norm_stderr': 0.04265792110940589},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.8367346938775511,\n",
       "    'acc_stderr': 0.02366169917709861,\n",
       "    'acc_norm': 0.8367346938775511,\n",
       "    'acc_norm_stderr': 0.02366169917709861},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8855721393034826,\n",
       "    'acc_stderr': 0.022509345325101706,\n",
       "    'acc_norm': 0.8855721393034826,\n",
       "    'acc_norm_stderr': 0.022509345325101706},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.89,\n",
       "    'acc_stderr': 0.03144660377352203,\n",
       "    'acc_norm': 0.89,\n",
       "    'acc_norm_stderr': 0.03144660377352203},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5843373493975904,\n",
       "    'acc_stderr': 0.03836722176598053,\n",
       "    'acc_norm': 0.5843373493975904,\n",
       "    'acc_norm_stderr': 0.03836722176598053},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8771929824561403,\n",
       "    'acc_stderr': 0.02517298435015577,\n",
       "    'acc_norm': 0.8771929824561403,\n",
       "    'acc_norm_stderr': 0.02517298435015577},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3843329253365973,\n",
       "    'mc1_stderr': 0.017028707301245203,\n",
       "    'mc2': 0.5536831362008046,\n",
       "    'mc2_stderr': 0.015524186394858242},\n",
       "   'harness|winogrande|5': {'acc': 0.8011049723756906,\n",
       "    'acc_stderr': 0.01121862997251531},\n",
       "   'harness|drop|3': {'em': 0.005138422818791947,\n",
       "    'em_stderr': 0.0007322104102794241,\n",
       "    'f1': 0.08032508389261797,\n",
       "    'f1_stderr': 0.001571649833831937},\n",
       "   'harness|gsm8k|5': {'acc': 0.3191811978771797,\n",
       "    'acc_stderr': 0.012840345676251648},\n",
       "   'all': {'acc': 0.7393930299846158,\n",
       "    'acc_stderr': 0.028807135333088364,\n",
       "    'acc_norm': 0.7489434623723922,\n",
       "    'acc_norm_stderr': 0.02935457295982731,\n",
       "    'mc1': 0.3843329253365973,\n",
       "    'mc1_stderr': 0.017028707301245203,\n",
       "    'mc2': 0.5536831362008046,\n",
       "    'mc2_stderr': 0.015524186394858242}}},\n",
       " 'zephyr-7b-alpha': {'key': 'zephyr-7b-alpha',\n",
       "  'Model': 'Zephyr-7b-alpha',\n",
       "  'MT-bench (score)': '6.88',\n",
       "  'MMLU': '-',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'HuggingFace',\n",
       "  'Link': 'https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha',\n",
       "  'results': {'harness|drop|3': {'em': 0.010171979865771811,\n",
       "    'em_stderr': 0.001027595678252703,\n",
       "    'f1': 0.0982225251677855,\n",
       "    'f1_stderr': 0.00197861649884212},\n",
       "   'harness|gsm8k|5': {'acc': 0.14025777103866566,\n",
       "    'acc_stderr': 0.00956510828142867},\n",
       "   'harness|winogrande|5': {'acc': 0.7861089187056038,\n",
       "    'acc_stderr': 0.011524466954090248},\n",
       "   'all': {'acc': 0.6137978230566867,\n",
       "    'acc_stderr': 0.03380754595328641,\n",
       "    'acc_norm': 0.6176702382672306,\n",
       "    'acc_norm_stderr': 0.03378555360789072,\n",
       "    'mc1': 0.42717258261933905,\n",
       "    'mc1_stderr': 0.017316834410963926,\n",
       "    'mc2': 0.5790339154881958,\n",
       "    'mc2_stderr': 0.015362629183533977},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5810580204778157,\n",
       "    'acc_stderr': 0.014418106953639011,\n",
       "    'acc_norm': 0.6100682593856656,\n",
       "    'acc_norm_stderr': 0.01425295984889289},\n",
       "   'harness|hellaswag|10': {'acc': 0.6409081856203943,\n",
       "    'acc_stderr': 0.004787537385153006,\n",
       "    'acc_norm': 0.8403704441346346,\n",
       "    'acc_norm_stderr': 0.0036551361115537096},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6447368421052632,\n",
       "    'acc_stderr': 0.038947344870133176,\n",
       "    'acc_norm': 0.6447368421052632,\n",
       "    'acc_norm_stderr': 0.038947344870133176},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620332,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620332},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6679245283018868,\n",
       "    'acc_stderr': 0.02898545565233439,\n",
       "    'acc_norm': 0.6679245283018868,\n",
       "    'acc_norm_stderr': 0.02898545565233439},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7013888888888888,\n",
       "    'acc_stderr': 0.03827052357950756,\n",
       "    'acc_norm': 0.7013888888888888,\n",
       "    'acc_norm_stderr': 0.03827052357950756},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.49,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.49,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6184971098265896,\n",
       "    'acc_stderr': 0.03703851193099521,\n",
       "    'acc_norm': 0.6184971098265896,\n",
       "    'acc_norm_stderr': 0.03703851193099521},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4215686274509804,\n",
       "    'acc_stderr': 0.049135952012744975,\n",
       "    'acc_norm': 0.4215686274509804,\n",
       "    'acc_norm_stderr': 0.049135952012744975},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5319148936170213,\n",
       "    'acc_stderr': 0.03261936918467382,\n",
       "    'acc_norm': 0.5319148936170213,\n",
       "    'acc_norm_stderr': 0.03261936918467382},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.43859649122807015,\n",
       "    'acc_stderr': 0.04668000738510455,\n",
       "    'acc_norm': 0.43859649122807015,\n",
       "    'acc_norm_stderr': 0.04668000738510455},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370333,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370333},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.38095238095238093,\n",
       "    'acc_stderr': 0.02501074911613761,\n",
       "    'acc_norm': 0.38095238095238093,\n",
       "    'acc_norm_stderr': 0.02501074911613761},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4126984126984127,\n",
       "    'acc_stderr': 0.04403438954768176,\n",
       "    'acc_norm': 0.4126984126984127,\n",
       "    'acc_norm_stderr': 0.04403438954768176},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.04793724854411019,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.04793724854411019},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7548387096774194,\n",
       "    'acc_stderr': 0.02447224384089553,\n",
       "    'acc_norm': 0.7548387096774194,\n",
       "    'acc_norm_stderr': 0.02447224384089553},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5073891625615764,\n",
       "    'acc_stderr': 0.035176035403610105,\n",
       "    'acc_norm': 0.5073891625615764,\n",
       "    'acc_norm_stderr': 0.035176035403610105},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7515151515151515,\n",
       "    'acc_stderr': 0.033744026441394036,\n",
       "    'acc_norm': 0.7515151515151515,\n",
       "    'acc_norm_stderr': 0.033744026441394036},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7828282828282829,\n",
       "    'acc_stderr': 0.029376616484945633,\n",
       "    'acc_norm': 0.7828282828282829,\n",
       "    'acc_norm_stderr': 0.029376616484945633},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8549222797927462,\n",
       "    'acc_stderr': 0.025416343096306433,\n",
       "    'acc_norm': 0.8549222797927462,\n",
       "    'acc_norm_stderr': 0.025416343096306433},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6153846153846154,\n",
       "    'acc_stderr': 0.024666744915187208,\n",
       "    'acc_norm': 0.6153846153846154,\n",
       "    'acc_norm_stderr': 0.024666744915187208},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.35555555555555557,\n",
       "    'acc_stderr': 0.029185714949857403,\n",
       "    'acc_norm': 0.35555555555555557,\n",
       "    'acc_norm_stderr': 0.029185714949857403},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.634453781512605,\n",
       "    'acc_stderr': 0.03128217706368461,\n",
       "    'acc_norm': 0.634453781512605,\n",
       "    'acc_norm_stderr': 0.03128217706368461},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3708609271523179,\n",
       "    'acc_stderr': 0.03943966699183629,\n",
       "    'acc_norm': 0.3708609271523179,\n",
       "    'acc_norm_stderr': 0.03943966699183629},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8018348623853211,\n",
       "    'acc_stderr': 0.017090573804217902,\n",
       "    'acc_norm': 0.8018348623853211,\n",
       "    'acc_norm_stderr': 0.017090573804217902},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5416666666666666,\n",
       "    'acc_stderr': 0.033981108902946366,\n",
       "    'acc_norm': 0.5416666666666666,\n",
       "    'acc_norm_stderr': 0.033981108902946366},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7598039215686274,\n",
       "    'acc_stderr': 0.02998373305591361,\n",
       "    'acc_norm': 0.7598039215686274,\n",
       "    'acc_norm_stderr': 0.02998373305591361},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7426160337552743,\n",
       "    'acc_stderr': 0.028458820991460285,\n",
       "    'acc_norm': 0.7426160337552743,\n",
       "    'acc_norm_stderr': 0.028458820991460285},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6367713004484304,\n",
       "    'acc_stderr': 0.032277904428505,\n",
       "    'acc_norm': 0.6367713004484304,\n",
       "    'acc_norm_stderr': 0.032277904428505},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6793893129770993,\n",
       "    'acc_stderr': 0.04093329229834278,\n",
       "    'acc_norm': 0.6793893129770993,\n",
       "    'acc_norm_stderr': 0.04093329229834278},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.743801652892562,\n",
       "    'acc_stderr': 0.03984979653302871,\n",
       "    'acc_norm': 0.743801652892562,\n",
       "    'acc_norm_stderr': 0.03984979653302871},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.043300437496507416,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.043300437496507416},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7361963190184049,\n",
       "    'acc_stderr': 0.03462419931615623,\n",
       "    'acc_norm': 0.7361963190184049,\n",
       "    'acc_norm_stderr': 0.03462419931615623},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.39285714285714285,\n",
       "    'acc_stderr': 0.04635550135609976,\n",
       "    'acc_norm': 0.39285714285714285,\n",
       "    'acc_norm_stderr': 0.04635550135609976},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7281553398058253,\n",
       "    'acc_stderr': 0.044052680241409216,\n",
       "    'acc_norm': 0.7281553398058253,\n",
       "    'acc_norm_stderr': 0.044052680241409216},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.02280138253459753,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.02280138253459753},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768078,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768078},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7905491698595147,\n",
       "    'acc_stderr': 0.014551310568143704,\n",
       "    'acc_norm': 0.7905491698595147,\n",
       "    'acc_norm_stderr': 0.014551310568143704},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6994219653179191,\n",
       "    'acc_stderr': 0.0246853168672578,\n",
       "    'acc_norm': 0.6994219653179191,\n",
       "    'acc_norm_stderr': 0.0246853168672578},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.37094972067039106,\n",
       "    'acc_stderr': 0.01615591072134177,\n",
       "    'acc_norm': 0.37094972067039106,\n",
       "    'acc_norm_stderr': 0.01615591072134177},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6862745098039216,\n",
       "    'acc_stderr': 0.02656892101545715,\n",
       "    'acc_norm': 0.6862745098039216,\n",
       "    'acc_norm_stderr': 0.02656892101545715},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7041800643086816,\n",
       "    'acc_stderr': 0.025922371788818774,\n",
       "    'acc_norm': 0.7041800643086816,\n",
       "    'acc_norm_stderr': 0.025922371788818774},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6790123456790124,\n",
       "    'acc_stderr': 0.02597656601086274,\n",
       "    'acc_norm': 0.6790123456790124,\n",
       "    'acc_norm_stderr': 0.02597656601086274},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.46099290780141844,\n",
       "    'acc_stderr': 0.029736592526424438,\n",
       "    'acc_norm': 0.46099290780141844,\n",
       "    'acc_norm_stderr': 0.029736592526424438},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.41003911342894395,\n",
       "    'acc_stderr': 0.012561837621962044,\n",
       "    'acc_norm': 0.41003911342894395,\n",
       "    'acc_norm_stderr': 0.012561837621962044},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6507352941176471,\n",
       "    'acc_stderr': 0.028959755196824866,\n",
       "    'acc_norm': 0.6507352941176471,\n",
       "    'acc_norm_stderr': 0.028959755196824866},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6290849673202614,\n",
       "    'acc_stderr': 0.019542101564854128,\n",
       "    'acc_norm': 0.6290849673202614,\n",
       "    'acc_norm_stderr': 0.019542101564854128},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.04607582090719976,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.04607582090719976},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6816326530612244,\n",
       "    'acc_stderr': 0.029822533793982066,\n",
       "    'acc_norm': 0.6816326530612244,\n",
       "    'acc_norm_stderr': 0.029822533793982066},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8009950248756219,\n",
       "    'acc_stderr': 0.028231365092758406,\n",
       "    'acc_norm': 0.8009950248756219,\n",
       "    'acc_norm_stderr': 0.028231365092758406},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5180722891566265,\n",
       "    'acc_stderr': 0.03889951252827216,\n",
       "    'acc_norm': 0.5180722891566265,\n",
       "    'acc_norm_stderr': 0.03889951252827216},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.42717258261933905,\n",
       "    'mc1_stderr': 0.017316834410963926,\n",
       "    'mc2': 0.5790339154881958,\n",
       "    'mc2_stderr': 0.015362629183533977}}},\n",
       " 'zephyr-7b-beta': {'key': 'zephyr-7b-beta',\n",
       "  'Model': 'Zephyr-7b-beta',\n",
       "  'MT-bench (score)': '7.34',\n",
       "  'MMLU': '0.614',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'HuggingFace',\n",
       "  'Link': 'https://huggingface.co/HuggingFaceH4/zephyr-7b-beta',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.590443686006826,\n",
       "    'acc_stderr': 0.014370358632472437,\n",
       "    'acc_norm': 0.6203071672354948,\n",
       "    'acc_norm_stderr': 0.01418211986697487},\n",
       "   'harness|hellaswag|10': {'acc': 0.6491734714200359,\n",
       "    'acc_stderr': 0.004762534245488399,\n",
       "    'acc_norm': 0.8435570603465445,\n",
       "    'acc_norm_stderr': 0.003625323221166244},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.04218506215368881,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.04218506215368881},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6052631578947368,\n",
       "    'acc_stderr': 0.039777499346220734,\n",
       "    'acc_norm': 0.6052631578947368,\n",
       "    'acc_norm_stderr': 0.039777499346220734},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.660377358490566,\n",
       "    'acc_stderr': 0.02914690474779833,\n",
       "    'acc_norm': 0.660377358490566,\n",
       "    'acc_norm_stderr': 0.02914690474779833},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7013888888888888,\n",
       "    'acc_stderr': 0.03827052357950756,\n",
       "    'acc_norm': 0.7013888888888888,\n",
       "    'acc_norm_stderr': 0.03827052357950756},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.04878317312145633,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.04878317312145633},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6416184971098265,\n",
       "    'acc_stderr': 0.03656343653353159,\n",
       "    'acc_norm': 0.6416184971098265,\n",
       "    'acc_norm_stderr': 0.03656343653353159},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4215686274509804,\n",
       "    'acc_stderr': 0.049135952012744975,\n",
       "    'acc_norm': 0.4215686274509804,\n",
       "    'acc_norm_stderr': 0.049135952012744975},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5191489361702127,\n",
       "    'acc_stderr': 0.032662042990646775,\n",
       "    'acc_norm': 0.5191489361702127,\n",
       "    'acc_norm_stderr': 0.032662042990646775},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.42105263157894735,\n",
       "    'acc_stderr': 0.046446020912223177,\n",
       "    'acc_norm': 0.42105263157894735,\n",
       "    'acc_norm_stderr': 0.046446020912223177},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5379310344827586,\n",
       "    'acc_stderr': 0.04154659671707548,\n",
       "    'acc_norm': 0.5379310344827586,\n",
       "    'acc_norm_stderr': 0.04154659671707548},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36772486772486773,\n",
       "    'acc_stderr': 0.02483383982556242,\n",
       "    'acc_norm': 0.36772486772486773,\n",
       "    'acc_norm_stderr': 0.02483383982556242},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.044444444444444495,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.044444444444444495},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.049236596391733084,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.049236596391733084},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7483870967741936,\n",
       "    'acc_stderr': 0.024685979286239963,\n",
       "    'acc_norm': 0.7483870967741936,\n",
       "    'acc_norm_stderr': 0.024685979286239963},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.035158955511656986,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.035158955511656986},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.04725815626252609,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.04725815626252609},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7575757575757576,\n",
       "    'acc_stderr': 0.03346409881055953,\n",
       "    'acc_norm': 0.7575757575757576,\n",
       "    'acc_norm_stderr': 0.03346409881055953},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7323232323232324,\n",
       "    'acc_stderr': 0.031544498882702866,\n",
       "    'acc_norm': 0.7323232323232324,\n",
       "    'acc_norm_stderr': 0.031544498882702866},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8238341968911918,\n",
       "    'acc_stderr': 0.02749350424454805,\n",
       "    'acc_norm': 0.8238341968911918,\n",
       "    'acc_norm_stderr': 0.02749350424454805},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6282051282051282,\n",
       "    'acc_stderr': 0.024503472557110936,\n",
       "    'acc_norm': 0.6282051282051282,\n",
       "    'acc_norm_stderr': 0.024503472557110936},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.34444444444444444,\n",
       "    'acc_stderr': 0.028972648884844267,\n",
       "    'acc_norm': 0.34444444444444444,\n",
       "    'acc_norm_stderr': 0.028972648884844267},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.0303883535518868,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.0303883535518868},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2980132450331126,\n",
       "    'acc_stderr': 0.037345356767871984,\n",
       "    'acc_norm': 0.2980132450331126,\n",
       "    'acc_norm_stderr': 0.037345356767871984},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8091743119266055,\n",
       "    'acc_stderr': 0.01684767640009109,\n",
       "    'acc_norm': 0.8091743119266055,\n",
       "    'acc_norm_stderr': 0.01684767640009109},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.033953227263757976,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.033953227263757976},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.02910225438967407,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.02910225438967407},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7341772151898734,\n",
       "    'acc_stderr': 0.028756799629658346,\n",
       "    'acc_norm': 0.7341772151898734,\n",
       "    'acc_norm_stderr': 0.028756799629658346},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6367713004484304,\n",
       "    'acc_stderr': 0.032277904428505,\n",
       "    'acc_norm': 0.6367713004484304,\n",
       "    'acc_norm_stderr': 0.032277904428505},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6717557251908397,\n",
       "    'acc_stderr': 0.04118438565806298,\n",
       "    'acc_norm': 0.6717557251908397,\n",
       "    'acc_norm_stderr': 0.04118438565806298},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7107438016528925,\n",
       "    'acc_stderr': 0.04139112727635463,\n",
       "    'acc_norm': 0.7107438016528925,\n",
       "    'acc_norm_stderr': 0.04139112727635463},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.040191074725573483,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.040191074725573483},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7055214723926381,\n",
       "    'acc_stderr': 0.03581165790474082,\n",
       "    'acc_norm': 0.7055214723926381,\n",
       "    'acc_norm_stderr': 0.03581165790474082},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.3482142857142857,\n",
       "    'acc_stderr': 0.04521829902833585,\n",
       "    'acc_norm': 0.3482142857142857,\n",
       "    'acc_norm_stderr': 0.04521829902833585},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.043546310772605956,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.043546310772605956},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8803418803418803,\n",
       "    'acc_stderr': 0.02126271940040697,\n",
       "    'acc_norm': 0.8803418803418803,\n",
       "    'acc_norm_stderr': 0.02126271940040697},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.014866821664709588,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.014866821664709588},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6705202312138728,\n",
       "    'acc_stderr': 0.02530525813187972,\n",
       "    'acc_norm': 0.6705202312138728,\n",
       "    'acc_norm_stderr': 0.02530525813187972},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.3418994413407821,\n",
       "    'acc_stderr': 0.015864506461604637,\n",
       "    'acc_norm': 0.3418994413407821,\n",
       "    'acc_norm_stderr': 0.015864506461604637},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6830065359477124,\n",
       "    'acc_stderr': 0.026643278474508755,\n",
       "    'acc_norm': 0.6830065359477124,\n",
       "    'acc_norm_stderr': 0.026643278474508755},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6688102893890675,\n",
       "    'acc_stderr': 0.02673062072800491,\n",
       "    'acc_norm': 0.6688102893890675,\n",
       "    'acc_norm_stderr': 0.02673062072800491},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.02622964917882117,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.02622964917882117},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4929078014184397,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.4929078014184397,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4276401564537158,\n",
       "    'acc_stderr': 0.012635799922765844,\n",
       "    'acc_norm': 0.4276401564537158,\n",
       "    'acc_norm_stderr': 0.012635799922765844},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6470588235294118,\n",
       "    'acc_stderr': 0.029029422815681397,\n",
       "    'acc_norm': 0.6470588235294118,\n",
       "    'acc_norm_stderr': 0.029029422815681397},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.619281045751634,\n",
       "    'acc_stderr': 0.019643801557924803,\n",
       "    'acc_norm': 0.619281045751634,\n",
       "    'acc_norm_stderr': 0.019643801557924803},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302506,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302506},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6816326530612244,\n",
       "    'acc_stderr': 0.029822533793982066,\n",
       "    'acc_norm': 0.6816326530612244,\n",
       "    'acc_norm_stderr': 0.029822533793982066},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8009950248756219,\n",
       "    'acc_stderr': 0.028231365092758406,\n",
       "    'acc_norm': 0.8009950248756219,\n",
       "    'acc_norm_stderr': 0.028231365092758406},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932262,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932262},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5180722891566265,\n",
       "    'acc_stderr': 0.03889951252827216,\n",
       "    'acc_norm': 0.5180722891566265,\n",
       "    'acc_norm_stderr': 0.03889951252827216},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8070175438596491,\n",
       "    'acc_stderr': 0.030267457554898458,\n",
       "    'acc_norm': 0.8070175438596491,\n",
       "    'acc_norm_stderr': 0.030267457554898458},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.40636474908200737,\n",
       "    'mc1_stderr': 0.017193835812093893,\n",
       "    'mc2': 0.5744916942762855,\n",
       "    'mc2_stderr': 0.015742095840959796},\n",
       "   'harness|winogrande|5': {'acc': 0.7774269928966061,\n",
       "    'acc_stderr': 0.011690933809712667},\n",
       "   'harness|drop|3': {'em': 0.004928691275167785,\n",
       "    'em_stderr': 0.0007171872517059793,\n",
       "    'f1': 0.09662437080536909,\n",
       "    'f1_stderr': 0.0018807376338089597},\n",
       "   'harness|gsm8k|5': {'acc': 0.12736921910538287,\n",
       "    'acc_stderr': 0.009183110326737829},\n",
       "   'all': {'acc': 0.6058001121844437,\n",
       "    'acc_stderr': 0.033164878802299444,\n",
       "    'acc_norm': 0.6148009779899025,\n",
       "    'acc_norm_stderr': 0.033912849373118566,\n",
       "    'mc1': 0.40636474908200737,\n",
       "    'mc1_stderr': 0.017193835812093893,\n",
       "    'mc2': 0.5744916942762855,\n",
       "    'mc2_stderr': 0.015742095840959796,\n",
       "    'em': 0.004928691275167785,\n",
       "    'em_stderr': 0.0007171872517059793,\n",
       "    'f1': 0.09662437080536909,\n",
       "    'f1_stderr': 0.0018807376338089597}}}}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_hf_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'harness|arc:challenge|25': {'acc': 0.590443686006826,\n",
       "  'acc_stderr': 0.014370358632472437,\n",
       "  'acc_norm': 0.6203071672354948,\n",
       "  'acc_norm_stderr': 0.01418211986697487},\n",
       " 'harness|hellaswag|10': {'acc': 0.6491734714200359,\n",
       "  'acc_stderr': 0.004762534245488399,\n",
       "  'acc_norm': 0.8435570603465445,\n",
       "  'acc_norm_stderr': 0.003625323221166244},\n",
       " 'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.36,\n",
       "  'acc_stderr': 0.04824181513244218,\n",
       "  'acc_norm': 0.36,\n",
       "  'acc_norm_stderr': 0.04824181513244218},\n",
       " 'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "  'acc_stderr': 0.04218506215368881,\n",
       "  'acc_norm': 0.6074074074074074,\n",
       "  'acc_norm_stderr': 0.04218506215368881},\n",
       " 'harness|hendrycksTest-astronomy|5': {'acc': 0.6052631578947368,\n",
       "  'acc_stderr': 0.039777499346220734,\n",
       "  'acc_norm': 0.6052631578947368,\n",
       "  'acc_norm_stderr': 0.039777499346220734},\n",
       " 'harness|hendrycksTest-business_ethics|5': {'acc': 0.56,\n",
       "  'acc_stderr': 0.04988876515698589,\n",
       "  'acc_norm': 0.56,\n",
       "  'acc_norm_stderr': 0.04988876515698589},\n",
       " 'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.660377358490566,\n",
       "  'acc_stderr': 0.02914690474779833,\n",
       "  'acc_norm': 0.660377358490566,\n",
       "  'acc_norm_stderr': 0.02914690474779833},\n",
       " 'harness|hendrycksTest-college_biology|5': {'acc': 0.7013888888888888,\n",
       "  'acc_stderr': 0.03827052357950756,\n",
       "  'acc_norm': 0.7013888888888888,\n",
       "  'acc_norm_stderr': 0.03827052357950756},\n",
       " 'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "  'acc_stderr': 0.050211673156867795,\n",
       "  'acc_norm': 0.48,\n",
       "  'acc_norm_stderr': 0.050211673156867795},\n",
       " 'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "  'acc_stderr': 0.050211673156867795,\n",
       "  'acc_norm': 0.48,\n",
       "  'acc_norm_stderr': 0.050211673156867795},\n",
       " 'harness|hendrycksTest-college_mathematics|5': {'acc': 0.38,\n",
       "  'acc_stderr': 0.04878317312145633,\n",
       "  'acc_norm': 0.38,\n",
       "  'acc_norm_stderr': 0.04878317312145633},\n",
       " 'harness|hendrycksTest-college_medicine|5': {'acc': 0.6416184971098265,\n",
       "  'acc_stderr': 0.03656343653353159,\n",
       "  'acc_norm': 0.6416184971098265,\n",
       "  'acc_norm_stderr': 0.03656343653353159},\n",
       " 'harness|hendrycksTest-college_physics|5': {'acc': 0.4215686274509804,\n",
       "  'acc_stderr': 0.049135952012744975,\n",
       "  'acc_norm': 0.4215686274509804,\n",
       "  'acc_norm_stderr': 0.049135952012744975},\n",
       " 'harness|hendrycksTest-computer_security|5': {'acc': 0.73,\n",
       "  'acc_stderr': 0.044619604333847394,\n",
       "  'acc_norm': 0.73,\n",
       "  'acc_norm_stderr': 0.044619604333847394},\n",
       " 'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5191489361702127,\n",
       "  'acc_stderr': 0.032662042990646775,\n",
       "  'acc_norm': 0.5191489361702127,\n",
       "  'acc_norm_stderr': 0.032662042990646775},\n",
       " 'harness|hendrycksTest-econometrics|5': {'acc': 0.42105263157894735,\n",
       "  'acc_stderr': 0.046446020912223177,\n",
       "  'acc_norm': 0.42105263157894735,\n",
       "  'acc_norm_stderr': 0.046446020912223177},\n",
       " 'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5379310344827586,\n",
       "  'acc_stderr': 0.04154659671707548,\n",
       "  'acc_norm': 0.5379310344827586,\n",
       "  'acc_norm_stderr': 0.04154659671707548},\n",
       " 'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36772486772486773,\n",
       "  'acc_stderr': 0.02483383982556242,\n",
       "  'acc_norm': 0.36772486772486773,\n",
       "  'acc_norm_stderr': 0.02483383982556242},\n",
       " 'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "  'acc_stderr': 0.044444444444444495,\n",
       "  'acc_norm': 0.4444444444444444,\n",
       "  'acc_norm_stderr': 0.044444444444444495},\n",
       " 'harness|hendrycksTest-global_facts|5': {'acc': 0.4,\n",
       "  'acc_stderr': 0.049236596391733084,\n",
       "  'acc_norm': 0.4,\n",
       "  'acc_norm_stderr': 0.049236596391733084},\n",
       " 'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7483870967741936,\n",
       "  'acc_stderr': 0.024685979286239963,\n",
       "  'acc_norm': 0.7483870967741936,\n",
       "  'acc_norm_stderr': 0.024685979286239963},\n",
       " 'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "  'acc_stderr': 0.035158955511656986,\n",
       "  'acc_norm': 0.5172413793103449,\n",
       "  'acc_norm_stderr': 0.035158955511656986},\n",
       " 'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.67,\n",
       "  'acc_stderr': 0.04725815626252609,\n",
       "  'acc_norm': 0.67,\n",
       "  'acc_norm_stderr': 0.04725815626252609},\n",
       " 'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7575757575757576,\n",
       "  'acc_stderr': 0.03346409881055953,\n",
       "  'acc_norm': 0.7575757575757576,\n",
       "  'acc_norm_stderr': 0.03346409881055953},\n",
       " 'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7323232323232324,\n",
       "  'acc_stderr': 0.031544498882702866,\n",
       "  'acc_norm': 0.7323232323232324,\n",
       "  'acc_norm_stderr': 0.031544498882702866},\n",
       " 'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8238341968911918,\n",
       "  'acc_stderr': 0.02749350424454805,\n",
       "  'acc_norm': 0.8238341968911918,\n",
       "  'acc_norm_stderr': 0.02749350424454805},\n",
       " 'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6282051282051282,\n",
       "  'acc_stderr': 0.024503472557110936,\n",
       "  'acc_norm': 0.6282051282051282,\n",
       "  'acc_norm_stderr': 0.024503472557110936},\n",
       " 'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.34444444444444444,\n",
       "  'acc_stderr': 0.028972648884844267,\n",
       "  'acc_norm': 0.34444444444444444,\n",
       "  'acc_norm_stderr': 0.028972648884844267},\n",
       " 'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6764705882352942,\n",
       "  'acc_stderr': 0.0303883535518868,\n",
       "  'acc_norm': 0.6764705882352942,\n",
       "  'acc_norm_stderr': 0.0303883535518868},\n",
       " 'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2980132450331126,\n",
       "  'acc_stderr': 0.037345356767871984,\n",
       "  'acc_norm': 0.2980132450331126,\n",
       "  'acc_norm_stderr': 0.037345356767871984},\n",
       " 'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8091743119266055,\n",
       "  'acc_stderr': 0.01684767640009109,\n",
       "  'acc_norm': 0.8091743119266055,\n",
       "  'acc_norm_stderr': 0.01684767640009109},\n",
       " 'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5462962962962963,\n",
       "  'acc_stderr': 0.033953227263757976,\n",
       "  'acc_norm': 0.5462962962962963,\n",
       "  'acc_norm_stderr': 0.033953227263757976},\n",
       " 'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "  'acc_stderr': 0.02910225438967407,\n",
       "  'acc_norm': 0.7794117647058824,\n",
       "  'acc_norm_stderr': 0.02910225438967407},\n",
       " 'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7341772151898734,\n",
       "  'acc_stderr': 0.028756799629658346,\n",
       "  'acc_norm': 0.7341772151898734,\n",
       "  'acc_norm_stderr': 0.028756799629658346},\n",
       " 'harness|hendrycksTest-human_aging|5': {'acc': 0.6367713004484304,\n",
       "  'acc_stderr': 0.032277904428505,\n",
       "  'acc_norm': 0.6367713004484304,\n",
       "  'acc_norm_stderr': 0.032277904428505},\n",
       " 'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6717557251908397,\n",
       "  'acc_stderr': 0.04118438565806298,\n",
       "  'acc_norm': 0.6717557251908397,\n",
       "  'acc_norm_stderr': 0.04118438565806298},\n",
       " 'harness|hendrycksTest-international_law|5': {'acc': 0.7107438016528925,\n",
       "  'acc_stderr': 0.04139112727635463,\n",
       "  'acc_norm': 0.7107438016528925,\n",
       "  'acc_norm_stderr': 0.04139112727635463},\n",
       " 'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7777777777777778,\n",
       "  'acc_stderr': 0.040191074725573483,\n",
       "  'acc_norm': 0.7777777777777778,\n",
       "  'acc_norm_stderr': 0.040191074725573483},\n",
       " 'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7055214723926381,\n",
       "  'acc_stderr': 0.03581165790474082,\n",
       "  'acc_norm': 0.7055214723926381,\n",
       "  'acc_norm_stderr': 0.03581165790474082},\n",
       " 'harness|hendrycksTest-machine_learning|5': {'acc': 0.3482142857142857,\n",
       "  'acc_stderr': 0.04521829902833585,\n",
       "  'acc_norm': 0.3482142857142857,\n",
       "  'acc_norm_stderr': 0.04521829902833585},\n",
       " 'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "  'acc_stderr': 0.043546310772605956,\n",
       "  'acc_norm': 0.7378640776699029,\n",
       "  'acc_norm_stderr': 0.043546310772605956},\n",
       " 'harness|hendrycksTest-marketing|5': {'acc': 0.8803418803418803,\n",
       "  'acc_stderr': 0.02126271940040697,\n",
       "  'acc_norm': 0.8803418803418803,\n",
       "  'acc_norm_stderr': 0.02126271940040697},\n",
       " 'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "  'acc_stderr': 0.045604802157206845,\n",
       "  'acc_norm': 0.71,\n",
       "  'acc_norm_stderr': 0.045604802157206845},\n",
       " 'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7777777777777778,\n",
       "  'acc_stderr': 0.014866821664709588,\n",
       "  'acc_norm': 0.7777777777777778,\n",
       "  'acc_norm_stderr': 0.014866821664709588},\n",
       " 'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6705202312138728,\n",
       "  'acc_stderr': 0.02530525813187972,\n",
       "  'acc_norm': 0.6705202312138728,\n",
       "  'acc_norm_stderr': 0.02530525813187972},\n",
       " 'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.3418994413407821,\n",
       "  'acc_stderr': 0.015864506461604637,\n",
       "  'acc_norm': 0.3418994413407821,\n",
       "  'acc_norm_stderr': 0.015864506461604637},\n",
       " 'harness|hendrycksTest-nutrition|5': {'acc': 0.6830065359477124,\n",
       "  'acc_stderr': 0.026643278474508755,\n",
       "  'acc_norm': 0.6830065359477124,\n",
       "  'acc_norm_stderr': 0.026643278474508755},\n",
       " 'harness|hendrycksTest-philosophy|5': {'acc': 0.6688102893890675,\n",
       "  'acc_stderr': 0.02673062072800491,\n",
       "  'acc_norm': 0.6688102893890675,\n",
       "  'acc_norm_stderr': 0.02673062072800491},\n",
       " 'harness|hendrycksTest-prehistory|5': {'acc': 0.6666666666666666,\n",
       "  'acc_stderr': 0.02622964917882117,\n",
       "  'acc_norm': 0.6666666666666666,\n",
       "  'acc_norm_stderr': 0.02622964917882117},\n",
       " 'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4929078014184397,\n",
       "  'acc_stderr': 0.02982449855912901,\n",
       "  'acc_norm': 0.4929078014184397,\n",
       "  'acc_norm_stderr': 0.02982449855912901},\n",
       " 'harness|hendrycksTest-professional_law|5': {'acc': 0.4276401564537158,\n",
       "  'acc_stderr': 0.012635799922765844,\n",
       "  'acc_norm': 0.4276401564537158,\n",
       "  'acc_norm_stderr': 0.012635799922765844},\n",
       " 'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6470588235294118,\n",
       "  'acc_stderr': 0.029029422815681397,\n",
       "  'acc_norm': 0.6470588235294118,\n",
       "  'acc_norm_stderr': 0.029029422815681397},\n",
       " 'harness|hendrycksTest-professional_psychology|5': {'acc': 0.619281045751634,\n",
       "  'acc_stderr': 0.019643801557924803,\n",
       "  'acc_norm': 0.619281045751634,\n",
       "  'acc_norm_stderr': 0.019643801557924803},\n",
       " 'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "  'acc_stderr': 0.04525393596302506,\n",
       "  'acc_norm': 0.6636363636363637,\n",
       "  'acc_norm_stderr': 0.04525393596302506},\n",
       " 'harness|hendrycksTest-security_studies|5': {'acc': 0.6816326530612244,\n",
       "  'acc_stderr': 0.029822533793982066,\n",
       "  'acc_norm': 0.6816326530612244,\n",
       "  'acc_norm_stderr': 0.029822533793982066},\n",
       " 'harness|hendrycksTest-sociology|5': {'acc': 0.8009950248756219,\n",
       "  'acc_stderr': 0.028231365092758406,\n",
       "  'acc_norm': 0.8009950248756219,\n",
       "  'acc_norm_stderr': 0.028231365092758406},\n",
       " 'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "  'acc_stderr': 0.04163331998932262,\n",
       "  'acc_norm': 0.78,\n",
       "  'acc_norm_stderr': 0.04163331998932262},\n",
       " 'harness|hendrycksTest-virology|5': {'acc': 0.5180722891566265,\n",
       "  'acc_stderr': 0.03889951252827216,\n",
       "  'acc_norm': 0.5180722891566265,\n",
       "  'acc_norm_stderr': 0.03889951252827216},\n",
       " 'harness|hendrycksTest-world_religions|5': {'acc': 0.8070175438596491,\n",
       "  'acc_stderr': 0.030267457554898458,\n",
       "  'acc_norm': 0.8070175438596491,\n",
       "  'acc_norm_stderr': 0.030267457554898458},\n",
       " 'harness|truthfulqa:mc|0': {'mc1': 0.40636474908200737,\n",
       "  'mc1_stderr': 0.017193835812093893,\n",
       "  'mc2': 0.5744916942762855,\n",
       "  'mc2_stderr': 0.015742095840959796},\n",
       " 'harness|winogrande|5': {'acc': 0.7774269928966061,\n",
       "  'acc_stderr': 0.011690933809712667},\n",
       " 'harness|drop|3': {'em': 0.004928691275167785,\n",
       "  'em_stderr': 0.0007171872517059793,\n",
       "  'f1': 0.09662437080536909,\n",
       "  'f1_stderr': 0.0018807376338089597},\n",
       " 'harness|gsm8k|5': {'acc': 0.12736921910538287,\n",
       "  'acc_stderr': 0.009183110326737829},\n",
       " 'all': {'acc': 0.6058001121844437,\n",
       "  'acc_stderr': 0.033164878802299444,\n",
       "  'acc_norm': 0.6148009779899025,\n",
       "  'acc_norm_stderr': 0.033912849373118566,\n",
       "  'mc1': 0.40636474908200737,\n",
       "  'mc1_stderr': 0.017193835812093893,\n",
       "  'mc2': 0.5744916942762855,\n",
       "  'mc2_stderr': 0.015742095840959796,\n",
       "  'em': 0.004928691275167785,\n",
       "  'em_stderr': 0.0007171872517059793,\n",
       "  'f1': 0.09662437080536909,\n",
       "  'f1_stderr': 0.0018807376338089597}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmarks = []\n",
    "for model in filtered_hf_models.values():\n",
    "    benchmarks.append(list(model['results'].keys()))\n",
    "    \n",
    "# Intersection of benchmarks\n",
    "common_benchmarks = benchmarks[0]\n",
    "for benchmark in benchmarks:\n",
    "    common_benchmarks = list(set(common_benchmarks) & set(benchmark))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_benchmarks = [x for x in common_benchmarks if '|' in x]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'chatglm2-6b': {'key': 'chatglm2-6b',\n",
       "  'Model': 'ChatGLM2-6B',\n",
       "  'MT-bench (score)': '4.96',\n",
       "  'MMLU': '0.455',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'Tsinghua',\n",
       "  'Link': 'https://huggingface.co/THUDM/chatglm2-6b',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.37457337883959047,\n",
       "    'acc_stderr': 0.014144193471893444,\n",
       "    'acc_norm': 0.38822525597269625,\n",
       "    'acc_norm_stderr': 0.014241614207414044},\n",
       "   'harness|hellaswag|10': {'acc': 0.4596693885680143,\n",
       "    'acc_stderr': 0.004973522582431221,\n",
       "    'acc_norm': 0.5902210714997013,\n",
       "    'acc_norm_stderr': 0.004907877144720029},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4148148148148148,\n",
       "    'acc_stderr': 0.042561937679014075,\n",
       "    'acc_norm': 0.4148148148148148,\n",
       "    'acc_norm_stderr': 0.042561937679014075},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5328947368421053,\n",
       "    'acc_stderr': 0.04060127035236395,\n",
       "    'acc_norm': 0.5328947368421053,\n",
       "    'acc_norm_stderr': 0.04060127035236395},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.61,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.61,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.4867924528301887,\n",
       "    'acc_stderr': 0.030762134874500476,\n",
       "    'acc_norm': 0.4867924528301887,\n",
       "    'acc_norm_stderr': 0.030762134874500476},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.04076663253918567,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.04076663253918567},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.42196531791907516,\n",
       "    'acc_stderr': 0.0376574669386515,\n",
       "    'acc_norm': 0.42196531791907516,\n",
       "    'acc_norm_stderr': 0.0376574669386515},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.21568627450980393,\n",
       "    'acc_stderr': 0.040925639582376556,\n",
       "    'acc_norm': 0.21568627450980393,\n",
       "    'acc_norm_stderr': 0.040925639582376556},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.03202563076101736,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.03202563076101736},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.30701754385964913,\n",
       "    'acc_stderr': 0.043391383225798615,\n",
       "    'acc_norm': 0.30701754385964913,\n",
       "    'acc_norm_stderr': 0.043391383225798615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4689655172413793,\n",
       "    'acc_stderr': 0.04158632762097828,\n",
       "    'acc_norm': 0.4689655172413793,\n",
       "    'acc_norm_stderr': 0.04158632762097828},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.31746031746031744,\n",
       "    'acc_stderr': 0.023973861998992072,\n",
       "    'acc_norm': 0.31746031746031744,\n",
       "    'acc_norm_stderr': 0.023973861998992072},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3492063492063492,\n",
       "    'acc_stderr': 0.04263906892795133,\n",
       "    'acc_norm': 0.3492063492063492,\n",
       "    'acc_norm_stderr': 0.04263906892795133},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.5580645161290323,\n",
       "    'acc_stderr': 0.02825155790684973,\n",
       "    'acc_norm': 0.5580645161290323,\n",
       "    'acc_norm_stderr': 0.02825155790684973},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.45320197044334976,\n",
       "    'acc_stderr': 0.03502544650845872,\n",
       "    'acc_norm': 0.45320197044334976,\n",
       "    'acc_norm_stderr': 0.03502544650845872},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.42,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.42,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.038254602783800246,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.038254602783800246},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.035402943770953675,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.035402943770953675},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.5699481865284974,\n",
       "    'acc_stderr': 0.03572954333144809,\n",
       "    'acc_norm': 0.5699481865284974,\n",
       "    'acc_norm_stderr': 0.03572954333144809},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.44871794871794873,\n",
       "    'acc_stderr': 0.025217315184846482,\n",
       "    'acc_norm': 0.44871794871794873,\n",
       "    'acc_norm_stderr': 0.025217315184846482},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.26296296296296295,\n",
       "    'acc_stderr': 0.026842057873833706,\n",
       "    'acc_norm': 0.26296296296296295,\n",
       "    'acc_norm_stderr': 0.026842057873833706},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.4327731092436975,\n",
       "    'acc_stderr': 0.03218358107742613,\n",
       "    'acc_norm': 0.4327731092436975,\n",
       "    'acc_norm_stderr': 0.03218358107742613},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2913907284768212,\n",
       "    'acc_stderr': 0.03710185726119994,\n",
       "    'acc_norm': 0.2913907284768212,\n",
       "    'acc_norm_stderr': 0.03710185726119994},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.5926605504587156,\n",
       "    'acc_stderr': 0.021065986244412895,\n",
       "    'acc_norm': 0.5926605504587156,\n",
       "    'acc_norm_stderr': 0.021065986244412895},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.33796296296296297,\n",
       "    'acc_stderr': 0.032259413526312945,\n",
       "    'acc_norm': 0.33796296296296297,\n",
       "    'acc_norm_stderr': 0.032259413526312945},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.5882352941176471,\n",
       "    'acc_stderr': 0.03454236585380609,\n",
       "    'acc_norm': 0.5882352941176471,\n",
       "    'acc_norm_stderr': 0.03454236585380609},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6624472573839663,\n",
       "    'acc_stderr': 0.03078154910202621,\n",
       "    'acc_norm': 0.6624472573839663,\n",
       "    'acc_norm_stderr': 0.03078154910202621},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.4663677130044843,\n",
       "    'acc_stderr': 0.033481800170603065,\n",
       "    'acc_norm': 0.4663677130044843,\n",
       "    'acc_norm_stderr': 0.033481800170603065},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.48091603053435117,\n",
       "    'acc_stderr': 0.04382094705550988,\n",
       "    'acc_norm': 0.48091603053435117,\n",
       "    'acc_norm_stderr': 0.04382094705550988},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.6033057851239669,\n",
       "    'acc_stderr': 0.04465869780531009,\n",
       "    'acc_norm': 0.6033057851239669,\n",
       "    'acc_norm_stderr': 0.04465869780531009},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04766075165356461,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04766075165356461},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.49079754601226994,\n",
       "    'acc_stderr': 0.03927705600787443,\n",
       "    'acc_norm': 0.49079754601226994,\n",
       "    'acc_norm_stderr': 0.03927705600787443},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.4375,\n",
       "    'acc_stderr': 0.04708567521880525,\n",
       "    'acc_norm': 0.4375,\n",
       "    'acc_norm_stderr': 0.04708567521880525},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6213592233009708,\n",
       "    'acc_stderr': 0.048026946982589726,\n",
       "    'acc_norm': 0.6213592233009708,\n",
       "    'acc_norm_stderr': 0.048026946982589726},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.6923076923076923,\n",
       "    'acc_stderr': 0.030236389942173102,\n",
       "    'acc_norm': 0.6923076923076923,\n",
       "    'acc_norm_stderr': 0.030236389942173102},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.5964240102171137,\n",
       "    'acc_stderr': 0.017544332237926417,\n",
       "    'acc_norm': 0.5964240102171137,\n",
       "    'acc_norm_stderr': 0.017544332237926417},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5289017341040463,\n",
       "    'acc_stderr': 0.026874085883518348,\n",
       "    'acc_norm': 0.5289017341040463,\n",
       "    'acc_norm_stderr': 0.026874085883518348},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.23575418994413408,\n",
       "    'acc_stderr': 0.014196375686290804,\n",
       "    'acc_norm': 0.23575418994413408,\n",
       "    'acc_norm_stderr': 0.014196375686290804},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5326797385620915,\n",
       "    'acc_stderr': 0.028568699752225868,\n",
       "    'acc_norm': 0.5326797385620915,\n",
       "    'acc_norm_stderr': 0.028568699752225868},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.4983922829581994,\n",
       "    'acc_stderr': 0.02839794490780661,\n",
       "    'acc_norm': 0.4983922829581994,\n",
       "    'acc_norm_stderr': 0.02839794490780661},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.49382716049382713,\n",
       "    'acc_stderr': 0.02781862396258329,\n",
       "    'acc_norm': 0.49382716049382713,\n",
       "    'acc_norm_stderr': 0.02781862396258329},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.02812163604063988,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.02812163604063988},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3559322033898305,\n",
       "    'acc_stderr': 0.012228645537277566,\n",
       "    'acc_norm': 0.3559322033898305,\n",
       "    'acc_norm_stderr': 0.012228645537277566},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.35294117647058826,\n",
       "    'acc_stderr': 0.029029422815681393,\n",
       "    'acc_norm': 0.35294117647058826,\n",
       "    'acc_norm_stderr': 0.029029422815681393},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.020102583895887188,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.020102583895887188},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5818181818181818,\n",
       "    'acc_stderr': 0.04724577405731572,\n",
       "    'acc_norm': 0.5818181818181818,\n",
       "    'acc_norm_stderr': 0.04724577405731572},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5673469387755102,\n",
       "    'acc_stderr': 0.031717528240626645,\n",
       "    'acc_norm': 0.5673469387755102,\n",
       "    'acc_norm_stderr': 0.031717528240626645},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6218905472636815,\n",
       "    'acc_stderr': 0.034288678487786564,\n",
       "    'acc_norm': 0.6218905472636815,\n",
       "    'acc_norm_stderr': 0.034288678487786564},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.41566265060240964,\n",
       "    'acc_stderr': 0.03836722176598052,\n",
       "    'acc_norm': 0.41566265060240964,\n",
       "    'acc_norm_stderr': 0.03836722176598052},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.038295098689947286,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.038295098689947286},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.2998776009791922,\n",
       "    'mc1_stderr': 0.016040352966713616,\n",
       "    'mc2': 0.48081249614578586,\n",
       "    'mc2_stderr': 0.01612488256095027},\n",
       "   'all': {'acc': 0.46492840933020524,\n",
       "    'acc_stderr': 0.03535258865761692,\n",
       "    'acc_norm': 0.46737253746689666,\n",
       "    'acc_norm_stderr': 0.035353127221986566,\n",
       "    'mc1': 0.2998776009791922,\n",
       "    'mc1_stderr': 0.016040352966713616,\n",
       "    'mc2': 0.48081249614578586,\n",
       "    'mc2_stderr': 0.01612488256095027}}},\n",
       " 'codellama-34b-instruct': {'key': 'codellama-34b-instruct',\n",
       "  'Model': 'CodeLlama-34B-instruct',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.537',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/codellama/CodeLlama-34b-Instruct-hf',\n",
       "  'results': {'harness|drop|3': {'em': 0.0014681208053691276,\n",
       "    'em_stderr': 0.00039210421902985756,\n",
       "    'f1': 0.05437604865771826,\n",
       "    'f1_stderr': 0.0011898517432786567},\n",
       "   'harness|gsm8k|5': {'acc': 0.3100833965125095,\n",
       "    'acc_stderr': 0.012740305717376268},\n",
       "   'harness|winogrande|5': {'acc': 0.745067087608524,\n",
       "    'acc_stderr': 0.012248806969376422},\n",
       "   'all': {'em': 0.0014681208053691276,\n",
       "    'em_stderr': 0.00039210421902985756,\n",
       "    'f1': 0.05437604865771826,\n",
       "    'f1_stderr': 0.0011898517432786567,\n",
       "    'acc': 0.5275752420605168,\n",
       "    'acc_stderr': 0.012494556343376345},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5093856655290102,\n",
       "    'acc_stderr': 0.014608816322065,\n",
       "    'acc_norm': 0.5426621160409556,\n",
       "    'acc_norm_stderr': 0.01455810654392406},\n",
       "   'harness|hellaswag|10': {'acc': 0.5637323242381995,\n",
       "    'acc_stderr': 0.004949080334816024,\n",
       "    'acc_norm': 0.7691694881497709,\n",
       "    'acc_norm_stderr': 0.004205030476886528},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.45925925925925926,\n",
       "    'acc_stderr': 0.04304979692464244,\n",
       "    'acc_norm': 0.45925925925925926,\n",
       "    'acc_norm_stderr': 0.04304979692464244},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5921052631578947,\n",
       "    'acc_stderr': 0.03999309712777472,\n",
       "    'acc_norm': 0.5921052631578947,\n",
       "    'acc_norm_stderr': 0.03999309712777472},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.61,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.61,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.49056603773584906,\n",
       "    'acc_stderr': 0.0307673947078081,\n",
       "    'acc_norm': 0.49056603773584906,\n",
       "    'acc_norm_stderr': 0.0307673947078081},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.04179596617581,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.04179596617581},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4508670520231214,\n",
       "    'acc_stderr': 0.03794012674697028,\n",
       "    'acc_norm': 0.4508670520231214,\n",
       "    'acc_norm_stderr': 0.03794012674697028},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3431372549019608,\n",
       "    'acc_stderr': 0.047240073523838876,\n",
       "    'acc_norm': 0.3431372549019608,\n",
       "    'acc_norm_stderr': 0.047240073523838876},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.69,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.69,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.49361702127659574,\n",
       "    'acc_stderr': 0.03268335899936336,\n",
       "    'acc_norm': 0.49361702127659574,\n",
       "    'acc_norm_stderr': 0.03268335899936336},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.42105263157894735,\n",
       "    'acc_stderr': 0.046446020912223177,\n",
       "    'acc_norm': 0.42105263157894735,\n",
       "    'acc_norm_stderr': 0.046446020912223177},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5241379310344828,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.5241379310344828,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3915343915343915,\n",
       "    'acc_stderr': 0.025138091388851102,\n",
       "    'acc_norm': 0.3915343915343915,\n",
       "    'acc_norm_stderr': 0.025138091388851102},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4603174603174603,\n",
       "    'acc_stderr': 0.04458029125470973,\n",
       "    'acc_norm': 0.4603174603174603,\n",
       "    'acc_norm_stderr': 0.04458029125470973},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6096774193548387,\n",
       "    'acc_stderr': 0.027751256636969576,\n",
       "    'acc_norm': 0.6096774193548387,\n",
       "    'acc_norm_stderr': 0.027751256636969576},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3891625615763547,\n",
       "    'acc_stderr': 0.034304624161038716,\n",
       "    'acc_norm': 0.3891625615763547,\n",
       "    'acc_norm_stderr': 0.034304624161038716},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.036639749943912434,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.036639749943912434},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.03191178226713549,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.03191178226713549},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7616580310880829,\n",
       "    'acc_stderr': 0.03074890536390989,\n",
       "    'acc_norm': 0.7616580310880829,\n",
       "    'acc_norm_stderr': 0.03074890536390989},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5153846153846153,\n",
       "    'acc_stderr': 0.02533900301010651,\n",
       "    'acc_norm': 0.5153846153846153,\n",
       "    'acc_norm_stderr': 0.02533900301010651},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.34074074074074073,\n",
       "    'acc_stderr': 0.02889774874113114,\n",
       "    'acc_norm': 0.34074074074074073,\n",
       "    'acc_norm_stderr': 0.02889774874113114},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5168067226890757,\n",
       "    'acc_stderr': 0.03246013680375308,\n",
       "    'acc_norm': 0.5168067226890757,\n",
       "    'acc_norm_stderr': 0.03246013680375308},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.36423841059602646,\n",
       "    'acc_stderr': 0.03929111781242742,\n",
       "    'acc_norm': 0.36423841059602646,\n",
       "    'acc_norm_stderr': 0.03929111781242742},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7064220183486238,\n",
       "    'acc_stderr': 0.019525151122639667,\n",
       "    'acc_norm': 0.7064220183486238,\n",
       "    'acc_norm_stderr': 0.019525151122639667},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.033247089118091176,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.033247089118091176},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.029102254389674082,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.029102254389674082},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7637130801687764,\n",
       "    'acc_stderr': 0.027652153144159256,\n",
       "    'acc_norm': 0.7637130801687764,\n",
       "    'acc_norm_stderr': 0.027652153144159256},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6098654708520179,\n",
       "    'acc_stderr': 0.03273766725459156,\n",
       "    'acc_norm': 0.6098654708520179,\n",
       "    'acc_norm_stderr': 0.03273766725459156},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6259541984732825,\n",
       "    'acc_stderr': 0.042438692422305246,\n",
       "    'acc_norm': 0.6259541984732825,\n",
       "    'acc_norm_stderr': 0.042438692422305246},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7355371900826446,\n",
       "    'acc_stderr': 0.040261875275912046,\n",
       "    'acc_norm': 0.7355371900826446,\n",
       "    'acc_norm_stderr': 0.040261875275912046},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.04557239513497751,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.04557239513497751},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6993865030674846,\n",
       "    'acc_stderr': 0.03602511318806771,\n",
       "    'acc_norm': 0.6993865030674846,\n",
       "    'acc_norm_stderr': 0.03602511318806771},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.38392857142857145,\n",
       "    'acc_stderr': 0.04616143075028547,\n",
       "    'acc_norm': 0.38392857142857145,\n",
       "    'acc_norm_stderr': 0.04616143075028547},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.046561471100123514,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.046561471100123514},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.811965811965812,\n",
       "    'acc_stderr': 0.025598193686652265,\n",
       "    'acc_norm': 0.811965811965812,\n",
       "    'acc_norm_stderr': 0.025598193686652265},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.0498887651569859,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.0498887651569859},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7075351213282248,\n",
       "    'acc_stderr': 0.016267000684598645,\n",
       "    'acc_norm': 0.7075351213282248,\n",
       "    'acc_norm_stderr': 0.016267000684598645},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5780346820809249,\n",
       "    'acc_stderr': 0.02658923114217426,\n",
       "    'acc_norm': 0.5780346820809249,\n",
       "    'acc_norm_stderr': 0.02658923114217426},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.31620111731843575,\n",
       "    'acc_stderr': 0.015551673652172554,\n",
       "    'acc_norm': 0.31620111731843575,\n",
       "    'acc_norm_stderr': 0.015551673652172554},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5915032679738562,\n",
       "    'acc_stderr': 0.028146405993096358,\n",
       "    'acc_norm': 0.5915032679738562,\n",
       "    'acc_norm_stderr': 0.028146405993096358},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.594855305466238,\n",
       "    'acc_stderr': 0.027882383791325963,\n",
       "    'acc_norm': 0.594855305466238,\n",
       "    'acc_norm_stderr': 0.027882383791325963},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6141975308641975,\n",
       "    'acc_stderr': 0.027085401226132143,\n",
       "    'acc_norm': 0.6141975308641975,\n",
       "    'acc_norm_stderr': 0.027085401226132143},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.40425531914893614,\n",
       "    'acc_stderr': 0.029275532159704725,\n",
       "    'acc_norm': 0.40425531914893614,\n",
       "    'acc_norm_stderr': 0.029275532159704725},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3891786179921773,\n",
       "    'acc_stderr': 0.012452613934287012,\n",
       "    'acc_norm': 0.3891786179921773,\n",
       "    'acc_norm_stderr': 0.012452613934287012},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.47794117647058826,\n",
       "    'acc_stderr': 0.030343264224213535,\n",
       "    'acc_norm': 0.47794117647058826,\n",
       "    'acc_norm_stderr': 0.030343264224213535},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.020227834851568375,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.020227834851568375},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6489795918367347,\n",
       "    'acc_stderr': 0.03055531675557364,\n",
       "    'acc_norm': 0.6489795918367347,\n",
       "    'acc_norm_stderr': 0.03055531675557364},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7611940298507462,\n",
       "    'acc_stderr': 0.03014777593540922,\n",
       "    'acc_norm': 0.7611940298507462,\n",
       "    'acc_norm_stderr': 0.03014777593540922},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.041633319989322626,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.041633319989322626},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.42168674698795183,\n",
       "    'acc_stderr': 0.03844453181770917,\n",
       "    'acc_norm': 0.42168674698795183,\n",
       "    'acc_norm_stderr': 0.03844453181770917},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7485380116959064,\n",
       "    'acc_stderr': 0.033275044238468436,\n",
       "    'acc_norm': 0.7485380116959064,\n",
       "    'acc_norm_stderr': 0.033275044238468436},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.44437538633055657,\n",
       "    'mc2_stderr': 0.014550940721814704}}},\n",
       " 'dolly-v2-12b': {'key': 'dolly-v2-12b',\n",
       "  'Model': 'Dolly-V2-12B',\n",
       "  'MT-bench (score)': '3.28',\n",
       "  'MMLU': '0.257',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'Databricks',\n",
       "  'Link': 'https://huggingface.co/databricks/dolly-v2-12b',\n",
       "  'results': {'harness|drop|3': {'em': 0.0016778523489932886,\n",
       "    'em_stderr': 0.0004191330178826844,\n",
       "    'f1': 0.06285968959731549,\n",
       "    'f1_stderr': 0.0014820300080071475},\n",
       "   'harness|gsm8k|5': {'acc': 0.012130401819560273,\n",
       "    'acc_stderr': 0.0030152942428909495},\n",
       "   'harness|winogrande|5': {'acc': 0.6085240726124704,\n",
       "    'acc_stderr': 0.013717487071290854},\n",
       "   'all': {'acc': 0.2661068958576839,\n",
       "    'acc_stderr': 0.03186337801557282,\n",
       "    'acc_norm': 0.26986300658126056,\n",
       "    'acc_norm_stderr': 0.0318588678937482,\n",
       "    'mc1': 0.200734394124847,\n",
       "    'mc1_stderr': 0.014022045717482156,\n",
       "    'mc2': 0.33827081326692277,\n",
       "    'mc2_stderr': 0.014862542606576355},\n",
       "   'harness|arc:challenge|25': {'acc': 0.38139931740614336,\n",
       "    'acc_stderr': 0.01419438908668526,\n",
       "    'acc_norm': 0.42406143344709896,\n",
       "    'acc_norm_stderr': 0.0144418896274644},\n",
       "   'harness|hellaswag|10': {'acc': 0.5463055168293168,\n",
       "    'acc_stderr': 0.0049683371441363675,\n",
       "    'acc_norm': 0.7252539334793866,\n",
       "    'acc_norm_stderr': 0.00445473941570504},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.04072314811876837,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.04072314811876837},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.2631578947368421,\n",
       "    'acc_stderr': 0.03583496176361064,\n",
       "    'acc_norm': 0.2631578947368421,\n",
       "    'acc_norm_stderr': 0.03583496176361064},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816507,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816507},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.22264150943396227,\n",
       "    'acc_stderr': 0.0256042334708991,\n",
       "    'acc_norm': 0.22264150943396227,\n",
       "    'acc_norm_stderr': 0.0256042334708991},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2638888888888889,\n",
       "    'acc_stderr': 0.03685651095897532,\n",
       "    'acc_norm': 0.2638888888888889,\n",
       "    'acc_norm_stderr': 0.03685651095897532},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.18,\n",
       "    'acc_stderr': 0.038612291966536955,\n",
       "    'acc_norm': 0.18,\n",
       "    'acc_norm_stderr': 0.038612291966536955},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.2,\n",
       "    'acc_stderr': 0.04020151261036845,\n",
       "    'acc_norm': 0.2,\n",
       "    'acc_norm_stderr': 0.04020151261036845},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.23699421965317918,\n",
       "    'acc_stderr': 0.03242414757483099,\n",
       "    'acc_norm': 0.23699421965317918,\n",
       "    'acc_norm_stderr': 0.03242414757483099},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.23529411764705882,\n",
       "    'acc_stderr': 0.04220773659171452,\n",
       "    'acc_norm': 0.23529411764705882,\n",
       "    'acc_norm_stderr': 0.04220773659171452},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.23829787234042554,\n",
       "    'acc_stderr': 0.027851252973889788,\n",
       "    'acc_norm': 0.23829787234042554,\n",
       "    'acc_norm_stderr': 0.027851252973889788},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2543859649122807,\n",
       "    'acc_stderr': 0.040969851398436695,\n",
       "    'acc_norm': 0.2543859649122807,\n",
       "    'acc_norm_stderr': 0.040969851398436695},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.2827586206896552,\n",
       "    'acc_stderr': 0.03752833958003337,\n",
       "    'acc_norm': 0.2827586206896552,\n",
       "    'acc_norm_stderr': 0.03752833958003337},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.2698412698412698,\n",
       "    'acc_stderr': 0.02286083830923207,\n",
       "    'acc_norm': 0.2698412698412698,\n",
       "    'acc_norm_stderr': 0.02286083830923207},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.1984126984126984,\n",
       "    'acc_stderr': 0.035670166752768635,\n",
       "    'acc_norm': 0.1984126984126984,\n",
       "    'acc_norm_stderr': 0.035670166752768635},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.26129032258064516,\n",
       "    'acc_stderr': 0.024993053397764822,\n",
       "    'acc_norm': 0.26129032258064516,\n",
       "    'acc_norm_stderr': 0.024993053397764822},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.2512315270935961,\n",
       "    'acc_stderr': 0.030516530732694436,\n",
       "    'acc_norm': 0.2512315270935961,\n",
       "    'acc_norm_stderr': 0.030516530732694436},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.20606060606060606,\n",
       "    'acc_stderr': 0.03158415324047708,\n",
       "    'acc_norm': 0.20606060606060606,\n",
       "    'acc_norm_stderr': 0.03158415324047708},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.20707070707070707,\n",
       "    'acc_stderr': 0.02886977846026705,\n",
       "    'acc_norm': 0.20707070707070707,\n",
       "    'acc_norm_stderr': 0.02886977846026705},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.23316062176165803,\n",
       "    'acc_stderr': 0.03051611137147601,\n",
       "    'acc_norm': 0.23316062176165803,\n",
       "    'acc_norm_stderr': 0.03051611137147601},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.23076923076923078,\n",
       "    'acc_stderr': 0.02136202772522273,\n",
       "    'acc_norm': 0.23076923076923078,\n",
       "    'acc_norm_stderr': 0.02136202772522273},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.26666666666666666,\n",
       "    'acc_stderr': 0.026962424325073835,\n",
       "    'acc_norm': 0.26666666666666666,\n",
       "    'acc_norm_stderr': 0.026962424325073835},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.226890756302521,\n",
       "    'acc_stderr': 0.02720537153827949,\n",
       "    'acc_norm': 0.226890756302521,\n",
       "    'acc_norm_stderr': 0.02720537153827949},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.23178807947019867,\n",
       "    'acc_stderr': 0.03445406271987054,\n",
       "    'acc_norm': 0.23178807947019867,\n",
       "    'acc_norm_stderr': 0.03445406271987054},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.26605504587155965,\n",
       "    'acc_stderr': 0.018946022322225593,\n",
       "    'acc_norm': 0.26605504587155965,\n",
       "    'acc_norm_stderr': 0.018946022322225593},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.20833333333333334,\n",
       "    'acc_stderr': 0.02769691071309394,\n",
       "    'acc_norm': 0.20833333333333334,\n",
       "    'acc_norm_stderr': 0.02769691071309394},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.2696078431372549,\n",
       "    'acc_stderr': 0.031145570659486782,\n",
       "    'acc_norm': 0.2696078431372549,\n",
       "    'acc_norm_stderr': 0.031145570659486782},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.32489451476793246,\n",
       "    'acc_stderr': 0.030486039389105307,\n",
       "    'acc_norm': 0.32489451476793246,\n",
       "    'acc_norm_stderr': 0.030486039389105307},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.29596412556053814,\n",
       "    'acc_stderr': 0.0306365913486998,\n",
       "    'acc_norm': 0.29596412556053814,\n",
       "    'acc_norm_stderr': 0.0306365913486998},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.22137404580152673,\n",
       "    'acc_stderr': 0.03641297081313729,\n",
       "    'acc_norm': 0.22137404580152673,\n",
       "    'acc_norm_stderr': 0.03641297081313729},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.38016528925619836,\n",
       "    'acc_stderr': 0.04431324501968431,\n",
       "    'acc_norm': 0.38016528925619836,\n",
       "    'acc_norm_stderr': 0.04431324501968431},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.2222222222222222,\n",
       "    'acc_stderr': 0.040191074725573483,\n",
       "    'acc_norm': 0.2222222222222222,\n",
       "    'acc_norm_stderr': 0.040191074725573483},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2392638036809816,\n",
       "    'acc_stderr': 0.03351953879521272,\n",
       "    'acc_norm': 0.2392638036809816,\n",
       "    'acc_norm_stderr': 0.03351953879521272},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.3482142857142857,\n",
       "    'acc_stderr': 0.045218299028335865,\n",
       "    'acc_norm': 0.3482142857142857,\n",
       "    'acc_norm_stderr': 0.045218299028335865},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.24271844660194175,\n",
       "    'acc_stderr': 0.04245022486384493,\n",
       "    'acc_norm': 0.24271844660194175,\n",
       "    'acc_norm_stderr': 0.04245022486384493},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.27350427350427353,\n",
       "    'acc_stderr': 0.029202540153431187,\n",
       "    'acc_norm': 0.27350427350427353,\n",
       "    'acc_norm_stderr': 0.029202540153431187},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.041633319989322695,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.041633319989322695},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.29246487867177523,\n",
       "    'acc_stderr': 0.016267000684598645,\n",
       "    'acc_norm': 0.29246487867177523,\n",
       "    'acc_norm_stderr': 0.016267000684598645},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.2630057803468208,\n",
       "    'acc_stderr': 0.023703099525258176,\n",
       "    'acc_norm': 0.2630057803468208,\n",
       "    'acc_norm_stderr': 0.023703099525258176},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961443,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961443},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.24509803921568626,\n",
       "    'acc_stderr': 0.02463004897982478,\n",
       "    'acc_norm': 0.24509803921568626,\n",
       "    'acc_norm_stderr': 0.02463004897982478},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.3054662379421222,\n",
       "    'acc_stderr': 0.026160584450140495,\n",
       "    'acc_norm': 0.3054662379421222,\n",
       "    'acc_norm_stderr': 0.026160584450140495},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.02409347123262133,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.02409347123262133},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2695035460992908,\n",
       "    'acc_stderr': 0.02646903681859062,\n",
       "    'acc_norm': 0.2695035460992908,\n",
       "    'acc_norm_stderr': 0.02646903681859062},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.2692307692307692,\n",
       "    'acc_stderr': 0.011328734403140308,\n",
       "    'acc_norm': 0.2692307692307692,\n",
       "    'acc_norm_stderr': 0.011328734403140308},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.18382352941176472,\n",
       "    'acc_stderr': 0.02352924218519311,\n",
       "    'acc_norm': 0.18382352941176472,\n",
       "    'acc_norm_stderr': 0.02352924218519311},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.01784808957491322,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.01784808957491322},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.2818181818181818,\n",
       "    'acc_stderr': 0.0430911870994646,\n",
       "    'acc_norm': 0.2818181818181818,\n",
       "    'acc_norm_stderr': 0.0430911870994646},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.17551020408163265,\n",
       "    'acc_stderr': 0.024352800722970015,\n",
       "    'acc_norm': 0.17551020408163265,\n",
       "    'acc_norm_stderr': 0.024352800722970015},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.2935323383084577,\n",
       "    'acc_stderr': 0.03220024104534205,\n",
       "    'acc_norm': 0.2935323383084577,\n",
       "    'acc_norm_stderr': 0.03220024104534205},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.044084400227680794,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.044084400227680794},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.26506024096385544,\n",
       "    'acc_stderr': 0.03436024037944967,\n",
       "    'acc_norm': 0.26506024096385544,\n",
       "    'acc_norm_stderr': 0.03436024037944967},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.03565079670708311,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.03565079670708311},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.200734394124847,\n",
       "    'mc1_stderr': 0.014022045717482156,\n",
       "    'mc2': 0.33827081326692277,\n",
       "    'mc2_stderr': 0.014862542606576355}}},\n",
       " 'dolphin-2.2.1-mistral-7b': {'key': 'dolphin-2.2.1-mistral-7b',\n",
       "  'Model': 'Dolphin-2.2.1-Mistral-7B',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '-',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'Cognitive Computations',\n",
       "  'Link': 'https://huggingface.co/ehartford/dolphin-2.2.1-mistral-7b',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6049488054607508,\n",
       "    'acc_stderr': 0.014285898292938167,\n",
       "    'acc_norm': 0.6331058020477816,\n",
       "    'acc_norm_stderr': 0.014084133118104301},\n",
       "   'harness|hellaswag|10': {'acc': 0.6431985660227046,\n",
       "    'acc_stderr': 0.004780764443411322,\n",
       "    'acc_norm': 0.8375821549492133,\n",
       "    'acc_norm_stderr': 0.0036807989505319113},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.04218506215368879,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.04218506215368879},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6776315789473685,\n",
       "    'acc_stderr': 0.03803510248351585,\n",
       "    'acc_norm': 0.6776315789473685,\n",
       "    'acc_norm_stderr': 0.03803510248351585},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.028727502957880267,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.028727502957880267},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03621034121889507,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03621034121889507},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.04878317312145633,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.04878317312145633},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6358381502890174,\n",
       "    'acc_stderr': 0.03669072477416907,\n",
       "    'acc_norm': 0.6358381502890174,\n",
       "    'acc_norm_stderr': 0.03669072477416907},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.35294117647058826,\n",
       "    'acc_stderr': 0.04755129616062947,\n",
       "    'acc_norm': 0.35294117647058826,\n",
       "    'acc_norm_stderr': 0.04755129616062947},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.04093601807403326,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.04093601807403326},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5446808510638298,\n",
       "    'acc_stderr': 0.03255525359340354,\n",
       "    'acc_norm': 0.5446808510638298,\n",
       "    'acc_norm_stderr': 0.03255525359340354},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.45614035087719296,\n",
       "    'acc_stderr': 0.04685473041907789,\n",
       "    'acc_norm': 0.45614035087719296,\n",
       "    'acc_norm_stderr': 0.04685473041907789},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.593103448275862,\n",
       "    'acc_stderr': 0.04093793981266236,\n",
       "    'acc_norm': 0.593103448275862,\n",
       "    'acc_norm_stderr': 0.04093793981266236},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3862433862433862,\n",
       "    'acc_stderr': 0.025075981767601684,\n",
       "    'acc_norm': 0.3862433862433862,\n",
       "    'acc_norm_stderr': 0.025075981767601684},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3968253968253968,\n",
       "    'acc_stderr': 0.04375888492727062,\n",
       "    'acc_norm': 0.3968253968253968,\n",
       "    'acc_norm_stderr': 0.04375888492727062},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7709677419354839,\n",
       "    'acc_stderr': 0.02390491431178265,\n",
       "    'acc_norm': 0.7709677419354839,\n",
       "    'acc_norm_stderr': 0.02390491431178265},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4876847290640394,\n",
       "    'acc_stderr': 0.035169204442208966,\n",
       "    'acc_norm': 0.4876847290640394,\n",
       "    'acc_norm_stderr': 0.035169204442208966},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.69,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.69,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7696969696969697,\n",
       "    'acc_stderr': 0.0328766675860349,\n",
       "    'acc_norm': 0.7696969696969697,\n",
       "    'acc_norm_stderr': 0.0328766675860349},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7929292929292929,\n",
       "    'acc_stderr': 0.028869778460267025,\n",
       "    'acc_norm': 0.7929292929292929,\n",
       "    'acc_norm_stderr': 0.028869778460267025},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8704663212435233,\n",
       "    'acc_stderr': 0.02423353229775873,\n",
       "    'acc_norm': 0.8704663212435233,\n",
       "    'acc_norm_stderr': 0.02423353229775873},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6512820512820513,\n",
       "    'acc_stderr': 0.02416278028401772,\n",
       "    'acc_norm': 0.6512820512820513,\n",
       "    'acc_norm_stderr': 0.02416278028401772},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028593,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.030388353551886783,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.030388353551886783},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.304635761589404,\n",
       "    'acc_stderr': 0.03757949922943343,\n",
       "    'acc_norm': 0.304635761589404,\n",
       "    'acc_norm_stderr': 0.03757949922943343},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8348623853211009,\n",
       "    'acc_stderr': 0.015919557829976044,\n",
       "    'acc_norm': 0.8348623853211009,\n",
       "    'acc_norm_stderr': 0.015919557829976044},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.034076320938540516,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.034076320938540516},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7696078431372549,\n",
       "    'acc_stderr': 0.02955429260569508,\n",
       "    'acc_norm': 0.7696078431372549,\n",
       "    'acc_norm_stderr': 0.02955429260569508},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7763713080168776,\n",
       "    'acc_stderr': 0.027123298205229966,\n",
       "    'acc_norm': 0.7763713080168776,\n",
       "    'acc_norm_stderr': 0.027123298205229966},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6771300448430493,\n",
       "    'acc_stderr': 0.03138147637575499,\n",
       "    'acc_norm': 0.6771300448430493,\n",
       "    'acc_norm_stderr': 0.03138147637575499},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7786259541984732,\n",
       "    'acc_stderr': 0.0364129708131373,\n",
       "    'acc_norm': 0.7786259541984732,\n",
       "    'acc_norm_stderr': 0.0364129708131373},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7933884297520661,\n",
       "    'acc_stderr': 0.03695980128098823,\n",
       "    'acc_norm': 0.7933884297520661,\n",
       "    'acc_norm_stderr': 0.03695980128098823},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8055555555555556,\n",
       "    'acc_stderr': 0.03826076324884866,\n",
       "    'acc_norm': 0.8055555555555556,\n",
       "    'acc_norm_stderr': 0.03826076324884866},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7607361963190185,\n",
       "    'acc_stderr': 0.033519538795212696,\n",
       "    'acc_norm': 0.7607361963190185,\n",
       "    'acc_norm_stderr': 0.033519538795212696},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7669902912621359,\n",
       "    'acc_stderr': 0.04185832598928315,\n",
       "    'acc_norm': 0.7669902912621359,\n",
       "    'acc_norm_stderr': 0.04185832598928315},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8632478632478633,\n",
       "    'acc_stderr': 0.022509033937077805,\n",
       "    'acc_norm': 0.8632478632478633,\n",
       "    'acc_norm_stderr': 0.022509033937077805},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768078,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768078},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8135376756066411,\n",
       "    'acc_stderr': 0.013927751372001506,\n",
       "    'acc_norm': 0.8135376756066411,\n",
       "    'acc_norm_stderr': 0.013927751372001506},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7138728323699421,\n",
       "    'acc_stderr': 0.02433214677913413,\n",
       "    'acc_norm': 0.7138728323699421,\n",
       "    'acc_norm_stderr': 0.02433214677913413},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.37988826815642457,\n",
       "    'acc_stderr': 0.016232826818678502,\n",
       "    'acc_norm': 0.37988826815642457,\n",
       "    'acc_norm_stderr': 0.016232826818678502},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7058823529411765,\n",
       "    'acc_stderr': 0.026090162504279053,\n",
       "    'acc_norm': 0.7058823529411765,\n",
       "    'acc_norm_stderr': 0.026090162504279053},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7202572347266881,\n",
       "    'acc_stderr': 0.025494259350694912,\n",
       "    'acc_norm': 0.7202572347266881,\n",
       "    'acc_norm_stderr': 0.025494259350694912},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7098765432098766,\n",
       "    'acc_stderr': 0.025251173936495026,\n",
       "    'acc_norm': 0.7098765432098766,\n",
       "    'acc_norm_stderr': 0.025251173936495026},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4787234042553192,\n",
       "    'acc_stderr': 0.029800481645628693,\n",
       "    'acc_norm': 0.4787234042553192,\n",
       "    'acc_norm_stderr': 0.029800481645628693},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4471968709256845,\n",
       "    'acc_stderr': 0.012698825252435111,\n",
       "    'acc_norm': 0.4471968709256845,\n",
       "    'acc_norm_stderr': 0.012698825252435111},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6470588235294118,\n",
       "    'acc_stderr': 0.029029422815681397,\n",
       "    'acc_norm': 0.6470588235294118,\n",
       "    'acc_norm_stderr': 0.029029422815681397},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6503267973856209,\n",
       "    'acc_stderr': 0.01929196189506638,\n",
       "    'acc_norm': 0.6503267973856209,\n",
       "    'acc_norm_stderr': 0.01929196189506638},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.726530612244898,\n",
       "    'acc_stderr': 0.02853556033712844,\n",
       "    'acc_norm': 0.726530612244898,\n",
       "    'acc_norm_stderr': 0.02853556033712844},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.835820895522388,\n",
       "    'acc_stderr': 0.026193923544454125,\n",
       "    'acc_norm': 0.835820895522388,\n",
       "    'acc_norm_stderr': 0.026193923544454125},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.03379976689896309,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.03379976689896309},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5481927710843374,\n",
       "    'acc_stderr': 0.03874371556587953,\n",
       "    'acc_norm': 0.5481927710843374,\n",
       "    'acc_norm_stderr': 0.03874371556587953},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8304093567251462,\n",
       "    'acc_stderr': 0.02878210810540171,\n",
       "    'acc_norm': 0.8304093567251462,\n",
       "    'acc_norm_stderr': 0.02878210810540171},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3659730722154223,\n",
       "    'mc1_stderr': 0.016862941684088365,\n",
       "    'mc2': 0.5311447373702662,\n",
       "    'mc2_stderr': 0.015062742496541512},\n",
       "   'harness|winogrande|5': {'acc': 0.7813733228097869,\n",
       "    'acc_stderr': 0.01161619821577323},\n",
       "   'harness|drop|3': {'em': 0.004299496644295302,\n",
       "    'em_stderr': 0.0006700586558630193,\n",
       "    'f1': 0.081867659395973,\n",
       "    'f1_stderr': 0.0016972961971096978},\n",
       "   'harness|gsm8k|5': {'acc': 0.4806671721000758,\n",
       "    'acc_stderr': 0.013762185709851354},\n",
       "   'all': {'acc': 0.6314567324183159,\n",
       "    'acc_stderr': 0.032318316802746,\n",
       "    'acc_norm': 0.6352434028495076,\n",
       "    'acc_norm_stderr': 0.032961647633460475,\n",
       "    'mc1': 0.3659730722154223,\n",
       "    'mc1_stderr': 0.016862941684088365,\n",
       "    'mc2': 0.5311447373702662,\n",
       "    'mc2_stderr': 0.015062742496541512}}},\n",
       " 'falcon-180b-chat': {'key': 'falcon-180b-chat',\n",
       "  'Model': 'falcon-180b-chat',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.680',\n",
       "  'License': 'Falcon-180B TII License',\n",
       "  'Organization': 'TII',\n",
       "  'Link': 'https://huggingface.co/tiiuae/falcon-180B-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6143344709897611,\n",
       "    'acc_stderr': 0.014224250973257182,\n",
       "    'acc_norm': 0.64419795221843,\n",
       "    'acc_norm_stderr': 0.01399057113791876},\n",
       "   'harness|hellaswag|10': {'acc': 0.6904999004182434,\n",
       "    'acc_stderr': 0.004613427745209517,\n",
       "    'acc_norm': 0.8804023102967536,\n",
       "    'acc_norm_stderr': 0.0032382732952847414},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.04725815626252606,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.04725815626252606},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6518518518518519,\n",
       "    'acc_stderr': 0.041153246103369526,\n",
       "    'acc_norm': 0.6518518518518519,\n",
       "    'acc_norm_stderr': 0.041153246103369526},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.743421052631579,\n",
       "    'acc_stderr': 0.0355418036802569,\n",
       "    'acc_norm': 0.743421052631579,\n",
       "    'acc_norm_stderr': 0.0355418036802569},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.7018867924528301,\n",
       "    'acc_stderr': 0.028152837942493857,\n",
       "    'acc_norm': 0.7018867924528301,\n",
       "    'acc_norm_stderr': 0.028152837942493857},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.8263888888888888,\n",
       "    'acc_stderr': 0.03167473383795718,\n",
       "    'acc_norm': 0.8263888888888888,\n",
       "    'acc_norm_stderr': 0.03167473383795718},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6473988439306358,\n",
       "    'acc_stderr': 0.036430371689585475,\n",
       "    'acc_norm': 0.6473988439306358,\n",
       "    'acc_norm_stderr': 0.036430371689585475},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3137254901960784,\n",
       "    'acc_stderr': 0.04617034827006717,\n",
       "    'acc_norm': 0.3137254901960784,\n",
       "    'acc_norm_stderr': 0.04617034827006717},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932263,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932263},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.6510638297872341,\n",
       "    'acc_stderr': 0.03115852213135778,\n",
       "    'acc_norm': 0.6510638297872341,\n",
       "    'acc_norm_stderr': 0.03115852213135778},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.4824561403508772,\n",
       "    'acc_stderr': 0.04700708033551038,\n",
       "    'acc_norm': 0.4824561403508772,\n",
       "    'acc_norm_stderr': 0.04700708033551038},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.6137931034482759,\n",
       "    'acc_stderr': 0.04057324734419036,\n",
       "    'acc_norm': 0.6137931034482759,\n",
       "    'acc_norm_stderr': 0.04057324734419036},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.455026455026455,\n",
       "    'acc_stderr': 0.02564692836104939,\n",
       "    'acc_norm': 0.455026455026455,\n",
       "    'acc_norm_stderr': 0.02564692836104939},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4365079365079365,\n",
       "    'acc_stderr': 0.04435932892851466,\n",
       "    'acc_norm': 0.4365079365079365,\n",
       "    'acc_norm_stderr': 0.04435932892851466},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7967741935483871,\n",
       "    'acc_stderr': 0.02289168798455495,\n",
       "    'acc_norm': 0.7967741935483871,\n",
       "    'acc_norm_stderr': 0.02289168798455495},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.541871921182266,\n",
       "    'acc_stderr': 0.03505630140785741,\n",
       "    'acc_norm': 0.541871921182266,\n",
       "    'acc_norm_stderr': 0.03505630140785741},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.03123475237772117,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.03123475237772117},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8484848484848485,\n",
       "    'acc_stderr': 0.025545650426603617,\n",
       "    'acc_norm': 0.8484848484848485,\n",
       "    'acc_norm_stderr': 0.025545650426603617},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9481865284974094,\n",
       "    'acc_stderr': 0.01599622932024412,\n",
       "    'acc_norm': 0.9481865284974094,\n",
       "    'acc_norm_stderr': 0.01599622932024412},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6820512820512821,\n",
       "    'acc_stderr': 0.02361088430892786,\n",
       "    'acc_norm': 0.6820512820512821,\n",
       "    'acc_norm_stderr': 0.02361088430892786},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028597,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028597},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7689075630252101,\n",
       "    'acc_stderr': 0.027381406927868886,\n",
       "    'acc_norm': 0.7689075630252101,\n",
       "    'acc_norm_stderr': 0.027381406927868886},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3973509933774834,\n",
       "    'acc_stderr': 0.039955240076816806,\n",
       "    'acc_norm': 0.3973509933774834,\n",
       "    'acc_norm_stderr': 0.039955240076816806},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8715596330275229,\n",
       "    'acc_stderr': 0.014344977542914318,\n",
       "    'acc_norm': 0.8715596330275229,\n",
       "    'acc_norm_stderr': 0.014344977542914318},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5416666666666666,\n",
       "    'acc_stderr': 0.033981108902946366,\n",
       "    'acc_norm': 0.5416666666666666,\n",
       "    'acc_norm_stderr': 0.033981108902946366},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8578431372549019,\n",
       "    'acc_stderr': 0.02450980392156862,\n",
       "    'acc_norm': 0.8578431372549019,\n",
       "    'acc_norm_stderr': 0.02450980392156862},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8607594936708861,\n",
       "    'acc_stderr': 0.0225355263526927,\n",
       "    'acc_norm': 0.8607594936708861,\n",
       "    'acc_norm_stderr': 0.0225355263526927},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7668161434977578,\n",
       "    'acc_stderr': 0.028380391147094702,\n",
       "    'acc_norm': 0.7668161434977578,\n",
       "    'acc_norm_stderr': 0.028380391147094702},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8396946564885496,\n",
       "    'acc_stderr': 0.03217829420744631,\n",
       "    'acc_norm': 0.8396946564885496,\n",
       "    'acc_norm_stderr': 0.03217829420744631},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03520893951097653,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03520893951097653},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8611111111111112,\n",
       "    'acc_stderr': 0.0334327006286962,\n",
       "    'acc_norm': 0.8611111111111112,\n",
       "    'acc_norm_stderr': 0.0334327006286962},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.8343558282208589,\n",
       "    'acc_stderr': 0.029208296231259104,\n",
       "    'acc_norm': 0.8343558282208589,\n",
       "    'acc_norm_stderr': 0.029208296231259104},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.5446428571428571,\n",
       "    'acc_stderr': 0.04726835553719098,\n",
       "    'acc_norm': 0.5446428571428571,\n",
       "    'acc_norm_stderr': 0.04726835553719098},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8446601941747572,\n",
       "    'acc_stderr': 0.03586594738573975,\n",
       "    'acc_norm': 0.8446601941747572,\n",
       "    'acc_norm_stderr': 0.03586594738573975},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8931623931623932,\n",
       "    'acc_stderr': 0.02023714900899093,\n",
       "    'acc_norm': 0.8931623931623932,\n",
       "    'acc_norm_stderr': 0.02023714900899093},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932261,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932261},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8671775223499362,\n",
       "    'acc_stderr': 0.012136303209884562,\n",
       "    'acc_norm': 0.8671775223499362,\n",
       "    'acc_norm_stderr': 0.012136303209884562},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7745664739884393,\n",
       "    'acc_stderr': 0.022497230190967558,\n",
       "    'acc_norm': 0.7745664739884393,\n",
       "    'acc_norm_stderr': 0.022497230190967558},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.4659217877094972,\n",
       "    'acc_stderr': 0.016683615837486863,\n",
       "    'acc_norm': 0.4659217877094972,\n",
       "    'acc_norm_stderr': 0.016683615837486863},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7287581699346405,\n",
       "    'acc_stderr': 0.02545775669666788,\n",
       "    'acc_norm': 0.7287581699346405,\n",
       "    'acc_norm_stderr': 0.02545775669666788},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7877813504823151,\n",
       "    'acc_stderr': 0.02322275679743512,\n",
       "    'acc_norm': 0.7877813504823151,\n",
       "    'acc_norm_stderr': 0.02322275679743512},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7870370370370371,\n",
       "    'acc_stderr': 0.022779719088733403,\n",
       "    'acc_norm': 0.7870370370370371,\n",
       "    'acc_norm_stderr': 0.022779719088733403},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5460992907801419,\n",
       "    'acc_stderr': 0.02970045324729146,\n",
       "    'acc_norm': 0.5460992907801419,\n",
       "    'acc_norm_stderr': 0.02970045324729146},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5345501955671447,\n",
       "    'acc_stderr': 0.012739711554045713,\n",
       "    'acc_norm': 0.5345501955671447,\n",
       "    'acc_norm_stderr': 0.012739711554045713},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6875,\n",
       "    'acc_stderr': 0.02815637344037142,\n",
       "    'acc_norm': 0.6875,\n",
       "    'acc_norm_stderr': 0.02815637344037142},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.7271241830065359,\n",
       "    'acc_stderr': 0.018020474148393577,\n",
       "    'acc_norm': 0.7271241830065359,\n",
       "    'acc_norm_stderr': 0.018020474148393577},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.763265306122449,\n",
       "    'acc_stderr': 0.027212835884073156,\n",
       "    'acc_norm': 0.763265306122449,\n",
       "    'acc_norm_stderr': 0.027212835884073156},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8258706467661692,\n",
       "    'acc_stderr': 0.026814951200421603,\n",
       "    'acc_norm': 0.8258706467661692,\n",
       "    'acc_norm_stderr': 0.026814951200421603},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.89,\n",
       "    'acc_stderr': 0.03144660377352203,\n",
       "    'acc_norm': 0.89,\n",
       "    'acc_norm_stderr': 0.03144660377352203},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5240963855421686,\n",
       "    'acc_stderr': 0.03887971849597264,\n",
       "    'acc_norm': 0.5240963855421686,\n",
       "    'acc_norm_stderr': 0.03887971849597264},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8421052631578947,\n",
       "    'acc_stderr': 0.02796678585916089,\n",
       "    'acc_norm': 0.8421052631578947,\n",
       "    'acc_norm_stderr': 0.02796678585916089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.37576499388004897,\n",
       "    'mc1_stderr': 0.016954584060214297,\n",
       "    'mc2': 0.5369457047083463,\n",
       "    'mc2_stderr': 0.014662579093423517},\n",
       "   'all': {'acc': 0.6793461177088693,\n",
       "    'acc_stderr': 0.031533098229447415,\n",
       "    'acc_norm': 0.6830709633208555,\n",
       "    'acc_norm_stderr': 0.03150582985173109,\n",
       "    'mc1': 0.37576499388004897,\n",
       "    'mc1_stderr': 0.016954584060214297,\n",
       "    'mc2': 0.5369457047083463,\n",
       "    'mc2_stderr': 0.014662579093423517}}},\n",
       " 'guanaco-33b': {'key': 'guanaco-33b',\n",
       "  'Model': 'Guanaco-33B',\n",
       "  'MT-bench (score)': '6.53',\n",
       "  'MMLU': '0.576',\n",
       "  'License': 'Non-commercial',\n",
       "  'Organization': 'UW',\n",
       "  'Link': 'https://huggingface.co/timdettmers/guanaco-33b-merged',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5870307167235495,\n",
       "    'acc_stderr': 0.014388344935398326,\n",
       "    'acc_norm': 0.6245733788395904,\n",
       "    'acc_norm_stderr': 0.014150631435111726},\n",
       "   'harness|hellaswag|10': {'acc': 0.6446922923720374,\n",
       "    'acc_stderr': 0.004776283203468098,\n",
       "    'acc_norm': 0.8447520414260108,\n",
       "    'acc_norm_stderr': 0.003614007841341989},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.047609522856952365,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.047609522856952365},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5657894736842105,\n",
       "    'acc_stderr': 0.04033565667848319,\n",
       "    'acc_norm': 0.5657894736842105,\n",
       "    'acc_norm_stderr': 0.04033565667848319},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620332,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620332},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5622641509433962,\n",
       "    'acc_stderr': 0.030533338430467516,\n",
       "    'acc_norm': 0.5622641509433962,\n",
       "    'acc_norm_stderr': 0.030533338430467516},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5625,\n",
       "    'acc_stderr': 0.04148415739394154,\n",
       "    'acc_norm': 0.5625,\n",
       "    'acc_norm_stderr': 0.04148415739394154},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3431372549019608,\n",
       "    'acc_stderr': 0.04724007352383889,\n",
       "    'acc_norm': 0.3431372549019608,\n",
       "    'acc_norm_stderr': 0.04724007352383889},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4595744680851064,\n",
       "    'acc_stderr': 0.03257901482099835,\n",
       "    'acc_norm': 0.4595744680851064,\n",
       "    'acc_norm_stderr': 0.03257901482099835},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3684210526315789,\n",
       "    'acc_stderr': 0.04537815354939391,\n",
       "    'acc_norm': 0.3684210526315789,\n",
       "    'acc_norm_stderr': 0.04537815354939391},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4413793103448276,\n",
       "    'acc_stderr': 0.04137931034482758,\n",
       "    'acc_norm': 0.4413793103448276,\n",
       "    'acc_norm_stderr': 0.04137931034482758},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.31216931216931215,\n",
       "    'acc_stderr': 0.0238652068369726,\n",
       "    'acc_norm': 0.31216931216931215,\n",
       "    'acc_norm_stderr': 0.0238652068369726},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.29365079365079366,\n",
       "    'acc_stderr': 0.04073524322147126,\n",
       "    'acc_norm': 0.29365079365079366,\n",
       "    'acc_norm_stderr': 0.04073524322147126},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6290322580645161,\n",
       "    'acc_stderr': 0.027480541887953593,\n",
       "    'acc_norm': 0.6290322580645161,\n",
       "    'acc_norm_stderr': 0.027480541887953593},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3793103448275862,\n",
       "    'acc_stderr': 0.03413963805906235,\n",
       "    'acc_norm': 0.3793103448275862,\n",
       "    'acc_norm_stderr': 0.03413963805906235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7212121212121212,\n",
       "    'acc_stderr': 0.035014387062967806,\n",
       "    'acc_norm': 0.7212121212121212,\n",
       "    'acc_norm_stderr': 0.035014387062967806},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.03191178226713547,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.03191178226713547},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7150259067357513,\n",
       "    'acc_stderr': 0.032577140777096614,\n",
       "    'acc_norm': 0.7150259067357513,\n",
       "    'acc_norm_stderr': 0.032577140777096614},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.49230769230769234,\n",
       "    'acc_stderr': 0.025348006031534778,\n",
       "    'acc_norm': 0.49230769230769234,\n",
       "    'acc_norm_stderr': 0.025348006031534778},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2777777777777778,\n",
       "    'acc_stderr': 0.02730914058823019,\n",
       "    'acc_norm': 0.2777777777777778,\n",
       "    'acc_norm_stderr': 0.02730914058823019},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03242225027115006,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03242225027115006},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.03822746937658752,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.03822746937658752},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7302752293577982,\n",
       "    'acc_stderr': 0.019028486711115438,\n",
       "    'acc_norm': 0.7302752293577982,\n",
       "    'acc_norm_stderr': 0.019028486711115438},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.375,\n",
       "    'acc_stderr': 0.033016908987210894,\n",
       "    'acc_norm': 0.375,\n",
       "    'acc_norm_stderr': 0.033016908987210894},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.02910225438967409,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.02910225438967409},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7679324894514767,\n",
       "    'acc_stderr': 0.02747974455080851,\n",
       "    'acc_norm': 0.7679324894514767,\n",
       "    'acc_norm_stderr': 0.02747974455080851},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5874439461883408,\n",
       "    'acc_stderr': 0.03304062175449297,\n",
       "    'acc_norm': 0.5874439461883408,\n",
       "    'acc_norm_stderr': 0.03304062175449297},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6412213740458015,\n",
       "    'acc_stderr': 0.04206739313864908,\n",
       "    'acc_norm': 0.6412213740458015,\n",
       "    'acc_norm_stderr': 0.04206739313864908},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7272727272727273,\n",
       "    'acc_stderr': 0.04065578140908705,\n",
       "    'acc_norm': 0.7272727272727273,\n",
       "    'acc_norm_stderr': 0.04065578140908705},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6111111111111112,\n",
       "    'acc_stderr': 0.04712821257426769,\n",
       "    'acc_norm': 0.6111111111111112,\n",
       "    'acc_norm_stderr': 0.04712821257426769},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6380368098159509,\n",
       "    'acc_stderr': 0.037757007291414416,\n",
       "    'acc_norm': 0.6380368098159509,\n",
       "    'acc_norm_stderr': 0.037757007291414416},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.38392857142857145,\n",
       "    'acc_stderr': 0.04616143075028546,\n",
       "    'acc_norm': 0.38392857142857145,\n",
       "    'acc_norm_stderr': 0.04616143075028546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6310679611650486,\n",
       "    'acc_stderr': 0.0477761518115674,\n",
       "    'acc_norm': 0.6310679611650486,\n",
       "    'acc_norm_stderr': 0.0477761518115674},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7863247863247863,\n",
       "    'acc_stderr': 0.026853450377009154,\n",
       "    'acc_norm': 0.7863247863247863,\n",
       "    'acc_norm_stderr': 0.026853450377009154},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6756066411238825,\n",
       "    'acc_stderr': 0.0167409290471627,\n",
       "    'acc_norm': 0.6756066411238825,\n",
       "    'acc_norm_stderr': 0.0167409290471627},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5491329479768786,\n",
       "    'acc_stderr': 0.026788811931562757,\n",
       "    'acc_norm': 0.5491329479768786,\n",
       "    'acc_norm_stderr': 0.026788811931562757},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2569832402234637,\n",
       "    'acc_stderr': 0.01461446582196632,\n",
       "    'acc_norm': 0.2569832402234637,\n",
       "    'acc_norm_stderr': 0.01461446582196632},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5718954248366013,\n",
       "    'acc_stderr': 0.028332397483664278,\n",
       "    'acc_norm': 0.5718954248366013,\n",
       "    'acc_norm_stderr': 0.028332397483664278},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6012861736334405,\n",
       "    'acc_stderr': 0.027809322585774496,\n",
       "    'acc_norm': 0.6012861736334405,\n",
       "    'acc_norm_stderr': 0.027809322585774496},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6172839506172839,\n",
       "    'acc_stderr': 0.027044538138402595,\n",
       "    'acc_norm': 0.6172839506172839,\n",
       "    'acc_norm_stderr': 0.027044538138402595},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.42907801418439717,\n",
       "    'acc_stderr': 0.029525914302558555,\n",
       "    'acc_norm': 0.42907801418439717,\n",
       "    'acc_norm_stderr': 0.029525914302558555},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.424380704041721,\n",
       "    'acc_stderr': 0.01262334375743002,\n",
       "    'acc_norm': 0.424380704041721,\n",
       "    'acc_norm_stderr': 0.01262334375743002},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5808823529411765,\n",
       "    'acc_stderr': 0.02997280717046462,\n",
       "    'acc_norm': 0.5808823529411765,\n",
       "    'acc_norm_stderr': 0.02997280717046462},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5212418300653595,\n",
       "    'acc_stderr': 0.020209572388600248,\n",
       "    'acc_norm': 0.5212418300653595,\n",
       "    'acc_norm_stderr': 0.020209572388600248},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.04607582090719976,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.04607582090719976},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5918367346938775,\n",
       "    'acc_stderr': 0.03146465712827424,\n",
       "    'acc_norm': 0.5918367346938775,\n",
       "    'acc_norm_stderr': 0.03146465712827424},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6915422885572139,\n",
       "    'acc_stderr': 0.032658195885126966,\n",
       "    'acc_norm': 0.6915422885572139,\n",
       "    'acc_norm_stderr': 0.032658195885126966},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.041633319989322626,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.041633319989322626},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4578313253012048,\n",
       "    'acc_stderr': 0.038786267710023595,\n",
       "    'acc_norm': 0.4578313253012048,\n",
       "    'acc_norm_stderr': 0.038786267710023595},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.34761321909424725,\n",
       "    'mc1_stderr': 0.016670769188897306,\n",
       "    'mc2': 0.5121992740888713,\n",
       "    'mc2_stderr': 0.014650490351006002},\n",
       "   'all': {'acc': 0.5404553018205109,\n",
       "    'acc_stderr': 0.03488622237927161,\n",
       "    'acc_norm': 0.5444824613318672,\n",
       "    'acc_norm_stderr': 0.03486249375448495,\n",
       "    'mc1': 0.34761321909424725,\n",
       "    'mc1_stderr': 0.016670769188897306,\n",
       "    'mc2': 0.5121992740888713,\n",
       "    'mc2_stderr': 0.014650490351006002}}},\n",
       " 'llama-2-13b-chat': {'key': 'llama-2-13b-chat',\n",
       "  'Model': 'Llama-2-13b-chat',\n",
       "  'MT-bench (score)': '6.65',\n",
       "  'MMLU': '0.536',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-13b-chat-hf',\n",
       "  'results': {'harness|drop|3': {'em': 0.1782718120805369,\n",
       "    'em_stderr': 0.003919630092588375,\n",
       "    'f1': 0.2387195889261742,\n",
       "    'f1_stderr': 0.003944947017182046},\n",
       "   'harness|gsm8k|5': {'acc': 0.15238817285822592,\n",
       "    'acc_stderr': 0.009899572254794204},\n",
       "   'harness|winogrande|5': {'acc': 0.745067087608524,\n",
       "    'acc_stderr': 0.012248806969376422},\n",
       "   'all': {'acc': 0.5479380524707899,\n",
       "    'acc_stderr': 0.03451142729909022,\n",
       "    'acc_norm': 0.5517368945804153,\n",
       "    'acc_norm_stderr': 0.03449229816957583,\n",
       "    'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.4411794590119937,\n",
       "    'mc2_stderr': 0.015755921757439843},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5563139931740614,\n",
       "    'acc_stderr': 0.014518421825670449,\n",
       "    'acc_norm': 0.590443686006826,\n",
       "    'acc_norm_stderr': 0.01437035863247244},\n",
       "   'harness|hellaswag|10': {'acc': 0.6293567018522207,\n",
       "    'acc_stderr': 0.004819899945342489,\n",
       "    'acc_norm': 0.8193586934873531,\n",
       "    'acc_norm_stderr': 0.0038393444971919545},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.046482319871173156,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.046482319871173156},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4740740740740741,\n",
       "    'acc_stderr': 0.04313531696750574,\n",
       "    'acc_norm': 0.4740740740740741,\n",
       "    'acc_norm_stderr': 0.04313531696750574},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5460526315789473,\n",
       "    'acc_stderr': 0.04051646342874142,\n",
       "    'acc_norm': 0.5460526315789473,\n",
       "    'acc_norm_stderr': 0.04051646342874142},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5849056603773585,\n",
       "    'acc_stderr': 0.03032594578928611,\n",
       "    'acc_norm': 0.5849056603773585,\n",
       "    'acc_norm_stderr': 0.03032594578928611},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04122728707651282,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04122728707651282},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4624277456647399,\n",
       "    'acc_stderr': 0.0380168510452446,\n",
       "    'acc_norm': 0.4624277456647399,\n",
       "    'acc_norm_stderr': 0.0380168510452446},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3137254901960784,\n",
       "    'acc_stderr': 0.04617034827006717,\n",
       "    'acc_norm': 0.3137254901960784,\n",
       "    'acc_norm_stderr': 0.04617034827006717},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.03202563076101735,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.03202563076101735},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.043727482902780064,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.043727482902780064},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.503448275862069,\n",
       "    'acc_stderr': 0.0416656757710158,\n",
       "    'acc_norm': 0.503448275862069,\n",
       "    'acc_norm_stderr': 0.0416656757710158},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.024419234966819064,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.024419234966819064},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.30952380952380953,\n",
       "    'acc_stderr': 0.04134913018303316,\n",
       "    'acc_norm': 0.30952380952380953,\n",
       "    'acc_norm_stderr': 0.04134913018303316},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6419354838709678,\n",
       "    'acc_stderr': 0.02727389059430064,\n",
       "    'acc_norm': 0.6419354838709678,\n",
       "    'acc_norm_stderr': 0.02727389059430064},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4630541871921182,\n",
       "    'acc_stderr': 0.035083705204426656,\n",
       "    'acc_norm': 0.4630541871921182,\n",
       "    'acc_norm_stderr': 0.035083705204426656},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.03663974994391244,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.03663974994391244},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.702020202020202,\n",
       "    'acc_stderr': 0.03258630383836556,\n",
       "    'acc_norm': 0.702020202020202,\n",
       "    'acc_norm_stderr': 0.03258630383836556},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7875647668393783,\n",
       "    'acc_stderr': 0.029519282616817234,\n",
       "    'acc_norm': 0.7875647668393783,\n",
       "    'acc_norm_stderr': 0.029519282616817234},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.49230769230769234,\n",
       "    'acc_stderr': 0.025348006031534788,\n",
       "    'acc_norm': 0.49230769230769234,\n",
       "    'acc_norm_stderr': 0.025348006031534788},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3111111111111111,\n",
       "    'acc_stderr': 0.028226446749683522,\n",
       "    'acc_norm': 0.3111111111111111,\n",
       "    'acc_norm_stderr': 0.028226446749683522},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03242225027115007,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03242225027115007},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.33774834437086093,\n",
       "    'acc_stderr': 0.038615575462551684,\n",
       "    'acc_norm': 0.33774834437086093,\n",
       "    'acc_norm_stderr': 0.038615575462551684},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7321100917431193,\n",
       "    'acc_stderr': 0.018987462257978652,\n",
       "    'acc_norm': 0.7321100917431193,\n",
       "    'acc_norm_stderr': 0.018987462257978652},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.03324708911809117,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.03324708911809117},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03039153369274154,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03039153369274154},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7172995780590717,\n",
       "    'acc_stderr': 0.02931281415395592,\n",
       "    'acc_norm': 0.7172995780590717,\n",
       "    'acc_norm_stderr': 0.02931281415395592},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6457399103139013,\n",
       "    'acc_stderr': 0.032100621541349864,\n",
       "    'acc_norm': 0.6457399103139013,\n",
       "    'acc_norm_stderr': 0.032100621541349864},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6335877862595419,\n",
       "    'acc_stderr': 0.04225875451969637,\n",
       "    'acc_norm': 0.6335877862595419,\n",
       "    'acc_norm_stderr': 0.04225875451969637},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.768595041322314,\n",
       "    'acc_stderr': 0.03849856098794089,\n",
       "    'acc_norm': 0.768595041322314,\n",
       "    'acc_norm_stderr': 0.03849856098794089},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6944444444444444,\n",
       "    'acc_stderr': 0.044531975073749834,\n",
       "    'acc_norm': 0.6944444444444444,\n",
       "    'acc_norm_stderr': 0.044531975073749834},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6503067484662577,\n",
       "    'acc_stderr': 0.037466683254700206,\n",
       "    'acc_norm': 0.6503067484662577,\n",
       "    'acc_norm_stderr': 0.037466683254700206},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.35714285714285715,\n",
       "    'acc_stderr': 0.04547960999764376,\n",
       "    'acc_norm': 0.35714285714285715,\n",
       "    'acc_norm_stderr': 0.04547960999764376},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.04354631077260595,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.04354631077260595},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7863247863247863,\n",
       "    'acc_stderr': 0.026853450377009175,\n",
       "    'acc_norm': 0.7863247863247863,\n",
       "    'acc_norm_stderr': 0.026853450377009175},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7471264367816092,\n",
       "    'acc_stderr': 0.015543377313719681,\n",
       "    'acc_norm': 0.7471264367816092,\n",
       "    'acc_norm_stderr': 0.015543377313719681},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6127167630057804,\n",
       "    'acc_stderr': 0.026226158605124655,\n",
       "    'acc_norm': 0.6127167630057804,\n",
       "    'acc_norm_stderr': 0.026226158605124655},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30502793296089387,\n",
       "    'acc_stderr': 0.015398723510916716,\n",
       "    'acc_norm': 0.30502793296089387,\n",
       "    'acc_norm_stderr': 0.015398723510916716},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5947712418300654,\n",
       "    'acc_stderr': 0.028110928492809068,\n",
       "    'acc_norm': 0.5947712418300654,\n",
       "    'acc_norm_stderr': 0.028110928492809068},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5884244372990354,\n",
       "    'acc_stderr': 0.02795048149440127,\n",
       "    'acc_norm': 0.5884244372990354,\n",
       "    'acc_norm_stderr': 0.02795048149440127},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6111111111111112,\n",
       "    'acc_stderr': 0.02712511551316687,\n",
       "    'acc_norm': 0.6111111111111112,\n",
       "    'acc_norm_stderr': 0.02712511551316687},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.38652482269503546,\n",
       "    'acc_stderr': 0.029049190342543454,\n",
       "    'acc_norm': 0.38652482269503546,\n",
       "    'acc_norm_stderr': 0.029049190342543454},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.39113428943937417,\n",
       "    'acc_stderr': 0.012463861839982058,\n",
       "    'acc_norm': 0.39113428943937417,\n",
       "    'acc_norm_stderr': 0.012463861839982058},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.030372836961539352,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.030372836961539352},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5424836601307189,\n",
       "    'acc_stderr': 0.020154685712590888,\n",
       "    'acc_norm': 0.5424836601307189,\n",
       "    'acc_norm_stderr': 0.020154685712590888},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302505,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302505},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6408163265306123,\n",
       "    'acc_stderr': 0.030713560455108493,\n",
       "    'acc_norm': 0.6408163265306123,\n",
       "    'acc_norm_stderr': 0.030713560455108493},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7512437810945274,\n",
       "    'acc_stderr': 0.030567675938916714,\n",
       "    'acc_norm': 0.7512437810945274,\n",
       "    'acc_norm_stderr': 0.030567675938916714},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4819277108433735,\n",
       "    'acc_stderr': 0.038899512528272166,\n",
       "    'acc_norm': 0.4819277108433735,\n",
       "    'acc_norm_stderr': 0.038899512528272166},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7309941520467836,\n",
       "    'acc_stderr': 0.03401052620104089,\n",
       "    'acc_norm': 0.7309941520467836,\n",
       "    'acc_norm_stderr': 0.03401052620104089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.28518971848225216,\n",
       "    'mc1_stderr': 0.015805827874454895,\n",
       "    'mc2': 0.4411794590119937,\n",
       "    'mc2_stderr': 0.015755921757439843}}},\n",
       " 'llama-2-70b-chat': {'key': 'llama-2-70b-chat',\n",
       "  'Model': 'Llama-2-70b-chat',\n",
       "  'MT-bench (score)': '6.86',\n",
       "  'MMLU': '0.630',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-70b-chat-hf',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6049488054607508,\n",
       "    'acc_stderr': 0.01428589829293817,\n",
       "    'acc_norm': 0.6459044368600683,\n",
       "    'acc_norm_stderr': 0.013975454122756564},\n",
       "   'harness|hellaswag|10': {'acc': 0.6693885680143398,\n",
       "    'acc_stderr': 0.004694718918225751,\n",
       "    'acc_norm': 0.8587930691097391,\n",
       "    'acc_norm_stderr': 0.003475231889452833},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3561811505507956,\n",
       "    'mc1_stderr': 0.016763790728446335,\n",
       "    'mc2': 0.5280473232260097,\n",
       "    'mc2_stderr': 0.01553022126123046},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.04793724854411021,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.04793724854411021},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5185185185185185,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.5185185185185185,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.7302631578947368,\n",
       "    'acc_stderr': 0.03611780560284898,\n",
       "    'acc_norm': 0.7302631578947368,\n",
       "    'acc_norm_stderr': 0.03611780560284898},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6377358490566037,\n",
       "    'acc_stderr': 0.029582245128384303,\n",
       "    'acc_norm': 0.6377358490566037,\n",
       "    'acc_norm_stderr': 0.029582245128384303},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03621034121889507,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03621034121889507},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237101,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237101},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6011560693641619,\n",
       "    'acc_stderr': 0.0373362665538351,\n",
       "    'acc_norm': 0.6011560693641619,\n",
       "    'acc_norm_stderr': 0.0373362665538351},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.04690650298201943,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.04690650298201943},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5829787234042553,\n",
       "    'acc_stderr': 0.032232762667117124,\n",
       "    'acc_norm': 0.5829787234042553,\n",
       "    'acc_norm_stderr': 0.032232762667117124},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.41228070175438597,\n",
       "    'acc_stderr': 0.04630653203366595,\n",
       "    'acc_norm': 0.41228070175438597,\n",
       "    'acc_norm_stderr': 0.04630653203366595},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5793103448275863,\n",
       "    'acc_stderr': 0.0411391498118926,\n",
       "    'acc_norm': 0.5793103448275863,\n",
       "    'acc_norm_stderr': 0.0411391498118926},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.41005291005291006,\n",
       "    'acc_stderr': 0.02533120243894442,\n",
       "    'acc_norm': 0.41005291005291006,\n",
       "    'acc_norm_stderr': 0.02533120243894442},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4126984126984127,\n",
       "    'acc_stderr': 0.04403438954768176,\n",
       "    'acc_norm': 0.4126984126984127,\n",
       "    'acc_norm_stderr': 0.04403438954768176},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7645161290322581,\n",
       "    'acc_stderr': 0.02413763242933771,\n",
       "    'acc_norm': 0.7645161290322581,\n",
       "    'acc_norm_stderr': 0.02413763242933771},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4630541871921182,\n",
       "    'acc_stderr': 0.035083705204426656,\n",
       "    'acc_norm': 0.4630541871921182,\n",
       "    'acc_norm_stderr': 0.035083705204426656},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03011768892950359,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03011768892950359},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.02805779167298902,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.02805779167298902},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8911917098445595,\n",
       "    'acc_stderr': 0.022473253332768783,\n",
       "    'acc_norm': 0.8911917098445595,\n",
       "    'acc_norm_stderr': 0.022473253332768783},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6410256410256411,\n",
       "    'acc_stderr': 0.02432173848460235,\n",
       "    'acc_norm': 0.6410256410256411,\n",
       "    'acc_norm_stderr': 0.02432173848460235},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.027940457136228416,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.027940457136228416},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6596638655462185,\n",
       "    'acc_stderr': 0.030778057422931673,\n",
       "    'acc_norm': 0.6596638655462185,\n",
       "    'acc_norm_stderr': 0.030778057422931673},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.423841059602649,\n",
       "    'acc_stderr': 0.04034846678603397,\n",
       "    'acc_norm': 0.423841059602649,\n",
       "    'acc_norm_stderr': 0.04034846678603397},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8385321100917431,\n",
       "    'acc_stderr': 0.015776239256163255,\n",
       "    'acc_norm': 0.8385321100917431,\n",
       "    'acc_norm_stderr': 0.015776239256163255},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.48148148148148145,\n",
       "    'acc_stderr': 0.03407632093854052,\n",
       "    'acc_norm': 0.48148148148148145,\n",
       "    'acc_norm_stderr': 0.03407632093854052},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8578431372549019,\n",
       "    'acc_stderr': 0.024509803921568606,\n",
       "    'acc_norm': 0.8578431372549019,\n",
       "    'acc_norm_stderr': 0.024509803921568606},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8438818565400844,\n",
       "    'acc_stderr': 0.02362715946031867,\n",
       "    'acc_norm': 0.8438818565400844,\n",
       "    'acc_norm_stderr': 0.02362715946031867},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.726457399103139,\n",
       "    'acc_stderr': 0.02991858670779883,\n",
       "    'acc_norm': 0.726457399103139,\n",
       "    'acc_norm_stderr': 0.02991858670779883},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7099236641221374,\n",
       "    'acc_stderr': 0.039800662464677665,\n",
       "    'acc_norm': 0.7099236641221374,\n",
       "    'acc_norm_stderr': 0.039800662464677665},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8016528925619835,\n",
       "    'acc_stderr': 0.03640118271990946,\n",
       "    'acc_norm': 0.8016528925619835,\n",
       "    'acc_norm_stderr': 0.03640118271990946},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8240740740740741,\n",
       "    'acc_stderr': 0.036809181416738807,\n",
       "    'acc_norm': 0.8240740740740741,\n",
       "    'acc_norm_stderr': 0.036809181416738807},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7607361963190185,\n",
       "    'acc_stderr': 0.033519538795212696,\n",
       "    'acc_norm': 0.7607361963190185,\n",
       "    'acc_norm_stderr': 0.033519538795212696},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8058252427184466,\n",
       "    'acc_stderr': 0.03916667762822584,\n",
       "    'acc_norm': 0.8058252427184466,\n",
       "    'acc_norm_stderr': 0.03916667762822584},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8717948717948718,\n",
       "    'acc_stderr': 0.02190190511507332,\n",
       "    'acc_norm': 0.8717948717948718,\n",
       "    'acc_norm_stderr': 0.02190190511507332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.65,\n",
       "    'acc_stderr': 0.047937248544110196,\n",
       "    'acc_norm': 0.65,\n",
       "    'acc_norm_stderr': 0.047937248544110196},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8275862068965517,\n",
       "    'acc_stderr': 0.013507943909371798,\n",
       "    'acc_norm': 0.8275862068965517,\n",
       "    'acc_norm_stderr': 0.013507943909371798},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7167630057803468,\n",
       "    'acc_stderr': 0.02425790170532338,\n",
       "    'acc_norm': 0.7167630057803468,\n",
       "    'acc_norm_stderr': 0.02425790170532338},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.39553072625698327,\n",
       "    'acc_stderr': 0.01635341541007577,\n",
       "    'acc_norm': 0.39553072625698327,\n",
       "    'acc_norm_stderr': 0.01635341541007577},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6993464052287581,\n",
       "    'acc_stderr': 0.026256053835718968,\n",
       "    'acc_norm': 0.6993464052287581,\n",
       "    'acc_norm_stderr': 0.026256053835718968},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7041800643086816,\n",
       "    'acc_stderr': 0.02592237178881877,\n",
       "    'acc_norm': 0.7041800643086816,\n",
       "    'acc_norm_stderr': 0.02592237178881877},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7098765432098766,\n",
       "    'acc_stderr': 0.025251173936495036,\n",
       "    'acc_norm': 0.7098765432098766,\n",
       "    'acc_norm_stderr': 0.025251173936495036},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5070921985815603,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.5070921985815603,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4771838331160365,\n",
       "    'acc_stderr': 0.012756933382823694,\n",
       "    'acc_norm': 0.4771838331160365,\n",
       "    'acc_norm_stderr': 0.012756933382823694},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5772058823529411,\n",
       "    'acc_stderr': 0.030008562845003476,\n",
       "    'acc_norm': 0.5772058823529411,\n",
       "    'acc_norm_stderr': 0.030008562845003476},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6699346405228758,\n",
       "    'acc_stderr': 0.019023726160724556,\n",
       "    'acc_norm': 0.6699346405228758,\n",
       "    'acc_norm_stderr': 0.019023726160724556},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7877551020408163,\n",
       "    'acc_stderr': 0.026176967197866767,\n",
       "    'acc_norm': 0.7877551020408163,\n",
       "    'acc_norm_stderr': 0.026176967197866767},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8706467661691543,\n",
       "    'acc_stderr': 0.023729830881018526,\n",
       "    'acc_norm': 0.8706467661691543,\n",
       "    'acc_norm_stderr': 0.023729830881018526},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.03379976689896309,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.03379976689896309},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5120481927710844,\n",
       "    'acc_stderr': 0.03891364495835817,\n",
       "    'acc_norm': 0.5120481927710844,\n",
       "    'acc_norm_stderr': 0.03891364495835817},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8187134502923976,\n",
       "    'acc_stderr': 0.029547741687640038,\n",
       "    'acc_norm': 0.8187134502923976,\n",
       "    'acc_norm_stderr': 0.029547741687640038},\n",
       "   'all': {'em': 0.040373322147651006,\n",
       "    'em_stderr': 0.0020157564185176837,\n",
       "    'f1': 0.1050272651006715,\n",
       "    'f1_stderr': 0.0023756238577676155,\n",
       "    'acc': 0.5359600711595986,\n",
       "    'acc_stderr': 0.011658939983913113},\n",
       "   'harness|drop|3': {'em': 0.040373322147651006,\n",
       "    'em_stderr': 0.0020157564185176837,\n",
       "    'f1': 0.1050272651006715,\n",
       "    'f1_stderr': 0.0023756238577676155},\n",
       "   'harness|gsm8k|5': {'acc': 0.266868840030326,\n",
       "    'acc_stderr': 0.012183780551887957},\n",
       "   'harness|winogrande|5': {'acc': 0.8050513022888713,\n",
       "    'acc_stderr': 0.011134099415938268}}},\n",
       " 'llama-2-7b-chat': {'key': 'llama-2-7b-chat',\n",
       "  'Model': 'Llama-2-7b-chat',\n",
       "  'MT-bench (score)': '6.27',\n",
       "  'MMLU': '0.458',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Meta',\n",
       "  'Link': 'https://huggingface.co/meta-llama/Llama-2-7b-chat-hf',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.4948805460750853,\n",
       "    'acc_stderr': 0.01461062489030916,\n",
       "    'acc_norm': 0.5290102389078498,\n",
       "    'acc_norm_stderr': 0.014586776355294323},\n",
       "   'harness|hellaswag|10': {'acc': 0.5978888667596096,\n",
       "    'acc_stderr': 0.004893220635011792,\n",
       "    'acc_norm': 0.7855008962358097,\n",
       "    'acc_norm_stderr': 0.004096355125117511},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542129,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542129},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.42962962962962964,\n",
       "    'acc_stderr': 0.04276349494376599,\n",
       "    'acc_norm': 0.42962962962962964,\n",
       "    'acc_norm_stderr': 0.04276349494376599},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.4868421052631579,\n",
       "    'acc_stderr': 0.04067533136309173,\n",
       "    'acc_norm': 0.4868421052631579,\n",
       "    'acc_norm_stderr': 0.04067533136309173},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5358490566037736,\n",
       "    'acc_stderr': 0.030693675018458003,\n",
       "    'acc_norm': 0.5358490566037736,\n",
       "    'acc_norm_stderr': 0.030693675018458003},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5208333333333334,\n",
       "    'acc_stderr': 0.041775789507399935,\n",
       "    'acc_norm': 0.5208333333333334,\n",
       "    'acc_norm_stderr': 0.041775789507399935},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.04560480215720684,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.04560480215720684},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.3988439306358382,\n",
       "    'acc_stderr': 0.037336266553835096,\n",
       "    'acc_norm': 0.3988439306358382,\n",
       "    'acc_norm_stderr': 0.037336266553835096},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.22549019607843138,\n",
       "    'acc_stderr': 0.041583075330832865,\n",
       "    'acc_norm': 0.22549019607843138,\n",
       "    'acc_norm_stderr': 0.041583075330832865},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.58,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.58,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4085106382978723,\n",
       "    'acc_stderr': 0.03213418026701576,\n",
       "    'acc_norm': 0.4085106382978723,\n",
       "    'acc_norm_stderr': 0.03213418026701576},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.37719298245614036,\n",
       "    'acc_stderr': 0.045595221419582166,\n",
       "    'acc_norm': 0.37719298245614036,\n",
       "    'acc_norm_stderr': 0.045595221419582166},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4896551724137931,\n",
       "    'acc_stderr': 0.04165774775728762,\n",
       "    'acc_norm': 0.4896551724137931,\n",
       "    'acc_norm_stderr': 0.04165774775728762},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.29894179894179895,\n",
       "    'acc_stderr': 0.023577604791655805,\n",
       "    'acc_norm': 0.29894179894179895,\n",
       "    'acc_norm_stderr': 0.023577604791655805},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.25396825396825395,\n",
       "    'acc_stderr': 0.03893259610604675,\n",
       "    'acc_norm': 0.25396825396825395,\n",
       "    'acc_norm_stderr': 0.03893259610604675},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.5225806451612903,\n",
       "    'acc_stderr': 0.02841498501970786,\n",
       "    'acc_norm': 0.5225806451612903,\n",
       "    'acc_norm_stderr': 0.02841498501970786},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3645320197044335,\n",
       "    'acc_stderr': 0.033864057460620905,\n",
       "    'acc_norm': 0.3645320197044335,\n",
       "    'acc_norm_stderr': 0.033864057460620905},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.5878787878787879,\n",
       "    'acc_stderr': 0.03843566993588718,\n",
       "    'acc_norm': 0.5878787878787879,\n",
       "    'acc_norm_stderr': 0.03843566993588718},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6060606060606061,\n",
       "    'acc_stderr': 0.034812853382329624,\n",
       "    'acc_norm': 0.6060606060606061,\n",
       "    'acc_norm_stderr': 0.034812853382329624},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7150259067357513,\n",
       "    'acc_stderr': 0.032577140777096614,\n",
       "    'acc_norm': 0.7150259067357513,\n",
       "    'acc_norm_stderr': 0.032577140777096614},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.4256410256410256,\n",
       "    'acc_stderr': 0.02506909438729654,\n",
       "    'acc_norm': 0.4256410256410256,\n",
       "    'acc_norm_stderr': 0.02506909438729654},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.25555555555555554,\n",
       "    'acc_stderr': 0.02659393910184408,\n",
       "    'acc_norm': 0.25555555555555554,\n",
       "    'acc_norm_stderr': 0.02659393910184408},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.42436974789915966,\n",
       "    'acc_stderr': 0.03210479051015776,\n",
       "    'acc_norm': 0.42436974789915966,\n",
       "    'acc_norm_stderr': 0.03210479051015776},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2913907284768212,\n",
       "    'acc_stderr': 0.03710185726119995,\n",
       "    'acc_norm': 0.2913907284768212,\n",
       "    'acc_norm_stderr': 0.03710185726119995},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.6752293577981652,\n",
       "    'acc_stderr': 0.020077729109310327,\n",
       "    'acc_norm': 0.6752293577981652,\n",
       "    'acc_norm_stderr': 0.020077729109310327},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.0321495214780275,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.0321495214780275},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.0328347205610856,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.0328347205610856},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.03068582059661079,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.03068582059661079},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5605381165919282,\n",
       "    'acc_stderr': 0.03331092511038179,\n",
       "    'acc_norm': 0.5605381165919282,\n",
       "    'acc_norm_stderr': 0.03331092511038179},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5725190839694656,\n",
       "    'acc_stderr': 0.04338920305792401,\n",
       "    'acc_norm': 0.5725190839694656,\n",
       "    'acc_norm_stderr': 0.04338920305792401},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.628099173553719,\n",
       "    'acc_stderr': 0.04412015806624504,\n",
       "    'acc_norm': 0.628099173553719,\n",
       "    'acc_norm_stderr': 0.04412015806624504},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.04766075165356461,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.04766075165356461},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.5521472392638037,\n",
       "    'acc_stderr': 0.03906947479456606,\n",
       "    'acc_norm': 0.5521472392638037,\n",
       "    'acc_norm_stderr': 0.03906947479456606},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.30357142857142855,\n",
       "    'acc_stderr': 0.04364226155841044,\n",
       "    'acc_norm': 0.30357142857142855,\n",
       "    'acc_norm_stderr': 0.04364226155841044},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.04656147110012351,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.04656147110012351},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7094017094017094,\n",
       "    'acc_stderr': 0.029745048572674074,\n",
       "    'acc_norm': 0.7094017094017094,\n",
       "    'acc_norm_stderr': 0.029745048572674074},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6756066411238825,\n",
       "    'acc_stderr': 0.0167409290471627,\n",
       "    'acc_norm': 0.6756066411238825,\n",
       "    'acc_norm_stderr': 0.0167409290471627},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.026907849856282542,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.026907849856282542},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2201117318435754,\n",
       "    'acc_stderr': 0.013856994024227175,\n",
       "    'acc_norm': 0.2201117318435754,\n",
       "    'acc_norm_stderr': 0.013856994024227175},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5196078431372549,\n",
       "    'acc_stderr': 0.028607893699576066,\n",
       "    'acc_norm': 0.5196078431372549,\n",
       "    'acc_norm_stderr': 0.028607893699576066},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5659163987138264,\n",
       "    'acc_stderr': 0.02815023224453559,\n",
       "    'acc_norm': 0.5659163987138264,\n",
       "    'acc_norm_stderr': 0.02815023224453559},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5679012345679012,\n",
       "    'acc_stderr': 0.027563010971606676,\n",
       "    'acc_norm': 0.5679012345679012,\n",
       "    'acc_norm_stderr': 0.027563010971606676},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3723404255319149,\n",
       "    'acc_stderr': 0.028838921471251458,\n",
       "    'acc_norm': 0.3723404255319149,\n",
       "    'acc_norm_stderr': 0.028838921471251458},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.3500651890482399,\n",
       "    'acc_stderr': 0.012182552313215175,\n",
       "    'acc_norm': 0.3500651890482399,\n",
       "    'acc_norm_stderr': 0.012182552313215175},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.45588235294117646,\n",
       "    'acc_stderr': 0.030254372573976684,\n",
       "    'acc_norm': 0.45588235294117646,\n",
       "    'acc_norm_stderr': 0.030254372573976684},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4803921568627451,\n",
       "    'acc_stderr': 0.020212274976302957,\n",
       "    'acc_norm': 0.4803921568627451,\n",
       "    'acc_norm_stderr': 0.020212274976302957},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5272727272727272,\n",
       "    'acc_stderr': 0.04782001791380061,\n",
       "    'acc_norm': 0.5272727272727272,\n",
       "    'acc_norm_stderr': 0.04782001791380061},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5265306122448979,\n",
       "    'acc_stderr': 0.03196412734523272,\n",
       "    'acc_norm': 0.5265306122448979,\n",
       "    'acc_norm_stderr': 0.03196412734523272},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6467661691542289,\n",
       "    'acc_stderr': 0.03379790611796777,\n",
       "    'acc_norm': 0.6467661691542289,\n",
       "    'acc_norm_stderr': 0.03379790611796777},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.43373493975903615,\n",
       "    'acc_stderr': 0.03858158940685517,\n",
       "    'acc_norm': 0.43373493975903615,\n",
       "    'acc_norm_stderr': 0.03858158940685517},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.034240429246915824,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.034240429246915824},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3011015911872705,\n",
       "    'mc1_stderr': 0.016058999026100616,\n",
       "    'mc2': 0.45570370195101134,\n",
       "    'mc2_stderr': 0.015691038880908878},\n",
       "   'all': {'em': 0.06763842281879194,\n",
       "    'em_stderr': 0.0025717489509556085,\n",
       "    'f1': 0.13085570469798627,\n",
       "    'f1_stderr': 0.0028825856446422905,\n",
       "    'acc': 0.39549166962367155,\n",
       "    'acc_stderr': 0.009921949302668327},\n",
       "   'harness|drop|3': {'em': 0.06763842281879194,\n",
       "    'em_stderr': 0.0025717489509556085,\n",
       "    'f1': 0.13085570469798627,\n",
       "    'f1_stderr': 0.0028825856446422905},\n",
       "   'harness|gsm8k|5': {'acc': 0.07354056103108415,\n",
       "    'acc_stderr': 0.0071898357543652685},\n",
       "   'harness|winogrande|5': {'acc': 0.7174427782162589,\n",
       "    'acc_stderr': 0.012654062850971384}}},\n",
       " 'mistral-7b-instruct': {'key': 'mistral-7b-instruct',\n",
       "  'Model': 'Mistral-7B-Instruct-v0.1',\n",
       "  'MT-bench (score)': '6.84',\n",
       "  'MMLU': '0.554',\n",
       "  'License': 'Apache 2.0',\n",
       "  'Organization': 'Mistral',\n",
       "  'Link': 'https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.1',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.523037542662116,\n",
       "    'acc_stderr': 0.014595873205358269,\n",
       "    'acc_norm': 0.5452218430034129,\n",
       "    'acc_norm_stderr': 0.014551507060836357},\n",
       "   'harness|hellaswag|10': {'acc': 0.5694084843656642,\n",
       "    'acc_stderr': 0.004941470620074867,\n",
       "    'acc_norm': 0.7563234415455089,\n",
       "    'acc_norm_stderr': 0.0042842240337755385},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.047609522856952365,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.047609522856952365},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4222222222222222,\n",
       "    'acc_stderr': 0.04266763404099582,\n",
       "    'acc_norm': 0.4222222222222222,\n",
       "    'acc_norm_stderr': 0.04266763404099582},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5657894736842105,\n",
       "    'acc_stderr': 0.040335656678483205,\n",
       "    'acc_norm': 0.5657894736842105,\n",
       "    'acc_norm_stderr': 0.040335656678483205},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5849056603773585,\n",
       "    'acc_stderr': 0.030325945789286105,\n",
       "    'acc_norm': 0.5849056603773585,\n",
       "    'acc_norm_stderr': 0.030325945789286105},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5902777777777778,\n",
       "    'acc_stderr': 0.04112490974670787,\n",
       "    'acc_norm': 0.5902777777777778,\n",
       "    'acc_norm_stderr': 0.04112490974670787},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5202312138728323,\n",
       "    'acc_stderr': 0.03809342081273956,\n",
       "    'acc_norm': 0.5202312138728323,\n",
       "    'acc_norm_stderr': 0.03809342081273956},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.30392156862745096,\n",
       "    'acc_stderr': 0.045766654032077636,\n",
       "    'acc_norm': 0.30392156862745096,\n",
       "    'acc_norm_stderr': 0.045766654032077636},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.047258156262526094,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.047258156262526094},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4808510638297872,\n",
       "    'acc_stderr': 0.032662042990646775,\n",
       "    'acc_norm': 0.4808510638297872,\n",
       "    'acc_norm_stderr': 0.032662042990646775},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.37719298245614036,\n",
       "    'acc_stderr': 0.04559522141958216,\n",
       "    'acc_norm': 0.37719298245614036,\n",
       "    'acc_norm_stderr': 0.04559522141958216},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5517241379310345,\n",
       "    'acc_stderr': 0.04144311810878151,\n",
       "    'acc_norm': 0.5517241379310345,\n",
       "    'acc_norm_stderr': 0.04144311810878151},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36507936507936506,\n",
       "    'acc_stderr': 0.024796060602699947,\n",
       "    'acc_norm': 0.36507936507936506,\n",
       "    'acc_norm_stderr': 0.024796060602699947},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.38095238095238093,\n",
       "    'acc_stderr': 0.043435254289490965,\n",
       "    'acc_norm': 0.38095238095238093,\n",
       "    'acc_norm_stderr': 0.043435254289490965},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6419354838709678,\n",
       "    'acc_stderr': 0.027273890594300642,\n",
       "    'acc_norm': 0.6419354838709678,\n",
       "    'acc_norm_stderr': 0.027273890594300642},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.41379310344827586,\n",
       "    'acc_stderr': 0.03465304488406795,\n",
       "    'acc_norm': 0.41379310344827586,\n",
       "    'acc_norm_stderr': 0.03465304488406795},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.036810508691615486,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.036810508691615486},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.031911782267135466,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.031911782267135466},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7253886010362695,\n",
       "    'acc_stderr': 0.03221024508041154,\n",
       "    'acc_norm': 0.7253886010362695,\n",
       "    'acc_norm_stderr': 0.03221024508041154},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5205128205128206,\n",
       "    'acc_stderr': 0.02532966316348994,\n",
       "    'acc_norm': 0.5205128205128206,\n",
       "    'acc_norm_stderr': 0.02532966316348994},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.027940457136228416,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.027940457136228416},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5462184873949579,\n",
       "    'acc_stderr': 0.032339434681820885,\n",
       "    'acc_norm': 0.5462184873949579,\n",
       "    'acc_norm_stderr': 0.032339434681820885},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.03822746937658753,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.03822746937658753},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.710091743119266,\n",
       "    'acc_stderr': 0.019453066609201597,\n",
       "    'acc_norm': 0.710091743119266,\n",
       "    'acc_norm_stderr': 0.019453066609201597},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4537037037037037,\n",
       "    'acc_stderr': 0.03395322726375797,\n",
       "    'acc_norm': 0.4537037037037037,\n",
       "    'acc_norm_stderr': 0.03395322726375797},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7058823529411765,\n",
       "    'acc_stderr': 0.03198001660115072,\n",
       "    'acc_norm': 0.7058823529411765,\n",
       "    'acc_norm_stderr': 0.03198001660115072},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.6962025316455697,\n",
       "    'acc_stderr': 0.0299366963871386,\n",
       "    'acc_norm': 0.6962025316455697,\n",
       "    'acc_norm_stderr': 0.0299366963871386},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6547085201793722,\n",
       "    'acc_stderr': 0.03191100192835794,\n",
       "    'acc_norm': 0.6547085201793722,\n",
       "    'acc_norm_stderr': 0.03191100192835794},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6717557251908397,\n",
       "    'acc_stderr': 0.04118438565806299,\n",
       "    'acc_norm': 0.6717557251908397,\n",
       "    'acc_norm_stderr': 0.04118438565806299},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.6776859504132231,\n",
       "    'acc_stderr': 0.042664163633521685,\n",
       "    'acc_norm': 0.6776859504132231,\n",
       "    'acc_norm_stderr': 0.042664163633521685},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.6944444444444444,\n",
       "    'acc_stderr': 0.044531975073749834,\n",
       "    'acc_norm': 0.6944444444444444,\n",
       "    'acc_norm_stderr': 0.044531975073749834},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6503067484662577,\n",
       "    'acc_stderr': 0.03746668325470021,\n",
       "    'acc_norm': 0.6503067484662577,\n",
       "    'acc_norm_stderr': 0.03746668325470021},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.4732142857142857,\n",
       "    'acc_stderr': 0.047389751192741546,\n",
       "    'acc_norm': 0.4732142857142857,\n",
       "    'acc_norm_stderr': 0.047389751192741546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6893203883495146,\n",
       "    'acc_stderr': 0.04582124160161551,\n",
       "    'acc_norm': 0.6893203883495146,\n",
       "    'acc_norm_stderr': 0.04582124160161551},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8418803418803419,\n",
       "    'acc_stderr': 0.023902325549560392,\n",
       "    'acc_norm': 0.8418803418803419,\n",
       "    'acc_norm_stderr': 0.023902325549560392},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.047258156262526094,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.047258156262526094},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7432950191570882,\n",
       "    'acc_stderr': 0.015620480263064533,\n",
       "    'acc_norm': 0.7432950191570882,\n",
       "    'acc_norm_stderr': 0.015620480263064533},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5895953757225434,\n",
       "    'acc_stderr': 0.026483392042098174,\n",
       "    'acc_norm': 0.5895953757225434,\n",
       "    'acc_norm_stderr': 0.026483392042098174},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2446927374301676,\n",
       "    'acc_stderr': 0.014378169884098417,\n",
       "    'acc_norm': 0.2446927374301676,\n",
       "    'acc_norm_stderr': 0.014378169884098417},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6143790849673203,\n",
       "    'acc_stderr': 0.02787074527829027,\n",
       "    'acc_norm': 0.6143790849673203,\n",
       "    'acc_norm_stderr': 0.02787074527829027},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6077170418006431,\n",
       "    'acc_stderr': 0.027731258647012,\n",
       "    'acc_norm': 0.6077170418006431,\n",
       "    'acc_norm_stderr': 0.027731258647012},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5771604938271605,\n",
       "    'acc_stderr': 0.027487472980871595,\n",
       "    'acc_norm': 0.5771604938271605,\n",
       "    'acc_norm_stderr': 0.027487472980871595},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3829787234042553,\n",
       "    'acc_stderr': 0.028999080904806185,\n",
       "    'acc_norm': 0.3829787234042553,\n",
       "    'acc_norm_stderr': 0.028999080904806185},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.40091264667535853,\n",
       "    'acc_stderr': 0.012516960350640824,\n",
       "    'acc_norm': 0.40091264667535853,\n",
       "    'acc_norm_stderr': 0.012516960350640824},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5808823529411765,\n",
       "    'acc_stderr': 0.02997280717046462,\n",
       "    'acc_norm': 0.5808823529411765,\n",
       "    'acc_norm_stderr': 0.02997280717046462},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5163398692810458,\n",
       "    'acc_stderr': 0.02021703065318646,\n",
       "    'acc_norm': 0.5163398692810458,\n",
       "    'acc_norm_stderr': 0.02021703065318646},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302505,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302505},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6775510204081633,\n",
       "    'acc_stderr': 0.02992310056368391,\n",
       "    'acc_norm': 0.6775510204081633,\n",
       "    'acc_norm_stderr': 0.02992310056368391},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.746268656716418,\n",
       "    'acc_stderr': 0.03076944496729602,\n",
       "    'acc_norm': 0.746268656716418,\n",
       "    'acc_norm_stderr': 0.03076944496729602},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.46987951807228917,\n",
       "    'acc_stderr': 0.03885425420866766,\n",
       "    'acc_norm': 0.46987951807228917,\n",
       "    'acc_norm_stderr': 0.03885425420866766},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3953488372093023,\n",
       "    'mc1_stderr': 0.017115815632418194,\n",
       "    'mc2': 0.5628382292113293,\n",
       "    'mc2_stderr': 0.015351892312006444},\n",
       "   'all': {'em': 0.37038590604026844,\n",
       "    'em_stderr': 0.00494543044549648,\n",
       "    'f1': 0.43100566275167973,\n",
       "    'f1_stderr': 0.00478990485809286,\n",
       "    'acc': 0.4398533245809979,\n",
       "    'acc_stderr': 0.01100025548646791},\n",
       "   'harness|drop|3': {'em': 0.37038590604026844,\n",
       "    'em_stderr': 0.00494543044549648,\n",
       "    'f1': 0.43100566275167973,\n",
       "    'f1_stderr': 0.00478990485809286},\n",
       "   'harness|gsm8k|5': {'acc': 0.1425322213798332,\n",
       "    'acc_stderr': 0.009629588445673814},\n",
       "   'harness|winogrande|5': {'acc': 0.7371744277821626,\n",
       "    'acc_stderr': 0.012370922527262006}}},\n",
       " 'mpt-30b-chat': {'key': 'mpt-30b-chat',\n",
       "  'Model': 'MPT-30B-chat',\n",
       "  'MT-bench (score)': '6.39',\n",
       "  'MMLU': '0.504',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'MosaicML',\n",
       "  'Link': 'https://huggingface.co/mosaicml/mpt-30b-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5477815699658704,\n",
       "    'acc_stderr': 0.014544519880633829,\n",
       "    'acc_norm': 0.5836177474402731,\n",
       "    'acc_norm_stderr': 0.014405618279436176},\n",
       "   'harness|hellaswag|10': {'acc': 0.6325433180641307,\n",
       "    'acc_stderr': 0.0048112699754506005,\n",
       "    'acc_norm': 0.8241386178052181,\n",
       "    'acc_norm_stderr': 0.0037992414085029525},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.45925925925925926,\n",
       "    'acc_stderr': 0.04304979692464243,\n",
       "    'acc_norm': 0.45925925925925926,\n",
       "    'acc_norm_stderr': 0.04304979692464243},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.04068942293855797,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.04068942293855797},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5509433962264151,\n",
       "    'acc_stderr': 0.030612730713641095,\n",
       "    'acc_norm': 0.5509433962264151,\n",
       "    'acc_norm_stderr': 0.030612730713641095},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5902777777777778,\n",
       "    'acc_stderr': 0.04112490974670788,\n",
       "    'acc_norm': 0.5902777777777778,\n",
       "    'acc_norm_stderr': 0.04112490974670788},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.043898699568087785,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.043898699568087785},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.04923659639173309,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.04923659639173309},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4297872340425532,\n",
       "    'acc_stderr': 0.03236214467715564,\n",
       "    'acc_norm': 0.4297872340425532,\n",
       "    'acc_norm_stderr': 0.03236214467715564},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2807017543859649,\n",
       "    'acc_stderr': 0.042270544512322,\n",
       "    'acc_norm': 0.2807017543859649,\n",
       "    'acc_norm_stderr': 0.042270544512322},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.47586206896551725,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.47586206896551725,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3333333333333333,\n",
       "    'acc_stderr': 0.0242785680243077,\n",
       "    'acc_norm': 0.3333333333333333,\n",
       "    'acc_norm_stderr': 0.0242785680243077},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.31746031746031744,\n",
       "    'acc_stderr': 0.04163453031302859,\n",
       "    'acc_norm': 0.31746031746031744,\n",
       "    'acc_norm_stderr': 0.04163453031302859},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6193548387096774,\n",
       "    'acc_stderr': 0.027621717832907046,\n",
       "    'acc_norm': 0.6193548387096774,\n",
       "    'acc_norm_stderr': 0.027621717832907046},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.3842364532019704,\n",
       "    'acc_stderr': 0.03422398565657551,\n",
       "    'acc_norm': 0.3842364532019704,\n",
       "    'acc_norm_stderr': 0.03422398565657551},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6303030303030303,\n",
       "    'acc_stderr': 0.03769430314512568,\n",
       "    'acc_norm': 0.6303030303030303,\n",
       "    'acc_norm_stderr': 0.03769430314512568},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6919191919191919,\n",
       "    'acc_stderr': 0.03289477330098616,\n",
       "    'acc_norm': 0.6919191919191919,\n",
       "    'acc_norm_stderr': 0.03289477330098616},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.6476683937823834,\n",
       "    'acc_stderr': 0.03447478286414357,\n",
       "    'acc_norm': 0.6476683937823834,\n",
       "    'acc_norm_stderr': 0.03447478286414357},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.45384615384615384,\n",
       "    'acc_stderr': 0.02524277098712618,\n",
       "    'acc_norm': 0.45384615384615384,\n",
       "    'acc_norm_stderr': 0.02524277098712618},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.28888888888888886,\n",
       "    'acc_stderr': 0.027634907264178544,\n",
       "    'acc_norm': 0.28888888888888886,\n",
       "    'acc_norm_stderr': 0.027634907264178544},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.47058823529411764,\n",
       "    'acc_stderr': 0.03242225027115006,\n",
       "    'acc_norm': 0.47058823529411764,\n",
       "    'acc_norm_stderr': 0.03242225027115006},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3443708609271523,\n",
       "    'acc_stderr': 0.038796870240733264,\n",
       "    'acc_norm': 0.3443708609271523,\n",
       "    'acc_norm_stderr': 0.038796870240733264},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.728440366972477,\n",
       "    'acc_stderr': 0.019069098363191442,\n",
       "    'acc_norm': 0.728440366972477,\n",
       "    'acc_norm_stderr': 0.019069098363191442},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4166666666666667,\n",
       "    'acc_stderr': 0.03362277436608044,\n",
       "    'acc_norm': 0.4166666666666667,\n",
       "    'acc_norm_stderr': 0.03362277436608044},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7598039215686274,\n",
       "    'acc_stderr': 0.02998373305591362,\n",
       "    'acc_norm': 0.7598039215686274,\n",
       "    'acc_norm_stderr': 0.02998373305591362},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7088607594936709,\n",
       "    'acc_stderr': 0.029571601065753374,\n",
       "    'acc_norm': 0.7088607594936709,\n",
       "    'acc_norm_stderr': 0.029571601065753374},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.5291479820627802,\n",
       "    'acc_stderr': 0.03350073248773403,\n",
       "    'acc_norm': 0.5291479820627802,\n",
       "    'acc_norm_stderr': 0.03350073248773403},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5725190839694656,\n",
       "    'acc_stderr': 0.04338920305792401,\n",
       "    'acc_norm': 0.5725190839694656,\n",
       "    'acc_norm_stderr': 0.04338920305792401},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.4793388429752066,\n",
       "    'acc_stderr': 0.04560456086387235,\n",
       "    'acc_norm': 0.4793388429752066,\n",
       "    'acc_norm_stderr': 0.04560456086387235},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.04812917324536823,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.04812917324536823},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6012269938650306,\n",
       "    'acc_stderr': 0.03847021420456023,\n",
       "    'acc_norm': 0.6012269938650306,\n",
       "    'acc_norm_stderr': 0.03847021420456023},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.36607142857142855,\n",
       "    'acc_stderr': 0.045723723587374296,\n",
       "    'acc_norm': 0.36607142857142855,\n",
       "    'acc_norm_stderr': 0.045723723587374296},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6990291262135923,\n",
       "    'acc_stderr': 0.045416094465039476,\n",
       "    'acc_norm': 0.6990291262135923,\n",
       "    'acc_norm_stderr': 0.045416094465039476},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7948717948717948,\n",
       "    'acc_stderr': 0.026453508054040332,\n",
       "    'acc_norm': 0.7948717948717948,\n",
       "    'acc_norm_stderr': 0.026453508054040332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.04999999999999999,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.04999999999999999},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.698595146871009,\n",
       "    'acc_stderr': 0.01640909109726878,\n",
       "    'acc_norm': 0.698595146871009,\n",
       "    'acc_norm_stderr': 0.01640909109726878},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5491329479768786,\n",
       "    'acc_stderr': 0.026788811931562757,\n",
       "    'acc_norm': 0.5491329479768786,\n",
       "    'acc_norm_stderr': 0.026788811931562757},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2860335195530726,\n",
       "    'acc_stderr': 0.015113972129062141,\n",
       "    'acc_norm': 0.2860335195530726,\n",
       "    'acc_norm_stderr': 0.015113972129062141},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5032679738562091,\n",
       "    'acc_stderr': 0.02862930519400354,\n",
       "    'acc_norm': 0.5032679738562091,\n",
       "    'acc_norm_stderr': 0.02862930519400354},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5594855305466238,\n",
       "    'acc_stderr': 0.028196400574197422,\n",
       "    'acc_norm': 0.5594855305466238,\n",
       "    'acc_norm_stderr': 0.028196400574197422},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.027431623722415012,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.027431623722415012},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.3829787234042553,\n",
       "    'acc_stderr': 0.028999080904806178,\n",
       "    'acc_norm': 0.3829787234042553,\n",
       "    'acc_norm_stderr': 0.028999080904806178},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.37157757496740546,\n",
       "    'acc_stderr': 0.012341828514528298,\n",
       "    'acc_norm': 0.37157757496740546,\n",
       "    'acc_norm_stderr': 0.012341828514528298},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.48161764705882354,\n",
       "    'acc_stderr': 0.03035230339535196,\n",
       "    'acc_norm': 0.48161764705882354,\n",
       "    'acc_norm_stderr': 0.03035230339535196},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.49019607843137253,\n",
       "    'acc_stderr': 0.0202239460050743,\n",
       "    'acc_norm': 0.49019607843137253,\n",
       "    'acc_norm_stderr': 0.0202239460050743},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.5727272727272728,\n",
       "    'acc_stderr': 0.04738198703545483,\n",
       "    'acc_norm': 0.5727272727272728,\n",
       "    'acc_norm_stderr': 0.04738198703545483},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.5755102040816327,\n",
       "    'acc_stderr': 0.031642094879429414,\n",
       "    'acc_norm': 0.5755102040816327,\n",
       "    'acc_norm_stderr': 0.031642094879429414},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6169154228855721,\n",
       "    'acc_stderr': 0.034375193373382504,\n",
       "    'acc_norm': 0.6169154228855721,\n",
       "    'acc_norm_stderr': 0.034375193373382504},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.77,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.77,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.45180722891566266,\n",
       "    'acc_stderr': 0.038743715565879536,\n",
       "    'acc_norm': 0.45180722891566266,\n",
       "    'acc_norm_stderr': 0.038743715565879536},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7251461988304093,\n",
       "    'acc_stderr': 0.03424042924691584,\n",
       "    'acc_norm': 0.7251461988304093,\n",
       "    'acc_norm_stderr': 0.03424042924691584},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3390452876376989,\n",
       "    'mc1_stderr': 0.01657179791062661,\n",
       "    'mc2': 0.5199824927914821,\n",
       "    'mc2_stderr': 0.01582403747940678},\n",
       "   'all': {'em': 0.0017827181208053692,\n",
       "    'em_stderr': 0.00043200973460389824,\n",
       "    'f1': 0.06156250000000011,\n",
       "    'f1_stderr': 0.001404482472524456,\n",
       "    'acc': 0.4371318828152442,\n",
       "    'acc_stderr': 0.010557145720065584},\n",
       "   'harness|drop|3': {'em': 0.0017827181208053692,\n",
       "    'em_stderr': 0.00043200973460389824,\n",
       "    'f1': 0.06156250000000011,\n",
       "    'f1_stderr': 0.001404482472524456},\n",
       "   'harness|gsm8k|5': {'acc': 0.12130401819560273,\n",
       "    'acc_stderr': 0.008992888497275597},\n",
       "   'harness|winogrande|5': {'acc': 0.7529597474348856,\n",
       "    'acc_stderr': 0.01212140294285557}}},\n",
       " 'mpt-7b-chat': {'key': 'mpt-7b-chat',\n",
       "  'Model': 'MPT-7B-Chat',\n",
       "  'MT-bench (score)': '5.42',\n",
       "  'MMLU': '0.320',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'MosaicML',\n",
       "  'Link': 'https://huggingface.co/mosaicml/mpt-7b-chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.431740614334471,\n",
       "    'acc_stderr': 0.014474591427196204,\n",
       "    'acc_norm': 0.46501706484641636,\n",
       "    'acc_norm_stderr': 0.014575583922019669},\n",
       "   'harness|hellaswag|10': {'acc': 0.5710017924716192,\n",
       "    'acc_stderr': 0.004939215682191771,\n",
       "    'acc_norm': 0.7551284604660427,\n",
       "    'acc_norm_stderr': 0.00429132188812274},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.3925925925925926,\n",
       "    'acc_stderr': 0.04218506215368879,\n",
       "    'acc_norm': 0.3925925925925926,\n",
       "    'acc_norm_stderr': 0.04218506215368879},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.3684210526315789,\n",
       "    'acc_stderr': 0.03925523381052932,\n",
       "    'acc_norm': 0.3684210526315789,\n",
       "    'acc_norm_stderr': 0.03925523381052932},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.3849056603773585,\n",
       "    'acc_stderr': 0.029946498567699948,\n",
       "    'acc_norm': 0.3849056603773585,\n",
       "    'acc_norm_stderr': 0.029946498567699948},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.04076663253918567,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.04076663253918567},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621504,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621504},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.32947976878612717,\n",
       "    'acc_stderr': 0.03583901754736411,\n",
       "    'acc_norm': 0.32947976878612717,\n",
       "    'acc_norm_stderr': 0.03583901754736411},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.17647058823529413,\n",
       "    'acc_stderr': 0.0379328118530781,\n",
       "    'acc_norm': 0.17647058823529413,\n",
       "    'acc_norm_stderr': 0.0379328118530781},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.049888765156985884,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.049888765156985884},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.3702127659574468,\n",
       "    'acc_stderr': 0.031565646822367836,\n",
       "    'acc_norm': 0.3702127659574468,\n",
       "    'acc_norm_stderr': 0.031565646822367836},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.21929824561403508,\n",
       "    'acc_stderr': 0.03892431106518754,\n",
       "    'acc_norm': 0.21929824561403508,\n",
       "    'acc_norm_stderr': 0.03892431106518754},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.45517241379310347,\n",
       "    'acc_stderr': 0.04149886942192117,\n",
       "    'acc_norm': 0.45517241379310347,\n",
       "    'acc_norm_stderr': 0.04149886942192117},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.24338624338624337,\n",
       "    'acc_stderr': 0.02210112878741543,\n",
       "    'acc_norm': 0.24338624338624337,\n",
       "    'acc_norm_stderr': 0.02210112878741543},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.25396825396825395,\n",
       "    'acc_stderr': 0.038932596106046755,\n",
       "    'acc_norm': 0.25396825396825395,\n",
       "    'acc_norm_stderr': 0.038932596106046755},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768077,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768077},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.4032258064516129,\n",
       "    'acc_stderr': 0.027906150826041143,\n",
       "    'acc_norm': 0.4032258064516129,\n",
       "    'acc_norm_stderr': 0.027906150826041143},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.2413793103448276,\n",
       "    'acc_stderr': 0.030108330718011625,\n",
       "    'acc_norm': 0.2413793103448276,\n",
       "    'acc_norm_stderr': 0.030108330718011625},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.3212121212121212,\n",
       "    'acc_stderr': 0.036462049632538115,\n",
       "    'acc_norm': 0.3212121212121212,\n",
       "    'acc_norm_stderr': 0.036462049632538115},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.36363636363636365,\n",
       "    'acc_stderr': 0.034273086529999365,\n",
       "    'acc_norm': 0.36363636363636365,\n",
       "    'acc_norm_stderr': 0.034273086529999365},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.49222797927461137,\n",
       "    'acc_stderr': 0.03608003225569653,\n",
       "    'acc_norm': 0.49222797927461137,\n",
       "    'acc_norm_stderr': 0.03608003225569653},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.3871794871794872,\n",
       "    'acc_stderr': 0.02469721693087894,\n",
       "    'acc_norm': 0.3871794871794872,\n",
       "    'acc_norm_stderr': 0.02469721693087894},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.21851851851851853,\n",
       "    'acc_stderr': 0.025195752251823786,\n",
       "    'acc_norm': 0.21851851851851853,\n",
       "    'acc_norm_stderr': 0.025195752251823786},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.3907563025210084,\n",
       "    'acc_stderr': 0.031693802357129965,\n",
       "    'acc_norm': 0.3907563025210084,\n",
       "    'acc_norm_stderr': 0.031693802357129965},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2847682119205298,\n",
       "    'acc_stderr': 0.03684881521389024,\n",
       "    'acc_norm': 0.2847682119205298,\n",
       "    'acc_norm_stderr': 0.03684881521389024},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.5100917431192661,\n",
       "    'acc_stderr': 0.02143295620345332,\n",
       "    'acc_norm': 0.5100917431192661,\n",
       "    'acc_norm_stderr': 0.02143295620345332},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.35185185185185186,\n",
       "    'acc_stderr': 0.03256850570293648,\n",
       "    'acc_norm': 0.35185185185185186,\n",
       "    'acc_norm_stderr': 0.03256850570293648},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.3284313725490196,\n",
       "    'acc_stderr': 0.032962451101722294,\n",
       "    'acc_norm': 0.3284313725490196,\n",
       "    'acc_norm_stderr': 0.032962451101722294},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.4092827004219409,\n",
       "    'acc_stderr': 0.032007041833595914,\n",
       "    'acc_norm': 0.4092827004219409,\n",
       "    'acc_norm_stderr': 0.032007041833595914},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.484304932735426,\n",
       "    'acc_stderr': 0.0335412657542081,\n",
       "    'acc_norm': 0.484304932735426,\n",
       "    'acc_norm_stderr': 0.0335412657542081},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.46564885496183206,\n",
       "    'acc_stderr': 0.04374928560599738,\n",
       "    'acc_norm': 0.46564885496183206,\n",
       "    'acc_norm_stderr': 0.04374928560599738},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.4049586776859504,\n",
       "    'acc_stderr': 0.044811377559424694,\n",
       "    'acc_norm': 0.4049586776859504,\n",
       "    'acc_norm_stderr': 0.044811377559424694},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.4074074074074074,\n",
       "    'acc_stderr': 0.047500773411999854,\n",
       "    'acc_norm': 0.4074074074074074,\n",
       "    'acc_norm_stderr': 0.047500773411999854},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2883435582822086,\n",
       "    'acc_stderr': 0.03559039531617342,\n",
       "    'acc_norm': 0.2883435582822086,\n",
       "    'acc_norm_stderr': 0.03559039531617342},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.33035714285714285,\n",
       "    'acc_stderr': 0.04464285714285713,\n",
       "    'acc_norm': 0.33035714285714285,\n",
       "    'acc_norm_stderr': 0.04464285714285713},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.36893203883495146,\n",
       "    'acc_stderr': 0.04777615181156739,\n",
       "    'acc_norm': 0.36893203883495146,\n",
       "    'acc_norm_stderr': 0.04777615181156739},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.5512820512820513,\n",
       "    'acc_stderr': 0.032583346493868806,\n",
       "    'acc_norm': 0.5512820512820513,\n",
       "    'acc_norm_stderr': 0.032583346493868806},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.5325670498084292,\n",
       "    'acc_stderr': 0.01784199575052087,\n",
       "    'acc_norm': 0.5325670498084292,\n",
       "    'acc_norm_stderr': 0.01784199575052087},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.41040462427745666,\n",
       "    'acc_stderr': 0.026483392042098187,\n",
       "    'acc_norm': 0.41040462427745666,\n",
       "    'acc_norm_stderr': 0.026483392042098187},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961443,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961443},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.4019607843137255,\n",
       "    'acc_stderr': 0.02807415894760065,\n",
       "    'acc_norm': 0.4019607843137255,\n",
       "    'acc_norm_stderr': 0.02807415894760065},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.40836012861736337,\n",
       "    'acc_stderr': 0.027917050748484624,\n",
       "    'acc_norm': 0.40836012861736337,\n",
       "    'acc_norm_stderr': 0.027917050748484624},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.36728395061728397,\n",
       "    'acc_stderr': 0.026822801759507894,\n",
       "    'acc_norm': 0.36728395061728397,\n",
       "    'acc_norm_stderr': 0.026822801759507894},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2695035460992908,\n",
       "    'acc_stderr': 0.026469036818590627,\n",
       "    'acc_norm': 0.2695035460992908,\n",
       "    'acc_norm_stderr': 0.026469036818590627},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.2907431551499348,\n",
       "    'acc_stderr': 0.011598062372851988,\n",
       "    'acc_norm': 0.2907431551499348,\n",
       "    'acc_norm_stderr': 0.011598062372851988},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.3897058823529412,\n",
       "    'acc_stderr': 0.029624663581159696,\n",
       "    'acc_norm': 0.3897058823529412,\n",
       "    'acc_norm_stderr': 0.029624663581159696},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.3202614379084967,\n",
       "    'acc_stderr': 0.018875682938069446,\n",
       "    'acc_norm': 0.3202614379084967,\n",
       "    'acc_norm_stderr': 0.018875682938069446},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.4636363636363636,\n",
       "    'acc_stderr': 0.047764491623961985,\n",
       "    'acc_norm': 0.4636363636363636,\n",
       "    'acc_norm_stderr': 0.047764491623961985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.4857142857142857,\n",
       "    'acc_stderr': 0.03199615232806286,\n",
       "    'acc_norm': 0.4857142857142857,\n",
       "    'acc_norm_stderr': 0.03199615232806286},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.48258706467661694,\n",
       "    'acc_stderr': 0.03533389234739245,\n",
       "    'acc_norm': 0.48258706467661694,\n",
       "    'acc_norm_stderr': 0.03533389234739245},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.04975698519562426,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.04975698519562426},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.42771084337349397,\n",
       "    'acc_stderr': 0.038515976837185335,\n",
       "    'acc_norm': 0.42771084337349397,\n",
       "    'acc_norm_stderr': 0.038515976837185335},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.49707602339181284,\n",
       "    'acc_stderr': 0.03834759370936839,\n",
       "    'acc_norm': 0.49707602339181284,\n",
       "    'acc_norm_stderr': 0.03834759370936839},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.27050183598531213,\n",
       "    'mc1_stderr': 0.015550778332842895,\n",
       "    'mc2': 0.40163647231251104,\n",
       "    'mc2_stderr': 0.014753108409806075},\n",
       "   'all': {'em': 0.06952600671140939,\n",
       "    'em_stderr': 0.002604746204517829,\n",
       "    'f1': 0.12196937919463072,\n",
       "    'f1_stderr': 0.002840521979064293,\n",
       "    'acc': 0.3626168565432783,\n",
       "    'acc_stderr': 0.009260585769647573},\n",
       "   'harness|drop|3': {'em': 0.06952600671140939,\n",
       "    'em_stderr': 0.002604746204517829,\n",
       "    'f1': 0.12196937919463072,\n",
       "    'f1_stderr': 0.002840521979064293},\n",
       "   'harness|gsm8k|5': {'acc': 0.04094010614101592,\n",
       "    'acc_stderr': 0.005458076796294338},\n",
       "   'harness|winogrande|5': {'acc': 0.6842936069455406,\n",
       "    'acc_stderr': 0.01306309474300081}}},\n",
       " 'oasst-pythia-12b': {'key': 'oasst-pythia-12b',\n",
       "  'Model': 'OpenAssistant-Pythia-12B',\n",
       "  'MT-bench (score)': '4.32',\n",
       "  'MMLU': '0.270',\n",
       "  'License': 'Apache 2.0',\n",
       "  'Organization': 'OpenAssistant',\n",
       "  'Link': 'https://huggingface.co/OpenAssistant/oasst-sft-4-pythia-12b-epoch-3.5',\n",
       "  'results': {'harness|drop|3': {'em': 0.001363255033557047,\n",
       "    'em_stderr': 0.00037786091964606887,\n",
       "    'f1': 0.059077181208053976,\n",
       "    'f1_stderr': 0.001394848925611238},\n",
       "   'harness|gsm8k|5': {'acc': 0.030326004548900682,\n",
       "    'acc_stderr': 0.004723487465514761},\n",
       "   'harness|winogrande|5': {'acc': 0.659037095501184,\n",
       "    'acc_stderr': 0.013322681435934807},\n",
       "   'all': {'acc': 0.2748440247593001,\n",
       "    'acc_stderr': 0.03228989271589345,\n",
       "    'acc_norm': 0.2784679867417539,\n",
       "    'acc_norm_stderr': 0.03228683184562667,\n",
       "    'mc1': 0.2386780905752754,\n",
       "    'mc1_stderr': 0.014922629695456418,\n",
       "    'mc2': 0.37807719406726975,\n",
       "    'mc2_stderr': 0.014701655079555453},\n",
       "   'harness|arc:challenge|25': {'acc': 0.4121160409556314,\n",
       "    'acc_stderr': 0.0143839153022254,\n",
       "    'acc_norm': 0.45733788395904434,\n",
       "    'acc_norm_stderr': 0.014558106543924067},\n",
       "   'harness|hellaswag|10': {'acc': 0.5173272256522605,\n",
       "    'acc_stderr': 0.004986784319771785,\n",
       "    'acc_norm': 0.6859191396136228,\n",
       "    'acc_norm_stderr': 0.004632001732332984},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768081,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768081},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.31851851851851853,\n",
       "    'acc_stderr': 0.04024778401977109,\n",
       "    'acc_norm': 0.31851851851851853,\n",
       "    'acc_norm_stderr': 0.04024778401977109},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.23026315789473684,\n",
       "    'acc_stderr': 0.03426059424403165,\n",
       "    'acc_norm': 0.23026315789473684,\n",
       "    'acc_norm_stderr': 0.03426059424403165},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.0479372485441102,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.0479372485441102},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.33584905660377357,\n",
       "    'acc_stderr': 0.029067220146644833,\n",
       "    'acc_norm': 0.33584905660377357,\n",
       "    'acc_norm_stderr': 0.029067220146644833},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2847222222222222,\n",
       "    'acc_stderr': 0.03773809990686936,\n",
       "    'acc_norm': 0.2847222222222222,\n",
       "    'acc_norm_stderr': 0.03773809990686936},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.2,\n",
       "    'acc_stderr': 0.040201512610368445,\n",
       "    'acc_norm': 0.2,\n",
       "    'acc_norm_stderr': 0.040201512610368445},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.24855491329479767,\n",
       "    'acc_stderr': 0.03295304696818317,\n",
       "    'acc_norm': 0.24855491329479767,\n",
       "    'acc_norm_stderr': 0.03295304696818317},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.28431372549019607,\n",
       "    'acc_stderr': 0.04488482852329017,\n",
       "    'acc_norm': 0.28431372549019607,\n",
       "    'acc_norm_stderr': 0.04488482852329017},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.26,\n",
       "    'acc_stderr': 0.04408440022768079,\n",
       "    'acc_norm': 0.26,\n",
       "    'acc_norm_stderr': 0.04408440022768079},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.30638297872340425,\n",
       "    'acc_stderr': 0.030135906478517563,\n",
       "    'acc_norm': 0.30638297872340425,\n",
       "    'acc_norm_stderr': 0.030135906478517563},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.22807017543859648,\n",
       "    'acc_stderr': 0.03947152782669416,\n",
       "    'acc_norm': 0.22807017543859648,\n",
       "    'acc_norm_stderr': 0.03947152782669416},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.25517241379310346,\n",
       "    'acc_stderr': 0.03632984052707841,\n",
       "    'acc_norm': 0.25517241379310346,\n",
       "    'acc_norm_stderr': 0.03632984052707841},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.25132275132275134,\n",
       "    'acc_stderr': 0.022340482339643895,\n",
       "    'acc_norm': 0.25132275132275134,\n",
       "    'acc_norm_stderr': 0.022340482339643895},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.1746031746031746,\n",
       "    'acc_stderr': 0.033954900208561116,\n",
       "    'acc_norm': 0.1746031746031746,\n",
       "    'acc_norm_stderr': 0.033954900208561116},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.04560480215720684,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.04560480215720684},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.267741935483871,\n",
       "    'acc_stderr': 0.025189006660212385,\n",
       "    'acc_norm': 0.267741935483871,\n",
       "    'acc_norm_stderr': 0.025189006660212385},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.21674876847290642,\n",
       "    'acc_stderr': 0.028990331252516235,\n",
       "    'acc_norm': 0.21674876847290642,\n",
       "    'acc_norm_stderr': 0.028990331252516235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.28484848484848485,\n",
       "    'acc_stderr': 0.03524390844511784,\n",
       "    'acc_norm': 0.28484848484848485,\n",
       "    'acc_norm_stderr': 0.03524390844511784},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.25757575757575757,\n",
       "    'acc_stderr': 0.031156269519646836,\n",
       "    'acc_norm': 0.25757575757575757,\n",
       "    'acc_norm_stderr': 0.031156269519646836},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.23834196891191708,\n",
       "    'acc_stderr': 0.03074890536390989,\n",
       "    'acc_norm': 0.23834196891191708,\n",
       "    'acc_norm_stderr': 0.03074890536390989},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.2641025641025641,\n",
       "    'acc_stderr': 0.022352193737453282,\n",
       "    'acc_norm': 0.2641025641025641,\n",
       "    'acc_norm_stderr': 0.022352193737453282},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2518518518518518,\n",
       "    'acc_stderr': 0.02646611753895991,\n",
       "    'acc_norm': 0.2518518518518518,\n",
       "    'acc_norm_stderr': 0.02646611753895991},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.24369747899159663,\n",
       "    'acc_stderr': 0.02788682807838058,\n",
       "    'acc_norm': 0.24369747899159663,\n",
       "    'acc_norm_stderr': 0.02788682807838058},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.26490066225165565,\n",
       "    'acc_stderr': 0.03603038545360384,\n",
       "    'acc_norm': 0.26490066225165565,\n",
       "    'acc_norm_stderr': 0.03603038545360384},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.24587155963302754,\n",
       "    'acc_stderr': 0.018461940968708446,\n",
       "    'acc_norm': 0.24587155963302754,\n",
       "    'acc_norm_stderr': 0.018461940968708446},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.24074074074074073,\n",
       "    'acc_stderr': 0.02915752218460561,\n",
       "    'acc_norm': 0.24074074074074073,\n",
       "    'acc_norm_stderr': 0.02915752218460561},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.29411764705882354,\n",
       "    'acc_stderr': 0.03198001660115071,\n",
       "    'acc_norm': 0.29411764705882354,\n",
       "    'acc_norm_stderr': 0.03198001660115071},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.23628691983122363,\n",
       "    'acc_stderr': 0.027652153144159263,\n",
       "    'acc_norm': 0.23628691983122363,\n",
       "    'acc_norm_stderr': 0.027652153144159263},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.3004484304932735,\n",
       "    'acc_stderr': 0.030769352008229136,\n",
       "    'acc_norm': 0.3004484304932735,\n",
       "    'acc_norm_stderr': 0.030769352008229136},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.22137404580152673,\n",
       "    'acc_stderr': 0.036412970813137276,\n",
       "    'acc_norm': 0.22137404580152673,\n",
       "    'acc_norm_stderr': 0.036412970813137276},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.3305785123966942,\n",
       "    'acc_stderr': 0.04294340845212094,\n",
       "    'acc_norm': 0.3305785123966942,\n",
       "    'acc_norm_stderr': 0.04294340845212094},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.28703703703703703,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.28703703703703703,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.2822085889570552,\n",
       "    'acc_stderr': 0.03536117886664743,\n",
       "    'acc_norm': 0.2822085889570552,\n",
       "    'acc_norm_stderr': 0.03536117886664743},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.26785714285714285,\n",
       "    'acc_stderr': 0.04203277291467764,\n",
       "    'acc_norm': 0.26785714285714285,\n",
       "    'acc_norm_stderr': 0.04203277291467764},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.18446601941747573,\n",
       "    'acc_stderr': 0.03840423627288276,\n",
       "    'acc_norm': 0.18446601941747573,\n",
       "    'acc_norm_stderr': 0.03840423627288276},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.29914529914529914,\n",
       "    'acc_stderr': 0.029996951858349497,\n",
       "    'acc_norm': 0.29914529914529914,\n",
       "    'acc_norm_stderr': 0.029996951858349497},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.28607918263090676,\n",
       "    'acc_stderr': 0.01616087140512754,\n",
       "    'acc_norm': 0.28607918263090676,\n",
       "    'acc_norm_stderr': 0.01616087140512754},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.25722543352601157,\n",
       "    'acc_stderr': 0.02353292543104429,\n",
       "    'acc_norm': 0.25722543352601157,\n",
       "    'acc_norm_stderr': 0.02353292543104429},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2424581005586592,\n",
       "    'acc_stderr': 0.014333522059217889,\n",
       "    'acc_norm': 0.2424581005586592,\n",
       "    'acc_norm_stderr': 0.014333522059217889},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.27124183006535946,\n",
       "    'acc_stderr': 0.025457756696667864,\n",
       "    'acc_norm': 0.27124183006535946,\n",
       "    'acc_norm_stderr': 0.025457756696667864},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.2861736334405145,\n",
       "    'acc_stderr': 0.02567025924218895,\n",
       "    'acc_norm': 0.2861736334405145,\n",
       "    'acc_norm_stderr': 0.02567025924218895},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.2716049382716049,\n",
       "    'acc_stderr': 0.02474862449053737,\n",
       "    'acc_norm': 0.2716049382716049,\n",
       "    'acc_norm_stderr': 0.02474862449053737},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.2730496453900709,\n",
       "    'acc_stderr': 0.026577860943307857,\n",
       "    'acc_norm': 0.2730496453900709,\n",
       "    'acc_norm_stderr': 0.026577860943307857},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.27835723598435463,\n",
       "    'acc_stderr': 0.011446990197380985,\n",
       "    'acc_norm': 0.27835723598435463,\n",
       "    'acc_norm_stderr': 0.011446990197380985},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.23529411764705882,\n",
       "    'acc_stderr': 0.025767252010855973,\n",
       "    'acc_norm': 0.23529411764705882,\n",
       "    'acc_norm_stderr': 0.025767252010855973},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.2777777777777778,\n",
       "    'acc_stderr': 0.018120224251484584,\n",
       "    'acc_norm': 0.2777777777777778,\n",
       "    'acc_norm_stderr': 0.018120224251484584},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.32727272727272727,\n",
       "    'acc_stderr': 0.04494290866252089,\n",
       "    'acc_norm': 0.32727272727272727,\n",
       "    'acc_norm_stderr': 0.04494290866252089},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.2571428571428571,\n",
       "    'acc_stderr': 0.027979823538744543,\n",
       "    'acc_norm': 0.2571428571428571,\n",
       "    'acc_norm_stderr': 0.027979823538744543},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.27860696517412936,\n",
       "    'acc_stderr': 0.031700561834973086,\n",
       "    'acc_norm': 0.27860696517412936,\n",
       "    'acc_norm_stderr': 0.031700561834973086},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.23,\n",
       "    'acc_stderr': 0.04229525846816506,\n",
       "    'acc_norm': 0.23,\n",
       "    'acc_norm_stderr': 0.04229525846816506},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.3373493975903614,\n",
       "    'acc_stderr': 0.0368078369072758,\n",
       "    'acc_norm': 0.3373493975903614,\n",
       "    'acc_norm_stderr': 0.0368078369072758},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.26900584795321636,\n",
       "    'acc_stderr': 0.03401052620104089,\n",
       "    'acc_norm': 0.26900584795321636,\n",
       "    'acc_norm_stderr': 0.03401052620104089},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.2386780905752754,\n",
       "    'mc1_stderr': 0.014922629695456418,\n",
       "    'mc2': 0.37807719406726975,\n",
       "    'mc2_stderr': 0.014701655079555453}}},\n",
       " 'openchat-3.5': {'key': 'openchat-3.5',\n",
       "  'Model': 'OpenChat-3.5',\n",
       "  'MT-bench (score)': '7.81',\n",
       "  'MMLU': '0.643',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'OpenChat',\n",
       "  'Link': 'https://huggingface.co/openchat/openchat_3.5',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5972696245733788,\n",
       "    'acc_stderr': 0.014332236306790147,\n",
       "    'acc_norm': 0.6382252559726962,\n",
       "    'acc_norm_stderr': 0.014041957945038078},\n",
       "   'harness|hellaswag|10': {'acc': 0.6579366660027883,\n",
       "    'acc_stderr': 0.004734311435009196,\n",
       "    'acc_norm': 0.8480382393945429,\n",
       "    'acc_norm_stderr': 0.003582501596564539},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5703703703703704,\n",
       "    'acc_stderr': 0.042763494943765995,\n",
       "    'acc_norm': 0.5703703703703704,\n",
       "    'acc_norm_stderr': 0.042763494943765995},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6973684210526315,\n",
       "    'acc_stderr': 0.037385206761196686,\n",
       "    'acc_norm': 0.6973684210526315,\n",
       "    'acc_norm_stderr': 0.037385206761196686},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.028544793319055326,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.028544793319055326},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7708333333333334,\n",
       "    'acc_stderr': 0.035146974678623884,\n",
       "    'acc_norm': 0.7708333333333334,\n",
       "    'acc_norm_stderr': 0.035146974678623884},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.048523658709391,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.048523658709391},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.653179190751445,\n",
       "    'acc_stderr': 0.036291466701596636,\n",
       "    'acc_norm': 0.653179190751445,\n",
       "    'acc_norm_stderr': 0.036291466701596636},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4117647058823529,\n",
       "    'acc_stderr': 0.04897104952726366,\n",
       "    'acc_norm': 0.4117647058823529,\n",
       "    'acc_norm_stderr': 0.04897104952726366},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.574468085106383,\n",
       "    'acc_stderr': 0.032321469162244675,\n",
       "    'acc_norm': 0.574468085106383,\n",
       "    'acc_norm_stderr': 0.032321469162244675},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5175438596491229,\n",
       "    'acc_stderr': 0.04700708033551038,\n",
       "    'acc_norm': 0.5175438596491229,\n",
       "    'acc_norm_stderr': 0.04700708033551038},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370332,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370332},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42328042328042326,\n",
       "    'acc_stderr': 0.02544636563440679,\n",
       "    'acc_norm': 0.42328042328042326,\n",
       "    'acc_norm_stderr': 0.02544636563440679},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5158730158730159,\n",
       "    'acc_stderr': 0.044698818540726076,\n",
       "    'acc_norm': 0.5158730158730159,\n",
       "    'acc_norm_stderr': 0.044698818540726076},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7967741935483871,\n",
       "    'acc_stderr': 0.02289168798455496,\n",
       "    'acc_norm': 0.7967741935483871,\n",
       "    'acc_norm_stderr': 0.02289168798455496},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5073891625615764,\n",
       "    'acc_stderr': 0.035176035403610105,\n",
       "    'acc_norm': 0.5073891625615764,\n",
       "    'acc_norm_stderr': 0.035176035403610105},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7757575757575758,\n",
       "    'acc_stderr': 0.032568666616811015,\n",
       "    'acc_norm': 0.7757575757575758,\n",
       "    'acc_norm_stderr': 0.032568666616811015},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.803030303030303,\n",
       "    'acc_stderr': 0.028335609732463362,\n",
       "    'acc_norm': 0.803030303030303,\n",
       "    'acc_norm_stderr': 0.028335609732463362},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9067357512953368,\n",
       "    'acc_stderr': 0.02098685459328974,\n",
       "    'acc_norm': 0.9067357512953368,\n",
       "    'acc_norm_stderr': 0.02098685459328974},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.023901157979402538,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.023901157979402538},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.36666666666666664,\n",
       "    'acc_stderr': 0.02938162072646507,\n",
       "    'acc_norm': 0.36666666666666664,\n",
       "    'acc_norm_stderr': 0.02938162072646507},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6680672268907563,\n",
       "    'acc_stderr': 0.03058869701378364,\n",
       "    'acc_norm': 0.6680672268907563,\n",
       "    'acc_norm_stderr': 0.03058869701378364},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.32450331125827814,\n",
       "    'acc_stderr': 0.038227469376587525,\n",
       "    'acc_norm': 0.32450331125827814,\n",
       "    'acc_norm_stderr': 0.038227469376587525},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8550458715596331,\n",
       "    'acc_stderr': 0.015094215699700469,\n",
       "    'acc_norm': 0.8550458715596331,\n",
       "    'acc_norm_stderr': 0.015094215699700469},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.03395322726375798,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.03395322726375798},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8333333333333334,\n",
       "    'acc_stderr': 0.026156867523931045,\n",
       "    'acc_norm': 0.8333333333333334,\n",
       "    'acc_norm_stderr': 0.026156867523931045},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8227848101265823,\n",
       "    'acc_stderr': 0.024856364184503234,\n",
       "    'acc_norm': 0.8227848101265823,\n",
       "    'acc_norm_stderr': 0.024856364184503234},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.726457399103139,\n",
       "    'acc_stderr': 0.029918586707798827,\n",
       "    'acc_norm': 0.726457399103139,\n",
       "    'acc_norm_stderr': 0.029918586707798827},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7786259541984732,\n",
       "    'acc_stderr': 0.03641297081313729,\n",
       "    'acc_norm': 0.7786259541984732,\n",
       "    'acc_norm_stderr': 0.03641297081313729},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8181818181818182,\n",
       "    'acc_stderr': 0.03520893951097653,\n",
       "    'acc_norm': 0.8181818181818182,\n",
       "    'acc_norm_stderr': 0.03520893951097653},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.0401910747255735,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.0401910747255735},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7791411042944786,\n",
       "    'acc_stderr': 0.03259177392742178,\n",
       "    'acc_norm': 0.7791411042944786,\n",
       "    'acc_norm_stderr': 0.03259177392742178},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.44642857142857145,\n",
       "    'acc_stderr': 0.04718471485219588,\n",
       "    'acc_norm': 0.44642857142857145,\n",
       "    'acc_norm_stderr': 0.04718471485219588},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8252427184466019,\n",
       "    'acc_stderr': 0.03760178006026621,\n",
       "    'acc_norm': 0.8252427184466019,\n",
       "    'acc_norm_stderr': 0.03760178006026621},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8760683760683761,\n",
       "    'acc_stderr': 0.02158649400128138,\n",
       "    'acc_norm': 0.8760683760683761,\n",
       "    'acc_norm_stderr': 0.02158649400128138},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8212005108556832,\n",
       "    'acc_stderr': 0.013702643715368982,\n",
       "    'acc_norm': 0.8212005108556832,\n",
       "    'acc_norm_stderr': 0.013702643715368982},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7456647398843931,\n",
       "    'acc_stderr': 0.023445826276545546,\n",
       "    'acc_norm': 0.7456647398843931,\n",
       "    'acc_norm_stderr': 0.023445826276545546},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.42681564245810055,\n",
       "    'acc_stderr': 0.016542401954631906,\n",
       "    'acc_norm': 0.42681564245810055,\n",
       "    'acc_norm_stderr': 0.016542401954631906},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7189542483660131,\n",
       "    'acc_stderr': 0.025738854797818733,\n",
       "    'acc_norm': 0.7189542483660131,\n",
       "    'acc_norm_stderr': 0.025738854797818733},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7202572347266881,\n",
       "    'acc_stderr': 0.025494259350694912,\n",
       "    'acc_norm': 0.7202572347266881,\n",
       "    'acc_norm_stderr': 0.025494259350694912},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7314814814814815,\n",
       "    'acc_stderr': 0.02465968518596728,\n",
       "    'acc_norm': 0.7314814814814815,\n",
       "    'acc_norm_stderr': 0.02465968518596728},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.48226950354609927,\n",
       "    'acc_stderr': 0.02980873964223777,\n",
       "    'acc_norm': 0.48226950354609927,\n",
       "    'acc_norm_stderr': 0.02980873964223777},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.47392438070404175,\n",
       "    'acc_stderr': 0.012752858346533134,\n",
       "    'acc_norm': 0.47392438070404175,\n",
       "    'acc_norm_stderr': 0.012752858346533134},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6801470588235294,\n",
       "    'acc_stderr': 0.02833295951403121,\n",
       "    'acc_norm': 0.6801470588235294,\n",
       "    'acc_norm_stderr': 0.02833295951403121},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6748366013071896,\n",
       "    'acc_stderr': 0.01895088677080631,\n",
       "    'acc_norm': 0.6748366013071896,\n",
       "    'acc_norm_stderr': 0.01895088677080631},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.0449429086625209,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.0449429086625209},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.028263889943784603,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.028263889943784603},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.835820895522388,\n",
       "    'acc_stderr': 0.026193923544454125,\n",
       "    'acc_norm': 0.835820895522388,\n",
       "    'acc_norm_stderr': 0.026193923544454125},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.88,\n",
       "    'acc_stderr': 0.03265986323710906,\n",
       "    'acc_norm': 0.88,\n",
       "    'acc_norm_stderr': 0.03265986323710906},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.536144578313253,\n",
       "    'acc_stderr': 0.038823108508905954,\n",
       "    'acc_norm': 0.536144578313253,\n",
       "    'acc_norm_stderr': 0.038823108508905954},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.30966952264381886,\n",
       "    'mc1_stderr': 0.0161857443551449,\n",
       "    'mc2': 0.46392847173087287,\n",
       "    'mc2_stderr': 0.015046687408684912},\n",
       "   'harness|winogrande|5': {'acc': 0.8074191002367798,\n",
       "    'acc_stderr': 0.011082538847491902},\n",
       "   'harness|drop|3': {'em': 0.0019924496644295304,\n",
       "    'em_stderr': 0.0004566676462666931,\n",
       "    'f1': 0.07226510067114073,\n",
       "    'f1_stderr': 0.0014840662284336832},\n",
       "   'harness|gsm8k|5': {'acc': 0.26611068991660347,\n",
       "    'acc_stderr': 0.012172750939040322},\n",
       "   'all': {'acc': 0.6453244910928128,\n",
       "    'acc_stderr': 0.031930893551459144,\n",
       "    'acc_norm': 0.6529207012084622,\n",
       "    'acc_norm_stderr': 0.03259469709580134,\n",
       "    'mc1': 0.30966952264381886,\n",
       "    'mc1_stderr': 0.0161857443551449,\n",
       "    'mc2': 0.46392847173087287,\n",
       "    'mc2_stderr': 0.015046687408684912,\n",
       "    'em': 0.0019924496644295304,\n",
       "    'em_stderr': 0.0004566676462666931,\n",
       "    'f1': 0.07226510067114073,\n",
       "    'f1_stderr': 0.0014840662284336832}}},\n",
       " 'openhermes-2.5-mistral-7b': {'key': 'openhermes-2.5-mistral-7b',\n",
       "  'Model': 'OpenHermes-2.5-Mistral-7b',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '-',\n",
       "  'License': 'Apache-2.0',\n",
       "  'Organization': 'NousResearch',\n",
       "  'Link': 'https://huggingface.co/teknium/OpenHermes-2.5-Mistral-7B',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6126279863481229,\n",
       "    'acc_stderr': 0.014235872487909869,\n",
       "    'acc_norm': 0.6493174061433447,\n",
       "    'acc_norm_stderr': 0.013944635930726099},\n",
       "   'harness|hellaswag|10': {'acc': 0.6519617606054571,\n",
       "    'acc_stderr': 0.004753746951620152,\n",
       "    'acc_norm': 0.8429595698068114,\n",
       "    'acc_norm_stderr': 0.003630952999843739},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695236,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695236},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.0421850621536888,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.0421850621536888},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6973684210526315,\n",
       "    'acc_stderr': 0.037385206761196686,\n",
       "    'acc_norm': 0.6973684210526315,\n",
       "    'acc_norm_stderr': 0.037385206761196686},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.02854479331905533,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.02854479331905533},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7569444444444444,\n",
       "    'acc_stderr': 0.03586879280080341,\n",
       "    'acc_norm': 0.7569444444444444,\n",
       "    'acc_norm_stderr': 0.03586879280080341},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.46,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.46,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6242774566473989,\n",
       "    'acc_stderr': 0.036928207672648664,\n",
       "    'acc_norm': 0.6242774566473989,\n",
       "    'acc_norm_stderr': 0.036928207672648664},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107223,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107223},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5659574468085107,\n",
       "    'acc_stderr': 0.03240038086792747,\n",
       "    'acc_norm': 0.5659574468085107,\n",
       "    'acc_norm_stderr': 0.03240038086792747},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.49122807017543857,\n",
       "    'acc_stderr': 0.04702880432049615,\n",
       "    'acc_norm': 0.49122807017543857,\n",
       "    'acc_norm_stderr': 0.04702880432049615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5241379310344828,\n",
       "    'acc_stderr': 0.0416180850350153,\n",
       "    'acc_norm': 0.5241379310344828,\n",
       "    'acc_norm_stderr': 0.0416180850350153},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42592592592592593,\n",
       "    'acc_stderr': 0.02546714904546955,\n",
       "    'acc_norm': 0.42592592592592593,\n",
       "    'acc_norm_stderr': 0.02546714904546955},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.46825396825396826,\n",
       "    'acc_stderr': 0.04463112720677172,\n",
       "    'acc_norm': 0.46825396825396826,\n",
       "    'acc_norm_stderr': 0.04463112720677172},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7935483870967742,\n",
       "    'acc_stderr': 0.02302589961718871,\n",
       "    'acc_norm': 0.7935483870967742,\n",
       "    'acc_norm_stderr': 0.02302589961718871},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.035158955511656986,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.035158955511656986},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7818181818181819,\n",
       "    'acc_stderr': 0.032250781083062896,\n",
       "    'acc_norm': 0.7818181818181819,\n",
       "    'acc_norm_stderr': 0.032250781083062896},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.028057791672989017,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.028057791672989017},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8911917098445595,\n",
       "    'acc_stderr': 0.022473253332768776,\n",
       "    'acc_norm': 0.8911917098445595,\n",
       "    'acc_norm_stderr': 0.022473253332768776},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6128205128205129,\n",
       "    'acc_stderr': 0.024697216930878937,\n",
       "    'acc_norm': 0.6128205128205129,\n",
       "    'acc_norm_stderr': 0.024697216930878937},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3037037037037037,\n",
       "    'acc_stderr': 0.02803792996911499,\n",
       "    'acc_norm': 0.3037037037037037,\n",
       "    'acc_norm_stderr': 0.02803792996911499},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.680672268907563,\n",
       "    'acc_stderr': 0.030283995525884396,\n",
       "    'acc_norm': 0.680672268907563,\n",
       "    'acc_norm_stderr': 0.030283995525884396},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.31788079470198677,\n",
       "    'acc_stderr': 0.038020397601079024,\n",
       "    'acc_norm': 0.31788079470198677,\n",
       "    'acc_norm_stderr': 0.038020397601079024},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8330275229357799,\n",
       "    'acc_stderr': 0.01599015488507338,\n",
       "    'acc_norm': 0.8330275229357799,\n",
       "    'acc_norm_stderr': 0.01599015488507338},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5092592592592593,\n",
       "    'acc_stderr': 0.034093869469927006,\n",
       "    'acc_norm': 0.5092592592592593,\n",
       "    'acc_norm_stderr': 0.034093869469927006},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7990196078431373,\n",
       "    'acc_stderr': 0.02812597226565437,\n",
       "    'acc_norm': 0.7990196078431373,\n",
       "    'acc_norm_stderr': 0.02812597226565437},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8143459915611815,\n",
       "    'acc_stderr': 0.025310495376944856,\n",
       "    'acc_norm': 0.8143459915611815,\n",
       "    'acc_norm_stderr': 0.025310495376944856},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7040358744394619,\n",
       "    'acc_stderr': 0.030636591348699803,\n",
       "    'acc_norm': 0.7040358744394619,\n",
       "    'acc_norm_stderr': 0.030636591348699803},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7938931297709924,\n",
       "    'acc_stderr': 0.035477710041594654,\n",
       "    'acc_norm': 0.7938931297709924,\n",
       "    'acc_norm_stderr': 0.035477710041594654},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7603305785123967,\n",
       "    'acc_stderr': 0.03896878985070416,\n",
       "    'acc_norm': 0.7603305785123967,\n",
       "    'acc_norm_stderr': 0.03896878985070416},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7870370370370371,\n",
       "    'acc_stderr': 0.039578354719809805,\n",
       "    'acc_norm': 0.7870370370370371,\n",
       "    'acc_norm_stderr': 0.039578354719809805},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7852760736196319,\n",
       "    'acc_stderr': 0.032262193772867744,\n",
       "    'acc_norm': 0.7852760736196319,\n",
       "    'acc_norm_stderr': 0.032262193772867744},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.5089285714285714,\n",
       "    'acc_stderr': 0.04745033255489123,\n",
       "    'acc_norm': 0.5089285714285714,\n",
       "    'acc_norm_stderr': 0.04745033255489123},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7766990291262136,\n",
       "    'acc_stderr': 0.04123553189891431,\n",
       "    'acc_norm': 0.7766990291262136,\n",
       "    'acc_norm_stderr': 0.04123553189891431},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.022801382534597528,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.022801382534597528},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8301404853128991,\n",
       "    'acc_stderr': 0.013428186370608306,\n",
       "    'acc_norm': 0.8301404853128991,\n",
       "    'acc_norm_stderr': 0.013428186370608306},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7167630057803468,\n",
       "    'acc_stderr': 0.02425790170532338,\n",
       "    'acc_norm': 0.7167630057803468,\n",
       "    'acc_norm_stderr': 0.02425790170532338},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30837988826815643,\n",
       "    'acc_stderr': 0.01544571691099888,\n",
       "    'acc_norm': 0.30837988826815643,\n",
       "    'acc_norm_stderr': 0.01544571691099888},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7549019607843137,\n",
       "    'acc_stderr': 0.024630048979824782,\n",
       "    'acc_norm': 0.7549019607843137,\n",
       "    'acc_norm_stderr': 0.024630048979824782},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.684887459807074,\n",
       "    'acc_stderr': 0.026385273703464485,\n",
       "    'acc_norm': 0.684887459807074,\n",
       "    'acc_norm_stderr': 0.026385273703464485},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7530864197530864,\n",
       "    'acc_stderr': 0.02399350170904211,\n",
       "    'acc_norm': 0.7530864197530864,\n",
       "    'acc_norm_stderr': 0.02399350170904211},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5070921985815603,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.5070921985815603,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.46936114732724904,\n",
       "    'acc_stderr': 0.012746237711716634,\n",
       "    'acc_norm': 0.46936114732724904,\n",
       "    'acc_norm_stderr': 0.012746237711716634},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.028418208619406762,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.028418208619406762},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.673202614379085,\n",
       "    'acc_stderr': 0.018975427920507215,\n",
       "    'acc_norm': 0.673202614379085,\n",
       "    'acc_norm_stderr': 0.018975427920507215},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.028263889943784596,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.028263889943784596},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8159203980099502,\n",
       "    'acc_stderr': 0.027403859410786845,\n",
       "    'acc_norm': 0.8159203980099502,\n",
       "    'acc_norm_stderr': 0.027403859410786845},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.033799766898963086,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.033799766898963086},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5542168674698795,\n",
       "    'acc_stderr': 0.03869543323472101,\n",
       "    'acc_norm': 0.5542168674698795,\n",
       "    'acc_norm_stderr': 0.03869543323472101},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3598531211750306,\n",
       "    'mc1_stderr': 0.01680186046667716,\n",
       "    'mc2': 0.5230564118100686,\n",
       "    'mc2_stderr': 0.015250230546286025},\n",
       "   'harness|winogrande|5': {'acc': 0.7790055248618785,\n",
       "    'acc_stderr': 0.011661223637643412},\n",
       "   'harness|drop|3': {'em': 0.30830536912751677,\n",
       "    'em_stderr': 0.004729196914949925,\n",
       "    'f1': 0.35989723154362524,\n",
       "    'f1_stderr': 0.004629324720589026},\n",
       "   'harness|gsm8k|5': {'acc': 0.25473843821076575,\n",
       "    'acc_stderr': 0.012001731232879136},\n",
       "   'all': {'acc': 0.6340440103659418,\n",
       "    'acc_stderr': 0.03220763540877311,\n",
       "    'acc_norm': 0.6418750491228201,\n",
       "    'acc_norm_stderr': 0.032874386009418256,\n",
       "    'mc1': 0.3598531211750306,\n",
       "    'mc1_stderr': 0.01680186046667716,\n",
       "    'mc2': 0.5230564118100686,\n",
       "    'mc2_stderr': 0.015250230546286025,\n",
       "    'em': 0.30830536912751677,\n",
       "    'em_stderr': 0.004729196914949925,\n",
       "    'f1': 0.35989723154362524,\n",
       "    'f1_stderr': 0.004629324720589026}}},\n",
       " 'solar-10.7b-instruct-v1.0': {'key': 'solar-10.7b-instruct-v1.0',\n",
       "  'Model': 'SOLAR-10.7B-Instruct-v1.0',\n",
       "  'MT-bench (score)': '7.58',\n",
       "  'MMLU': '0.662',\n",
       "  'License': 'CC-BY-NC-4.0',\n",
       "  'Organization': 'Upstage AI',\n",
       "  'Link': 'https://huggingface.co/upstage/SOLAR-10.7B-Instruct-v1.0',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6808873720136519,\n",
       "    'acc_stderr': 0.013621696119173307,\n",
       "    'acc_norm': 0.7107508532423208,\n",
       "    'acc_norm_stderr': 0.01325001257939344},\n",
       "   'harness|hellaswag|10': {'acc': 0.7070304720175263,\n",
       "    'acc_stderr': 0.004541944342035901,\n",
       "    'acc_norm': 0.8815972913762199,\n",
       "    'acc_norm_stderr': 0.003224240722351317},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.41,\n",
       "    'acc_stderr': 0.049431107042371025,\n",
       "    'acc_norm': 0.41,\n",
       "    'acc_norm_stderr': 0.049431107042371025},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.7368421052631579,\n",
       "    'acc_stderr': 0.03583496176361072,\n",
       "    'acc_norm': 0.7368421052631579,\n",
       "    'acc_norm_stderr': 0.03583496176361072},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.02872750295788027,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.02872750295788027},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7638888888888888,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.7638888888888888,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.52,\n",
       "    'acc_stderr': 0.05021167315686779,\n",
       "    'acc_norm': 0.52,\n",
       "    'acc_norm_stderr': 0.05021167315686779},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6647398843930635,\n",
       "    'acc_stderr': 0.03599586301247077,\n",
       "    'acc_norm': 0.6647398843930635,\n",
       "    'acc_norm_stderr': 0.03599586301247077},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107223,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107223},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.6297872340425532,\n",
       "    'acc_stderr': 0.03156564682236785,\n",
       "    'acc_norm': 0.6297872340425532,\n",
       "    'acc_norm_stderr': 0.03156564682236785},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.047036043419179864,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.047036043419179864},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.6413793103448275,\n",
       "    'acc_stderr': 0.039966295748767186,\n",
       "    'acc_norm': 0.6413793103448275,\n",
       "    'acc_norm_stderr': 0.039966295748767186},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.47883597883597884,\n",
       "    'acc_stderr': 0.025728230952130726,\n",
       "    'acc_norm': 0.47883597883597884,\n",
       "    'acc_norm_stderr': 0.025728230952130726},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.044444444444444495,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.044444444444444495},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.8032258064516129,\n",
       "    'acc_stderr': 0.022616409420742025,\n",
       "    'acc_norm': 0.8032258064516129,\n",
       "    'acc_norm_stderr': 0.022616409420742025},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.03515895551165698,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.03515895551165698},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.72,\n",
       "    'acc_stderr': 0.04512608598542128,\n",
       "    'acc_norm': 0.72,\n",
       "    'acc_norm_stderr': 0.04512608598542128},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.031234752377721175,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.031234752377721175},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8737373737373737,\n",
       "    'acc_stderr': 0.02366435940288023,\n",
       "    'acc_norm': 0.8737373737373737,\n",
       "    'acc_norm_stderr': 0.02366435940288023},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9067357512953368,\n",
       "    'acc_stderr': 0.02098685459328973,\n",
       "    'acc_norm': 0.9067357512953368,\n",
       "    'acc_norm_stderr': 0.02098685459328973},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6615384615384615,\n",
       "    'acc_stderr': 0.023991500500313036,\n",
       "    'acc_norm': 0.6615384615384615,\n",
       "    'acc_norm_stderr': 0.023991500500313036},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3814814814814815,\n",
       "    'acc_stderr': 0.029616718927497593,\n",
       "    'acc_norm': 0.3814814814814815,\n",
       "    'acc_norm_stderr': 0.029616718927497593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7184873949579832,\n",
       "    'acc_stderr': 0.02921354941437217,\n",
       "    'acc_norm': 0.7184873949579832,\n",
       "    'acc_norm_stderr': 0.02921354941437217},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3708609271523179,\n",
       "    'acc_stderr': 0.03943966699183629,\n",
       "    'acc_norm': 0.3708609271523179,\n",
       "    'acc_norm_stderr': 0.03943966699183629},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.03388857118502325,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.03388857118502325},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8480392156862745,\n",
       "    'acc_stderr': 0.0251956584289318,\n",
       "    'acc_norm': 0.8480392156862745,\n",
       "    'acc_norm_stderr': 0.0251956584289318},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8565400843881856,\n",
       "    'acc_stderr': 0.022818291821017012,\n",
       "    'acc_norm': 0.8565400843881856,\n",
       "    'acc_norm_stderr': 0.022818291821017012},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6816143497757847,\n",
       "    'acc_stderr': 0.03126580522513713,\n",
       "    'acc_norm': 0.6816143497757847,\n",
       "    'acc_norm_stderr': 0.03126580522513713},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7480916030534351,\n",
       "    'acc_stderr': 0.03807387116306086,\n",
       "    'acc_norm': 0.7480916030534351,\n",
       "    'acc_norm_stderr': 0.03807387116306086},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7768595041322314,\n",
       "    'acc_stderr': 0.03800754475228733,\n",
       "    'acc_norm': 0.7768595041322314,\n",
       "    'acc_norm_stderr': 0.03800754475228733},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8055555555555556,\n",
       "    'acc_stderr': 0.038260763248848646,\n",
       "    'acc_norm': 0.8055555555555556,\n",
       "    'acc_norm_stderr': 0.038260763248848646},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.754601226993865,\n",
       "    'acc_stderr': 0.03380939813943354,\n",
       "    'acc_norm': 0.754601226993865,\n",
       "    'acc_norm_stderr': 0.03380939813943354},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.44642857142857145,\n",
       "    'acc_stderr': 0.047184714852195886,\n",
       "    'acc_norm': 0.44642857142857145,\n",
       "    'acc_norm_stderr': 0.047184714852195886},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8252427184466019,\n",
       "    'acc_stderr': 0.03760178006026621,\n",
       "    'acc_norm': 0.8252427184466019,\n",
       "    'acc_norm_stderr': 0.03760178006026621},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.02280138253459753,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.02280138253459753},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8033205619412516,\n",
       "    'acc_stderr': 0.014214138556913917,\n",
       "    'acc_norm': 0.8033205619412516,\n",
       "    'acc_norm_stderr': 0.014214138556913917},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7601156069364162,\n",
       "    'acc_stderr': 0.022989592543123567,\n",
       "    'acc_norm': 0.7601156069364162,\n",
       "    'acc_norm_stderr': 0.022989592543123567},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.39329608938547483,\n",
       "    'acc_stderr': 0.016337268694270112,\n",
       "    'acc_norm': 0.39329608938547483,\n",
       "    'acc_norm_stderr': 0.016337268694270112},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7581699346405228,\n",
       "    'acc_stderr': 0.024518195641879334,\n",
       "    'acc_norm': 0.7581699346405228,\n",
       "    'acc_norm_stderr': 0.024518195641879334},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.729903536977492,\n",
       "    'acc_stderr': 0.02521804037341062,\n",
       "    'acc_norm': 0.729903536977492,\n",
       "    'acc_norm_stderr': 0.02521804037341062},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7901234567901234,\n",
       "    'acc_stderr': 0.02265834408598137,\n",
       "    'acc_norm': 0.7901234567901234,\n",
       "    'acc_norm_stderr': 0.02265834408598137},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.49645390070921985,\n",
       "    'acc_stderr': 0.02982674915328092,\n",
       "    'acc_norm': 0.49645390070921985,\n",
       "    'acc_norm_stderr': 0.02982674915328092},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4934810951760104,\n",
       "    'acc_stderr': 0.012769150688867503,\n",
       "    'acc_norm': 0.4934810951760104,\n",
       "    'acc_norm_stderr': 0.012769150688867503},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.7389705882352942,\n",
       "    'acc_stderr': 0.026679252270103135,\n",
       "    'acc_norm': 0.7389705882352942,\n",
       "    'acc_norm_stderr': 0.026679252270103135},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.018690850273595294,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.018690850273595294},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6909090909090909,\n",
       "    'acc_stderr': 0.044262946482000985,\n",
       "    'acc_norm': 0.6909090909090909,\n",
       "    'acc_norm_stderr': 0.044262946482000985},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7346938775510204,\n",
       "    'acc_stderr': 0.0282638899437846,\n",
       "    'acc_norm': 0.7346938775510204,\n",
       "    'acc_norm_stderr': 0.0282638899437846},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8407960199004975,\n",
       "    'acc_stderr': 0.02587064676616913,\n",
       "    'acc_norm': 0.8407960199004975,\n",
       "    'acc_norm_stderr': 0.02587064676616913},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.9,\n",
       "    'acc_stderr': 0.030151134457776334,\n",
       "    'acc_norm': 0.9,\n",
       "    'acc_norm_stderr': 0.030151134457776334},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5843373493975904,\n",
       "    'acc_stderr': 0.03836722176598052,\n",
       "    'acc_norm': 0.5843373493975904,\n",
       "    'acc_norm_stderr': 0.03836722176598052},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7894736842105263,\n",
       "    'acc_stderr': 0.03126781714663179,\n",
       "    'acc_norm': 0.7894736842105263,\n",
       "    'acc_norm_stderr': 0.03126781714663179},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.5667074663402693,\n",
       "    'mc1_stderr': 0.017347024450107485,\n",
       "    'mc2': 0.7142943510205136,\n",
       "    'mc2_stderr': 0.015024530295000761},\n",
       "   'harness|winogrande|5': {'acc': 0.8358326756116812,\n",
       "    'acc_stderr': 0.01041084977522279},\n",
       "   'harness|gsm8k|5': {'acc': 0.6474601971190296,\n",
       "    'acc_stderr': 0.013159909755930337},\n",
       "   'all': {'acc': 0.6657586984797939,\n",
       "    'acc_stderr': 0.03165995758526614,\n",
       "    'acc_norm': 0.6666511531376961,\n",
       "    'acc_norm_stderr': 0.0323050384069596,\n",
       "    'mc1': 0.5667074663402693,\n",
       "    'mc1_stderr': 0.017347024450107485,\n",
       "    'mc2': 0.7142943510205136,\n",
       "    'mc2_stderr': 0.015024530295000761}}},\n",
       " 'stablelm-tuned-alpha-7b': {'key': 'stablelm-tuned-alpha-7b',\n",
       "  'Model': 'StableLM-Tuned-Alpha-7B',\n",
       "  'MT-bench (score)': '2.75',\n",
       "  'MMLU': '0.244',\n",
       "  'License': 'CC-BY-NC-SA-4.0',\n",
       "  'Organization': 'Stability AI',\n",
       "  'Link': 'https://huggingface.co/stabilityai/stablelm-tuned-alpha-7b',\n",
       "  'results': {'harness|drop|3': {'em': 0.0041946308724832215,\n",
       "    'em_stderr': 0.0006618716168266466,\n",
       "    'f1': 0.05621224832214779,\n",
       "    'f1_stderr': 0.0014117433231649174},\n",
       "   'harness|gsm8k|5': {'acc': 0.008339651250947688,\n",
       "    'acc_stderr': 0.002504942226860537},\n",
       "   'harness|winogrande|5': {'acc': 0.5311760063141279,\n",
       "    'acc_stderr': 0.014025142640639513},\n",
       "   'all': {'acc': 0.2479374638777667,\n",
       "    'acc_stderr': 0.03127884661282089,\n",
       "    'acc_norm': 0.2503417754412081,\n",
       "    'acc_norm_stderr': 0.031283748741553784,\n",
       "    'mc1': 0.23990208078335373,\n",
       "    'mc1_stderr': 0.014948812679062137,\n",
       "    'mc2': 0.4037161827493261,\n",
       "    'mc2_stderr': 0.014483731026265276},\n",
       "   'harness|arc:challenge|25': {'acc': 0.3003412969283277,\n",
       "    'acc_stderr': 0.013395909309956997,\n",
       "    'acc_norm': 0.3191126279863481,\n",
       "    'acc_norm_stderr': 0.0136216961191733},\n",
       "   'harness|hellaswag|10': {'acc': 0.41286596295558653,\n",
       "    'acc_stderr': 0.004913429010559072,\n",
       "    'acc_norm': 0.5359490141406095,\n",
       "    'acc_norm_stderr': 0.004976867796583554},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.25,\n",
       "    'acc_stderr': 0.04351941398892446,\n",
       "    'acc_norm': 0.25,\n",
       "    'acc_norm_stderr': 0.04351941398892446},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.26666666666666666,\n",
       "    'acc_stderr': 0.03820169914517904,\n",
       "    'acc_norm': 0.26666666666666666,\n",
       "    'acc_norm_stderr': 0.03820169914517904},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.17105263157894737,\n",
       "    'acc_stderr': 0.030643607071677088,\n",
       "    'acc_norm': 0.17105263157894737,\n",
       "    'acc_norm_stderr': 0.030643607071677088},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.24,\n",
       "    'acc_stderr': 0.04292346959909281,\n",
       "    'acc_norm': 0.24,\n",
       "    'acc_norm_stderr': 0.04292346959909281},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.27547169811320754,\n",
       "    'acc_stderr': 0.027495663683724064,\n",
       "    'acc_norm': 0.27547169811320754,\n",
       "    'acc_norm_stderr': 0.027495663683724064},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.2361111111111111,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.2361111111111111,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.041633319989322695,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.041633319989322695},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.23121387283236994,\n",
       "    'acc_stderr': 0.0321473730202947,\n",
       "    'acc_norm': 0.23121387283236994,\n",
       "    'acc_norm_stderr': 0.0321473730202947},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2549019607843137,\n",
       "    'acc_stderr': 0.04336432707993176,\n",
       "    'acc_norm': 0.2549019607843137,\n",
       "    'acc_norm_stderr': 0.04336432707993176},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542127,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542127},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.3148936170212766,\n",
       "    'acc_stderr': 0.03036358219723816,\n",
       "    'acc_norm': 0.3148936170212766,\n",
       "    'acc_norm_stderr': 0.03036358219723816},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.21052631578947367,\n",
       "    'acc_stderr': 0.038351539543994194,\n",
       "    'acc_norm': 0.21052631578947367,\n",
       "    'acc_norm_stderr': 0.038351539543994194},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.27586206896551724,\n",
       "    'acc_stderr': 0.037245636197746325,\n",
       "    'acc_norm': 0.27586206896551724,\n",
       "    'acc_norm_stderr': 0.037245636197746325},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.24338624338624337,\n",
       "    'acc_stderr': 0.022101128787415426,\n",
       "    'acc_norm': 0.24338624338624337,\n",
       "    'acc_norm_stderr': 0.022101128787415426},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.24603174603174602,\n",
       "    'acc_stderr': 0.03852273364924316,\n",
       "    'acc_norm': 0.24603174603174602,\n",
       "    'acc_norm_stderr': 0.03852273364924316},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.19,\n",
       "    'acc_stderr': 0.03942772444036624,\n",
       "    'acc_norm': 0.19,\n",
       "    'acc_norm_stderr': 0.03942772444036624},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.2129032258064516,\n",
       "    'acc_stderr': 0.023287665127268525,\n",
       "    'acc_norm': 0.2129032258064516,\n",
       "    'acc_norm_stderr': 0.023287665127268525},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.18226600985221675,\n",
       "    'acc_stderr': 0.02716334085964515,\n",
       "    'acc_norm': 0.18226600985221675,\n",
       "    'acc_norm_stderr': 0.02716334085964515},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.04163331998932269,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.04163331998932269},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.22424242424242424,\n",
       "    'acc_stderr': 0.03256866661681102,\n",
       "    'acc_norm': 0.22424242424242424,\n",
       "    'acc_norm_stderr': 0.03256866661681102},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.18686868686868688,\n",
       "    'acc_stderr': 0.027772533334218977,\n",
       "    'acc_norm': 0.18686868686868688,\n",
       "    'acc_norm_stderr': 0.027772533334218977},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.21243523316062177,\n",
       "    'acc_stderr': 0.029519282616817254,\n",
       "    'acc_norm': 0.21243523316062177,\n",
       "    'acc_norm_stderr': 0.029519282616817254},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.2743589743589744,\n",
       "    'acc_stderr': 0.022622765767493218,\n",
       "    'acc_norm': 0.2743589743589744,\n",
       "    'acc_norm_stderr': 0.022622765767493218},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.25925925925925924,\n",
       "    'acc_stderr': 0.026719240783712163,\n",
       "    'acc_norm': 0.25925925925925924,\n",
       "    'acc_norm_stderr': 0.026719240783712163},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.19747899159663865,\n",
       "    'acc_stderr': 0.02585916412205145,\n",
       "    'acc_norm': 0.19747899159663865,\n",
       "    'acc_norm_stderr': 0.02585916412205145},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2185430463576159,\n",
       "    'acc_stderr': 0.03374235550425694,\n",
       "    'acc_norm': 0.2185430463576159,\n",
       "    'acc_norm_stderr': 0.03374235550425694},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.22935779816513763,\n",
       "    'acc_stderr': 0.018025349724618684,\n",
       "    'acc_norm': 0.22935779816513763,\n",
       "    'acc_norm_stderr': 0.018025349724618684},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.35185185185185186,\n",
       "    'acc_stderr': 0.03256850570293647,\n",
       "    'acc_norm': 0.35185185185185186,\n",
       "    'acc_norm_stderr': 0.03256850570293647},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.24019607843137256,\n",
       "    'acc_stderr': 0.02998373305591361,\n",
       "    'acc_norm': 0.24019607843137256,\n",
       "    'acc_norm_stderr': 0.02998373305591361},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.2869198312236287,\n",
       "    'acc_stderr': 0.02944377302259469,\n",
       "    'acc_norm': 0.2869198312236287,\n",
       "    'acc_norm_stderr': 0.02944377302259469},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.3542600896860987,\n",
       "    'acc_stderr': 0.03210062154134987,\n",
       "    'acc_norm': 0.3542600896860987,\n",
       "    'acc_norm_stderr': 0.03210062154134987},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.2900763358778626,\n",
       "    'acc_stderr': 0.03980066246467765,\n",
       "    'acc_norm': 0.2900763358778626,\n",
       "    'acc_norm_stderr': 0.03980066246467765},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.2396694214876033,\n",
       "    'acc_stderr': 0.03896878985070417,\n",
       "    'acc_norm': 0.2396694214876033,\n",
       "    'acc_norm_stderr': 0.03896878985070417},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.28703703703703703,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.28703703703703703,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.27607361963190186,\n",
       "    'acc_stderr': 0.0351238528370505,\n",
       "    'acc_norm': 0.27607361963190186,\n",
       "    'acc_norm_stderr': 0.0351238528370505},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.24107142857142858,\n",
       "    'acc_stderr': 0.04059867246952687,\n",
       "    'acc_norm': 0.24107142857142858,\n",
       "    'acc_norm_stderr': 0.04059867246952687},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.1650485436893204,\n",
       "    'acc_stderr': 0.036756688322331886,\n",
       "    'acc_norm': 0.1650485436893204,\n",
       "    'acc_norm_stderr': 0.036756688322331886},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.24786324786324787,\n",
       "    'acc_stderr': 0.028286324075564397,\n",
       "    'acc_norm': 0.24786324786324787,\n",
       "    'acc_norm_stderr': 0.028286324075564397},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.23499361430395913,\n",
       "    'acc_stderr': 0.015162024152278434,\n",
       "    'acc_norm': 0.23499361430395913,\n",
       "    'acc_norm_stderr': 0.015162024152278434},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.2254335260115607,\n",
       "    'acc_stderr': 0.02249723019096756,\n",
       "    'acc_norm': 0.2254335260115607,\n",
       "    'acc_norm_stderr': 0.02249723019096756},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24134078212290502,\n",
       "    'acc_stderr': 0.014310999547961459,\n",
       "    'acc_norm': 0.24134078212290502,\n",
       "    'acc_norm_stderr': 0.014310999547961459},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.25163398692810457,\n",
       "    'acc_stderr': 0.024848018263875195,\n",
       "    'acc_norm': 0.25163398692810457,\n",
       "    'acc_norm_stderr': 0.024848018263875195},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.21864951768488747,\n",
       "    'acc_stderr': 0.023475581417861106,\n",
       "    'acc_norm': 0.21864951768488747,\n",
       "    'acc_norm_stderr': 0.023475581417861106},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.2222222222222222,\n",
       "    'acc_stderr': 0.023132376234543325,\n",
       "    'acc_norm': 0.2222222222222222,\n",
       "    'acc_norm_stderr': 0.023132376234543325},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.24822695035460993,\n",
       "    'acc_stderr': 0.025770015644290403,\n",
       "    'acc_norm': 0.24822695035460993,\n",
       "    'acc_norm_stderr': 0.025770015644290403},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.22946544980443284,\n",
       "    'acc_stderr': 0.010739489382279503,\n",
       "    'acc_norm': 0.22946544980443284,\n",
       "    'acc_norm_stderr': 0.010739489382279503},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.16544117647058823,\n",
       "    'acc_stderr': 0.022571771025494767,\n",
       "    'acc_norm': 0.16544117647058823,\n",
       "    'acc_norm_stderr': 0.022571771025494767},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.25163398692810457,\n",
       "    'acc_stderr': 0.017555818091322263,\n",
       "    'acc_norm': 0.25163398692810457,\n",
       "    'acc_norm_stderr': 0.017555818091322263},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.23636363636363636,\n",
       "    'acc_stderr': 0.04069306319721378,\n",
       "    'acc_norm': 0.23636363636363636,\n",
       "    'acc_norm_stderr': 0.04069306319721378},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.1673469387755102,\n",
       "    'acc_stderr': 0.023897144768914524,\n",
       "    'acc_norm': 0.1673469387755102,\n",
       "    'acc_norm_stderr': 0.023897144768914524},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.263681592039801,\n",
       "    'acc_stderr': 0.03115715086935558,\n",
       "    'acc_norm': 0.263681592039801,\n",
       "    'acc_norm_stderr': 0.03115715086935558},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.24,\n",
       "    'acc_stderr': 0.042923469599092816,\n",
       "    'acc_norm': 0.24,\n",
       "    'acc_norm_stderr': 0.042923469599092816},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.2891566265060241,\n",
       "    'acc_stderr': 0.03529486801511115,\n",
       "    'acc_norm': 0.2891566265060241,\n",
       "    'acc_norm_stderr': 0.03529486801511115},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.24561403508771928,\n",
       "    'acc_stderr': 0.03301405946987249,\n",
       "    'acc_norm': 0.24561403508771928,\n",
       "    'acc_norm_stderr': 0.03301405946987249},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.23990208078335373,\n",
       "    'mc1_stderr': 0.014948812679062137,\n",
       "    'mc2': 0.4037161827493261,\n",
       "    'mc2_stderr': 0.014483731026265276}}},\n",
       " 'starling-lm-7b-alpha': {'key': 'starling-lm-7b-alpha',\n",
       "  'Model': 'Starling-LM-7B-alpha',\n",
       "  'MT-bench (score)': '8.09',\n",
       "  'MMLU': '0.639',\n",
       "  'License': 'CC-BY-NC-4.0',\n",
       "  'Organization': 'UC Berkeley',\n",
       "  'Link': 'https://huggingface.co/berkeley-nest/Starling-LM-7B-alpha',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5998293515358362,\n",
       "    'acc_stderr': 0.014317197787809172,\n",
       "    'acc_norm': 0.6382252559726962,\n",
       "    'acc_norm_stderr': 0.01404195794503808},\n",
       "   'harness|hellaswag|10': {'acc': 0.665803624775941,\n",
       "    'acc_stderr': 0.004707447244200621,\n",
       "    'acc_norm': 0.8490340569607648,\n",
       "    'acc_norm_stderr': 0.0035728399695219874},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6907894736842105,\n",
       "    'acc_stderr': 0.037610708698674805,\n",
       "    'acc_norm': 0.6907894736842105,\n",
       "    'acc_norm_stderr': 0.037610708698674805},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.63,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.63,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6867924528301886,\n",
       "    'acc_stderr': 0.028544793319055326,\n",
       "    'acc_norm': 0.6867924528301886,\n",
       "    'acc_norm_stderr': 0.028544793319055326},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7708333333333334,\n",
       "    'acc_stderr': 0.035146974678623884,\n",
       "    'acc_norm': 0.7708333333333334,\n",
       "    'acc_norm_stderr': 0.035146974678623884},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.050251890762960605,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.050251890762960605},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.51,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.51,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6589595375722543,\n",
       "    'acc_stderr': 0.03614665424180826,\n",
       "    'acc_norm': 0.6589595375722543,\n",
       "    'acc_norm_stderr': 0.03614665424180826},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4411764705882353,\n",
       "    'acc_stderr': 0.049406356306056595,\n",
       "    'acc_norm': 0.4411764705882353,\n",
       "    'acc_norm_stderr': 0.049406356306056595},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768079,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768079},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5531914893617021,\n",
       "    'acc_stderr': 0.032500536843658404,\n",
       "    'acc_norm': 0.5531914893617021,\n",
       "    'acc_norm_stderr': 0.032500536843658404},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.49122807017543857,\n",
       "    'acc_stderr': 0.04702880432049615,\n",
       "    'acc_norm': 0.49122807017543857,\n",
       "    'acc_norm_stderr': 0.04702880432049615},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370332,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370332},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.41005291005291006,\n",
       "    'acc_stderr': 0.02533120243894444,\n",
       "    'acc_norm': 0.41005291005291006,\n",
       "    'acc_norm_stderr': 0.02533120243894444},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5238095238095238,\n",
       "    'acc_stderr': 0.04467062628403273,\n",
       "    'acc_norm': 0.5238095238095238,\n",
       "    'acc_norm_stderr': 0.04467062628403273},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.22,\n",
       "    'acc_stderr': 0.04163331998932269,\n",
       "    'acc_norm': 0.22,\n",
       "    'acc_norm_stderr': 0.04163331998932269},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7903225806451613,\n",
       "    'acc_stderr': 0.023157879349083525,\n",
       "    'acc_norm': 0.7903225806451613,\n",
       "    'acc_norm_stderr': 0.023157879349083525},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.46798029556650245,\n",
       "    'acc_stderr': 0.035107665979592154,\n",
       "    'acc_norm': 0.46798029556650245,\n",
       "    'acc_norm_stderr': 0.035107665979592154},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7636363636363637,\n",
       "    'acc_stderr': 0.03317505930009181,\n",
       "    'acc_norm': 0.7636363636363637,\n",
       "    'acc_norm_stderr': 0.03317505930009181},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.797979797979798,\n",
       "    'acc_stderr': 0.028606204289229865,\n",
       "    'acc_norm': 0.797979797979798,\n",
       "    'acc_norm_stderr': 0.028606204289229865},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9119170984455959,\n",
       "    'acc_stderr': 0.02045374660160103,\n",
       "    'acc_norm': 0.9119170984455959,\n",
       "    'acc_norm_stderr': 0.02045374660160103},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.676923076923077,\n",
       "    'acc_stderr': 0.02371088850197057,\n",
       "    'acc_norm': 0.676923076923077,\n",
       "    'acc_norm_stderr': 0.02371088850197057},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028593,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028593},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.680672268907563,\n",
       "    'acc_stderr': 0.0302839955258844,\n",
       "    'acc_norm': 0.680672268907563,\n",
       "    'acc_norm_stderr': 0.0302839955258844},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.37748344370860926,\n",
       "    'acc_stderr': 0.03958027231121569,\n",
       "    'acc_norm': 0.37748344370860926,\n",
       "    'acc_norm_stderr': 0.03958027231121569},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5,\n",
       "    'acc_stderr': 0.034099716973523674,\n",
       "    'acc_norm': 0.5,\n",
       "    'acc_norm_stderr': 0.034099716973523674},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8235294117647058,\n",
       "    'acc_stderr': 0.026756401538078966,\n",
       "    'acc_norm': 0.8235294117647058,\n",
       "    'acc_norm_stderr': 0.026756401538078966},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8270042194092827,\n",
       "    'acc_stderr': 0.024621562866768434,\n",
       "    'acc_norm': 0.8270042194092827,\n",
       "    'acc_norm_stderr': 0.024621562866768434},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7130044843049327,\n",
       "    'acc_stderr': 0.030360379710291947,\n",
       "    'acc_norm': 0.7130044843049327,\n",
       "    'acc_norm_stderr': 0.030360379710291947},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.7862595419847328,\n",
       "    'acc_stderr': 0.0359546161177469,\n",
       "    'acc_norm': 0.7862595419847328,\n",
       "    'acc_norm_stderr': 0.0359546161177469},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8264462809917356,\n",
       "    'acc_stderr': 0.0345727283691767,\n",
       "    'acc_norm': 0.8264462809917356,\n",
       "    'acc_norm_stderr': 0.0345727283691767},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7407407407407407,\n",
       "    'acc_stderr': 0.042365112580946336,\n",
       "    'acc_norm': 0.7407407407407407,\n",
       "    'acc_norm_stderr': 0.042365112580946336},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7730061349693251,\n",
       "    'acc_stderr': 0.03291099578615769,\n",
       "    'acc_norm': 0.7730061349693251,\n",
       "    'acc_norm_stderr': 0.03291099578615769},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.41964285714285715,\n",
       "    'acc_stderr': 0.04684099321077106,\n",
       "    'acc_norm': 0.41964285714285715,\n",
       "    'acc_norm_stderr': 0.04684099321077106},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8446601941747572,\n",
       "    'acc_stderr': 0.03586594738573973,\n",
       "    'acc_norm': 0.8446601941747572,\n",
       "    'acc_norm_stderr': 0.03586594738573973},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8760683760683761,\n",
       "    'acc_stderr': 0.02158649400128138,\n",
       "    'acc_norm': 0.8760683760683761,\n",
       "    'acc_norm_stderr': 0.02158649400128138},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.0446196043338474,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.0446196043338474},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8263090676883781,\n",
       "    'acc_stderr': 0.01354741565866226,\n",
       "    'acc_norm': 0.8263090676883781,\n",
       "    'acc_norm_stderr': 0.01354741565866226},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7369942196531792,\n",
       "    'acc_stderr': 0.023703099525258176,\n",
       "    'acc_norm': 0.7369942196531792,\n",
       "    'acc_norm_stderr': 0.023703099525258176},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.47039106145251397,\n",
       "    'acc_stderr': 0.016693154927383557,\n",
       "    'acc_norm': 0.47039106145251397,\n",
       "    'acc_norm_stderr': 0.016693154927383557},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.7352941176470589,\n",
       "    'acc_stderr': 0.025261691219729484,\n",
       "    'acc_norm': 0.7352941176470589,\n",
       "    'acc_norm_stderr': 0.025261691219729484},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6977491961414791,\n",
       "    'acc_stderr': 0.02608270069539966,\n",
       "    'acc_norm': 0.6977491961414791,\n",
       "    'acc_norm_stderr': 0.02608270069539966},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7345679012345679,\n",
       "    'acc_stderr': 0.024569223600460845,\n",
       "    'acc_norm': 0.7345679012345679,\n",
       "    'acc_norm_stderr': 0.024569223600460845},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4645390070921986,\n",
       "    'acc_stderr': 0.029752389657427047,\n",
       "    'acc_norm': 0.4645390070921986,\n",
       "    'acc_norm_stderr': 0.029752389657427047},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4641460234680574,\n",
       "    'acc_stderr': 0.012737361318730581,\n",
       "    'acc_norm': 0.4641460234680574,\n",
       "    'acc_norm_stderr': 0.012737361318730581},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.028064998167040094,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.028064998167040094},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6633986928104575,\n",
       "    'acc_stderr': 0.019117213911495144,\n",
       "    'acc_norm': 0.6633986928104575,\n",
       "    'acc_norm_stderr': 0.019117213911495144},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.726530612244898,\n",
       "    'acc_stderr': 0.028535560337128448,\n",
       "    'acc_norm': 0.726530612244898,\n",
       "    'acc_norm_stderr': 0.028535560337128448},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.845771144278607,\n",
       "    'acc_stderr': 0.025538433368578334,\n",
       "    'acc_norm': 0.845771144278607,\n",
       "    'acc_norm_stderr': 0.025538433368578334},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.88,\n",
       "    'acc_stderr': 0.032659863237109066,\n",
       "    'acc_norm': 0.88,\n",
       "    'acc_norm_stderr': 0.032659863237109066},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5301204819277109,\n",
       "    'acc_stderr': 0.03885425420866767,\n",
       "    'acc_norm': 0.5301204819277109,\n",
       "    'acc_norm_stderr': 0.03885425420866767},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.847953216374269,\n",
       "    'acc_stderr': 0.027539122889061456,\n",
       "    'acc_norm': 0.847953216374269,\n",
       "    'acc_norm_stderr': 0.027539122889061456},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3047735618115055,\n",
       "    'mc1_stderr': 0.01611412415688245,\n",
       "    'mc2': 0.463936332301049,\n",
       "    'mc2_stderr': 0.015153266555511496},\n",
       "   'harness|winogrande|5': {'acc': 0.8058405682715075,\n",
       "    'acc_stderr': 0.01111698339239267},\n",
       "   'harness|drop|3': {'em': 0.0012583892617449664,\n",
       "    'em_stderr': 0.0003630560893118993,\n",
       "    'f1': 0.07203124999999957,\n",
       "    'f1_stderr': 0.0014672437300568956},\n",
       "   'harness|gsm8k|5': {'acc': 0.623199393479909,\n",
       "    'acc_stderr': 0.013347858757829158},\n",
       "   'all': {'acc': 0.623199393479909, 'acc_stderr': 0.013347858757829158}}},\n",
       " 'vicuna-13b': {'key': 'vicuna-13b',\n",
       "  'Model': 'Vicuna-13B',\n",
       "  'MT-bench (score)': '6.57',\n",
       "  'MMLU': '0.558',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-13b-v1.5',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5349829351535836,\n",
       "    'acc_stderr': 0.014575583922019672,\n",
       "    'acc_norm': 0.5656996587030717,\n",
       "    'acc_norm_stderr': 0.014484703048857357},\n",
       "   'harness|hellaswag|10': {'acc': 0.6115315674168492,\n",
       "    'acc_stderr': 0.004864058877626272,\n",
       "    'acc_norm': 0.8108942441744672,\n",
       "    'acc_norm_stderr': 0.0039079230108406094},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.04512608598542129,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.04512608598542129},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.4888888888888889,\n",
       "    'acc_stderr': 0.04318275491977976,\n",
       "    'acc_norm': 0.4888888888888889,\n",
       "    'acc_norm_stderr': 0.04318275491977976},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5855263157894737,\n",
       "    'acc_stderr': 0.04008973785779205,\n",
       "    'acc_norm': 0.5855263157894737,\n",
       "    'acc_norm_stderr': 0.04008973785779205},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.630188679245283,\n",
       "    'acc_stderr': 0.02971142188010793,\n",
       "    'acc_norm': 0.630188679245283,\n",
       "    'acc_norm_stderr': 0.02971142188010793},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5972222222222222,\n",
       "    'acc_stderr': 0.04101405519842426,\n",
       "    'acc_norm': 0.5972222222222222,\n",
       "    'acc_norm_stderr': 0.04101405519842426},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.049999999999999996,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.049999999999999996},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5433526011560693,\n",
       "    'acc_stderr': 0.03798106566014498,\n",
       "    'acc_norm': 0.5433526011560693,\n",
       "    'acc_norm_stderr': 0.03798106566014498},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.3235294117647059,\n",
       "    'acc_stderr': 0.046550104113196156,\n",
       "    'acc_norm': 0.3235294117647059,\n",
       "    'acc_norm_stderr': 0.046550104113196156},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.451063829787234,\n",
       "    'acc_stderr': 0.032529096196131965,\n",
       "    'acc_norm': 0.451063829787234,\n",
       "    'acc_norm_stderr': 0.032529096196131965},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2894736842105263,\n",
       "    'acc_stderr': 0.04266339443159394,\n",
       "    'acc_norm': 0.2894736842105263,\n",
       "    'acc_norm_stderr': 0.04266339443159394},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.47586206896551725,\n",
       "    'acc_stderr': 0.041618085035015295,\n",
       "    'acc_norm': 0.47586206896551725,\n",
       "    'acc_norm_stderr': 0.041618085035015295},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3306878306878307,\n",
       "    'acc_stderr': 0.024229965298425075,\n",
       "    'acc_norm': 0.3306878306878307,\n",
       "    'acc_norm_stderr': 0.024229965298425075},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.04240799327574924,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.04240799327574924},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001974,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001974},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6774193548387096,\n",
       "    'acc_stderr': 0.02659308451657227,\n",
       "    'acc_norm': 0.6774193548387096,\n",
       "    'acc_norm_stderr': 0.02659308451657227},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4482758620689655,\n",
       "    'acc_stderr': 0.034991131376767445,\n",
       "    'acc_norm': 0.4482758620689655,\n",
       "    'acc_norm_stderr': 0.034991131376767445},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620333,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620333},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.696969696969697,\n",
       "    'acc_stderr': 0.03588624800091707,\n",
       "    'acc_norm': 0.696969696969697,\n",
       "    'acc_norm_stderr': 0.03588624800091707},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7070707070707071,\n",
       "    'acc_stderr': 0.03242497958178815,\n",
       "    'acc_norm': 0.7070707070707071,\n",
       "    'acc_norm_stderr': 0.03242497958178815},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8134715025906736,\n",
       "    'acc_stderr': 0.028112091210117467,\n",
       "    'acc_norm': 0.8134715025906736,\n",
       "    'acc_norm_stderr': 0.028112091210117467},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5487179487179488,\n",
       "    'acc_stderr': 0.025230381238934833,\n",
       "    'acc_norm': 0.5487179487179488,\n",
       "    'acc_norm_stderr': 0.025230381238934833},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.0284934650910286,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.0284934650910286},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5798319327731093,\n",
       "    'acc_stderr': 0.03206183783236153,\n",
       "    'acc_norm': 0.5798319327731093,\n",
       "    'acc_norm_stderr': 0.03206183783236153},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.33112582781456956,\n",
       "    'acc_stderr': 0.038425817186598696,\n",
       "    'acc_norm': 0.33112582781456956,\n",
       "    'acc_norm_stderr': 0.038425817186598696},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7541284403669725,\n",
       "    'acc_stderr': 0.01846194096870842,\n",
       "    'acc_norm': 0.7541284403669725,\n",
       "    'acc_norm_stderr': 0.01846194096870842},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4583333333333333,\n",
       "    'acc_stderr': 0.03398110890294636,\n",
       "    'acc_norm': 0.4583333333333333,\n",
       "    'acc_norm_stderr': 0.03398110890294636},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7450980392156863,\n",
       "    'acc_stderr': 0.03058759135160426,\n",
       "    'acc_norm': 0.7450980392156863,\n",
       "    'acc_norm_stderr': 0.03058759135160426},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7637130801687764,\n",
       "    'acc_stderr': 0.027652153144159256,\n",
       "    'acc_norm': 0.7637130801687764,\n",
       "    'acc_norm_stderr': 0.027652153144159256},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6502242152466368,\n",
       "    'acc_stderr': 0.03200736719484503,\n",
       "    'acc_norm': 0.6502242152466368,\n",
       "    'acc_norm_stderr': 0.03200736719484503},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6412213740458015,\n",
       "    'acc_stderr': 0.04206739313864908,\n",
       "    'acc_norm': 0.6412213740458015,\n",
       "    'acc_norm_stderr': 0.04206739313864908},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7768595041322314,\n",
       "    'acc_stderr': 0.03800754475228732,\n",
       "    'acc_norm': 0.7768595041322314,\n",
       "    'acc_norm_stderr': 0.03800754475228732},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.043300437496507416,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.043300437496507416},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6932515337423313,\n",
       "    'acc_stderr': 0.03623089915724147,\n",
       "    'acc_norm': 0.6932515337423313,\n",
       "    'acc_norm_stderr': 0.03623089915724147},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.42857142857142855,\n",
       "    'acc_stderr': 0.04697113923010212,\n",
       "    'acc_norm': 0.42857142857142855,\n",
       "    'acc_norm_stderr': 0.04697113923010212},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.043546310772605956,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.043546310772605956},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8461538461538461,\n",
       "    'acc_stderr': 0.023636873317489288,\n",
       "    'acc_norm': 0.8461538461538461,\n",
       "    'acc_norm_stderr': 0.023636873317489288},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.58,\n",
       "    'acc_stderr': 0.049604496374885836,\n",
       "    'acc_norm': 0.58,\n",
       "    'acc_norm_stderr': 0.049604496374885836},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.756066411238825,\n",
       "    'acc_stderr': 0.015357212665829461,\n",
       "    'acc_norm': 0.756066411238825,\n",
       "    'acc_norm_stderr': 0.015357212665829461},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6069364161849711,\n",
       "    'acc_stderr': 0.026296227915613663,\n",
       "    'acc_norm': 0.6069364161849711,\n",
       "    'acc_norm_stderr': 0.026296227915613663},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.2782122905027933,\n",
       "    'acc_stderr': 0.01498732543996355,\n",
       "    'acc_norm': 0.2782122905027933,\n",
       "    'acc_norm_stderr': 0.01498732543996355},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6372549019607843,\n",
       "    'acc_stderr': 0.027530078447110307,\n",
       "    'acc_norm': 0.6372549019607843,\n",
       "    'acc_norm_stderr': 0.027530078447110307},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6366559485530546,\n",
       "    'acc_stderr': 0.027316847674192717,\n",
       "    'acc_norm': 0.6366559485530546,\n",
       "    'acc_norm_stderr': 0.027316847674192717},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6388888888888888,\n",
       "    'acc_stderr': 0.026725868809100793,\n",
       "    'acc_norm': 0.6388888888888888,\n",
       "    'acc_norm_stderr': 0.026725868809100793},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.41134751773049644,\n",
       "    'acc_stderr': 0.02935491115994098,\n",
       "    'acc_norm': 0.41134751773049644,\n",
       "    'acc_norm_stderr': 0.02935491115994098},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.43285528031290743,\n",
       "    'acc_stderr': 0.012654565234622862,\n",
       "    'acc_norm': 0.43285528031290743,\n",
       "    'acc_norm_stderr': 0.012654565234622862},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5367647058823529,\n",
       "    'acc_stderr': 0.03029061918048569,\n",
       "    'acc_norm': 0.5367647058823529,\n",
       "    'acc_norm_stderr': 0.03029061918048569},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5833333333333334,\n",
       "    'acc_stderr': 0.019944914136873583,\n",
       "    'acc_norm': 0.5833333333333334,\n",
       "    'acc_norm_stderr': 0.019944914136873583},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6545454545454545,\n",
       "    'acc_stderr': 0.04554619617541054,\n",
       "    'acc_norm': 0.6545454545454545,\n",
       "    'acc_norm_stderr': 0.04554619617541054},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6122448979591837,\n",
       "    'acc_stderr': 0.031192230726795656,\n",
       "    'acc_norm': 0.6122448979591837,\n",
       "    'acc_norm_stderr': 0.031192230726795656},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.7860696517412935,\n",
       "    'acc_stderr': 0.02899690969332891,\n",
       "    'acc_norm': 0.7860696517412935,\n",
       "    'acc_norm_stderr': 0.02899690969332891},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.82,\n",
       "    'acc_stderr': 0.038612291966536934,\n",
       "    'acc_norm': 0.82,\n",
       "    'acc_norm_stderr': 0.038612291966536934},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.45180722891566266,\n",
       "    'acc_stderr': 0.03874371556587953,\n",
       "    'acc_norm': 0.45180722891566266,\n",
       "    'acc_norm_stderr': 0.03874371556587953},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.783625730994152,\n",
       "    'acc_stderr': 0.03158149539338734,\n",
       "    'acc_norm': 0.783625730994152,\n",
       "    'acc_norm_stderr': 0.03158149539338734},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.34516523867809057,\n",
       "    'mc1_stderr': 0.01664310331927494,\n",
       "    'mc2': 0.5107449529759277,\n",
       "    'mc2_stderr': 0.015464470932268133},\n",
       "   'all': {'em': 0.21403104026845637,\n",
       "    'em_stderr': 0.004200304057589016,\n",
       "    'f1': 0.2773447986577177,\n",
       "    'f1_stderr': 0.004194161726605588,\n",
       "    'acc': 0.4298049932592257,\n",
       "    'acc_stderr': 0.010471546731533343},\n",
       "   'harness|drop|3': {'em': 0.21403104026845637,\n",
       "    'em_stderr': 0.004200304057589016,\n",
       "    'f1': 0.2773447986577177,\n",
       "    'f1_stderr': 0.004194161726605588},\n",
       "   'harness|gsm8k|5': {'acc': 0.11296436694465505,\n",
       "    'acc_stderr': 0.008719339028833057},\n",
       "   'harness|winogrande|5': {'acc': 0.7466456195737964,\n",
       "    'acc_stderr': 0.01222375443423363}}},\n",
       " 'vicuna-33b': {'key': 'vicuna-33b',\n",
       "  'Model': 'Vicuna-33B',\n",
       "  'MT-bench (score)': '7.12',\n",
       "  'MMLU': '0.592',\n",
       "  'License': 'Non-commercial',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-33b-v1.3',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5921501706484642,\n",
       "    'acc_stderr': 0.014361097288449707,\n",
       "    'acc_norm': 0.6194539249146758,\n",
       "    'acc_norm_stderr': 0.014188277712349807},\n",
       "   'harness|hellaswag|10': {'acc': 0.6249751045608445,\n",
       "    'acc_stderr': 0.00483139921850023,\n",
       "    'acc_norm': 0.82194781915953,\n",
       "    'acc_norm_stderr': 0.003817748269107777},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.27,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.27,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5333333333333333,\n",
       "    'acc_stderr': 0.043097329010363554,\n",
       "    'acc_norm': 0.5333333333333333,\n",
       "    'acc_norm_stderr': 0.043097329010363554},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.625,\n",
       "    'acc_stderr': 0.039397364351956274,\n",
       "    'acc_norm': 0.625,\n",
       "    'acc_norm_stderr': 0.039397364351956274},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6037735849056604,\n",
       "    'acc_stderr': 0.030102793781791197,\n",
       "    'acc_norm': 0.6037735849056604,\n",
       "    'acc_norm_stderr': 0.030102793781791197},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.6319444444444444,\n",
       "    'acc_stderr': 0.040329990539607195,\n",
       "    'acc_norm': 0.6319444444444444,\n",
       "    'acc_norm_stderr': 0.040329990539607195},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.49,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.49,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.04725815626252604,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.04725815626252604},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.5144508670520231,\n",
       "    'acc_stderr': 0.03810871630454764,\n",
       "    'acc_norm': 0.5144508670520231,\n",
       "    'acc_norm_stderr': 0.03810871630454764},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.27450980392156865,\n",
       "    'acc_stderr': 0.04440521906179326,\n",
       "    'acc_norm': 0.27450980392156865,\n",
       "    'acc_norm_stderr': 0.04440521906179326},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.46808510638297873,\n",
       "    'acc_stderr': 0.03261936918467382,\n",
       "    'acc_norm': 0.46808510638297873,\n",
       "    'acc_norm_stderr': 0.03261936918467382},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.34210526315789475,\n",
       "    'acc_stderr': 0.04462917535336936,\n",
       "    'acc_norm': 0.34210526315789475,\n",
       "    'acc_norm_stderr': 0.04462917535336936},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.04164188720169375,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.04164188720169375},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36507936507936506,\n",
       "    'acc_stderr': 0.024796060602699958,\n",
       "    'acc_norm': 0.36507936507936506,\n",
       "    'acc_norm_stderr': 0.024796060602699958},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.04240799327574924,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.04240799327574924},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.6870967741935484,\n",
       "    'acc_stderr': 0.02637756702864586,\n",
       "    'acc_norm': 0.6870967741935484,\n",
       "    'acc_norm_stderr': 0.02637756702864586},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.39901477832512317,\n",
       "    'acc_stderr': 0.03445487686264716,\n",
       "    'acc_norm': 0.39901477832512317,\n",
       "    'acc_norm_stderr': 0.03445487686264716},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237102,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237102},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7151515151515152,\n",
       "    'acc_stderr': 0.03524390844511781,\n",
       "    'acc_norm': 0.7151515151515152,\n",
       "    'acc_norm_stderr': 0.03524390844511781},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7727272727272727,\n",
       "    'acc_stderr': 0.02985751567338642,\n",
       "    'acc_norm': 0.7727272727272727,\n",
       "    'acc_norm_stderr': 0.02985751567338642},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8393782383419689,\n",
       "    'acc_stderr': 0.02649905770139744,\n",
       "    'acc_norm': 0.8393782383419689,\n",
       "    'acc_norm_stderr': 0.02649905770139744},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5871794871794872,\n",
       "    'acc_stderr': 0.024962683564331806,\n",
       "    'acc_norm': 0.5871794871794872,\n",
       "    'acc_norm_stderr': 0.024962683564331806},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.2740740740740741,\n",
       "    'acc_stderr': 0.027195934804085626,\n",
       "    'acc_norm': 0.2740740740740741,\n",
       "    'acc_norm_stderr': 0.027195934804085626},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5840336134453782,\n",
       "    'acc_stderr': 0.03201650100739611,\n",
       "    'acc_norm': 0.5840336134453782,\n",
       "    'acc_norm_stderr': 0.03201650100739611},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.40397350993377484,\n",
       "    'acc_stderr': 0.04006485685365342,\n",
       "    'acc_norm': 0.40397350993377484,\n",
       "    'acc_norm_stderr': 0.04006485685365342},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7761467889908257,\n",
       "    'acc_stderr': 0.017871217767790232,\n",
       "    'acc_norm': 0.7761467889908257,\n",
       "    'acc_norm_stderr': 0.017871217767790232},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4027777777777778,\n",
       "    'acc_stderr': 0.033448873829978666,\n",
       "    'acc_norm': 0.4027777777777778,\n",
       "    'acc_norm_stderr': 0.033448873829978666},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7745098039215687,\n",
       "    'acc_stderr': 0.029331162294251735,\n",
       "    'acc_norm': 0.7745098039215687,\n",
       "    'acc_norm_stderr': 0.029331162294251735},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8143459915611815,\n",
       "    'acc_stderr': 0.025310495376944863,\n",
       "    'acc_norm': 0.8143459915611815,\n",
       "    'acc_norm_stderr': 0.025310495376944863},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6771300448430493,\n",
       "    'acc_stderr': 0.03138147637575498,\n",
       "    'acc_norm': 0.6771300448430493,\n",
       "    'acc_norm_stderr': 0.03138147637575498},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6946564885496184,\n",
       "    'acc_stderr': 0.040393149787245626,\n",
       "    'acc_norm': 0.6946564885496184,\n",
       "    'acc_norm_stderr': 0.040393149787245626},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7520661157024794,\n",
       "    'acc_stderr': 0.03941897526516303,\n",
       "    'acc_norm': 0.7520661157024794,\n",
       "    'acc_norm_stderr': 0.03941897526516303},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7129629629629629,\n",
       "    'acc_stderr': 0.043733130409147614,\n",
       "    'acc_norm': 0.7129629629629629,\n",
       "    'acc_norm_stderr': 0.043733130409147614},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6993865030674846,\n",
       "    'acc_stderr': 0.03602511318806771,\n",
       "    'acc_norm': 0.6993865030674846,\n",
       "    'acc_norm_stderr': 0.03602511318806771},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.45535714285714285,\n",
       "    'acc_stderr': 0.04726835553719099,\n",
       "    'acc_norm': 0.45535714285714285,\n",
       "    'acc_norm_stderr': 0.04726835553719099},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7184466019417476,\n",
       "    'acc_stderr': 0.044532548363264673,\n",
       "    'acc_norm': 0.7184466019417476,\n",
       "    'acc_norm_stderr': 0.044532548363264673},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8803418803418803,\n",
       "    'acc_stderr': 0.021262719400406978,\n",
       "    'acc_norm': 0.8803418803418803,\n",
       "    'acc_norm_stderr': 0.021262719400406978},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7662835249042146,\n",
       "    'acc_stderr': 0.015133383278988829,\n",
       "    'acc_norm': 0.7662835249042146,\n",
       "    'acc_norm_stderr': 0.015133383278988829},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6705202312138728,\n",
       "    'acc_stderr': 0.0253052581318797,\n",
       "    'acc_norm': 0.6705202312138728,\n",
       "    'acc_norm_stderr': 0.0253052581318797},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.43798882681564244,\n",
       "    'acc_stderr': 0.016593394227564843,\n",
       "    'acc_norm': 0.43798882681564244,\n",
       "    'acc_norm_stderr': 0.016593394227564843},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6601307189542484,\n",
       "    'acc_stderr': 0.027121956071388856,\n",
       "    'acc_norm': 0.6601307189542484,\n",
       "    'acc_norm_stderr': 0.027121956071388856},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6688102893890675,\n",
       "    'acc_stderr': 0.026730620728004906,\n",
       "    'acc_norm': 0.6688102893890675,\n",
       "    'acc_norm_stderr': 0.026730620728004906},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6851851851851852,\n",
       "    'acc_stderr': 0.02584224870090217,\n",
       "    'acc_norm': 0.6851851851851852,\n",
       "    'acc_norm_stderr': 0.02584224870090217},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.425531914893617,\n",
       "    'acc_stderr': 0.02949482760014438,\n",
       "    'acc_norm': 0.425531914893617,\n",
       "    'acc_norm_stderr': 0.02949482760014438},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.46740547588005216,\n",
       "    'acc_stderr': 0.012743072942653352,\n",
       "    'acc_norm': 0.46740547588005216,\n",
       "    'acc_norm_stderr': 0.012743072942653352},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5772058823529411,\n",
       "    'acc_stderr': 0.03000856284500348,\n",
       "    'acc_norm': 0.5772058823529411,\n",
       "    'acc_norm_stderr': 0.03000856284500348},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6372549019607843,\n",
       "    'acc_stderr': 0.01945076843250551,\n",
       "    'acc_norm': 0.6372549019607843,\n",
       "    'acc_norm_stderr': 0.01945076843250551},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.046075820907199756,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.046075820907199756},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.689795918367347,\n",
       "    'acc_stderr': 0.029613459872484375,\n",
       "    'acc_norm': 0.689795918367347,\n",
       "    'acc_norm_stderr': 0.029613459872484375},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8109452736318408,\n",
       "    'acc_stderr': 0.027686913588013024,\n",
       "    'acc_norm': 0.8109452736318408,\n",
       "    'acc_norm_stderr': 0.027686913588013024},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.84,\n",
       "    'acc_stderr': 0.03684529491774708,\n",
       "    'acc_norm': 0.84,\n",
       "    'acc_norm_stderr': 0.03684529491774708},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4939759036144578,\n",
       "    'acc_stderr': 0.03892212195333045,\n",
       "    'acc_norm': 0.4939759036144578,\n",
       "    'acc_norm_stderr': 0.03892212195333045},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7719298245614035,\n",
       "    'acc_stderr': 0.03218093795602357,\n",
       "    'acc_norm': 0.7719298245614035,\n",
       "    'acc_norm_stderr': 0.03218093795602357},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3525091799265606,\n",
       "    'mc1_stderr': 0.016724646380756544,\n",
       "    'mc2': 0.5480260004479156,\n",
       "    'mc2_stderr': 0.01562805066884067},\n",
       "   'all': {'acc': 0.5856610701363549,\n",
       "    'acc_stderr': 0.0338972114882748,\n",
       "    'acc_norm': 0.5894623662188109,\n",
       "    'acc_norm_stderr': 0.0338771018183512,\n",
       "    'mc1': 0.3525091799265606,\n",
       "    'mc1_stderr': 0.016724646380756544,\n",
       "    'mc2': 0.5480260004479156,\n",
       "    'mc2_stderr': 0.01562805066884067},\n",
       "   'harness|drop|3': {'em': 0.24611996644295303,\n",
       "    'em_stderr': 0.004411275638567265,\n",
       "    'f1': 0.3191652684563765,\n",
       "    'f1_stderr': 0.004369271114420946},\n",
       "   'harness|gsm8k|5': {'acc': 0.1372251705837756,\n",
       "    'acc_stderr': 0.00947780824460041},\n",
       "   'harness|winogrande|5': {'acc': 0.7703235990528808,\n",
       "    'acc_stderr': 0.011821645601838243}}},\n",
       " 'vicuna-7b': {'key': 'vicuna-7b',\n",
       "  'Model': 'Vicuna-7B',\n",
       "  'MT-bench (score)': '6.17',\n",
       "  'MMLU': '0.498',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'LMSYS',\n",
       "  'Link': 'https://huggingface.co/lmsys/vicuna-7b-v1.5',\n",
       "  'results': {'harness|drop|3': {'em': 0.017932046979865772,\n",
       "    'em_stderr': 0.0013590184569504276,\n",
       "    'f1': 0.08961094798657747,\n",
       "    'f1_stderr': 0.002014243406072028},\n",
       "   'harness|gsm8k|5': {'acc': 0.08188021228203184,\n",
       "    'acc_stderr': 0.007552338527716956},\n",
       "   'harness|winogrande|5': {'acc': 0.7213891081294396,\n",
       "    'acc_stderr': 0.012599896649493878},\n",
       "   'all': {'acc': 0.509411874714016,\n",
       "    'acc_stderr': 0.03492615918680034,\n",
       "    'acc_norm': 0.5131451594579193,\n",
       "    'acc_norm_stderr': 0.03491300641703413,\n",
       "    'mc1': 0.33047735618115054,\n",
       "    'mc1_stderr': 0.016466769613698303,\n",
       "    'mc2': 0.5032970216179807,\n",
       "    'mc2_stderr': 0.015653721119290503},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5025597269624573,\n",
       "    'acc_stderr': 0.014611199329843784,\n",
       "    'acc_norm': 0.5324232081911263,\n",
       "    'acc_norm_stderr': 0.01458063756999542},\n",
       "   'harness|hellaswag|10': {'acc': 0.5835490938060147,\n",
       "    'acc_stderr': 0.0049196263806455115,\n",
       "    'acc_norm': 0.7739494124676359,\n",
       "    'acc_norm_stderr': 0.004174174724288079},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.28,\n",
       "    'acc_stderr': 0.045126085985421276,\n",
       "    'acc_norm': 0.28,\n",
       "    'acc_norm_stderr': 0.045126085985421276},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5037037037037037,\n",
       "    'acc_stderr': 0.04319223625811331,\n",
       "    'acc_norm': 0.5037037037037037,\n",
       "    'acc_norm_stderr': 0.04319223625811331},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.04063302731486671,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.04063302731486671},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.5433962264150943,\n",
       "    'acc_stderr': 0.030656748696739428,\n",
       "    'acc_norm': 0.5433962264150943,\n",
       "    'acc_norm_stderr': 0.030656748696739428},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.04179596617581,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.04179596617581},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.39,\n",
       "    'acc_stderr': 0.04902071300001975,\n",
       "    'acc_norm': 0.39,\n",
       "    'acc_norm_stderr': 0.04902071300001975},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4682080924855491,\n",
       "    'acc_stderr': 0.03804749744364763,\n",
       "    'acc_norm': 0.4682080924855491,\n",
       "    'acc_norm_stderr': 0.03804749744364763},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.17647058823529413,\n",
       "    'acc_stderr': 0.0379328118530781,\n",
       "    'acc_norm': 0.17647058823529413,\n",
       "    'acc_norm_stderr': 0.0379328118530781},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.4553191489361702,\n",
       "    'acc_stderr': 0.03255525359340355,\n",
       "    'acc_norm': 0.4553191489361702,\n",
       "    'acc_norm_stderr': 0.03255525359340355},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.3157894736842105,\n",
       "    'acc_stderr': 0.04372748290278006,\n",
       "    'acc_norm': 0.3157894736842105,\n",
       "    'acc_norm_stderr': 0.04372748290278006},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.42758620689655175,\n",
       "    'acc_stderr': 0.04122737111370331,\n",
       "    'acc_norm': 0.42758620689655175,\n",
       "    'acc_norm_stderr': 0.04122737111370331},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.30687830687830686,\n",
       "    'acc_stderr': 0.023752928712112143,\n",
       "    'acc_norm': 0.30687830687830686,\n",
       "    'acc_norm_stderr': 0.023752928712112143},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.373015873015873,\n",
       "    'acc_stderr': 0.04325506042017086,\n",
       "    'acc_norm': 0.373015873015873,\n",
       "    'acc_norm_stderr': 0.04325506042017086},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.33,\n",
       "    'acc_stderr': 0.047258156262526045,\n",
       "    'acc_norm': 0.33,\n",
       "    'acc_norm_stderr': 0.047258156262526045},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.535483870967742,\n",
       "    'acc_stderr': 0.028372287797962935,\n",
       "    'acc_norm': 0.535483870967742,\n",
       "    'acc_norm_stderr': 0.028372287797962935},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.4039408866995074,\n",
       "    'acc_stderr': 0.0345245390382204,\n",
       "    'acc_norm': 0.4039408866995074,\n",
       "    'acc_norm_stderr': 0.0345245390382204},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.43,\n",
       "    'acc_stderr': 0.04975698519562428,\n",
       "    'acc_norm': 0.43,\n",
       "    'acc_norm_stderr': 0.04975698519562428},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6303030303030303,\n",
       "    'acc_stderr': 0.037694303145125674,\n",
       "    'acc_norm': 0.6303030303030303,\n",
       "    'acc_norm_stderr': 0.037694303145125674},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6161616161616161,\n",
       "    'acc_stderr': 0.034648816750163396,\n",
       "    'acc_norm': 0.6161616161616161,\n",
       "    'acc_norm_stderr': 0.034648816750163396},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.7357512953367875,\n",
       "    'acc_stderr': 0.03182155050916645,\n",
       "    'acc_norm': 0.7357512953367875,\n",
       "    'acc_norm_stderr': 0.03182155050916645},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.4794871794871795,\n",
       "    'acc_stderr': 0.025329663163489943,\n",
       "    'acc_norm': 0.4794871794871795,\n",
       "    'acc_norm_stderr': 0.025329663163489943},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.24444444444444444,\n",
       "    'acc_stderr': 0.02620276653465215,\n",
       "    'acc_norm': 0.24444444444444444,\n",
       "    'acc_norm_stderr': 0.02620276653465215},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.453781512605042,\n",
       "    'acc_stderr': 0.03233943468182088,\n",
       "    'acc_norm': 0.453781512605042,\n",
       "    'acc_norm_stderr': 0.03233943468182088},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.271523178807947,\n",
       "    'acc_stderr': 0.03631329803969653,\n",
       "    'acc_norm': 0.271523178807947,\n",
       "    'acc_norm_stderr': 0.03631329803969653},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.6972477064220184,\n",
       "    'acc_stderr': 0.019698711434756336,\n",
       "    'acc_norm': 0.6972477064220184,\n",
       "    'acc_norm_stderr': 0.019698711434756336},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.3888888888888889,\n",
       "    'acc_stderr': 0.03324708911809117,\n",
       "    'acc_norm': 0.3888888888888889,\n",
       "    'acc_norm_stderr': 0.03324708911809117},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7156862745098039,\n",
       "    'acc_stderr': 0.031660096793998116,\n",
       "    'acc_norm': 0.7156862745098039,\n",
       "    'acc_norm_stderr': 0.031660096793998116},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7215189873417721,\n",
       "    'acc_stderr': 0.02917868230484253,\n",
       "    'acc_norm': 0.7215189873417721,\n",
       "    'acc_norm_stderr': 0.02917868230484253},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6188340807174888,\n",
       "    'acc_stderr': 0.03259625118416827,\n",
       "    'acc_norm': 0.6188340807174888,\n",
       "    'acc_norm_stderr': 0.03259625118416827},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6335877862595419,\n",
       "    'acc_stderr': 0.04225875451969637,\n",
       "    'acc_norm': 0.6335877862595419,\n",
       "    'acc_norm_stderr': 0.04225875451969637},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.5950413223140496,\n",
       "    'acc_stderr': 0.04481137755942469,\n",
       "    'acc_norm': 0.5950413223140496,\n",
       "    'acc_norm_stderr': 0.04481137755942469},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.5648148148148148,\n",
       "    'acc_stderr': 0.04792898170907061,\n",
       "    'acc_norm': 0.5648148148148148,\n",
       "    'acc_norm_stderr': 0.04792898170907061},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.5337423312883436,\n",
       "    'acc_stderr': 0.039194155450484096,\n",
       "    'acc_norm': 0.5337423312883436,\n",
       "    'acc_norm_stderr': 0.039194155450484096},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.42857142857142855,\n",
       "    'acc_stderr': 0.04697113923010212,\n",
       "    'acc_norm': 0.42857142857142855,\n",
       "    'acc_norm_stderr': 0.04697113923010212},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6699029126213593,\n",
       "    'acc_stderr': 0.04656147110012349,\n",
       "    'acc_norm': 0.6699029126213593,\n",
       "    'acc_norm_stderr': 0.04656147110012349},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.7692307692307693,\n",
       "    'acc_stderr': 0.027601921381417593,\n",
       "    'acc_norm': 0.7692307692307693,\n",
       "    'acc_norm_stderr': 0.027601921381417593},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.57,\n",
       "    'acc_stderr': 0.049756985195624284,\n",
       "    'acc_norm': 0.57,\n",
       "    'acc_norm_stderr': 0.049756985195624284},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.6883780332056194,\n",
       "    'acc_stderr': 0.016562433867284176,\n",
       "    'acc_norm': 0.6883780332056194,\n",
       "    'acc_norm_stderr': 0.016562433867284176},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5578034682080925,\n",
       "    'acc_stderr': 0.026738603643807403,\n",
       "    'acc_norm': 0.5578034682080925,\n",
       "    'acc_norm_stderr': 0.026738603643807403},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.24022346368715083,\n",
       "    'acc_stderr': 0.014288343803925296,\n",
       "    'acc_norm': 0.24022346368715083,\n",
       "    'acc_norm_stderr': 0.014288343803925296},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.5751633986928104,\n",
       "    'acc_stderr': 0.02830457667314112,\n",
       "    'acc_norm': 0.5751633986928104,\n",
       "    'acc_norm_stderr': 0.02830457667314112},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5852090032154341,\n",
       "    'acc_stderr': 0.027982680459759567,\n",
       "    'acc_norm': 0.5852090032154341,\n",
       "    'acc_norm_stderr': 0.027982680459759567},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5555555555555556,\n",
       "    'acc_stderr': 0.027648477877413324,\n",
       "    'acc_norm': 0.5555555555555556,\n",
       "    'acc_norm_stderr': 0.027648477877413324},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.36524822695035464,\n",
       "    'acc_stderr': 0.028723863853281285,\n",
       "    'acc_norm': 0.36524822695035464,\n",
       "    'acc_norm_stderr': 0.028723863853281285},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.37353324641460234,\n",
       "    'acc_stderr': 0.012354994823515271,\n",
       "    'acc_norm': 0.37353324641460234,\n",
       "    'acc_norm_stderr': 0.012354994823515271},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5294117647058824,\n",
       "    'acc_stderr': 0.03032024326500413,\n",
       "    'acc_norm': 0.5294117647058824,\n",
       "    'acc_norm_stderr': 0.03032024326500413},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.4852941176470588,\n",
       "    'acc_stderr': 0.020219083895133924,\n",
       "    'acc_norm': 0.4852941176470588,\n",
       "    'acc_norm_stderr': 0.020219083895133924},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6272727272727273,\n",
       "    'acc_stderr': 0.04631381319425465,\n",
       "    'acc_norm': 0.6272727272727273,\n",
       "    'acc_norm_stderr': 0.04631381319425465},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6244897959183674,\n",
       "    'acc_stderr': 0.03100120903989484,\n",
       "    'acc_norm': 0.6244897959183674,\n",
       "    'acc_norm_stderr': 0.03100120903989484},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.03333333333333335,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.03333333333333335},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.76,\n",
       "    'acc_stderr': 0.04292346959909282,\n",
       "    'acc_norm': 0.76,\n",
       "    'acc_norm_stderr': 0.04292346959909282},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.4397590361445783,\n",
       "    'acc_stderr': 0.03864139923699121,\n",
       "    'acc_norm': 0.4397590361445783,\n",
       "    'acc_norm_stderr': 0.03864139923699121},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7192982456140351,\n",
       "    'acc_stderr': 0.034462962170884265,\n",
       "    'acc_norm': 0.7192982456140351,\n",
       "    'acc_norm_stderr': 0.034462962170884265},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.33047735618115054,\n",
       "    'mc1_stderr': 0.016466769613698303,\n",
       "    'mc2': 0.5032970216179807,\n",
       "    'mc2_stderr': 0.015653721119290503}}},\n",
       " 'wizardlm-13b': {'key': 'wizardlm-13b',\n",
       "  'Model': 'WizardLM-13b-v1.2',\n",
       "  'MT-bench (score)': '7.20',\n",
       "  'MMLU': '0.527',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Microsoft',\n",
       "  'Link': 'https://huggingface.co/WizardLM/WizardLM-13B-V1.2',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.5443686006825939,\n",
       "    'acc_stderr': 0.014553749939306861,\n",
       "    'acc_norm': 0.590443686006826,\n",
       "    'acc_norm_stderr': 0.014370358632472439},\n",
       "   'harness|hellaswag|10': {'acc': 0.6294562836088429,\n",
       "    'acc_stderr': 0.00481963366883254,\n",
       "    'acc_norm': 0.8221469826727743,\n",
       "    'acc_norm_stderr': 0.0038160747120605347},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5111111111111111,\n",
       "    'acc_stderr': 0.04318275491977976,\n",
       "    'acc_norm': 0.5111111111111111,\n",
       "    'acc_norm_stderr': 0.04318275491977976},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.5394736842105263,\n",
       "    'acc_stderr': 0.04056242252249034,\n",
       "    'acc_norm': 0.5394736842105263,\n",
       "    'acc_norm_stderr': 0.04056242252249034},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.55,\n",
       "    'acc_stderr': 0.04999999999999999,\n",
       "    'acc_norm': 0.55,\n",
       "    'acc_norm_stderr': 0.04999999999999999},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6113207547169811,\n",
       "    'acc_stderr': 0.030000485448675986,\n",
       "    'acc_norm': 0.6113207547169811,\n",
       "    'acc_norm_stderr': 0.030000485448675986},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.5694444444444444,\n",
       "    'acc_stderr': 0.04140685639111503,\n",
       "    'acc_norm': 0.5694444444444444,\n",
       "    'acc_norm_stderr': 0.04140685639111503},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.37,\n",
       "    'acc_stderr': 0.04852365870939099,\n",
       "    'acc_norm': 0.37,\n",
       "    'acc_norm_stderr': 0.04852365870939099},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.31,\n",
       "    'acc_stderr': 0.04648231987117316,\n",
       "    'acc_norm': 0.31,\n",
       "    'acc_norm_stderr': 0.04648231987117316},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.4913294797687861,\n",
       "    'acc_stderr': 0.03811890988940412,\n",
       "    'acc_norm': 0.4913294797687861,\n",
       "    'acc_norm_stderr': 0.03811890988940412},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.2647058823529412,\n",
       "    'acc_stderr': 0.04389869956808777,\n",
       "    'acc_norm': 0.2647058823529412,\n",
       "    'acc_norm_stderr': 0.04389869956808777},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.62,\n",
       "    'acc_stderr': 0.048783173121456316,\n",
       "    'acc_norm': 0.62,\n",
       "    'acc_norm_stderr': 0.048783173121456316},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.42127659574468085,\n",
       "    'acc_stderr': 0.03227834510146268,\n",
       "    'acc_norm': 0.42127659574468085,\n",
       "    'acc_norm_stderr': 0.03227834510146268},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.2894736842105263,\n",
       "    'acc_stderr': 0.04266339443159394,\n",
       "    'acc_norm': 0.2894736842105263,\n",
       "    'acc_norm_stderr': 0.04266339443159394},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.4482758620689655,\n",
       "    'acc_stderr': 0.04144311810878151,\n",
       "    'acc_norm': 0.4482758620689655,\n",
       "    'acc_norm_stderr': 0.04144311810878151},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.3412698412698413,\n",
       "    'acc_stderr': 0.02441923496681907,\n",
       "    'acc_norm': 0.3412698412698413,\n",
       "    'acc_norm_stderr': 0.02441923496681907},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.3492063492063492,\n",
       "    'acc_stderr': 0.04263906892795132,\n",
       "    'acc_norm': 0.3492063492063492,\n",
       "    'acc_norm_stderr': 0.04263906892795132},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.29,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.29,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.632258064516129,\n",
       "    'acc_stderr': 0.02743086657997347,\n",
       "    'acc_norm': 0.632258064516129,\n",
       "    'acc_norm_stderr': 0.02743086657997347},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.43842364532019706,\n",
       "    'acc_stderr': 0.03491207857486518,\n",
       "    'acc_norm': 0.43842364532019706,\n",
       "    'acc_norm_stderr': 0.03491207857486518},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.6,\n",
       "    'acc_stderr': 0.049236596391733084,\n",
       "    'acc_norm': 0.6,\n",
       "    'acc_norm_stderr': 0.049236596391733084},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.6787878787878788,\n",
       "    'acc_stderr': 0.036462049632538115,\n",
       "    'acc_norm': 0.6787878787878788,\n",
       "    'acc_norm_stderr': 0.036462049632538115},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.6919191919191919,\n",
       "    'acc_stderr': 0.03289477330098616,\n",
       "    'acc_norm': 0.6919191919191919,\n",
       "    'acc_norm_stderr': 0.03289477330098616},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8031088082901554,\n",
       "    'acc_stderr': 0.028697873971860695,\n",
       "    'acc_norm': 0.8031088082901554,\n",
       "    'acc_norm_stderr': 0.028697873971860695},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.5230769230769231,\n",
       "    'acc_stderr': 0.025323990861736236,\n",
       "    'acc_norm': 0.5230769230769231,\n",
       "    'acc_norm_stderr': 0.025323990861736236},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.32222222222222224,\n",
       "    'acc_stderr': 0.028493465091028604,\n",
       "    'acc_norm': 0.32222222222222224,\n",
       "    'acc_norm_stderr': 0.028493465091028604},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.5798319327731093,\n",
       "    'acc_stderr': 0.03206183783236152,\n",
       "    'acc_norm': 0.5798319327731093,\n",
       "    'acc_norm_stderr': 0.03206183783236152},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.304635761589404,\n",
       "    'acc_stderr': 0.03757949922943343,\n",
       "    'acc_norm': 0.304635761589404,\n",
       "    'acc_norm_stderr': 0.03757949922943343},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.7321100917431193,\n",
       "    'acc_stderr': 0.018987462257978652,\n",
       "    'acc_norm': 0.7321100917431193,\n",
       "    'acc_norm_stderr': 0.018987462257978652},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.4027777777777778,\n",
       "    'acc_stderr': 0.033448873829978666,\n",
       "    'acc_norm': 0.4027777777777778,\n",
       "    'acc_norm_stderr': 0.033448873829978666},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03039153369274154,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03039153369274154},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7257383966244726,\n",
       "    'acc_stderr': 0.029041333510598028,\n",
       "    'acc_norm': 0.7257383966244726,\n",
       "    'acc_norm_stderr': 0.029041333510598028},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6322869955156951,\n",
       "    'acc_stderr': 0.03236198350928276,\n",
       "    'acc_norm': 0.6322869955156951,\n",
       "    'acc_norm_stderr': 0.03236198350928276},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.5954198473282443,\n",
       "    'acc_stderr': 0.043046937953806645,\n",
       "    'acc_norm': 0.5954198473282443,\n",
       "    'acc_norm_stderr': 0.043046937953806645},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7520661157024794,\n",
       "    'acc_stderr': 0.03941897526516303,\n",
       "    'acc_norm': 0.7520661157024794,\n",
       "    'acc_norm_stderr': 0.03941897526516303},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7314814814814815,\n",
       "    'acc_stderr': 0.042844679680521934,\n",
       "    'acc_norm': 0.7314814814814815,\n",
       "    'acc_norm_stderr': 0.042844679680521934},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.6441717791411042,\n",
       "    'acc_stderr': 0.03761521380046734,\n",
       "    'acc_norm': 0.6441717791411042,\n",
       "    'acc_norm_stderr': 0.03761521380046734},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.33035714285714285,\n",
       "    'acc_stderr': 0.04464285714285712,\n",
       "    'acc_norm': 0.33035714285714285,\n",
       "    'acc_norm_stderr': 0.04464285714285712},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.6601941747572816,\n",
       "    'acc_stderr': 0.046897659372781335,\n",
       "    'acc_norm': 0.6601941747572816,\n",
       "    'acc_norm_stderr': 0.046897659372781335},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8162393162393162,\n",
       "    'acc_stderr': 0.02537213967172293,\n",
       "    'acc_norm': 0.8162393162393162,\n",
       "    'acc_norm_stderr': 0.02537213967172293},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7279693486590039,\n",
       "    'acc_stderr': 0.015913367447500503,\n",
       "    'acc_norm': 0.7279693486590039,\n",
       "    'acc_norm_stderr': 0.015913367447500503},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.5982658959537572,\n",
       "    'acc_stderr': 0.026394104177643634,\n",
       "    'acc_norm': 0.5982658959537572,\n",
       "    'acc_norm_stderr': 0.026394104177643634},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.30502793296089387,\n",
       "    'acc_stderr': 0.015398723510916713,\n",
       "    'acc_norm': 0.30502793296089387,\n",
       "    'acc_norm_stderr': 0.015398723510916713},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6013071895424836,\n",
       "    'acc_stderr': 0.028036092273891776,\n",
       "    'acc_norm': 0.6013071895424836,\n",
       "    'acc_norm_stderr': 0.028036092273891776},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.5980707395498392,\n",
       "    'acc_stderr': 0.02784647600593047,\n",
       "    'acc_norm': 0.5980707395498392,\n",
       "    'acc_norm_stderr': 0.02784647600593047},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.5895061728395061,\n",
       "    'acc_stderr': 0.027371350925124764,\n",
       "    'acc_norm': 0.5895061728395061,\n",
       "    'acc_norm_stderr': 0.027371350925124764},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4219858156028369,\n",
       "    'acc_stderr': 0.029462189233370597,\n",
       "    'acc_norm': 0.4219858156028369,\n",
       "    'acc_norm_stderr': 0.029462189233370597},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4165580182529335,\n",
       "    'acc_stderr': 0.012591153245057388,\n",
       "    'acc_norm': 0.4165580182529335,\n",
       "    'acc_norm_stderr': 0.012591153245057388},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.5625,\n",
       "    'acc_stderr': 0.030134614954403924,\n",
       "    'acc_norm': 0.5625,\n",
       "    'acc_norm_stderr': 0.030134614954403924},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.5310457516339869,\n",
       "    'acc_stderr': 0.02018880445636189,\n",
       "    'acc_norm': 0.5310457516339869,\n",
       "    'acc_norm_stderr': 0.02018880445636189},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6727272727272727,\n",
       "    'acc_stderr': 0.0449429086625209,\n",
       "    'acc_norm': 0.6727272727272727,\n",
       "    'acc_norm_stderr': 0.0449429086625209},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6693877551020408,\n",
       "    'acc_stderr': 0.0301164262965406,\n",
       "    'acc_norm': 0.6693877551020408,\n",
       "    'acc_norm_stderr': 0.0301164262965406},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.6865671641791045,\n",
       "    'acc_stderr': 0.032801882053486435,\n",
       "    'acc_norm': 0.6865671641791045,\n",
       "    'acc_norm_stderr': 0.032801882053486435},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.463855421686747,\n",
       "    'acc_stderr': 0.03882310850890593,\n",
       "    'acc_norm': 0.463855421686747,\n",
       "    'acc_norm_stderr': 0.03882310850890593},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.7543859649122807,\n",
       "    'acc_stderr': 0.0330140594698725,\n",
       "    'acc_norm': 0.7543859649122807,\n",
       "    'acc_norm_stderr': 0.0330140594698725},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.32558139534883723,\n",
       "    'mc1_stderr': 0.01640398946990783,\n",
       "    'mc2': 0.47267763319871686,\n",
       "    'mc2_stderr': 0.01512716043041388},\n",
       "   'all': {'em': 0.09133808724832215,\n",
       "    'em_stderr': 0.002950304012601038,\n",
       "    'f1': 0.1617292365771806,\n",
       "    'f1_stderr': 0.0032231699829319426,\n",
       "    'acc': 0.4269860152120696,\n",
       "    'acc_stderr': 0.011021928189223498},\n",
       "   'harness|drop|3': {'em': 0.09133808724832215,\n",
       "    'em_stderr': 0.002950304012601038,\n",
       "    'f1': 0.1617292365771806,\n",
       "    'f1_stderr': 0.0032231699829319426},\n",
       "   'harness|gsm8k|5': {'acc': 0.13495072024260804,\n",
       "    'acc_stderr': 0.009411315282571171},\n",
       "   'harness|winogrande|5': {'acc': 0.7190213101815311,\n",
       "    'acc_stderr': 0.012632541095875825}}},\n",
       " 'wizardlm-70b': {'key': 'wizardlm-70b',\n",
       "  'Model': 'WizardLM-70B-v1.0',\n",
       "  'MT-bench (score)': '7.71',\n",
       "  'MMLU': '0.637',\n",
       "  'License': 'Llama 2 Community',\n",
       "  'Organization': 'Microsoft',\n",
       "  'Link': 'https://huggingface.co/WizardLM/WizardLM-70B-V1.0',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.606655290102389,\n",
       "    'acc_stderr': 0.014275101465693024,\n",
       "    'acc_norm': 0.6407849829351536,\n",
       "    'acc_norm_stderr': 0.014020224155839157},\n",
       "   'harness|hellaswag|10': {'acc': 0.6654052977494523,\n",
       "    'acc_stderr': 0.0047088426001774385,\n",
       "    'acc_norm': 0.854511053574985,\n",
       "    'acc_norm_stderr': 0.003518725257365601},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.5185185185185185,\n",
       "    'acc_stderr': 0.043163785995113245,\n",
       "    'acc_norm': 0.5185185185185185,\n",
       "    'acc_norm_stderr': 0.043163785995113245},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.75,\n",
       "    'acc_stderr': 0.03523807393012047,\n",
       "    'acc_norm': 0.75,\n",
       "    'acc_norm_stderr': 0.03523807393012047},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621503,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621503},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6792452830188679,\n",
       "    'acc_stderr': 0.02872750295788027,\n",
       "    'acc_norm': 0.6792452830188679,\n",
       "    'acc_norm_stderr': 0.02872750295788027},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7638888888888888,\n",
       "    'acc_stderr': 0.03551446610810826,\n",
       "    'acc_norm': 0.7638888888888888,\n",
       "    'acc_norm_stderr': 0.03551446610810826},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.3,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.3,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6473988439306358,\n",
       "    'acc_stderr': 0.036430371689585475,\n",
       "    'acc_norm': 0.6473988439306358,\n",
       "    'acc_norm_stderr': 0.036430371689585475},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.38235294117647056,\n",
       "    'acc_stderr': 0.04835503696107224,\n",
       "    'acc_norm': 0.38235294117647056,\n",
       "    'acc_norm_stderr': 0.04835503696107224},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.04461960433384739,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.04461960433384739},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5957446808510638,\n",
       "    'acc_stderr': 0.03208115750788684,\n",
       "    'acc_norm': 0.5957446808510638,\n",
       "    'acc_norm_stderr': 0.03208115750788684},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.47368421052631576,\n",
       "    'acc_stderr': 0.046970851366478626,\n",
       "    'acc_norm': 0.47368421052631576,\n",
       "    'acc_norm_stderr': 0.046970851366478626},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5448275862068965,\n",
       "    'acc_stderr': 0.04149886942192117,\n",
       "    'acc_norm': 0.5448275862068965,\n",
       "    'acc_norm_stderr': 0.04149886942192117},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.42328042328042326,\n",
       "    'acc_stderr': 0.02544636563440676,\n",
       "    'acc_norm': 0.42328042328042326,\n",
       "    'acc_norm_stderr': 0.02544636563440676},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.04444444444444449,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.04444444444444449},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.44,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.44,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7645161290322581,\n",
       "    'acc_stderr': 0.02413763242933771,\n",
       "    'acc_norm': 0.7645161290322581,\n",
       "    'acc_norm_stderr': 0.02413763242933771},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.49261083743842365,\n",
       "    'acc_stderr': 0.03517603540361008,\n",
       "    'acc_norm': 0.49261083743842365,\n",
       "    'acc_norm_stderr': 0.03517603540361008},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.7,\n",
       "    'acc_stderr': 0.046056618647183814,\n",
       "    'acc_norm': 0.7,\n",
       "    'acc_norm_stderr': 0.046056618647183814},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8121212121212121,\n",
       "    'acc_stderr': 0.03050193405942914,\n",
       "    'acc_norm': 0.8121212121212121,\n",
       "    'acc_norm_stderr': 0.03050193405942914},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.8080808080808081,\n",
       "    'acc_stderr': 0.02805779167298902,\n",
       "    'acc_norm': 0.8080808080808081,\n",
       "    'acc_norm_stderr': 0.02805779167298902},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9119170984455959,\n",
       "    'acc_stderr': 0.02045374660160103,\n",
       "    'acc_norm': 0.9119170984455959,\n",
       "    'acc_norm_stderr': 0.02045374660160103},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6461538461538462,\n",
       "    'acc_stderr': 0.024243783994062153,\n",
       "    'acc_norm': 0.6461538461538462,\n",
       "    'acc_norm_stderr': 0.024243783994062153},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.3111111111111111,\n",
       "    'acc_stderr': 0.028226446749683515,\n",
       "    'acc_norm': 0.3111111111111111,\n",
       "    'acc_norm_stderr': 0.028226446749683515},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.7142857142857143,\n",
       "    'acc_stderr': 0.029344572500634342,\n",
       "    'acc_norm': 0.7142857142857143,\n",
       "    'acc_norm_stderr': 0.029344572500634342},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.4105960264900662,\n",
       "    'acc_stderr': 0.04016689594849929,\n",
       "    'acc_norm': 0.4105960264900662,\n",
       "    'acc_norm_stderr': 0.04016689594849929},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8477064220183487,\n",
       "    'acc_stderr': 0.015405084393157074,\n",
       "    'acc_norm': 0.8477064220183487,\n",
       "    'acc_norm_stderr': 0.015405084393157074},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5138888888888888,\n",
       "    'acc_stderr': 0.03408655867977749,\n",
       "    'acc_norm': 0.5138888888888888,\n",
       "    'acc_norm_stderr': 0.03408655867977749},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.8480392156862745,\n",
       "    'acc_stderr': 0.025195658428931792,\n",
       "    'acc_norm': 0.8480392156862745,\n",
       "    'acc_norm_stderr': 0.025195658428931792},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.8396624472573839,\n",
       "    'acc_stderr': 0.02388438092596567,\n",
       "    'acc_norm': 0.8396624472573839,\n",
       "    'acc_norm_stderr': 0.02388438092596567},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.7040358744394619,\n",
       "    'acc_stderr': 0.03063659134869981,\n",
       "    'acc_norm': 0.7040358744394619,\n",
       "    'acc_norm_stderr': 0.03063659134869981},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8015267175572519,\n",
       "    'acc_stderr': 0.034981493854624714,\n",
       "    'acc_norm': 0.8015267175572519,\n",
       "    'acc_norm_stderr': 0.034981493854624714},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8264462809917356,\n",
       "    'acc_stderr': 0.0345727283691767,\n",
       "    'acc_norm': 0.8264462809917356,\n",
       "    'acc_norm_stderr': 0.0345727283691767},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8240740740740741,\n",
       "    'acc_stderr': 0.036809181416738807,\n",
       "    'acc_norm': 0.8240740740740741,\n",
       "    'acc_norm_stderr': 0.036809181416738807},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7791411042944786,\n",
       "    'acc_stderr': 0.03259177392742178,\n",
       "    'acc_norm': 0.7791411042944786,\n",
       "    'acc_norm_stderr': 0.03259177392742178},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.48214285714285715,\n",
       "    'acc_stderr': 0.047427623612430116,\n",
       "    'acc_norm': 0.48214285714285715,\n",
       "    'acc_norm_stderr': 0.047427623612430116},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7961165048543689,\n",
       "    'acc_stderr': 0.0398913985953177,\n",
       "    'acc_norm': 0.7961165048543689,\n",
       "    'acc_norm_stderr': 0.0398913985953177},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8717948717948718,\n",
       "    'acc_stderr': 0.02190190511507332,\n",
       "    'acc_norm': 0.8717948717948718,\n",
       "    'acc_norm_stderr': 0.02190190511507332},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.64,\n",
       "    'acc_stderr': 0.048241815132442176,\n",
       "    'acc_norm': 0.64,\n",
       "    'acc_norm_stderr': 0.048241815132442176},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8288633461047255,\n",
       "    'acc_stderr': 0.013468201614066307,\n",
       "    'acc_norm': 0.8288633461047255,\n",
       "    'acc_norm_stderr': 0.013468201614066307},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.7283236994219653,\n",
       "    'acc_stderr': 0.023948512905468365,\n",
       "    'acc_norm': 0.7283236994219653,\n",
       "    'acc_norm_stderr': 0.023948512905468365},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.29832402234636873,\n",
       "    'acc_stderr': 0.01530184004512928,\n",
       "    'acc_norm': 0.29832402234636873,\n",
       "    'acc_norm_stderr': 0.01530184004512928},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6797385620915033,\n",
       "    'acc_stderr': 0.026716118380156847,\n",
       "    'acc_norm': 0.6797385620915033,\n",
       "    'acc_norm_stderr': 0.026716118380156847},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6881028938906752,\n",
       "    'acc_stderr': 0.02631185807185416,\n",
       "    'acc_norm': 0.6881028938906752,\n",
       "    'acc_norm_stderr': 0.02631185807185416},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.7067901234567902,\n",
       "    'acc_stderr': 0.02532988817190092,\n",
       "    'acc_norm': 0.7067901234567902,\n",
       "    'acc_norm_stderr': 0.02532988817190092},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.5425531914893617,\n",
       "    'acc_stderr': 0.029719281272236837,\n",
       "    'acc_norm': 0.5425531914893617,\n",
       "    'acc_norm_stderr': 0.029719281272236837},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5189048239895697,\n",
       "    'acc_stderr': 0.012761104871472652,\n",
       "    'acc_norm': 0.5189048239895697,\n",
       "    'acc_norm_stderr': 0.012761104871472652},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6544117647058824,\n",
       "    'acc_stderr': 0.028888193103988626,\n",
       "    'acc_norm': 0.6544117647058824,\n",
       "    'acc_norm_stderr': 0.028888193103988626},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6911764705882353,\n",
       "    'acc_stderr': 0.01869085027359529,\n",
       "    'acc_norm': 0.6911764705882353,\n",
       "    'acc_norm_stderr': 0.01869085027359529},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.7090909090909091,\n",
       "    'acc_stderr': 0.04350271442923243,\n",
       "    'acc_norm': 0.7090909090909091,\n",
       "    'acc_norm_stderr': 0.04350271442923243},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.7714285714285715,\n",
       "    'acc_stderr': 0.026882144922307744,\n",
       "    'acc_norm': 0.7714285714285715,\n",
       "    'acc_norm_stderr': 0.026882144922307744},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8557213930348259,\n",
       "    'acc_stderr': 0.024845753212306042,\n",
       "    'acc_norm': 0.8557213930348259,\n",
       "    'acc_norm_stderr': 0.024845753212306042},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.83,\n",
       "    'acc_stderr': 0.03775251680686371,\n",
       "    'acc_norm': 0.83,\n",
       "    'acc_norm_stderr': 0.03775251680686371},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.536144578313253,\n",
       "    'acc_stderr': 0.03882310850890594,\n",
       "    'acc_norm': 0.536144578313253,\n",
       "    'acc_norm_stderr': 0.03882310850890594},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8538011695906432,\n",
       "    'acc_stderr': 0.027097290118070792,\n",
       "    'acc_norm': 0.8538011695906432,\n",
       "    'acc_norm_stderr': 0.027097290118070792},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.386780905752754,\n",
       "    'mc1_stderr': 0.01704885701051511,\n",
       "    'mc2': 0.5476558772806095,\n",
       "    'mc2_stderr': 0.015451111828036913},\n",
       "   'all': {'acc': 0.6477083045829946,\n",
       "    'acc_stderr': 0.032583732941869364,\n",
       "    'acc_norm': 0.6514919562551693,\n",
       "    'acc_norm_stderr': 0.03255924150707842,\n",
       "    'mc1': 0.386780905752754,\n",
       "    'mc1_stderr': 0.01704885701051511,\n",
       "    'mc2': 0.5476558772806095,\n",
       "    'mc2_stderr': 0.015451111828036913},\n",
       "   'harness|drop|3': {'em': 0.26541526845637586,\n",
       "    'em_stderr': 0.004521927044730418,\n",
       "    'f1': 0.3270648070469802,\n",
       "    'f1_stderr': 0.004444377320494032},\n",
       "   'harness|gsm8k|5': {'acc': 0.17968157695223655,\n",
       "    'acc_stderr': 0.010575119964242244},\n",
       "   'harness|winogrande|5': {'acc': 0.8082083662194159,\n",
       "    'acc_stderr': 0.011065209664659527}}},\n",
       " 'yi-34b-chat': {'key': 'yi-34b-chat',\n",
       "  'Model': 'Yi-34B-Chat',\n",
       "  'MT-bench (score)': '-',\n",
       "  'MMLU': '0.735',\n",
       "  'License': 'Yi License',\n",
       "  'Organization': '01 AI',\n",
       "  'Link': 'https://huggingface.co/01-ai/Yi-34B-Chat',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.6373720136518771,\n",
       "    'acc_stderr': 0.014049106564955012,\n",
       "    'acc_norm': 0.6544368600682594,\n",
       "    'acc_norm_stderr': 0.013896938461145678},\n",
       "   'harness|hellaswag|10': {'acc': 0.6536546504680343,\n",
       "    'acc_stderr': 0.004748324319714274,\n",
       "    'acc_norm': 0.8415654252141008,\n",
       "    'acc_norm_stderr': 0.003644017383711605},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.45,\n",
       "    'acc_stderr': 0.05,\n",
       "    'acc_norm': 0.45,\n",
       "    'acc_norm_stderr': 0.05},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.7111111111111111,\n",
       "    'acc_stderr': 0.03915450630414251,\n",
       "    'acc_norm': 0.7111111111111111,\n",
       "    'acc_norm_stderr': 0.03915450630414251},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.8552631578947368,\n",
       "    'acc_stderr': 0.028631951845930387,\n",
       "    'acc_norm': 0.8552631578947368,\n",
       "    'acc_norm_stderr': 0.028631951845930387},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.7924528301886793,\n",
       "    'acc_stderr': 0.02495991802891127,\n",
       "    'acc_norm': 0.7924528301886793,\n",
       "    'acc_norm_stderr': 0.02495991802891127},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.8472222222222222,\n",
       "    'acc_stderr': 0.030085743248565666,\n",
       "    'acc_norm': 0.8472222222222222,\n",
       "    'acc_norm_stderr': 0.030085743248565666},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.53,\n",
       "    'acc_stderr': 0.05016135580465919,\n",
       "    'acc_norm': 0.53,\n",
       "    'acc_norm_stderr': 0.05016135580465919},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.59,\n",
       "    'acc_stderr': 0.04943110704237101,\n",
       "    'acc_norm': 0.59,\n",
       "    'acc_norm_stderr': 0.04943110704237101},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6994219653179191,\n",
       "    'acc_stderr': 0.034961014811911786,\n",
       "    'acc_norm': 0.6994219653179191,\n",
       "    'acc_norm_stderr': 0.034961014811911786},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.46078431372549017,\n",
       "    'acc_stderr': 0.049598599663841815,\n",
       "    'acc_norm': 0.46078431372549017,\n",
       "    'acc_norm_stderr': 0.049598599663841815},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.82,\n",
       "    'acc_stderr': 0.03861229196653695,\n",
       "    'acc_norm': 0.82,\n",
       "    'acc_norm_stderr': 0.03861229196653695},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.7659574468085106,\n",
       "    'acc_stderr': 0.02767845257821239,\n",
       "    'acc_norm': 0.7659574468085106,\n",
       "    'acc_norm_stderr': 0.02767845257821239},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.5526315789473685,\n",
       "    'acc_stderr': 0.046774730044911984,\n",
       "    'acc_norm': 0.5526315789473685,\n",
       "    'acc_norm_stderr': 0.046774730044911984},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.8,\n",
       "    'acc_stderr': 0.0333333333333333,\n",
       "    'acc_norm': 0.8,\n",
       "    'acc_norm_stderr': 0.0333333333333333},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.6349206349206349,\n",
       "    'acc_stderr': 0.024796060602699965,\n",
       "    'acc_norm': 0.6349206349206349,\n",
       "    'acc_norm_stderr': 0.024796060602699965},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.5396825396825397,\n",
       "    'acc_stderr': 0.04458029125470973,\n",
       "    'acc_norm': 0.5396825396825397,\n",
       "    'acc_norm_stderr': 0.04458029125470973},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.049888765156985884,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.049888765156985884},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.8709677419354839,\n",
       "    'acc_stderr': 0.019070889254792767,\n",
       "    'acc_norm': 0.8709677419354839,\n",
       "    'acc_norm_stderr': 0.019070889254792767},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.6206896551724138,\n",
       "    'acc_stderr': 0.03413963805906235,\n",
       "    'acc_norm': 0.6206896551724138,\n",
       "    'acc_norm_stderr': 0.03413963805906235},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.81,\n",
       "    'acc_stderr': 0.03942772444036625,\n",
       "    'acc_norm': 0.81,\n",
       "    'acc_norm_stderr': 0.03942772444036625},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.8545454545454545,\n",
       "    'acc_stderr': 0.027530196355066573,\n",
       "    'acc_norm': 0.8545454545454545,\n",
       "    'acc_norm_stderr': 0.027530196355066573},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.898989898989899,\n",
       "    'acc_stderr': 0.021469735576055343,\n",
       "    'acc_norm': 0.898989898989899,\n",
       "    'acc_norm_stderr': 0.021469735576055343},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.9533678756476683,\n",
       "    'acc_stderr': 0.015216761819262585,\n",
       "    'acc_norm': 0.9533678756476683,\n",
       "    'acc_norm_stderr': 0.015216761819262585},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.7846153846153846,\n",
       "    'acc_stderr': 0.020843034557462878,\n",
       "    'acc_norm': 0.7846153846153846,\n",
       "    'acc_norm_stderr': 0.020843034557462878},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.35555555555555557,\n",
       "    'acc_stderr': 0.029185714949857403,\n",
       "    'acc_norm': 0.35555555555555557,\n",
       "    'acc_norm_stderr': 0.029185714949857403},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.8361344537815126,\n",
       "    'acc_stderr': 0.024044054940440488,\n",
       "    'acc_norm': 0.8361344537815126,\n",
       "    'acc_norm_stderr': 0.024044054940440488},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.5033112582781457,\n",
       "    'acc_stderr': 0.04082393379449654,\n",
       "    'acc_norm': 0.5033112582781457,\n",
       "    'acc_norm_stderr': 0.04082393379449654},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.908256880733945,\n",
       "    'acc_stderr': 0.012376323409137123,\n",
       "    'acc_norm': 0.908256880733945,\n",
       "    'acc_norm_stderr': 0.012376323409137123},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.6342592592592593,\n",
       "    'acc_stderr': 0.03284738857647206,\n",
       "    'acc_norm': 0.6342592592592593,\n",
       "    'acc_norm_stderr': 0.03284738857647206},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.9068627450980392,\n",
       "    'acc_stderr': 0.020397853969426998,\n",
       "    'acc_norm': 0.9068627450980392,\n",
       "    'acc_norm_stderr': 0.020397853969426998},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.9029535864978903,\n",
       "    'acc_stderr': 0.019269323025640255,\n",
       "    'acc_norm': 0.9029535864978903,\n",
       "    'acc_norm_stderr': 0.019269323025640255},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.8116591928251121,\n",
       "    'acc_stderr': 0.026241132996407256,\n",
       "    'acc_norm': 0.8116591928251121,\n",
       "    'acc_norm_stderr': 0.026241132996407256},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.8931297709923665,\n",
       "    'acc_stderr': 0.027096548624883733,\n",
       "    'acc_norm': 0.8931297709923665,\n",
       "    'acc_norm_stderr': 0.027096548624883733},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.8925619834710744,\n",
       "    'acc_stderr': 0.028268812192540627,\n",
       "    'acc_norm': 0.8925619834710744,\n",
       "    'acc_norm_stderr': 0.028268812192540627},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.8888888888888888,\n",
       "    'acc_stderr': 0.03038159675665167,\n",
       "    'acc_norm': 0.8888888888888888,\n",
       "    'acc_norm_stderr': 0.03038159675665167},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.852760736196319,\n",
       "    'acc_stderr': 0.027839915278339657,\n",
       "    'acc_norm': 0.852760736196319,\n",
       "    'acc_norm_stderr': 0.027839915278339657},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.6160714285714286,\n",
       "    'acc_stderr': 0.04616143075028546,\n",
       "    'acc_norm': 0.6160714285714286,\n",
       "    'acc_norm_stderr': 0.04616143075028546},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.8737864077669902,\n",
       "    'acc_stderr': 0.0328818027880863,\n",
       "    'acc_norm': 0.8737864077669902,\n",
       "    'acc_norm_stderr': 0.0328818027880863},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.9145299145299145,\n",
       "    'acc_stderr': 0.01831589168562586,\n",
       "    'acc_norm': 0.9145299145299145,\n",
       "    'acc_norm_stderr': 0.01831589168562586},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.87,\n",
       "    'acc_stderr': 0.033799766898963086,\n",
       "    'acc_norm': 0.87,\n",
       "    'acc_norm_stderr': 0.033799766898963086},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.8991060025542784,\n",
       "    'acc_stderr': 0.01077047201488672,\n",
       "    'acc_norm': 0.8991060025542784,\n",
       "    'acc_norm_stderr': 0.01077047201488672},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.8092485549132948,\n",
       "    'acc_stderr': 0.021152676966575277,\n",
       "    'acc_norm': 0.8092485549132948,\n",
       "    'acc_norm_stderr': 0.021152676966575277},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.7016759776536313,\n",
       "    'acc_stderr': 0.01530184004512928,\n",
       "    'acc_norm': 0.7016759776536313,\n",
       "    'acc_norm_stderr': 0.01530184004512928},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.8300653594771242,\n",
       "    'acc_stderr': 0.02150538312123137,\n",
       "    'acc_norm': 0.8300653594771242,\n",
       "    'acc_norm_stderr': 0.02150538312123137},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.8102893890675241,\n",
       "    'acc_stderr': 0.02226819625878322,\n",
       "    'acc_norm': 0.8102893890675241,\n",
       "    'acc_norm_stderr': 0.02226819625878322},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.8703703703703703,\n",
       "    'acc_stderr': 0.018689725721062072,\n",
       "    'acc_norm': 0.8703703703703703,\n",
       "    'acc_norm_stderr': 0.018689725721062072},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.6170212765957447,\n",
       "    'acc_stderr': 0.02899908090480618,\n",
       "    'acc_norm': 0.6170212765957447,\n",
       "    'acc_norm_stderr': 0.02899908090480618},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.5495436766623207,\n",
       "    'acc_stderr': 0.012707390438502348,\n",
       "    'acc_norm': 0.5495436766623207,\n",
       "    'acc_norm_stderr': 0.012707390438502348},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.7830882352941176,\n",
       "    'acc_stderr': 0.025035845227711274,\n",
       "    'acc_norm': 0.7830882352941176,\n",
       "    'acc_norm_stderr': 0.025035845227711274},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.826797385620915,\n",
       "    'acc_stderr': 0.015309329266969138,\n",
       "    'acc_norm': 0.826797385620915,\n",
       "    'acc_norm_stderr': 0.015309329266969138},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.7272727272727273,\n",
       "    'acc_stderr': 0.04265792110940589,\n",
       "    'acc_norm': 0.7272727272727273,\n",
       "    'acc_norm_stderr': 0.04265792110940589},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.8367346938775511,\n",
       "    'acc_stderr': 0.02366169917709861,\n",
       "    'acc_norm': 0.8367346938775511,\n",
       "    'acc_norm_stderr': 0.02366169917709861},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8855721393034826,\n",
       "    'acc_stderr': 0.022509345325101706,\n",
       "    'acc_norm': 0.8855721393034826,\n",
       "    'acc_norm_stderr': 0.022509345325101706},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.89,\n",
       "    'acc_stderr': 0.03144660377352203,\n",
       "    'acc_norm': 0.89,\n",
       "    'acc_norm_stderr': 0.03144660377352203},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5843373493975904,\n",
       "    'acc_stderr': 0.03836722176598053,\n",
       "    'acc_norm': 0.5843373493975904,\n",
       "    'acc_norm_stderr': 0.03836722176598053},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8771929824561403,\n",
       "    'acc_stderr': 0.02517298435015577,\n",
       "    'acc_norm': 0.8771929824561403,\n",
       "    'acc_norm_stderr': 0.02517298435015577},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.3843329253365973,\n",
       "    'mc1_stderr': 0.017028707301245203,\n",
       "    'mc2': 0.5536831362008046,\n",
       "    'mc2_stderr': 0.015524186394858242},\n",
       "   'harness|winogrande|5': {'acc': 0.8011049723756906,\n",
       "    'acc_stderr': 0.01121862997251531},\n",
       "   'harness|drop|3': {'em': 0.005138422818791947,\n",
       "    'em_stderr': 0.0007322104102794241,\n",
       "    'f1': 0.08032508389261797,\n",
       "    'f1_stderr': 0.001571649833831937},\n",
       "   'harness|gsm8k|5': {'acc': 0.3191811978771797,\n",
       "    'acc_stderr': 0.012840345676251648},\n",
       "   'all': {'acc': 0.7393930299846158,\n",
       "    'acc_stderr': 0.028807135333088364,\n",
       "    'acc_norm': 0.7489434623723922,\n",
       "    'acc_norm_stderr': 0.02935457295982731,\n",
       "    'mc1': 0.3843329253365973,\n",
       "    'mc1_stderr': 0.017028707301245203,\n",
       "    'mc2': 0.5536831362008046,\n",
       "    'mc2_stderr': 0.015524186394858242}}},\n",
       " 'zephyr-7b-alpha': {'key': 'zephyr-7b-alpha',\n",
       "  'Model': 'Zephyr-7b-alpha',\n",
       "  'MT-bench (score)': '6.88',\n",
       "  'MMLU': '-',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'HuggingFace',\n",
       "  'Link': 'https://huggingface.co/HuggingFaceH4/zephyr-7b-alpha',\n",
       "  'results': {'harness|drop|3': {'em': 0.010171979865771811,\n",
       "    'em_stderr': 0.001027595678252703,\n",
       "    'f1': 0.0982225251677855,\n",
       "    'f1_stderr': 0.00197861649884212},\n",
       "   'harness|gsm8k|5': {'acc': 0.14025777103866566,\n",
       "    'acc_stderr': 0.00956510828142867},\n",
       "   'harness|winogrande|5': {'acc': 0.7861089187056038,\n",
       "    'acc_stderr': 0.011524466954090248},\n",
       "   'all': {'acc': 0.6137978230566867,\n",
       "    'acc_stderr': 0.03380754595328641,\n",
       "    'acc_norm': 0.6176702382672306,\n",
       "    'acc_norm_stderr': 0.03378555360789072,\n",
       "    'mc1': 0.42717258261933905,\n",
       "    'mc1_stderr': 0.017316834410963926,\n",
       "    'mc2': 0.5790339154881958,\n",
       "    'mc2_stderr': 0.015362629183533977},\n",
       "   'harness|arc:challenge|25': {'acc': 0.5810580204778157,\n",
       "    'acc_stderr': 0.014418106953639011,\n",
       "    'acc_norm': 0.6100682593856656,\n",
       "    'acc_norm_stderr': 0.01425295984889289},\n",
       "   'harness|hellaswag|10': {'acc': 0.6409081856203943,\n",
       "    'acc_stderr': 0.004787537385153006,\n",
       "    'acc_norm': 0.8403704441346346,\n",
       "    'acc_norm_stderr': 0.0036551361115537096},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.32,\n",
       "    'acc_stderr': 0.046882617226215034,\n",
       "    'acc_norm': 0.32,\n",
       "    'acc_norm_stderr': 0.046882617226215034},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6148148148148148,\n",
       "    'acc_stderr': 0.04203921040156279,\n",
       "    'acc_norm': 0.6148148148148148,\n",
       "    'acc_norm_stderr': 0.04203921040156279},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6447368421052632,\n",
       "    'acc_stderr': 0.038947344870133176,\n",
       "    'acc_norm': 0.6447368421052632,\n",
       "    'acc_norm_stderr': 0.038947344870133176},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.54,\n",
       "    'acc_stderr': 0.05009082659620332,\n",
       "    'acc_norm': 0.54,\n",
       "    'acc_norm_stderr': 0.05009082659620332},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.6679245283018868,\n",
       "    'acc_stderr': 0.02898545565233439,\n",
       "    'acc_norm': 0.6679245283018868,\n",
       "    'acc_norm_stderr': 0.02898545565233439},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7013888888888888,\n",
       "    'acc_stderr': 0.03827052357950756,\n",
       "    'acc_norm': 0.7013888888888888,\n",
       "    'acc_norm_stderr': 0.03827052357950756},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.49,\n",
       "    'acc_stderr': 0.05024183937956912,\n",
       "    'acc_norm': 0.49,\n",
       "    'acc_norm_stderr': 0.05024183937956912},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.47,\n",
       "    'acc_stderr': 0.050161355804659205,\n",
       "    'acc_norm': 0.47,\n",
       "    'acc_norm_stderr': 0.050161355804659205},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.34,\n",
       "    'acc_stderr': 0.04760952285695235,\n",
       "    'acc_norm': 0.34,\n",
       "    'acc_norm_stderr': 0.04760952285695235},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6184971098265896,\n",
       "    'acc_stderr': 0.03703851193099521,\n",
       "    'acc_norm': 0.6184971098265896,\n",
       "    'acc_norm_stderr': 0.03703851193099521},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4215686274509804,\n",
       "    'acc_stderr': 0.049135952012744975,\n",
       "    'acc_norm': 0.4215686274509804,\n",
       "    'acc_norm_stderr': 0.049135952012744975},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.0440844002276808,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.0440844002276808},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5319148936170213,\n",
       "    'acc_stderr': 0.03261936918467382,\n",
       "    'acc_norm': 0.5319148936170213,\n",
       "    'acc_norm_stderr': 0.03261936918467382},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.43859649122807015,\n",
       "    'acc_stderr': 0.04668000738510455,\n",
       "    'acc_norm': 0.43859649122807015,\n",
       "    'acc_norm_stderr': 0.04668000738510455},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5724137931034483,\n",
       "    'acc_stderr': 0.04122737111370333,\n",
       "    'acc_norm': 0.5724137931034483,\n",
       "    'acc_norm_stderr': 0.04122737111370333},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.38095238095238093,\n",
       "    'acc_stderr': 0.02501074911613761,\n",
       "    'acc_norm': 0.38095238095238093,\n",
       "    'acc_norm_stderr': 0.02501074911613761},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4126984126984127,\n",
       "    'acc_stderr': 0.04403438954768176,\n",
       "    'acc_norm': 0.4126984126984127,\n",
       "    'acc_norm_stderr': 0.04403438954768176},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.35,\n",
       "    'acc_stderr': 0.04793724854411019,\n",
       "    'acc_norm': 0.35,\n",
       "    'acc_norm_stderr': 0.04793724854411019},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7548387096774194,\n",
       "    'acc_stderr': 0.02447224384089553,\n",
       "    'acc_norm': 0.7548387096774194,\n",
       "    'acc_norm_stderr': 0.02447224384089553},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5073891625615764,\n",
       "    'acc_stderr': 0.035176035403610105,\n",
       "    'acc_norm': 0.5073891625615764,\n",
       "    'acc_norm_stderr': 0.035176035403610105},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.68,\n",
       "    'acc_stderr': 0.04688261722621505,\n",
       "    'acc_norm': 0.68,\n",
       "    'acc_norm_stderr': 0.04688261722621505},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7515151515151515,\n",
       "    'acc_stderr': 0.033744026441394036,\n",
       "    'acc_norm': 0.7515151515151515,\n",
       "    'acc_norm_stderr': 0.033744026441394036},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7828282828282829,\n",
       "    'acc_stderr': 0.029376616484945633,\n",
       "    'acc_norm': 0.7828282828282829,\n",
       "    'acc_norm_stderr': 0.029376616484945633},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8549222797927462,\n",
       "    'acc_stderr': 0.025416343096306433,\n",
       "    'acc_norm': 0.8549222797927462,\n",
       "    'acc_norm_stderr': 0.025416343096306433},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6153846153846154,\n",
       "    'acc_stderr': 0.024666744915187208,\n",
       "    'acc_norm': 0.6153846153846154,\n",
       "    'acc_norm_stderr': 0.024666744915187208},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.35555555555555557,\n",
       "    'acc_stderr': 0.029185714949857403,\n",
       "    'acc_norm': 0.35555555555555557,\n",
       "    'acc_norm_stderr': 0.029185714949857403},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.634453781512605,\n",
       "    'acc_stderr': 0.03128217706368461,\n",
       "    'acc_norm': 0.634453781512605,\n",
       "    'acc_norm_stderr': 0.03128217706368461},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.3708609271523179,\n",
       "    'acc_stderr': 0.03943966699183629,\n",
       "    'acc_norm': 0.3708609271523179,\n",
       "    'acc_norm_stderr': 0.03943966699183629},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8018348623853211,\n",
       "    'acc_stderr': 0.017090573804217902,\n",
       "    'acc_norm': 0.8018348623853211,\n",
       "    'acc_norm_stderr': 0.017090573804217902},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5416666666666666,\n",
       "    'acc_stderr': 0.033981108902946366,\n",
       "    'acc_norm': 0.5416666666666666,\n",
       "    'acc_norm_stderr': 0.033981108902946366},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7598039215686274,\n",
       "    'acc_stderr': 0.02998373305591361,\n",
       "    'acc_norm': 0.7598039215686274,\n",
       "    'acc_norm_stderr': 0.02998373305591361},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7426160337552743,\n",
       "    'acc_stderr': 0.028458820991460285,\n",
       "    'acc_norm': 0.7426160337552743,\n",
       "    'acc_norm_stderr': 0.028458820991460285},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6367713004484304,\n",
       "    'acc_stderr': 0.032277904428505,\n",
       "    'acc_norm': 0.6367713004484304,\n",
       "    'acc_norm_stderr': 0.032277904428505},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6793893129770993,\n",
       "    'acc_stderr': 0.04093329229834278,\n",
       "    'acc_norm': 0.6793893129770993,\n",
       "    'acc_norm_stderr': 0.04093329229834278},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.743801652892562,\n",
       "    'acc_stderr': 0.03984979653302871,\n",
       "    'acc_norm': 0.743801652892562,\n",
       "    'acc_norm_stderr': 0.03984979653302871},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7222222222222222,\n",
       "    'acc_stderr': 0.043300437496507416,\n",
       "    'acc_norm': 0.7222222222222222,\n",
       "    'acc_norm_stderr': 0.043300437496507416},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7361963190184049,\n",
       "    'acc_stderr': 0.03462419931615623,\n",
       "    'acc_norm': 0.7361963190184049,\n",
       "    'acc_norm_stderr': 0.03462419931615623},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.39285714285714285,\n",
       "    'acc_stderr': 0.04635550135609976,\n",
       "    'acc_norm': 0.39285714285714285,\n",
       "    'acc_norm_stderr': 0.04635550135609976},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7281553398058253,\n",
       "    'acc_stderr': 0.044052680241409216,\n",
       "    'acc_norm': 0.7281553398058253,\n",
       "    'acc_norm_stderr': 0.044052680241409216},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8589743589743589,\n",
       "    'acc_stderr': 0.02280138253459753,\n",
       "    'acc_norm': 0.8589743589743589,\n",
       "    'acc_norm_stderr': 0.02280138253459753},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.74,\n",
       "    'acc_stderr': 0.04408440022768078,\n",
       "    'acc_norm': 0.74,\n",
       "    'acc_norm_stderr': 0.04408440022768078},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7905491698595147,\n",
       "    'acc_stderr': 0.014551310568143704,\n",
       "    'acc_norm': 0.7905491698595147,\n",
       "    'acc_norm_stderr': 0.014551310568143704},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6994219653179191,\n",
       "    'acc_stderr': 0.0246853168672578,\n",
       "    'acc_norm': 0.6994219653179191,\n",
       "    'acc_norm_stderr': 0.0246853168672578},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.37094972067039106,\n",
       "    'acc_stderr': 0.01615591072134177,\n",
       "    'acc_norm': 0.37094972067039106,\n",
       "    'acc_norm_stderr': 0.01615591072134177},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6862745098039216,\n",
       "    'acc_stderr': 0.02656892101545715,\n",
       "    'acc_norm': 0.6862745098039216,\n",
       "    'acc_norm_stderr': 0.02656892101545715},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.7041800643086816,\n",
       "    'acc_stderr': 0.025922371788818774,\n",
       "    'acc_norm': 0.7041800643086816,\n",
       "    'acc_norm_stderr': 0.025922371788818774},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6790123456790124,\n",
       "    'acc_stderr': 0.02597656601086274,\n",
       "    'acc_norm': 0.6790123456790124,\n",
       "    'acc_norm_stderr': 0.02597656601086274},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.46099290780141844,\n",
       "    'acc_stderr': 0.029736592526424438,\n",
       "    'acc_norm': 0.46099290780141844,\n",
       "    'acc_norm_stderr': 0.029736592526424438},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.41003911342894395,\n",
       "    'acc_stderr': 0.012561837621962044,\n",
       "    'acc_norm': 0.41003911342894395,\n",
       "    'acc_norm_stderr': 0.012561837621962044},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6507352941176471,\n",
       "    'acc_stderr': 0.028959755196824866,\n",
       "    'acc_norm': 0.6507352941176471,\n",
       "    'acc_norm_stderr': 0.028959755196824866},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.6290849673202614,\n",
       "    'acc_stderr': 0.019542101564854128,\n",
       "    'acc_norm': 0.6290849673202614,\n",
       "    'acc_norm_stderr': 0.019542101564854128},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6363636363636364,\n",
       "    'acc_stderr': 0.04607582090719976,\n",
       "    'acc_norm': 0.6363636363636364,\n",
       "    'acc_norm_stderr': 0.04607582090719976},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6816326530612244,\n",
       "    'acc_stderr': 0.029822533793982066,\n",
       "    'acc_norm': 0.6816326530612244,\n",
       "    'acc_norm_stderr': 0.029822533793982066},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8009950248756219,\n",
       "    'acc_stderr': 0.028231365092758406,\n",
       "    'acc_norm': 0.8009950248756219,\n",
       "    'acc_norm_stderr': 0.028231365092758406},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.79,\n",
       "    'acc_stderr': 0.040936018074033256,\n",
       "    'acc_norm': 0.79,\n",
       "    'acc_norm_stderr': 0.040936018074033256},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5180722891566265,\n",
       "    'acc_stderr': 0.03889951252827216,\n",
       "    'acc_norm': 0.5180722891566265,\n",
       "    'acc_norm_stderr': 0.03889951252827216},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8362573099415205,\n",
       "    'acc_stderr': 0.028380919596145866,\n",
       "    'acc_norm': 0.8362573099415205,\n",
       "    'acc_norm_stderr': 0.028380919596145866},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.42717258261933905,\n",
       "    'mc1_stderr': 0.017316834410963926,\n",
       "    'mc2': 0.5790339154881958,\n",
       "    'mc2_stderr': 0.015362629183533977}}},\n",
       " 'zephyr-7b-beta': {'key': 'zephyr-7b-beta',\n",
       "  'Model': 'Zephyr-7b-beta',\n",
       "  'MT-bench (score)': '7.34',\n",
       "  'MMLU': '0.614',\n",
       "  'License': 'MIT',\n",
       "  'Organization': 'HuggingFace',\n",
       "  'Link': 'https://huggingface.co/HuggingFaceH4/zephyr-7b-beta',\n",
       "  'results': {'harness|arc:challenge|25': {'acc': 0.590443686006826,\n",
       "    'acc_stderr': 0.014370358632472437,\n",
       "    'acc_norm': 0.6203071672354948,\n",
       "    'acc_norm_stderr': 0.01418211986697487},\n",
       "   'harness|hellaswag|10': {'acc': 0.6491734714200359,\n",
       "    'acc_stderr': 0.004762534245488399,\n",
       "    'acc_norm': 0.8435570603465445,\n",
       "    'acc_norm_stderr': 0.003625323221166244},\n",
       "   'harness|hendrycksTest-abstract_algebra|5': {'acc': 0.36,\n",
       "    'acc_stderr': 0.04824181513244218,\n",
       "    'acc_norm': 0.36,\n",
       "    'acc_norm_stderr': 0.04824181513244218},\n",
       "   'harness|hendrycksTest-anatomy|5': {'acc': 0.6074074074074074,\n",
       "    'acc_stderr': 0.04218506215368881,\n",
       "    'acc_norm': 0.6074074074074074,\n",
       "    'acc_norm_stderr': 0.04218506215368881},\n",
       "   'harness|hendrycksTest-astronomy|5': {'acc': 0.6052631578947368,\n",
       "    'acc_stderr': 0.039777499346220734,\n",
       "    'acc_norm': 0.6052631578947368,\n",
       "    'acc_norm_stderr': 0.039777499346220734},\n",
       "   'harness|hendrycksTest-business_ethics|5': {'acc': 0.56,\n",
       "    'acc_stderr': 0.04988876515698589,\n",
       "    'acc_norm': 0.56,\n",
       "    'acc_norm_stderr': 0.04988876515698589},\n",
       "   'harness|hendrycksTest-clinical_knowledge|5': {'acc': 0.660377358490566,\n",
       "    'acc_stderr': 0.02914690474779833,\n",
       "    'acc_norm': 0.660377358490566,\n",
       "    'acc_norm_stderr': 0.02914690474779833},\n",
       "   'harness|hendrycksTest-college_biology|5': {'acc': 0.7013888888888888,\n",
       "    'acc_stderr': 0.03827052357950756,\n",
       "    'acc_norm': 0.7013888888888888,\n",
       "    'acc_norm_stderr': 0.03827052357950756},\n",
       "   'harness|hendrycksTest-college_chemistry|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_computer_science|5': {'acc': 0.48,\n",
       "    'acc_stderr': 0.050211673156867795,\n",
       "    'acc_norm': 0.48,\n",
       "    'acc_norm_stderr': 0.050211673156867795},\n",
       "   'harness|hendrycksTest-college_mathematics|5': {'acc': 0.38,\n",
       "    'acc_stderr': 0.04878317312145633,\n",
       "    'acc_norm': 0.38,\n",
       "    'acc_norm_stderr': 0.04878317312145633},\n",
       "   'harness|hendrycksTest-college_medicine|5': {'acc': 0.6416184971098265,\n",
       "    'acc_stderr': 0.03656343653353159,\n",
       "    'acc_norm': 0.6416184971098265,\n",
       "    'acc_norm_stderr': 0.03656343653353159},\n",
       "   'harness|hendrycksTest-college_physics|5': {'acc': 0.4215686274509804,\n",
       "    'acc_stderr': 0.049135952012744975,\n",
       "    'acc_norm': 0.4215686274509804,\n",
       "    'acc_norm_stderr': 0.049135952012744975},\n",
       "   'harness|hendrycksTest-computer_security|5': {'acc': 0.73,\n",
       "    'acc_stderr': 0.044619604333847394,\n",
       "    'acc_norm': 0.73,\n",
       "    'acc_norm_stderr': 0.044619604333847394},\n",
       "   'harness|hendrycksTest-conceptual_physics|5': {'acc': 0.5191489361702127,\n",
       "    'acc_stderr': 0.032662042990646775,\n",
       "    'acc_norm': 0.5191489361702127,\n",
       "    'acc_norm_stderr': 0.032662042990646775},\n",
       "   'harness|hendrycksTest-econometrics|5': {'acc': 0.42105263157894735,\n",
       "    'acc_stderr': 0.046446020912223177,\n",
       "    'acc_norm': 0.42105263157894735,\n",
       "    'acc_norm_stderr': 0.046446020912223177},\n",
       "   'harness|hendrycksTest-electrical_engineering|5': {'acc': 0.5379310344827586,\n",
       "    'acc_stderr': 0.04154659671707548,\n",
       "    'acc_norm': 0.5379310344827586,\n",
       "    'acc_norm_stderr': 0.04154659671707548},\n",
       "   'harness|hendrycksTest-elementary_mathematics|5': {'acc': 0.36772486772486773,\n",
       "    'acc_stderr': 0.02483383982556242,\n",
       "    'acc_norm': 0.36772486772486773,\n",
       "    'acc_norm_stderr': 0.02483383982556242},\n",
       "   'harness|hendrycksTest-formal_logic|5': {'acc': 0.4444444444444444,\n",
       "    'acc_stderr': 0.044444444444444495,\n",
       "    'acc_norm': 0.4444444444444444,\n",
       "    'acc_norm_stderr': 0.044444444444444495},\n",
       "   'harness|hendrycksTest-global_facts|5': {'acc': 0.4,\n",
       "    'acc_stderr': 0.049236596391733084,\n",
       "    'acc_norm': 0.4,\n",
       "    'acc_norm_stderr': 0.049236596391733084},\n",
       "   'harness|hendrycksTest-high_school_biology|5': {'acc': 0.7483870967741936,\n",
       "    'acc_stderr': 0.024685979286239963,\n",
       "    'acc_norm': 0.7483870967741936,\n",
       "    'acc_norm_stderr': 0.024685979286239963},\n",
       "   'harness|hendrycksTest-high_school_chemistry|5': {'acc': 0.5172413793103449,\n",
       "    'acc_stderr': 0.035158955511656986,\n",
       "    'acc_norm': 0.5172413793103449,\n",
       "    'acc_norm_stderr': 0.035158955511656986},\n",
       "   'harness|hendrycksTest-high_school_computer_science|5': {'acc': 0.67,\n",
       "    'acc_stderr': 0.04725815626252609,\n",
       "    'acc_norm': 0.67,\n",
       "    'acc_norm_stderr': 0.04725815626252609},\n",
       "   'harness|hendrycksTest-high_school_european_history|5': {'acc': 0.7575757575757576,\n",
       "    'acc_stderr': 0.03346409881055953,\n",
       "    'acc_norm': 0.7575757575757576,\n",
       "    'acc_norm_stderr': 0.03346409881055953},\n",
       "   'harness|hendrycksTest-high_school_geography|5': {'acc': 0.7323232323232324,\n",
       "    'acc_stderr': 0.031544498882702866,\n",
       "    'acc_norm': 0.7323232323232324,\n",
       "    'acc_norm_stderr': 0.031544498882702866},\n",
       "   'harness|hendrycksTest-high_school_government_and_politics|5': {'acc': 0.8238341968911918,\n",
       "    'acc_stderr': 0.02749350424454805,\n",
       "    'acc_norm': 0.8238341968911918,\n",
       "    'acc_norm_stderr': 0.02749350424454805},\n",
       "   'harness|hendrycksTest-high_school_macroeconomics|5': {'acc': 0.6282051282051282,\n",
       "    'acc_stderr': 0.024503472557110936,\n",
       "    'acc_norm': 0.6282051282051282,\n",
       "    'acc_norm_stderr': 0.024503472557110936},\n",
       "   'harness|hendrycksTest-high_school_mathematics|5': {'acc': 0.34444444444444444,\n",
       "    'acc_stderr': 0.028972648884844267,\n",
       "    'acc_norm': 0.34444444444444444,\n",
       "    'acc_norm_stderr': 0.028972648884844267},\n",
       "   'harness|hendrycksTest-high_school_microeconomics|5': {'acc': 0.6764705882352942,\n",
       "    'acc_stderr': 0.0303883535518868,\n",
       "    'acc_norm': 0.6764705882352942,\n",
       "    'acc_norm_stderr': 0.0303883535518868},\n",
       "   'harness|hendrycksTest-high_school_physics|5': {'acc': 0.2980132450331126,\n",
       "    'acc_stderr': 0.037345356767871984,\n",
       "    'acc_norm': 0.2980132450331126,\n",
       "    'acc_norm_stderr': 0.037345356767871984},\n",
       "   'harness|hendrycksTest-high_school_psychology|5': {'acc': 0.8091743119266055,\n",
       "    'acc_stderr': 0.01684767640009109,\n",
       "    'acc_norm': 0.8091743119266055,\n",
       "    'acc_norm_stderr': 0.01684767640009109},\n",
       "   'harness|hendrycksTest-high_school_statistics|5': {'acc': 0.5462962962962963,\n",
       "    'acc_stderr': 0.033953227263757976,\n",
       "    'acc_norm': 0.5462962962962963,\n",
       "    'acc_norm_stderr': 0.033953227263757976},\n",
       "   'harness|hendrycksTest-high_school_us_history|5': {'acc': 0.7794117647058824,\n",
       "    'acc_stderr': 0.02910225438967407,\n",
       "    'acc_norm': 0.7794117647058824,\n",
       "    'acc_norm_stderr': 0.02910225438967407},\n",
       "   'harness|hendrycksTest-high_school_world_history|5': {'acc': 0.7341772151898734,\n",
       "    'acc_stderr': 0.028756799629658346,\n",
       "    'acc_norm': 0.7341772151898734,\n",
       "    'acc_norm_stderr': 0.028756799629658346},\n",
       "   'harness|hendrycksTest-human_aging|5': {'acc': 0.6367713004484304,\n",
       "    'acc_stderr': 0.032277904428505,\n",
       "    'acc_norm': 0.6367713004484304,\n",
       "    'acc_norm_stderr': 0.032277904428505},\n",
       "   'harness|hendrycksTest-human_sexuality|5': {'acc': 0.6717557251908397,\n",
       "    'acc_stderr': 0.04118438565806298,\n",
       "    'acc_norm': 0.6717557251908397,\n",
       "    'acc_norm_stderr': 0.04118438565806298},\n",
       "   'harness|hendrycksTest-international_law|5': {'acc': 0.7107438016528925,\n",
       "    'acc_stderr': 0.04139112727635463,\n",
       "    'acc_norm': 0.7107438016528925,\n",
       "    'acc_norm_stderr': 0.04139112727635463},\n",
       "   'harness|hendrycksTest-jurisprudence|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.040191074725573483,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.040191074725573483},\n",
       "   'harness|hendrycksTest-logical_fallacies|5': {'acc': 0.7055214723926381,\n",
       "    'acc_stderr': 0.03581165790474082,\n",
       "    'acc_norm': 0.7055214723926381,\n",
       "    'acc_norm_stderr': 0.03581165790474082},\n",
       "   'harness|hendrycksTest-machine_learning|5': {'acc': 0.3482142857142857,\n",
       "    'acc_stderr': 0.04521829902833585,\n",
       "    'acc_norm': 0.3482142857142857,\n",
       "    'acc_norm_stderr': 0.04521829902833585},\n",
       "   'harness|hendrycksTest-management|5': {'acc': 0.7378640776699029,\n",
       "    'acc_stderr': 0.043546310772605956,\n",
       "    'acc_norm': 0.7378640776699029,\n",
       "    'acc_norm_stderr': 0.043546310772605956},\n",
       "   'harness|hendrycksTest-marketing|5': {'acc': 0.8803418803418803,\n",
       "    'acc_stderr': 0.02126271940040697,\n",
       "    'acc_norm': 0.8803418803418803,\n",
       "    'acc_norm_stderr': 0.02126271940040697},\n",
       "   'harness|hendrycksTest-medical_genetics|5': {'acc': 0.71,\n",
       "    'acc_stderr': 0.045604802157206845,\n",
       "    'acc_norm': 0.71,\n",
       "    'acc_norm_stderr': 0.045604802157206845},\n",
       "   'harness|hendrycksTest-miscellaneous|5': {'acc': 0.7777777777777778,\n",
       "    'acc_stderr': 0.014866821664709588,\n",
       "    'acc_norm': 0.7777777777777778,\n",
       "    'acc_norm_stderr': 0.014866821664709588},\n",
       "   'harness|hendrycksTest-moral_disputes|5': {'acc': 0.6705202312138728,\n",
       "    'acc_stderr': 0.02530525813187972,\n",
       "    'acc_norm': 0.6705202312138728,\n",
       "    'acc_norm_stderr': 0.02530525813187972},\n",
       "   'harness|hendrycksTest-moral_scenarios|5': {'acc': 0.3418994413407821,\n",
       "    'acc_stderr': 0.015864506461604637,\n",
       "    'acc_norm': 0.3418994413407821,\n",
       "    'acc_norm_stderr': 0.015864506461604637},\n",
       "   'harness|hendrycksTest-nutrition|5': {'acc': 0.6830065359477124,\n",
       "    'acc_stderr': 0.026643278474508755,\n",
       "    'acc_norm': 0.6830065359477124,\n",
       "    'acc_norm_stderr': 0.026643278474508755},\n",
       "   'harness|hendrycksTest-philosophy|5': {'acc': 0.6688102893890675,\n",
       "    'acc_stderr': 0.02673062072800491,\n",
       "    'acc_norm': 0.6688102893890675,\n",
       "    'acc_norm_stderr': 0.02673062072800491},\n",
       "   'harness|hendrycksTest-prehistory|5': {'acc': 0.6666666666666666,\n",
       "    'acc_stderr': 0.02622964917882117,\n",
       "    'acc_norm': 0.6666666666666666,\n",
       "    'acc_norm_stderr': 0.02622964917882117},\n",
       "   'harness|hendrycksTest-professional_accounting|5': {'acc': 0.4929078014184397,\n",
       "    'acc_stderr': 0.02982449855912901,\n",
       "    'acc_norm': 0.4929078014184397,\n",
       "    'acc_norm_stderr': 0.02982449855912901},\n",
       "   'harness|hendrycksTest-professional_law|5': {'acc': 0.4276401564537158,\n",
       "    'acc_stderr': 0.012635799922765844,\n",
       "    'acc_norm': 0.4276401564537158,\n",
       "    'acc_norm_stderr': 0.012635799922765844},\n",
       "   'harness|hendrycksTest-professional_medicine|5': {'acc': 0.6470588235294118,\n",
       "    'acc_stderr': 0.029029422815681397,\n",
       "    'acc_norm': 0.6470588235294118,\n",
       "    'acc_norm_stderr': 0.029029422815681397},\n",
       "   'harness|hendrycksTest-professional_psychology|5': {'acc': 0.619281045751634,\n",
       "    'acc_stderr': 0.019643801557924803,\n",
       "    'acc_norm': 0.619281045751634,\n",
       "    'acc_norm_stderr': 0.019643801557924803},\n",
       "   'harness|hendrycksTest-public_relations|5': {'acc': 0.6636363636363637,\n",
       "    'acc_stderr': 0.04525393596302506,\n",
       "    'acc_norm': 0.6636363636363637,\n",
       "    'acc_norm_stderr': 0.04525393596302506},\n",
       "   'harness|hendrycksTest-security_studies|5': {'acc': 0.6816326530612244,\n",
       "    'acc_stderr': 0.029822533793982066,\n",
       "    'acc_norm': 0.6816326530612244,\n",
       "    'acc_norm_stderr': 0.029822533793982066},\n",
       "   'harness|hendrycksTest-sociology|5': {'acc': 0.8009950248756219,\n",
       "    'acc_stderr': 0.028231365092758406,\n",
       "    'acc_norm': 0.8009950248756219,\n",
       "    'acc_norm_stderr': 0.028231365092758406},\n",
       "   'harness|hendrycksTest-us_foreign_policy|5': {'acc': 0.78,\n",
       "    'acc_stderr': 0.04163331998932262,\n",
       "    'acc_norm': 0.78,\n",
       "    'acc_norm_stderr': 0.04163331998932262},\n",
       "   'harness|hendrycksTest-virology|5': {'acc': 0.5180722891566265,\n",
       "    'acc_stderr': 0.03889951252827216,\n",
       "    'acc_norm': 0.5180722891566265,\n",
       "    'acc_norm_stderr': 0.03889951252827216},\n",
       "   'harness|hendrycksTest-world_religions|5': {'acc': 0.8070175438596491,\n",
       "    'acc_stderr': 0.030267457554898458,\n",
       "    'acc_norm': 0.8070175438596491,\n",
       "    'acc_norm_stderr': 0.030267457554898458},\n",
       "   'harness|truthfulqa:mc|0': {'mc1': 0.40636474908200737,\n",
       "    'mc1_stderr': 0.017193835812093893,\n",
       "    'mc2': 0.5744916942762855,\n",
       "    'mc2_stderr': 0.015742095840959796},\n",
       "   'harness|winogrande|5': {'acc': 0.7774269928966061,\n",
       "    'acc_stderr': 0.011690933809712667},\n",
       "   'harness|drop|3': {'em': 0.004928691275167785,\n",
       "    'em_stderr': 0.0007171872517059793,\n",
       "    'f1': 0.09662437080536909,\n",
       "    'f1_stderr': 0.0018807376338089597},\n",
       "   'harness|gsm8k|5': {'acc': 0.12736921910538287,\n",
       "    'acc_stderr': 0.009183110326737829},\n",
       "   'all': {'acc': 0.6058001121844437,\n",
       "    'acc_stderr': 0.033164878802299444,\n",
       "    'acc_norm': 0.6148009779899025,\n",
       "    'acc_norm_stderr': 0.033912849373118566,\n",
       "    'mc1': 0.40636474908200737,\n",
       "    'mc1_stderr': 0.017193835812093893,\n",
       "    'mc2': 0.5744916942762855,\n",
       "    'mc2_stderr': 0.015742095840959796,\n",
       "    'em': 0.004928691275167785,\n",
       "    'em_stderr': 0.0007171872517059793,\n",
       "    'f1': 0.09662437080536909,\n",
       "    'f1_stderr': 0.0018807376338089597}}}}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_hf_models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hendrycksTest-high_school_statistics\n",
      "0.7416547068048434\n",
      "hendrycksTest-medical_genetics\n",
      "0.6907544373283913\n",
      "hendrycksTest-high_school_psychology\n",
      "0.8345597578739297\n",
      "hendrycksTest-moral_scenarios\n",
      "0.7934990071790254\n",
      "hendrycksTest-management\n",
      "0.7924669152096665\n",
      "hendrycksTest-college_mathematics\n",
      "0.032663410259844795\n",
      "hendrycksTest-high_school_computer_science\n",
      "0.7871949080998777\n",
      "hendrycksTest-astronomy\n",
      "0.8248974490616756\n",
      "hendrycksTest-high_school_world_history\n",
      "0.8279754246178759\n",
      "hendrycksTest-high_school_us_history\n",
      "0.8240336380878359\n",
      "hendrycksTest-marketing\n",
      "0.8356734983682623\n",
      "hendrycksTest-high_school_mathematics\n",
      "0.6143861143657399\n",
      "hendrycksTest-college_physics\n",
      "0.703305063609025\n",
      "hendrycksTest-high_school_european_history\n",
      "0.8363264312738408\n",
      "hendrycksTest-human_sexuality\n",
      "0.7970572304932209\n",
      "hendrycksTest-nutrition\n",
      "0.79008547008547\n",
      "hendrycksTest-professional_psychology\n",
      "0.83775005498629\n",
      "hendrycksTest-electrical_engineering\n",
      "0.690623225047368\n",
      "hendrycksTest-human_aging\n",
      "0.8032844849915253\n",
      "hendrycksTest-machine_learning\n",
      "0.6115398459618884\n",
      "hendrycksTest-miscellaneous\n",
      "0.833646789410847\n",
      "hendrycksTest-professional_law\n",
      "0.8345299145299144\n",
      "hendrycksTest-high_school_government_and_politics\n",
      "0.8371538052701287\n",
      "hendrycksTest-international_law\n",
      "0.8221007740351939\n",
      "hendrycksTest-world_religions\n",
      "0.8464726206755547\n",
      "hendrycksTest-abstract_algebra\n",
      "0.5774269944978343\n",
      "hendrycksTest-college_chemistry\n",
      "0.700175646624084\n",
      "hendrycksTest-econometrics\n",
      "0.7079557765646415\n",
      "hendrycksTest-high_school_physics\n",
      "0.7629647782221062\n",
      "hellaswag\n",
      "0.7299145299145299\n",
      "arc:challenge\n",
      "0.8290014164699427\n",
      "hendrycksTest-computer_security\n",
      "0.756507958382953\n",
      "hendrycksTest-moral_disputes\n",
      "0.8127245810661129\n",
      "hendrycksTest-security_studies\n",
      "0.8449696445202932\n",
      "hendrycksTest-professional_medicine\n",
      "0.7429451978432647\n",
      "truthfulqa:mc\n",
      "0.5405128205128205\n",
      "hendrycksTest-sociology\n",
      "0.870041090552613\n",
      "hendrycksTest-professional_accounting\n",
      "0.8195656970683529\n",
      "hendrycksTest-business_ethics\n",
      "0.7525724190297646\n",
      "hendrycksTest-clinical_knowledge\n",
      "0.7933582436134913\n",
      "hendrycksTest-college_medicine\n",
      "0.7822073007379267\n",
      "hendrycksTest-public_relations\n",
      "0.6988576678759426\n",
      "hendrycksTest-virology\n",
      "0.8156006014564721\n",
      "hendrycksTest-high_school_chemistry\n",
      "0.6488709785380623\n",
      "hendrycksTest-global_facts\n",
      "0.30225294867053554\n",
      "hendrycksTest-formal_logic\n",
      "0.7387887789753013\n",
      "hendrycksTest-conceptual_physics\n",
      "0.7686784177996653\n",
      "hendrycksTest-high_school_biology\n",
      "0.8038311302632007\n",
      "hendrycksTest-jurisprudence\n",
      "0.803355017715246\n",
      "hendrycksTest-prehistory\n",
      "0.8343306670067543\n",
      "hendrycksTest-college_computer_science\n",
      "0.7356537004948399\n",
      "hendrycksTest-elementary_mathematics\n",
      "0.7969207364036006\n",
      "hendrycksTest-us_foreign_policy\n",
      "0.8040441788763094\n",
      "hendrycksTest-high_school_microeconomics\n",
      "0.816284830148969\n",
      "hendrycksTest-college_biology\n",
      "0.8211541063054432\n",
      "hendrycksTest-philosophy\n",
      "0.7593638762486408\n",
      "hendrycksTest-logical_fallacies\n",
      "0.8309957470376553\n",
      "hendrycksTest-high_school_macroeconomics\n",
      "0.8080013795643279\n",
      "hendrycksTest-high_school_geography\n",
      "0.8139026953255367\n",
      "hendrycksTest-anatomy\n",
      "0.753083295828028\n"
     ]
    }
   ],
   "source": [
    "# Check Spearman correlation between elo and each benchmark\n",
    "corr_dict = {}\n",
    "\n",
    "for benchmark in common_benchmarks:\n",
    "    name = benchmark.split('|')[1]\n",
    "    print(name)\n",
    "    elo_scores = []\n",
    "    benchmark_scores = []\n",
    "    for model in filtered_hf_models:\n",
    "        elo_scores.append(elo.loc[model]['rating'])\n",
    "        result = filtered_hf_models[model]['results'][benchmark]\n",
    "        if 'acc_norm' in result:\n",
    "            benchmark_scores.append(result['acc_norm'])\n",
    "        elif 'mc2' in result:\n",
    "            benchmark_scores.append(result['mc2'])\n",
    "    corr = spearmanr(elo_scores, benchmark_scores).correlation\n",
    "    corr_dict[name] = corr\n",
    "\n",
    "    print(corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by correlation\n",
    "corr_dict = {k: v for k, v in sorted(corr_dict.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'hendrycksTest-sociology': 0.870041090552613,\n",
       " 'hendrycksTest-world_religions': 0.8464726206755547,\n",
       " 'hendrycksTest-security_studies': 0.8449696445202932,\n",
       " 'hendrycksTest-professional_psychology': 0.83775005498629,\n",
       " 'hendrycksTest-high_school_government_and_politics': 0.8371538052701287,\n",
       " 'hendrycksTest-high_school_european_history': 0.8363264312738408,\n",
       " 'hendrycksTest-marketing': 0.8356734983682623,\n",
       " 'hendrycksTest-high_school_psychology': 0.8345597578739297,\n",
       " 'hendrycksTest-professional_law': 0.8345299145299144,\n",
       " 'hendrycksTest-prehistory': 0.8343306670067543,\n",
       " 'hendrycksTest-miscellaneous': 0.833646789410847,\n",
       " 'hendrycksTest-logical_fallacies': 0.8309957470376553,\n",
       " 'arc:challenge': 0.8290014164699427,\n",
       " 'hendrycksTest-high_school_world_history': 0.8279754246178759,\n",
       " 'hendrycksTest-astronomy': 0.8248974490616756,\n",
       " 'hendrycksTest-high_school_us_history': 0.8240336380878359,\n",
       " 'hendrycksTest-international_law': 0.8221007740351939,\n",
       " 'hendrycksTest-college_biology': 0.8211541063054432,\n",
       " 'hendrycksTest-professional_accounting': 0.8195656970683529,\n",
       " 'hendrycksTest-high_school_microeconomics': 0.816284830148969,\n",
       " 'hendrycksTest-virology': 0.8156006014564721,\n",
       " 'hendrycksTest-high_school_geography': 0.8139026953255367,\n",
       " 'hendrycksTest-moral_disputes': 0.8127245810661129,\n",
       " 'hendrycksTest-high_school_macroeconomics': 0.8080013795643279,\n",
       " 'hendrycksTest-us_foreign_policy': 0.8040441788763094,\n",
       " 'hendrycksTest-high_school_biology': 0.8038311302632007,\n",
       " 'hendrycksTest-jurisprudence': 0.803355017715246,\n",
       " 'hendrycksTest-human_aging': 0.8032844849915253,\n",
       " 'hendrycksTest-human_sexuality': 0.7970572304932209,\n",
       " 'hendrycksTest-elementary_mathematics': 0.7969207364036006,\n",
       " 'hendrycksTest-moral_scenarios': 0.7934990071790254,\n",
       " 'hendrycksTest-clinical_knowledge': 0.7933582436134913,\n",
       " 'hendrycksTest-management': 0.7924669152096665,\n",
       " 'hendrycksTest-nutrition': 0.79008547008547,\n",
       " 'hendrycksTest-high_school_computer_science': 0.7871949080998777,\n",
       " 'hendrycksTest-college_medicine': 0.7822073007379267,\n",
       " 'hendrycksTest-conceptual_physics': 0.7686784177996653,\n",
       " 'hendrycksTest-high_school_physics': 0.7629647782221062,\n",
       " 'hendrycksTest-philosophy': 0.7593638762486408,\n",
       " 'hendrycksTest-computer_security': 0.756507958382953,\n",
       " 'hendrycksTest-anatomy': 0.753083295828028,\n",
       " 'hendrycksTest-business_ethics': 0.7525724190297646,\n",
       " 'hendrycksTest-professional_medicine': 0.7429451978432647,\n",
       " 'hendrycksTest-high_school_statistics': 0.7416547068048434,\n",
       " 'hendrycksTest-formal_logic': 0.7387887789753013,\n",
       " 'hendrycksTest-college_computer_science': 0.7356537004948399,\n",
       " 'hellaswag': 0.7299145299145299,\n",
       " 'hendrycksTest-econometrics': 0.7079557765646415,\n",
       " 'hendrycksTest-college_physics': 0.703305063609025,\n",
       " 'hendrycksTest-college_chemistry': 0.700175646624084,\n",
       " 'hendrycksTest-public_relations': 0.6988576678759426,\n",
       " 'hendrycksTest-medical_genetics': 0.6907544373283913,\n",
       " 'hendrycksTest-electrical_engineering': 0.690623225047368,\n",
       " 'hendrycksTest-high_school_chemistry': 0.6488709785380623,\n",
       " 'hendrycksTest-high_school_mathematics': 0.6143861143657399,\n",
       " 'hendrycksTest-machine_learning': 0.6115398459618884,\n",
       " 'hendrycksTest-abstract_algebra': 0.5774269944978343,\n",
       " 'truthfulqa:mc': 0.5405128205128205,\n",
       " 'hendrycksTest-global_facts': 0.30225294867053554,\n",
       " 'hendrycksTest-college_mathematics': 0.032663410259844795}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corr_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot correlation\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.bar(corr_dict.keys(), corr_dict.values())\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Spearman correlation between elo and benchmark scores')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_theme(style=\"whitegrid\")\n",
    "sns.set(rc={'figure.figsize':(10, 10)})\n",
    "sns.set(font_scale=1.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot top 5 benchmarks and use different colors for each benchmark\n",
    "plt.title('Top 5 benchmarks')\n",
    "# plt.xticks(rotation=30)\n",
    "plt.xlabel('Benchmarks')\n",
    "plt.ylabel('Spearman correlation')\n",
    "plt.ylim(0.82, 0.88)\n",
    "for i, benchmark in enumerate(list(corr_dict.keys())[:5]):\n",
    "    plt.bar(benchmark.replace(\"hendrycksTest-\", \"\").replace(\"_\", \"\\n\"), corr_dict[benchmark], color=plt.cm.Set1(i))\n",
    "plt.savefig('assets/top5_benchmarks.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot bottom 5 benchmarks and use different colors for each benchmark\n",
    "plt.title('Bottom 5 benchmarks')\n",
    "# plt.xticks(rotation=30)\n",
    "plt.xlabel('Benchmarks')\n",
    "plt.ylabel('Spearman correlation')\n",
    "plt.ylim(0.0, 0.65)\n",
    "for i, benchmark in enumerate(list(corr_dict.keys())[-5:]):\n",
    "    plt.bar(benchmark.replace(\"hendrycksTest-\", \"\").replace(\"_\", \"\\n\"), corr_dict[benchmark], color=plt.cm.Set1(i))\n",
    "plt.savefig('assets/bottom5_benchmarks.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use LASSO to select benchmarks\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "for model in filtered_hf_models:\n",
    "    x = []\n",
    "    for benchmark in common_benchmarks:\n",
    "        result = filtered_hf_models[model]['results'][benchmark]\n",
    "        if 'acc_norm' in result:\n",
    "            x.append(result['acc_norm'])\n",
    "        elif 'mc2' in result:\n",
    "            x.append(result['mc2'])\n",
    "\n",
    "    X.append(x)\n",
    "    y.append(elo.loc[model]['rating'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.789712919150614"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array(X)\n",
    "y = np.array(y)\n",
    "\n",
    "# Randomly split into train and test\n",
    "np.random.seed(0)\n",
    "idx = np.random.permutation(len(X))\n",
    "X_train = X[idx[:int(0.8 * len(X))]]\n",
    "y_train = y[idx[:int(0.8 * len(X))]]\n",
    "X_test = X[idx[int(0.8 * len(X)):]]\n",
    "y_test = y[idx[int(0.8 * len(X)):]]\n",
    "\n",
    "reg = Lasso(alpha=0.5, max_iter=10000)\n",
    "\n",
    "reg.fit(X_train, y_train)\n",
    "\n",
    "reg.score(X_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  0.        ,   0.        ,  18.08683519,   0.        ,\n",
       "         0.        ,  -0.        ,   0.        ,   0.        ,\n",
       "         0.        ,  98.1793419 ,  43.72065542,  -0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,  -0.        ,\n",
       "         0.        ,   0.        ,  10.49289352,   0.        ,\n",
       "         0.        ,  -0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "       100.61841831,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,  -0.        ,\n",
       "        -0.        ,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,   0.        ,\n",
       "       115.81335801,   0.        ,   0.        ,   0.        ,\n",
       "         0.        ,   0.        ,   0.        ,  -0.        ])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg.coef_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hendrycksTest-high_school_psychology 18.08683518807439\n",
      "hendrycksTest-high_school_us_history 98.17934189519255\n",
      "hendrycksTest-marketing 43.720655418874664\n",
      "hendrycksTest-high_school_government_and_politics 10.492893523456766\n",
      "hendrycksTest-sociology 100.61841831495344\n",
      "hendrycksTest-us_foreign_policy 115.81335800984122\n"
     ]
    }
   ],
   "source": [
    "# Print selected benchmarks\n",
    "selected_benchmarks = {}\n",
    "for i in range(len(common_benchmarks)):\n",
    "    if np.abs(reg.coef_[i]) > 1e-3:\n",
    "        selected_benchmarks[common_benchmarks[i]] = reg.coef_[i]\n",
    "        print(common_benchmarks[i].split('|')[1], reg.coef_[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sort by coefficient\n",
    "selected_benchmarks = {k: v for k, v in sorted(selected_benchmarks.items(), key=lambda item: item[1], reverse=True)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 720x720 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot selected benchmarks\n",
    "plt.title('Selected benchmarks')\n",
    "# plt.xticks(rotation=30)\n",
    "plt.xlabel('Benchmarks')\n",
    "plt.ylabel('LASSO coefficient')\n",
    "for i, benchmark in enumerate(selected_benchmarks.keys()):\n",
    "    plt.bar(benchmark.split('|')[1].replace(\"hendrycksTest-\", \"\").replace(\"_\", \"\\n\"), selected_benchmarks[benchmark], color=plt.cm.Set1(i))\n",
    "plt.savefig('assets/selected_benchmarks.png')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.942857142857143\n"
     ]
    }
   ],
   "source": [
    "# Check Spearman correlation between elo and each benchmark\n",
    "\n",
    "elo_scores = []\n",
    "benchmark_scores = []\n",
    "for i in idx[int(0.8 * len(X)):]:\n",
    "    model = list(filtered_hf_models.keys())[i]\n",
    "    elo_scores.append(elo.loc[model]['rating'])\n",
    "    benchmark_score = 0\n",
    "    for benchmark in selected_benchmarks.keys():\n",
    "        result = filtered_hf_models[model]['results'][benchmark]\n",
    "        if 'acc_norm' in result:\n",
    "            benchmark_score += result['acc_norm'] * selected_benchmarks[benchmark]\n",
    "        elif 'mc2' in result:\n",
    "            benchmark_score += result['mc2'] * selected_benchmarks[benchmark]\n",
    "    benchmark_scores.append(benchmark_score)\n",
    "corr = spearmanr(elo_scores, benchmark_scores).correlation\n",
    "\n",
    "print(corr)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
