{
  "results": {
    "harness|arc:challenge|25": {
      "acc": 0.5827645051194539,
      "acc_stderr": 0.014409825518403077,
      "acc_norm": 0.6160409556313993,
      "acc_norm_stderr": 0.01421244498065189
    },
    "harness|hellaswag|10": {
      "acc": 0.6461860187213703,
      "acc_stderr": 0.004771751187407024,
      "acc_norm": 0.8430591515634336,
      "acc_norm_stderr": 0.0036300159898964017
    },
    "harness|hendrycksTest-abstract_algebra|5": {
      "acc": 0.29,
      "acc_stderr": 0.04560480215720683,
      "acc_norm": 0.29,
      "acc_norm_stderr": 0.04560480215720683
    },
    "harness|hendrycksTest-anatomy|5": {
      "acc": 0.5407407407407407,
      "acc_stderr": 0.04304979692464242,
      "acc_norm": 0.5407407407407407,
      "acc_norm_stderr": 0.04304979692464242
    },
    "harness|hendrycksTest-astronomy|5": {
      "acc": 0.5526315789473685,
      "acc_stderr": 0.04046336883978251,
      "acc_norm": 0.5526315789473685,
      "acc_norm_stderr": 0.04046336883978251
    },
    "harness|hendrycksTest-business_ethics|5": {
      "acc": 0.51,
      "acc_stderr": 0.05024183937956912,
      "acc_norm": 0.51,
      "acc_norm_stderr": 0.05024183937956912
    },
    "harness|hendrycksTest-clinical_knowledge|5": {
      "acc": 0.5924528301886792,
      "acc_stderr": 0.030242233800854494,
      "acc_norm": 0.5924528301886792,
      "acc_norm_stderr": 0.030242233800854494
    },
    "harness|hendrycksTest-college_biology|5": {
      "acc": 0.6111111111111112,
      "acc_stderr": 0.04076663253918567,
      "acc_norm": 0.6111111111111112,
      "acc_norm_stderr": 0.04076663253918567
    },
    "harness|hendrycksTest-college_chemistry|5": {
      "acc": 0.42,
      "acc_stderr": 0.04960449637488584,
      "acc_norm": 0.42,
      "acc_norm_stderr": 0.04960449637488584
    },
    "harness|hendrycksTest-college_computer_science|5": {
      "acc": 0.48,
      "acc_stderr": 0.050211673156867795,
      "acc_norm": 0.48,
      "acc_norm_stderr": 0.050211673156867795
    },
    "harness|hendrycksTest-college_mathematics|5": {
      "acc": 0.37,
      "acc_stderr": 0.048523658709391,
      "acc_norm": 0.37,
      "acc_norm_stderr": 0.048523658709391
    },
    "harness|hendrycksTest-college_medicine|5": {
      "acc": 0.5317919075144508,
      "acc_stderr": 0.03804749744364764,
      "acc_norm": 0.5317919075144508,
      "acc_norm_stderr": 0.03804749744364764
    },
    "harness|hendrycksTest-college_physics|5": {
      "acc": 0.29411764705882354,
      "acc_stderr": 0.04533838195929775,
      "acc_norm": 0.29411764705882354,
      "acc_norm_stderr": 0.04533838195929775
    },
    "harness|hendrycksTest-computer_security|5": {
      "acc": 0.59,
      "acc_stderr": 0.049431107042371025,
      "acc_norm": 0.59,
      "acc_norm_stderr": 0.049431107042371025
    },
    "harness|hendrycksTest-conceptual_physics|5": {
      "acc": 0.4340425531914894,
      "acc_stderr": 0.03240038086792747,
      "acc_norm": 0.4340425531914894,
      "acc_norm_stderr": 0.03240038086792747
    },
    "harness|hendrycksTest-econometrics|5": {
      "acc": 0.2631578947368421,
      "acc_stderr": 0.041424397194893624,
      "acc_norm": 0.2631578947368421,
      "acc_norm_stderr": 0.041424397194893624
    },
    "harness|hendrycksTest-electrical_engineering|5": {
      "acc": 0.496551724137931,
      "acc_stderr": 0.041665675771015785,
      "acc_norm": 0.496551724137931,
      "acc_norm_stderr": 0.041665675771015785
    },
    "harness|hendrycksTest-elementary_mathematics|5": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.024278568024307706,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.024278568024307706
    },
    "harness|hendrycksTest-formal_logic|5": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.04216370213557835,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.04216370213557835
    },
    "harness|hendrycksTest-global_facts|5": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695235,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695235
    },
    "harness|hendrycksTest-high_school_biology|5": {
      "acc": 0.6516129032258065,
      "acc_stderr": 0.027104826328100944,
      "acc_norm": 0.6516129032258065,
      "acc_norm_stderr": 0.027104826328100944
    },
    "harness|hendrycksTest-high_school_chemistry|5": {
      "acc": 0.4236453201970443,
      "acc_stderr": 0.034767257476490364,
      "acc_norm": 0.4236453201970443,
      "acc_norm_stderr": 0.034767257476490364
    },
    "harness|hendrycksTest-high_school_computer_science|5": {
      "acc": 0.53,
      "acc_stderr": 0.05016135580465919,
      "acc_norm": 0.53,
      "acc_norm_stderr": 0.05016135580465919
    },
    "harness|hendrycksTest-high_school_european_history|5": {
      "acc": 0.6727272727272727,
      "acc_stderr": 0.03663974994391243,
      "acc_norm": 0.6727272727272727,
      "acc_norm_stderr": 0.03663974994391243
    },
    "harness|hendrycksTest-high_school_geography|5": {
      "acc": 0.7424242424242424,
      "acc_stderr": 0.03115626951964683,
      "acc_norm": 0.7424242424242424,
      "acc_norm_stderr": 0.03115626951964683
    },
    "harness|hendrycksTest-high_school_government_and_politics|5": {
      "acc": 0.7875647668393783,
      "acc_stderr": 0.029519282616817223,
      "acc_norm": 0.7875647668393783,
      "acc_norm_stderr": 0.029519282616817223
    },
    "harness|hendrycksTest-high_school_macroeconomics|5": {
      "acc": 0.5512820512820513,
      "acc_stderr": 0.025217315184846486,
      "acc_norm": 0.5512820512820513,
      "acc_norm_stderr": 0.025217315184846486
    },
    "harness|hendrycksTest-high_school_mathematics|5": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.028742040903948496,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.028742040903948496
    },
    "harness|hendrycksTest-high_school_microeconomics|5": {
      "acc": 0.5714285714285714,
      "acc_stderr": 0.03214536859788639,
      "acc_norm": 0.5714285714285714,
      "acc_norm_stderr": 0.03214536859788639
    },
    "harness|hendrycksTest-high_school_physics|5": {
      "acc": 0.3443708609271523,
      "acc_stderr": 0.03879687024073327,
      "acc_norm": 0.3443708609271523,
      "acc_norm_stderr": 0.03879687024073327
    },
    "harness|hendrycksTest-high_school_psychology|5": {
      "acc": 0.7541284403669725,
      "acc_stderr": 0.018461940968708433,
      "acc_norm": 0.7541284403669725,
      "acc_norm_stderr": 0.018461940968708433
    },
    "harness|hendrycksTest-high_school_statistics|5": {
      "acc": 0.4305555555555556,
      "acc_stderr": 0.03376922151252336,
      "acc_norm": 0.4305555555555556,
      "acc_norm_stderr": 0.03376922151252336
    },
    "harness|hendrycksTest-high_school_us_history|5": {
      "acc": 0.7107843137254902,
      "acc_stderr": 0.031822318676475544,
      "acc_norm": 0.7107843137254902,
      "acc_norm_stderr": 0.031822318676475544
    },
    "harness|hendrycksTest-high_school_world_history|5": {
      "acc": 0.679324894514768,
      "acc_stderr": 0.03038193194999041,
      "acc_norm": 0.679324894514768,
      "acc_norm_stderr": 0.03038193194999041
    },
    "harness|hendrycksTest-human_aging|5": {
      "acc": 0.6591928251121076,
      "acc_stderr": 0.0318114974705536,
      "acc_norm": 0.6591928251121076,
      "acc_norm_stderr": 0.0318114974705536
    },
    "harness|hendrycksTest-human_sexuality|5": {
      "acc": 0.6717557251908397,
      "acc_stderr": 0.04118438565806298,
      "acc_norm": 0.6717557251908397,
      "acc_norm_stderr": 0.04118438565806298
    },
    "harness|hendrycksTest-international_law|5": {
      "acc": 0.6942148760330579,
      "acc_stderr": 0.04205953933884123,
      "acc_norm": 0.6942148760330579,
      "acc_norm_stderr": 0.04205953933884123
    },
    "harness|hendrycksTest-jurisprudence|5": {
      "acc": 0.6481481481481481,
      "acc_stderr": 0.046166311118017125,
      "acc_norm": 0.6481481481481481,
      "acc_norm_stderr": 0.046166311118017125
    },
    "harness|hendrycksTest-logical_fallacies|5": {
      "acc": 0.6687116564417178,
      "acc_stderr": 0.03697983910025588,
      "acc_norm": 0.6687116564417178,
      "acc_norm_stderr": 0.03697983910025588
    },
    "harness|hendrycksTest-machine_learning|5": {
      "acc": 0.25892857142857145,
      "acc_stderr": 0.041577515398656284,
      "acc_norm": 0.25892857142857145,
      "acc_norm_stderr": 0.041577515398656284
    },
    "harness|hendrycksTest-management|5": {
      "acc": 0.7572815533980582,
      "acc_stderr": 0.04245022486384495,
      "acc_norm": 0.7572815533980582,
      "acc_norm_stderr": 0.04245022486384495
    },
    "harness|hendrycksTest-marketing|5": {
      "acc": 0.8205128205128205,
      "acc_stderr": 0.02514093595033544,
      "acc_norm": 0.8205128205128205,
      "acc_norm_stderr": 0.02514093595033544
    },
    "harness|hendrycksTest-medical_genetics|5": {
      "acc": 0.64,
      "acc_stderr": 0.04824181513244218,
      "acc_norm": 0.64,
      "acc_norm_stderr": 0.04824181513244218
    },
    "harness|hendrycksTest-miscellaneous|5": {
      "acc": 0.7420178799489144,
      "acc_stderr": 0.01564583018834895,
      "acc_norm": 0.7420178799489144,
      "acc_norm_stderr": 0.01564583018834895
    },
    "harness|hendrycksTest-moral_disputes|5": {
      "acc": 0.6127167630057804,
      "acc_stderr": 0.026226158605124655,
      "acc_norm": 0.6127167630057804,
      "acc_norm_stderr": 0.026226158605124655
    },
    "harness|hendrycksTest-moral_scenarios|5": {
      "acc": 0.3206703910614525,
      "acc_stderr": 0.015609929559348406,
      "acc_norm": 0.3206703910614525,
      "acc_norm_stderr": 0.015609929559348406
    },
    "harness|hendrycksTest-nutrition|5": {
      "acc": 0.6274509803921569,
      "acc_stderr": 0.027684181883302898,
      "acc_norm": 0.6274509803921569,
      "acc_norm_stderr": 0.027684181883302898
    },
    "harness|hendrycksTest-philosophy|5": {
      "acc": 0.639871382636656,
      "acc_stderr": 0.02726429759980402,
      "acc_norm": 0.639871382636656,
      "acc_norm_stderr": 0.02726429759980402
    },
    "harness|hendrycksTest-prehistory|5": {
      "acc": 0.6203703703703703,
      "acc_stderr": 0.027002521034516475,
      "acc_norm": 0.6203703703703703,
      "acc_norm_stderr": 0.027002521034516475
    },
    "harness|hendrycksTest-professional_accounting|5": {
      "acc": 0.3900709219858156,
      "acc_stderr": 0.02909767559946393,
      "acc_norm": 0.3900709219858156,
      "acc_norm_stderr": 0.02909767559946393
    },
    "harness|hendrycksTest-professional_law|5": {
      "acc": 0.40221642764015647,
      "acc_stderr": 0.012523646856180178,
      "acc_norm": 0.40221642764015647,
      "acc_norm_stderr": 0.012523646856180178
    },
    "harness|hendrycksTest-professional_medicine|5": {
      "acc": 0.5808823529411765,
      "acc_stderr": 0.02997280717046462,
      "acc_norm": 0.5808823529411765,
      "acc_norm_stderr": 0.02997280717046462
    },
    "harness|hendrycksTest-professional_psychology|5": {
      "acc": 0.5196078431372549,
      "acc_stderr": 0.020212274976302954,
      "acc_norm": 0.5196078431372549,
      "acc_norm_stderr": 0.020212274976302954
    },
    "harness|hendrycksTest-public_relations|5": {
      "acc": 0.6636363636363637,
      "acc_stderr": 0.04525393596302505,
      "acc_norm": 0.6636363636363637,
      "acc_norm_stderr": 0.04525393596302505
    },
    "harness|hendrycksTest-security_studies|5": {
      "acc": 0.6326530612244898,
      "acc_stderr": 0.030862144921087555,
      "acc_norm": 0.6326530612244898,
      "acc_norm_stderr": 0.030862144921087555
    },
    "harness|hendrycksTest-sociology|5": {
      "acc": 0.7661691542288557,
      "acc_stderr": 0.02992941540834839,
      "acc_norm": 0.7661691542288557,
      "acc_norm_stderr": 0.02992941540834839
    },
    "harness|hendrycksTest-us_foreign_policy|5": {
      "acc": 0.83,
      "acc_stderr": 0.03775251680686371,
      "acc_norm": 0.83,
      "acc_norm_stderr": 0.03775251680686371
    },
    "harness|hendrycksTest-virology|5": {
      "acc": 0.4939759036144578,
      "acc_stderr": 0.03892212195333045,
      "acc_norm": 0.4939759036144578,
      "acc_norm_stderr": 0.03892212195333045
    },
    "harness|hendrycksTest-world_religions|5": {
      "acc": 0.7777777777777778,
      "acc_stderr": 0.03188578017686399,
      "acc_norm": 0.7777777777777778,
      "acc_norm_stderr": 0.03188578017686399
    },
    "harness|truthfulqa:mc|0": {
      "mc1": 0.3769889840881273,
      "mc1_stderr": 0.016965517578930354,
      "mc2": 0.5251632785674454,
      "mc2_stderr": 0.015030281725523202
    },
    "all": {
      "acc": 0.5565123975347385,
      "acc_stderr": 0.034413429904106986,
      "acc_norm": 0.5604132379305692,
      "acc_norm_stderr": 0.03439073302723814,
      "mc1": 0.3769889840881273,
      "mc1_stderr": 0.016965517578930354,
      "mc2": 0.5251632785674454,
      "mc2_stderr": 0.015030281725523202
    }
  },
  "versions": {
    "harness|arc:challenge|25": 0,
    "harness|hellaswag|10": 0,
    "harness|hendrycksTest-abstract_algebra|5": 1,
    "harness|hendrycksTest-anatomy|5": 1,
    "harness|hendrycksTest-astronomy|5": 1,
    "harness|hendrycksTest-business_ethics|5": 1,
    "harness|hendrycksTest-clinical_knowledge|5": 1,
    "harness|hendrycksTest-college_biology|5": 1,
    "harness|hendrycksTest-college_chemistry|5": 1,
    "harness|hendrycksTest-college_computer_science|5": 1,
    "harness|hendrycksTest-college_mathematics|5": 1,
    "harness|hendrycksTest-college_medicine|5": 1,
    "harness|hendrycksTest-college_physics|5": 1,
    "harness|hendrycksTest-computer_security|5": 1,
    "harness|hendrycksTest-conceptual_physics|5": 1,
    "harness|hendrycksTest-econometrics|5": 1,
    "harness|hendrycksTest-electrical_engineering|5": 1,
    "harness|hendrycksTest-elementary_mathematics|5": 1,
    "harness|hendrycksTest-formal_logic|5": 1,
    "harness|hendrycksTest-global_facts|5": 1,
    "harness|hendrycksTest-high_school_biology|5": 1,
    "harness|hendrycksTest-high_school_chemistry|5": 1,
    "harness|hendrycksTest-high_school_computer_science|5": 1,
    "harness|hendrycksTest-high_school_european_history|5": 1,
    "harness|hendrycksTest-high_school_geography|5": 1,
    "harness|hendrycksTest-high_school_government_and_politics|5": 1,
    "harness|hendrycksTest-high_school_macroeconomics|5": 1,
    "harness|hendrycksTest-high_school_mathematics|5": 1,
    "harness|hendrycksTest-high_school_microeconomics|5": 1,
    "harness|hendrycksTest-high_school_physics|5": 1,
    "harness|hendrycksTest-high_school_psychology|5": 1,
    "harness|hendrycksTest-high_school_statistics|5": 1,
    "harness|hendrycksTest-high_school_us_history|5": 1,
    "harness|hendrycksTest-high_school_world_history|5": 1,
    "harness|hendrycksTest-human_aging|5": 1,
    "harness|hendrycksTest-human_sexuality|5": 1,
    "harness|hendrycksTest-international_law|5": 1,
    "harness|hendrycksTest-jurisprudence|5": 1,
    "harness|hendrycksTest-logical_fallacies|5": 1,
    "harness|hendrycksTest-machine_learning|5": 1,
    "harness|hendrycksTest-management|5": 1,
    "harness|hendrycksTest-marketing|5": 1,
    "harness|hendrycksTest-medical_genetics|5": 1,
    "harness|hendrycksTest-miscellaneous|5": 1,
    "harness|hendrycksTest-moral_disputes|5": 1,
    "harness|hendrycksTest-moral_scenarios|5": 1,
    "harness|hendrycksTest-nutrition|5": 1,
    "harness|hendrycksTest-philosophy|5": 1,
    "harness|hendrycksTest-prehistory|5": 1,
    "harness|hendrycksTest-professional_accounting|5": 1,
    "harness|hendrycksTest-professional_law|5": 1,
    "harness|hendrycksTest-professional_medicine|5": 1,
    "harness|hendrycksTest-professional_psychology|5": 1,
    "harness|hendrycksTest-public_relations|5": 1,
    "harness|hendrycksTest-security_studies|5": 1,
    "harness|hendrycksTest-sociology|5": 1,
    "harness|hendrycksTest-us_foreign_policy|5": 1,
    "harness|hendrycksTest-virology|5": 1,
    "harness|hendrycksTest-world_religions|5": 1,
    "harness|truthfulqa:mc|0": 1,
    "all": 0
  },
  "task_config": {
    "harness|arc:challenge": "LM Harness task",
    "harness|hellaswag": "LM Harness task",
    "harness|hendrycksTest-abstract_algebra": "LM Harness task",
    "harness|hendrycksTest-anatomy": "LM Harness task",
    "harness|hendrycksTest-astronomy": "LM Harness task",
    "harness|hendrycksTest-business_ethics": "LM Harness task",
    "harness|hendrycksTest-clinical_knowledge": "LM Harness task",
    "harness|hendrycksTest-college_biology": "LM Harness task",
    "harness|hendrycksTest-college_chemistry": "LM Harness task",
    "harness|hendrycksTest-college_computer_science": "LM Harness task",
    "harness|hendrycksTest-college_mathematics": "LM Harness task",
    "harness|hendrycksTest-college_medicine": "LM Harness task",
    "harness|hendrycksTest-college_physics": "LM Harness task",
    "harness|hendrycksTest-computer_security": "LM Harness task",
    "harness|hendrycksTest-conceptual_physics": "LM Harness task",
    "harness|hendrycksTest-econometrics": "LM Harness task",
    "harness|hendrycksTest-electrical_engineering": "LM Harness task",
    "harness|hendrycksTest-elementary_mathematics": "LM Harness task",
    "harness|hendrycksTest-formal_logic": "LM Harness task",
    "harness|hendrycksTest-global_facts": "LM Harness task",
    "harness|hendrycksTest-high_school_biology": "LM Harness task",
    "harness|hendrycksTest-high_school_chemistry": "LM Harness task",
    "harness|hendrycksTest-high_school_computer_science": "LM Harness task",
    "harness|hendrycksTest-high_school_european_history": "LM Harness task",
    "harness|hendrycksTest-high_school_geography": "LM Harness task",
    "harness|hendrycksTest-high_school_government_and_politics": "LM Harness task",
    "harness|hendrycksTest-high_school_macroeconomics": "LM Harness task",
    "harness|hendrycksTest-high_school_mathematics": "LM Harness task",
    "harness|hendrycksTest-high_school_microeconomics": "LM Harness task",
    "harness|hendrycksTest-high_school_physics": "LM Harness task",
    "harness|hendrycksTest-high_school_psychology": "LM Harness task",
    "harness|hendrycksTest-high_school_statistics": "LM Harness task",
    "harness|hendrycksTest-high_school_us_history": "LM Harness task",
    "harness|hendrycksTest-high_school_world_history": "LM Harness task",
    "harness|hendrycksTest-human_aging": "LM Harness task",
    "harness|hendrycksTest-human_sexuality": "LM Harness task",
    "harness|hendrycksTest-international_law": "LM Harness task",
    "harness|hendrycksTest-jurisprudence": "LM Harness task",
    "harness|hendrycksTest-logical_fallacies": "LM Harness task",
    "harness|hendrycksTest-machine_learning": "LM Harness task",
    "harness|hendrycksTest-management": "LM Harness task",
    "harness|hendrycksTest-marketing": "LM Harness task",
    "harness|hendrycksTest-medical_genetics": "LM Harness task",
    "harness|hendrycksTest-miscellaneous": "LM Harness task",
    "harness|hendrycksTest-moral_disputes": "LM Harness task",
    "harness|hendrycksTest-moral_scenarios": "LM Harness task",
    "harness|hendrycksTest-nutrition": "LM Harness task",
    "harness|hendrycksTest-philosophy": "LM Harness task",
    "harness|hendrycksTest-prehistory": "LM Harness task",
    "harness|hendrycksTest-professional_accounting": "LM Harness task",
    "harness|hendrycksTest-professional_law": "LM Harness task",
    "harness|hendrycksTest-professional_medicine": "LM Harness task",
    "harness|hendrycksTest-professional_psychology": "LM Harness task",
    "harness|hendrycksTest-public_relations": "LM Harness task",
    "harness|hendrycksTest-security_studies": "LM Harness task",
    "harness|hendrycksTest-sociology": "LM Harness task",
    "harness|hendrycksTest-us_foreign_policy": "LM Harness task",
    "harness|hendrycksTest-virology": "LM Harness task",
    "harness|hendrycksTest-world_religions": "LM Harness task",
    "harness|truthfulqa:mc": "LM Harness task"
  },
  "config_general": {
    "model_name": "tiiuae/falcon-40b-instruct",
    "model_sha": "7475ff8cfc36ed9a962b658ae3c33391566a85a5",
    "model_dtype": "torch.bfloat16",
    "lighteval_sha": "1982e5669ed61622a77b3a79436ff5d00583e4ff",
    "num_few_shot_default": 0,
    "num_fewshot_seeds": 1,
    "override_batch_size": 1,
    "max_samples": null
  }
}