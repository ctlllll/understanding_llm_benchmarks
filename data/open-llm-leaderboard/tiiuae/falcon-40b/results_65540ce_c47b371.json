{
  "results": {
    "harness|arc:challenge|25": {
      "acc": 0.5784982935153583,
      "acc_stderr": 0.014430197069326021,
      "acc_norm": 0.6194539249146758,
      "acc_norm_stderr": 0.01418827771234982
    },
    "harness|hellaswag|10": {
      "acc": 0.6560446126269668,
      "acc_stderr": 0.00474055578214217,
      "acc_norm": 0.8528181637124079,
      "acc_norm_stderr": 0.0035356302890914583
    },
    "harness|hendrycksTest-abstract_algebra|5": {
      "acc": 0.34,
      "acc_stderr": 0.04760952285695236,
      "acc_norm": 0.34,
      "acc_norm_stderr": 0.04760952285695236
    },
    "harness|hendrycksTest-anatomy|5": {
      "acc": 0.5333333333333333,
      "acc_stderr": 0.043097329010363554,
      "acc_norm": 0.5333333333333333,
      "acc_norm_stderr": 0.043097329010363554
    },
    "harness|hendrycksTest-astronomy|5": {
      "acc": 0.5723684210526315,
      "acc_stderr": 0.04026097083296564,
      "acc_norm": 0.5723684210526315,
      "acc_norm_stderr": 0.04026097083296564
    },
    "harness|hendrycksTest-business_ethics|5": {
      "acc": 0.54,
      "acc_stderr": 0.05009082659620332,
      "acc_norm": 0.54,
      "acc_norm_stderr": 0.05009082659620332
    },
    "harness|hendrycksTest-clinical_knowledge|5": {
      "acc": 0.6075471698113207,
      "acc_stderr": 0.030052580579557845,
      "acc_norm": 0.6075471698113207,
      "acc_norm_stderr": 0.030052580579557845
    },
    "harness|hendrycksTest-college_biology|5": {
      "acc": 0.6666666666666666,
      "acc_stderr": 0.03942082639927213,
      "acc_norm": 0.6666666666666666,
      "acc_norm_stderr": 0.03942082639927213
    },
    "harness|hendrycksTest-college_chemistry|5": {
      "acc": 0.45,
      "acc_stderr": 0.05,
      "acc_norm": 0.45,
      "acc_norm_stderr": 0.05
    },
    "harness|hendrycksTest-college_computer_science|5": {
      "acc": 0.5,
      "acc_stderr": 0.050251890762960605,
      "acc_norm": 0.5,
      "acc_norm_stderr": 0.050251890762960605
    },
    "harness|hendrycksTest-college_mathematics|5": {
      "acc": 0.39,
      "acc_stderr": 0.04902071300001974,
      "acc_norm": 0.39,
      "acc_norm_stderr": 0.04902071300001974
    },
    "harness|hendrycksTest-college_medicine|5": {
      "acc": 0.5202312138728323,
      "acc_stderr": 0.03809342081273956,
      "acc_norm": 0.5202312138728323,
      "acc_norm_stderr": 0.03809342081273956
    },
    "harness|hendrycksTest-college_physics|5": {
      "acc": 0.27450980392156865,
      "acc_stderr": 0.044405219061793254,
      "acc_norm": 0.27450980392156865,
      "acc_norm_stderr": 0.044405219061793254
    },
    "harness|hendrycksTest-computer_security|5": {
      "acc": 0.63,
      "acc_stderr": 0.04852365870939099,
      "acc_norm": 0.63,
      "acc_norm_stderr": 0.04852365870939099
    },
    "harness|hendrycksTest-conceptual_physics|5": {
      "acc": 0.4127659574468085,
      "acc_stderr": 0.03218471141400351,
      "acc_norm": 0.4127659574468085,
      "acc_norm_stderr": 0.03218471141400351
    },
    "harness|hendrycksTest-econometrics|5": {
      "acc": 0.3333333333333333,
      "acc_stderr": 0.044346007015849245,
      "acc_norm": 0.3333333333333333,
      "acc_norm_stderr": 0.044346007015849245
    },
    "harness|hendrycksTest-electrical_engineering|5": {
      "acc": 0.5310344827586206,
      "acc_stderr": 0.04158632762097828,
      "acc_norm": 0.5310344827586206,
      "acc_norm_stderr": 0.04158632762097828
    },
    "harness|hendrycksTest-elementary_mathematics|5": {
      "acc": 0.3386243386243386,
      "acc_stderr": 0.024373197867983056,
      "acc_norm": 0.3386243386243386,
      "acc_norm_stderr": 0.024373197867983056
    },
    "harness|hendrycksTest-formal_logic|5": {
      "acc": 0.30158730158730157,
      "acc_stderr": 0.04104947269903394,
      "acc_norm": 0.30158730158730157,
      "acc_norm_stderr": 0.04104947269903394
    },
    "harness|hendrycksTest-global_facts|5": {
      "acc": 0.32,
      "acc_stderr": 0.046882617226215034,
      "acc_norm": 0.32,
      "acc_norm_stderr": 0.046882617226215034
    },
    "harness|hendrycksTest-high_school_biology|5": {
      "acc": 0.6709677419354839,
      "acc_stderr": 0.02672949906834996,
      "acc_norm": 0.6709677419354839,
      "acc_norm_stderr": 0.02672949906834996
    },
    "harness|hendrycksTest-high_school_chemistry|5": {
      "acc": 0.4433497536945813,
      "acc_stderr": 0.03495334582162934,
      "acc_norm": 0.4433497536945813,
      "acc_norm_stderr": 0.03495334582162934
    },
    "harness|hendrycksTest-high_school_computer_science|5": {
      "acc": 0.62,
      "acc_stderr": 0.04878317312145633,
      "acc_norm": 0.62,
      "acc_norm_stderr": 0.04878317312145633
    },
    "harness|hendrycksTest-high_school_european_history|5": {
      "acc": 0.696969696969697,
      "acc_stderr": 0.03588624800091707,
      "acc_norm": 0.696969696969697,
      "acc_norm_stderr": 0.03588624800091707
    },
    "harness|hendrycksTest-high_school_geography|5": {
      "acc": 0.7424242424242424,
      "acc_stderr": 0.03115626951964683,
      "acc_norm": 0.7424242424242424,
      "acc_norm_stderr": 0.03115626951964683
    },
    "harness|hendrycksTest-high_school_government_and_politics|5": {
      "acc": 0.772020725388601,
      "acc_stderr": 0.030276909945178256,
      "acc_norm": 0.772020725388601,
      "acc_norm_stderr": 0.030276909945178256
    },
    "harness|hendrycksTest-high_school_macroeconomics|5": {
      "acc": 0.5538461538461539,
      "acc_stderr": 0.02520357177302833,
      "acc_norm": 0.5538461538461539,
      "acc_norm_stderr": 0.02520357177302833
    },
    "harness|hendrycksTest-high_school_mathematics|5": {
      "acc": 0.3148148148148148,
      "acc_stderr": 0.028317533496066475,
      "acc_norm": 0.3148148148148148,
      "acc_norm_stderr": 0.028317533496066475
    },
    "harness|hendrycksTest-high_school_microeconomics|5": {
      "acc": 0.5546218487394958,
      "acc_stderr": 0.0322841062671639,
      "acc_norm": 0.5546218487394958,
      "acc_norm_stderr": 0.0322841062671639
    },
    "harness|hendrycksTest-high_school_physics|5": {
      "acc": 0.31788079470198677,
      "acc_stderr": 0.03802039760107903,
      "acc_norm": 0.31788079470198677,
      "acc_norm_stderr": 0.03802039760107903
    },
    "harness|hendrycksTest-high_school_psychology|5": {
      "acc": 0.7761467889908257,
      "acc_stderr": 0.01787121776779022,
      "acc_norm": 0.7761467889908257,
      "acc_norm_stderr": 0.01787121776779022
    },
    "harness|hendrycksTest-high_school_statistics|5": {
      "acc": 0.47685185185185186,
      "acc_stderr": 0.034063153607115065,
      "acc_norm": 0.47685185185185186,
      "acc_norm_stderr": 0.034063153607115065
    },
    "harness|hendrycksTest-high_school_us_history|5": {
      "acc": 0.7156862745098039,
      "acc_stderr": 0.03166009679399813,
      "acc_norm": 0.7156862745098039,
      "acc_norm_stderr": 0.03166009679399813
    },
    "harness|hendrycksTest-high_school_world_history|5": {
      "acc": 0.7130801687763713,
      "acc_stderr": 0.029443773022594693,
      "acc_norm": 0.7130801687763713,
      "acc_norm_stderr": 0.029443773022594693
    },
    "harness|hendrycksTest-human_aging|5": {
      "acc": 0.7354260089686099,
      "acc_stderr": 0.02960510321703832,
      "acc_norm": 0.7354260089686099,
      "acc_norm_stderr": 0.02960510321703832
    },
    "harness|hendrycksTest-human_sexuality|5": {
      "acc": 0.7251908396946565,
      "acc_stderr": 0.03915345408847835,
      "acc_norm": 0.7251908396946565,
      "acc_norm_stderr": 0.03915345408847835
    },
    "harness|hendrycksTest-international_law|5": {
      "acc": 0.6528925619834711,
      "acc_stderr": 0.043457245702925335,
      "acc_norm": 0.6528925619834711,
      "acc_norm_stderr": 0.043457245702925335
    },
    "harness|hendrycksTest-jurisprudence|5": {
      "acc": 0.6944444444444444,
      "acc_stderr": 0.044531975073749834,
      "acc_norm": 0.6944444444444444,
      "acc_norm_stderr": 0.044531975073749834
    },
    "harness|hendrycksTest-logical_fallacies|5": {
      "acc": 0.656441717791411,
      "acc_stderr": 0.03731133519673893,
      "acc_norm": 0.656441717791411,
      "acc_norm_stderr": 0.03731133519673893
    },
    "harness|hendrycksTest-machine_learning|5": {
      "acc": 0.29464285714285715,
      "acc_stderr": 0.043270409325787275,
      "acc_norm": 0.29464285714285715,
      "acc_norm_stderr": 0.043270409325787275
    },
    "harness|hendrycksTest-management|5": {
      "acc": 0.7864077669902912,
      "acc_stderr": 0.040580420156460344,
      "acc_norm": 0.7864077669902912,
      "acc_norm_stderr": 0.040580420156460344
    },
    "harness|hendrycksTest-marketing|5": {
      "acc": 0.7905982905982906,
      "acc_stderr": 0.026655699653922744,
      "acc_norm": 0.7905982905982906,
      "acc_norm_stderr": 0.026655699653922744
    },
    "harness|hendrycksTest-medical_genetics|5": {
      "acc": 0.66,
      "acc_stderr": 0.04760952285695237,
      "acc_norm": 0.66,
      "acc_norm_stderr": 0.04760952285695237
    },
    "harness|hendrycksTest-miscellaneous|5": {
      "acc": 0.7586206896551724,
      "acc_stderr": 0.0153023801235421,
      "acc_norm": 0.7586206896551724,
      "acc_norm_stderr": 0.0153023801235421
    },
    "harness|hendrycksTest-moral_disputes|5": {
      "acc": 0.6271676300578035,
      "acc_stderr": 0.026033890613576288,
      "acc_norm": 0.6271676300578035,
      "acc_norm_stderr": 0.026033890613576288
    },
    "harness|hendrycksTest-moral_scenarios|5": {
      "acc": 0.27039106145251396,
      "acc_stderr": 0.014854993938010068,
      "acc_norm": 0.27039106145251396,
      "acc_norm_stderr": 0.014854993938010068
    },
    "harness|hendrycksTest-nutrition|5": {
      "acc": 0.673202614379085,
      "acc_stderr": 0.026857294663281416,
      "acc_norm": 0.673202614379085,
      "acc_norm_stderr": 0.026857294663281416
    },
    "harness|hendrycksTest-philosophy|5": {
      "acc": 0.6688102893890675,
      "acc_stderr": 0.02673062072800491,
      "acc_norm": 0.6688102893890675,
      "acc_norm_stderr": 0.02673062072800491
    },
    "harness|hendrycksTest-prehistory|5": {
      "acc": 0.6327160493827161,
      "acc_stderr": 0.0268228017595079,
      "acc_norm": 0.6327160493827161,
      "acc_norm_stderr": 0.0268228017595079
    },
    "harness|hendrycksTest-professional_accounting|5": {
      "acc": 0.43617021276595747,
      "acc_stderr": 0.029583452036284066,
      "acc_norm": 0.43617021276595747,
      "acc_norm_stderr": 0.029583452036284066
    },
    "harness|hendrycksTest-professional_law|5": {
      "acc": 0.4315514993481095,
      "acc_stderr": 0.01265000799946388,
      "acc_norm": 0.4315514993481095,
      "acc_norm_stderr": 0.01265000799946388
    },
    "harness|hendrycksTest-professional_medicine|5": {
      "acc": 0.6102941176470589,
      "acc_stderr": 0.02962466358115969,
      "acc_norm": 0.6102941176470589,
      "acc_norm_stderr": 0.02962466358115969
    },
    "harness|hendrycksTest-professional_psychology|5": {
      "acc": 0.565359477124183,
      "acc_stderr": 0.02005426920072646,
      "acc_norm": 0.565359477124183,
      "acc_norm_stderr": 0.02005426920072646
    },
    "harness|hendrycksTest-public_relations|5": {
      "acc": 0.6272727272727273,
      "acc_stderr": 0.04631381319425465,
      "acc_norm": 0.6272727272727273,
      "acc_norm_stderr": 0.04631381319425465
    },
    "harness|hendrycksTest-security_studies|5": {
      "acc": 0.6612244897959184,
      "acc_stderr": 0.030299506562154185,
      "acc_norm": 0.6612244897959184,
      "acc_norm_stderr": 0.030299506562154185
    },
    "harness|hendrycksTest-sociology|5": {
      "acc": 0.7960199004975125,
      "acc_stderr": 0.02849317624532607,
      "acc_norm": 0.7960199004975125,
      "acc_norm_stderr": 0.02849317624532607
    },
    "harness|hendrycksTest-us_foreign_policy|5": {
      "acc": 0.81,
      "acc_stderr": 0.03942772444036625,
      "acc_norm": 0.81,
      "acc_norm_stderr": 0.03942772444036625
    },
    "harness|hendrycksTest-virology|5": {
      "acc": 0.4759036144578313,
      "acc_stderr": 0.038879718495972646,
      "acc_norm": 0.4759036144578313,
      "acc_norm_stderr": 0.038879718495972646
    },
    "harness|hendrycksTest-world_religions|5": {
      "acc": 0.8070175438596491,
      "acc_stderr": 0.030267457554898458,
      "acc_norm": 0.8070175438596491,
      "acc_norm_stderr": 0.030267457554898458
    },
    "harness|truthfulqa:mc|0": {
      "mc1": 0.27539779681762544,
      "mc1_stderr": 0.01563813566777552,
      "mc2": 0.4171692245008122,
      "mc2_stderr": 0.01379036544689385
    },
    "all": {
      "acc": 0.5714063083032939,
      "acc_stderr": 0.034227801314107555,
      "acc_norm": 0.5754356164810016,
      "acc_norm_stderr": 0.03420327852003931,
      "mc1": 0.27539779681762544,
      "mc1_stderr": 0.01563813566777552,
      "mc2": 0.4171692245008122,
      "mc2_stderr": 0.01379036544689385
    }
  },
  "versions": {
    "harness|arc:challenge|25": 0,
    "harness|hellaswag|10": 0,
    "harness|hendrycksTest-abstract_algebra|5": 1,
    "harness|hendrycksTest-anatomy|5": 1,
    "harness|hendrycksTest-astronomy|5": 1,
    "harness|hendrycksTest-business_ethics|5": 1,
    "harness|hendrycksTest-clinical_knowledge|5": 1,
    "harness|hendrycksTest-college_biology|5": 1,
    "harness|hendrycksTest-college_chemistry|5": 1,
    "harness|hendrycksTest-college_computer_science|5": 1,
    "harness|hendrycksTest-college_mathematics|5": 1,
    "harness|hendrycksTest-college_medicine|5": 1,
    "harness|hendrycksTest-college_physics|5": 1,
    "harness|hendrycksTest-computer_security|5": 1,
    "harness|hendrycksTest-conceptual_physics|5": 1,
    "harness|hendrycksTest-econometrics|5": 1,
    "harness|hendrycksTest-electrical_engineering|5": 1,
    "harness|hendrycksTest-elementary_mathematics|5": 1,
    "harness|hendrycksTest-formal_logic|5": 1,
    "harness|hendrycksTest-global_facts|5": 1,
    "harness|hendrycksTest-high_school_biology|5": 1,
    "harness|hendrycksTest-high_school_chemistry|5": 1,
    "harness|hendrycksTest-high_school_computer_science|5": 1,
    "harness|hendrycksTest-high_school_european_history|5": 1,
    "harness|hendrycksTest-high_school_geography|5": 1,
    "harness|hendrycksTest-high_school_government_and_politics|5": 1,
    "harness|hendrycksTest-high_school_macroeconomics|5": 1,
    "harness|hendrycksTest-high_school_mathematics|5": 1,
    "harness|hendrycksTest-high_school_microeconomics|5": 1,
    "harness|hendrycksTest-high_school_physics|5": 1,
    "harness|hendrycksTest-high_school_psychology|5": 1,
    "harness|hendrycksTest-high_school_statistics|5": 1,
    "harness|hendrycksTest-high_school_us_history|5": 1,
    "harness|hendrycksTest-high_school_world_history|5": 1,
    "harness|hendrycksTest-human_aging|5": 1,
    "harness|hendrycksTest-human_sexuality|5": 1,
    "harness|hendrycksTest-international_law|5": 1,
    "harness|hendrycksTest-jurisprudence|5": 1,
    "harness|hendrycksTest-logical_fallacies|5": 1,
    "harness|hendrycksTest-machine_learning|5": 1,
    "harness|hendrycksTest-management|5": 1,
    "harness|hendrycksTest-marketing|5": 1,
    "harness|hendrycksTest-medical_genetics|5": 1,
    "harness|hendrycksTest-miscellaneous|5": 1,
    "harness|hendrycksTest-moral_disputes|5": 1,
    "harness|hendrycksTest-moral_scenarios|5": 1,
    "harness|hendrycksTest-nutrition|5": 1,
    "harness|hendrycksTest-philosophy|5": 1,
    "harness|hendrycksTest-prehistory|5": 1,
    "harness|hendrycksTest-professional_accounting|5": 1,
    "harness|hendrycksTest-professional_law|5": 1,
    "harness|hendrycksTest-professional_medicine|5": 1,
    "harness|hendrycksTest-professional_psychology|5": 1,
    "harness|hendrycksTest-public_relations|5": 1,
    "harness|hendrycksTest-security_studies|5": 1,
    "harness|hendrycksTest-sociology|5": 1,
    "harness|hendrycksTest-us_foreign_policy|5": 1,
    "harness|hendrycksTest-virology|5": 1,
    "harness|hendrycksTest-world_religions|5": 1,
    "harness|truthfulqa:mc|0": 1,
    "all": 0
  },
  "task_config": {
    "harness|arc:challenge": "LM Harness task",
    "harness|hellaswag": "LM Harness task",
    "harness|hendrycksTest-abstract_algebra": "LM Harness task",
    "harness|hendrycksTest-anatomy": "LM Harness task",
    "harness|hendrycksTest-astronomy": "LM Harness task",
    "harness|hendrycksTest-business_ethics": "LM Harness task",
    "harness|hendrycksTest-clinical_knowledge": "LM Harness task",
    "harness|hendrycksTest-college_biology": "LM Harness task",
    "harness|hendrycksTest-college_chemistry": "LM Harness task",
    "harness|hendrycksTest-college_computer_science": "LM Harness task",
    "harness|hendrycksTest-college_mathematics": "LM Harness task",
    "harness|hendrycksTest-college_medicine": "LM Harness task",
    "harness|hendrycksTest-college_physics": "LM Harness task",
    "harness|hendrycksTest-computer_security": "LM Harness task",
    "harness|hendrycksTest-conceptual_physics": "LM Harness task",
    "harness|hendrycksTest-econometrics": "LM Harness task",
    "harness|hendrycksTest-electrical_engineering": "LM Harness task",
    "harness|hendrycksTest-elementary_mathematics": "LM Harness task",
    "harness|hendrycksTest-formal_logic": "LM Harness task",
    "harness|hendrycksTest-global_facts": "LM Harness task",
    "harness|hendrycksTest-high_school_biology": "LM Harness task",
    "harness|hendrycksTest-high_school_chemistry": "LM Harness task",
    "harness|hendrycksTest-high_school_computer_science": "LM Harness task",
    "harness|hendrycksTest-high_school_european_history": "LM Harness task",
    "harness|hendrycksTest-high_school_geography": "LM Harness task",
    "harness|hendrycksTest-high_school_government_and_politics": "LM Harness task",
    "harness|hendrycksTest-high_school_macroeconomics": "LM Harness task",
    "harness|hendrycksTest-high_school_mathematics": "LM Harness task",
    "harness|hendrycksTest-high_school_microeconomics": "LM Harness task",
    "harness|hendrycksTest-high_school_physics": "LM Harness task",
    "harness|hendrycksTest-high_school_psychology": "LM Harness task",
    "harness|hendrycksTest-high_school_statistics": "LM Harness task",
    "harness|hendrycksTest-high_school_us_history": "LM Harness task",
    "harness|hendrycksTest-high_school_world_history": "LM Harness task",
    "harness|hendrycksTest-human_aging": "LM Harness task",
    "harness|hendrycksTest-human_sexuality": "LM Harness task",
    "harness|hendrycksTest-international_law": "LM Harness task",
    "harness|hendrycksTest-jurisprudence": "LM Harness task",
    "harness|hendrycksTest-logical_fallacies": "LM Harness task",
    "harness|hendrycksTest-machine_learning": "LM Harness task",
    "harness|hendrycksTest-management": "LM Harness task",
    "harness|hendrycksTest-marketing": "LM Harness task",
    "harness|hendrycksTest-medical_genetics": "LM Harness task",
    "harness|hendrycksTest-miscellaneous": "LM Harness task",
    "harness|hendrycksTest-moral_disputes": "LM Harness task",
    "harness|hendrycksTest-moral_scenarios": "LM Harness task",
    "harness|hendrycksTest-nutrition": "LM Harness task",
    "harness|hendrycksTest-philosophy": "LM Harness task",
    "harness|hendrycksTest-prehistory": "LM Harness task",
    "harness|hendrycksTest-professional_accounting": "LM Harness task",
    "harness|hendrycksTest-professional_law": "LM Harness task",
    "harness|hendrycksTest-professional_medicine": "LM Harness task",
    "harness|hendrycksTest-professional_psychology": "LM Harness task",
    "harness|hendrycksTest-public_relations": "LM Harness task",
    "harness|hendrycksTest-security_studies": "LM Harness task",
    "harness|hendrycksTest-sociology": "LM Harness task",
    "harness|hendrycksTest-us_foreign_policy": "LM Harness task",
    "harness|hendrycksTest-virology": "LM Harness task",
    "harness|hendrycksTest-world_religions": "LM Harness task",
    "harness|truthfulqa:mc": "LM Harness task"
  },
  "config_general": {
    "model_name": "tiiuae/falcon-40b",
    "model_sha": "c47b371b31a68349c233104050ac76680b8485db",
    "model_dtype": "torch.bfloat16",
    "lighteval_sha": "65540cead19c612dc684266f1fb08adbfe9cf8f5",
    "num_few_shot_default": 0,
    "num_fewshot_seeds": 1,
    "override_batch_size": 2,
    "max_samples": null
  }
}